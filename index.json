[{"id":3,"pagetitle":"Home","title":"ManifoldsBase.jl","ref":"/manifoldsbase/stable/#ManifoldsBase.jl","content":" ManifoldsBase.jl ManifoldsBase.jl  is a lightweight interface for manifolds. This packages has two main purposes. You can add it as a dependency if you plan to work on manifolds (generically) or if you plan to define own manifolds in a package. For a package that (only) depends on  ManifoldsBase.jl , see  Manopt.jl , which implements optimization algorithms on manifolds using this interface. These optimisation algorithms can hence be used with any manifold implemented based on  ManifoldsBase.jl . For a library of manifolds implemented using this interface  Manifolds.jl . Your package is using  ManifoldsBase ? We would like to add that here as well. Either  write an issue  or add yourself by forking, editing this file and  opening a PR ."},{"id":4,"pagetitle":"Home","title":"Citation","ref":"/manifoldsbase/stable/#Citation","content":" Citation If you use  ManifoldsBase.jl  in your work, please cite the following paper, which covers both the basic interface as well as the performance for  Manifolds.jl . @article{AxenBaranBergmannRzecki:2023,\n    AUTHOR    = {Axen, Seth D. and Baran, Mateusz and Bergmann, Ronny and Rzecki, Krzysztof},\n    ARTICLENO = {33},\n    DOI       = {10.1145/3618296},\n    JOURNAL   = {ACM Transactions on Mathematical Software},\n    MONTH     = {dec},\n    NUMBER    = {4},\n    TITLE     = {Manifolds.Jl: An Extensible Julia Framework for Data Analysis on Manifolds},\n    VOLUME    = {49},\n    YEAR      = {2023}\n} Note that the citation is in  BibLaTeX  format."},{"id":7,"pagetitle":"Bases for tangent spaces","title":"Bases for tangent spaces","ref":"/manifoldsbase/stable/bases/#bases","content":" Bases for tangent spaces The following functions and types provide support for bases of the tangent space of different manifolds. Moreover, bases of the cotangent space are also supported, though this description focuses on the tangent space. An orthonormal basis of the tangent space  $T_p \\mathcal M$  of (real) dimension  $n$  has a real-coefficient basis  $e_1, e_2, ‚Ä¶, e_n$  if  $\\mathrm{Re}(g_p(e_i, e_j)) = Œ¥_{ij}$  for each  $i,j ‚àà \\{1, 2, ‚Ä¶, n\\}$  where  $g_p$  is the Riemannian metric at point  $p$ . A vector  $X$  from the tangent space  $T_p \\mathcal M$  can be expressed in Einstein notation as a sum  $X = X^i e_i$ , where (real) coefficients  $X^i$  are calculated as  $X^i = \\mathrm{Re}(g_p(X, e_i))$ . The main types are: DefaultOrthonormalBasis , which is designed to work when no special properties of the tangent space basis are required.  It is designed to make  get_coordinates  and  get_vector  fast. DiagonalizingOrthonormalBasis , which diagonalizes the curvature tensor and makes the curvature in the selected direction equal to 0. ProjectedOrthonormalBasis , which projects a basis of the ambient space and orthonormalizes projections to obtain a basis in a generic way. CachedBasis , which stores (explicitly or implicitly) a precomputed basis at a certain point. The main functions are: get_basis  precomputes a basis at a certain point. get_coordinates  returns coordinates of a tangent vector. get_vector  returns a vector for the specified coordinates. get_vectors  returns a vector of basis vectors. Calling it should be avoided for high-dimensional manifolds."},{"id":8,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.AbstractBasis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.AbstractBasis","content":" ManifoldsBase.AbstractBasis  ‚Äî  Type AbstractBasis{ùîΩ,VST<:VectorSpaceType} Abstract type that represents a basis of vector space of type  VST  on a manifold or a subset of it. The type parameter  ùîΩ  denotes the  AbstractNumbers  that will be used as coefficients in linear combinations of the basis vectors. See also VectorSpaceType source"},{"id":9,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.AbstractOrthogonalBasis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.AbstractOrthogonalBasis","content":" ManifoldsBase.AbstractOrthogonalBasis  ‚Äî  Type AbstractOrthogonalBasis{ùîΩ,VST<:VectorSpaceType} Abstract type that represents an orthonormal basis of vector space of type  VST  on a manifold or a subset of it. The type parameter  ùîΩ  denotes the  AbstractNumbers  that will be used as coefficients in linear combinations of the basis vectors. See also VectorSpaceType source"},{"id":10,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.AbstractOrthonormalBasis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.AbstractOrthonormalBasis","content":" ManifoldsBase.AbstractOrthonormalBasis  ‚Äî  Type AbstractOrthonormalBasis{ùîΩ,VST<:VectorSpaceType} Abstract type that represents an orthonormal basis of vector space of type  VST  on a manifold or a subset of it. The type parameter  ùîΩ  denotes the  AbstractNumbers  that will be used as coefficients in linear combinations of the basis vectors. See also VectorSpaceType source"},{"id":11,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.CachedBasis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.CachedBasis","content":" ManifoldsBase.CachedBasis  ‚Äî  Type CachedBasis{ùîΩ,V,<:AbstractBasis{ùîΩ}} <: AbstractBasis{ùîΩ} A cached version of the given  basis  with precomputed basis vectors. The basis vectors are stored in  data , either explicitly (like in cached variants of  ProjectedOrthonormalBasis ) or implicitly. Constructor CachedBasis(basis::AbstractBasis, data) source"},{"id":12,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.CotangentSpaceType","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.CotangentSpaceType","content":" ManifoldsBase.CotangentSpaceType  ‚Äî  Type struct CotangentSpaceType <: VectorSpaceType end A type that indicates that a  Fiber  is a  CotangentSpace . source"},{"id":13,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.DefaultBasis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.DefaultBasis","content":" ManifoldsBase.DefaultBasis  ‚Äî  Type DefaultBasis{ùîΩ,VST<:VectorSpaceType} An arbitrary basis of vector space of type  VST  on a manifold. This will usually be the fastest basis available for a manifold. The type parameter  ùîΩ  denotes the  AbstractNumbers  that will be used as coefficients in linear combinations of the basis vectors. See also VectorSpaceType source"},{"id":14,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.DefaultOrthogonalBasis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.DefaultOrthogonalBasis","content":" ManifoldsBase.DefaultOrthogonalBasis  ‚Äî  Type DefaultOrthogonalBasis{ùîΩ,VST<:VectorSpaceType} An arbitrary orthogonal basis of vector space of type  VST  on a manifold. This will usually be the fastest orthogonal basis available for a manifold. The type parameter  ùîΩ  denotes the  AbstractNumbers  that will be used as coefficients in linear combinations of the basis vectors. See also VectorSpaceType source"},{"id":15,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.DefaultOrthonormalBasis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.DefaultOrthonormalBasis","content":" ManifoldsBase.DefaultOrthonormalBasis  ‚Äî  Type DefaultOrthonormalBasis(ùîΩ::AbstractNumbers = ‚Ñù, vs::VectorSpaceType = TangentSpaceType()) An arbitrary orthonormal basis of vector space of type  VST  on a manifold. This will usually be the fastest orthonormal basis available for a manifold. The type parameter  ùîΩ  denotes the  AbstractNumbers  that will be used as coefficients in linear combinations of the basis vectors. See also VectorSpaceType source"},{"id":16,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.DiagonalizingOrthonormalBasis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.DiagonalizingOrthonormalBasis","content":" ManifoldsBase.DiagonalizingOrthonormalBasis  ‚Äî  Type DiagonalizingOrthonormalBasis{ùîΩ,TV} <: AbstractOrthonormalBasis{ùîΩ,TangentSpaceType} An orthonormal basis  Œû  as a vector of tangent vectors (of length determined by  manifold_dimension ) in the tangent space that diagonalizes the curvature tensor  $R(u,v)w$  and where the direction  frame_direction $v$  has curvature  0 . The type parameter  ùîΩ  denotes the  AbstractNumbers  that will be used as coefficients in linear combinations of the basis vectors. Constructor DiagonalizingOrthonormalBasis(frame_direction, ùîΩ::AbstractNumbers = ‚Ñù) source"},{"id":17,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.GramSchmidtOrthonormalBasis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.GramSchmidtOrthonormalBasis","content":" ManifoldsBase.GramSchmidtOrthonormalBasis  ‚Äî  Type GramSchmidtOrthonormalBasis{ùîΩ} <: AbstractOrthonormalBasis{ùîΩ} An orthonormal basis obtained from a basis. Constructor GramSchmidtOrthonormalBasis(ùîΩ::AbstractNumbers = ‚Ñù) source"},{"id":18,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.ProjectedOrthonormalBasis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.ProjectedOrthonormalBasis","content":" ManifoldsBase.ProjectedOrthonormalBasis  ‚Äî  Type ProjectedOrthonormalBasis(method::Symbol, ùîΩ::AbstractNumbers = ‚Ñù) An orthonormal basis that comes from orthonormalization of basis vectors of the ambient space projected onto the subspace representing the tangent space at a given point. The type parameter  ùîΩ  denotes the  AbstractNumbers  that will be used as coefficients in linear combinations of the basis vectors. Available methods: :gram_schmidt  uses a modified Gram-Schmidt orthonormalization. :svd  uses SVD decomposition to orthogonalize projected vectors. The SVD-based method should be more numerically stable at the cost of an additional assumption (local metric tensor at a point where the basis is calculated has to be diagonal). source"},{"id":19,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.TangentSpaceType","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.TangentSpaceType","content":" ManifoldsBase.TangentSpaceType  ‚Äî  Type struct TangentSpaceType <: VectorSpaceType end A type that indicates that a  Fiber  is a  TangentSpace . source"},{"id":20,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.VectorSpaceType","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.VectorSpaceType","content":" ManifoldsBase.VectorSpaceType  ‚Äî  Type VectorSpaceType Abstract type for tangent spaces, cotangent spaces, their tensor products, exterior products, etc. Every vector space  fiber  is supposed to provide: a method of constructing vectors, basic operations: addition, subtraction, multiplication by a scalar and negation (unary minus), zero_vector(fiber, p)  to construct zero vectors at point  p , allocate(X)  and  allocate(X, T)  for vector  X  and type  T , copyto!(X, Y)  for vectors  X  and  Y , number_eltype(X)  for vector  X , vector_space_dimension . Optionally: inner product via  inner  (used to provide Riemannian metric on vector bundles), flat  and  sharp , norm  (by default uses  inner ), project  (for embedded vector spaces), representation_size , broadcasting for basic operations. source"},{"id":21,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.allocate_coordinates","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.allocate_coordinates-Tuple{AbstractManifold, Any, Any, Int64}","content":" ManifoldsBase.allocate_coordinates  ‚Äî  Method allocate_coordinates(M::AbstractManifold, p, T, n::Int) Allocate vector of coordinates of length  n  of type  T  of a vector at point  p  on manifold  M . source"},{"id":22,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.allocation_promotion_function","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.allocation_promotion_function-Tuple{AbstractManifold, Any, Tuple}","content":" ManifoldsBase.allocation_promotion_function  ‚Äî  Method allocation_promotion_function(M::AbstractManifold, f, args::Tuple) Determine the function that must be used to ensure that the allocated representation is of the right type. This is needed for  get_vector  when a point on a complex manifold is represented by a real-valued vectors with a real-coefficient basis, so that a complex-valued vector representation is allocated. source"},{"id":23,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.change_basis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.change_basis-Tuple{AbstractManifold, Any, Any, ManifoldsBase.AbstractBasis, ManifoldsBase.AbstractBasis}","content":" ManifoldsBase.change_basis  ‚Äî  Method change_basis(M::AbstractManifold, p, c, B_in::AbstractBasis, B_out::AbstractBasis) Given a vector with coordinates  c  at point  p  from manifold  M  in basis  B_in , compute coordinates of the same vector in basis  B_out . source"},{"id":24,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.coordinate_eltype","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.coordinate_eltype-Tuple{AbstractManifold, Any, ManifoldsBase.ComplexNumbers}","content":" ManifoldsBase.coordinate_eltype  ‚Äî  Method coordinate_eltype(M::AbstractManifold, p, ùîΩ::AbstractNumbers) Get the element type for ùîΩ-field coordinates of the tangent space at a point  p  from manifold  M . This default assumes that usually complex bases of complex manifolds have real coordinates but it can be overridden by a more specific method. source"},{"id":25,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.default_basis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.default_basis-Tuple{AbstractManifold}","content":" ManifoldsBase.default_basis  ‚Äî  Method default_basis(M::AbstractManifold, ::typeof(p); kwargs...)\ndefault_basis(M::AbstractManifold; kwargs...) Provide a default basis for a manifold's tangent space. This can be specific for different points  p  on  M  The global default for both is the  DefaultOrthonormalBasis  with the same number type as  M . This method can also be specified more precisely with a point type  T , for the case that on a  M  there are two different representations of points, which provide different inverse retraction methods. Keyword arguments field:: AbstractNumbers  field for the coefficients of the basis source"},{"id":26,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.default_basis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.default_basis-Union{Tuple{T}, Tuple{AbstractManifold, Type{T}}} where T","content":" ManifoldsBase.default_basis  ‚Äî  Method default_basis(M::AbstractManifold, ::typeof(p); kwargs...)\ndefault_basis(M::AbstractManifold; kwargs...) Provide a default basis for a manifold's tangent space. This can be specific for different points  p  on  M  The global default for both is the  DefaultOrthonormalBasis  with the same number type as  M . This method can also be specified more precisely with a point type  T , for the case that on a  M  there are two different representations of points, which provide different inverse retraction methods. Keyword arguments field:: AbstractNumbers  field for the coefficients of the basis source"},{"id":27,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.dual_basis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.dual_basis-Tuple{AbstractManifold, Any, ManifoldsBase.AbstractBasis}","content":" ManifoldsBase.dual_basis  ‚Äî  Method dual_basis(M::AbstractManifold, p, B::AbstractBasis) Get the dual basis to  B , a basis of a vector space at point  p  from manifold  M . The dual to the  $i$ th vector  $v_i$  from basis  B  is a vector  $v^i$  from the dual space such that  $v^i(v_j) = Œ¥^i_j$ , where  $Œ¥^i_j$  is the Kronecker delta symbol: \\[Œ¥^i_j = \\begin{cases}\n1 & \\text{ if } i=j, \\\\\n0 & \\text{ otherwise.}\n\\end{cases}\\] source"},{"id":28,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.get_basis","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.get_basis-Tuple{AbstractManifold, Any, ManifoldsBase.AbstractBasis}","content":" ManifoldsBase.get_basis  ‚Äî  Method get_basis(M::AbstractManifold, p, B::AbstractBasis; kwargs...) -> CachedBasis Compute the basis vectors of the tangent space at a point on manifold  M  represented by  p . Returned object derives from  AbstractBasis  and may have a field  .vectors  that stores tangent vectors or it may store them implicitly, in which case the function  get_vectors  needs to be used to retrieve the basis vectors. See also:  get_coordinates ,  get_vector source"},{"id":29,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.get_coordinates","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.get_coordinates","content":" ManifoldsBase.get_coordinates  ‚Äî  Function get_coordinates(M::AbstractManifold, p, X, B::AbstractBasis=default_basis(M, typeof(p)))\nget_coordinates(M::AbstractManifold, p, X, B::CachedBasis) Compute a one-dimensional vector of coefficients of the tangent vector  X  at point denoted by  p  on manifold  M  in basis  B . Depending on the basis,  p  may not directly represent a point on the manifold. For example if a basis transported along a curve is used,  p  may be the coordinate along the curve. If a  CachedBasis  is provided, their stored vectors are used, otherwise the user has to provide a method to compute the coordinates. For the  CachedBasis  keep in mind that the reconstruction with  get_vector  requires either a dual basis or the cached basis to be selfdual, for example orthonormal See also:  get_vector ,  get_basis source"},{"id":30,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.get_vector","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.get_vector","content":" ManifoldsBase.get_vector  ‚Äî  Function X = get_vector(M::AbstractManifold, p, c, B::AbstractBasis=default_basis(M, typeof(p))) Convert a one-dimensional vector of coefficients in a basis  B  of the tangent space at  p  on manifold  M  to a tangent vector  X  at  p . Depending on the basis,  p  may not directly represent a point on the manifold. For example if a basis transported along a curve is used,  p  may be the coordinate along the curve. For the  CachedBasis  keep in mind that the reconstruction from  get_coordinates  requires either a dual basis or the cached basis to be selfdual, for example orthonormal See also:  get_coordinates ,  get_basis ,  default_basis source"},{"id":31,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.get_vectors","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.get_vectors-Tuple{AbstractManifold, Any, ManifoldsBase.AbstractBasis}","content":" ManifoldsBase.get_vectors  ‚Äî  Method get_vectors(M::AbstractManifold, p, B::AbstractBasis=get_basis(M, p, DefaultOrthonormalBasis())) Get the basis vectors of basis  B  of the tangent space at point  p . The function may or may not work if passed a basis other than a  CachedBasis . A  CachedBasis  can be obtained by calling  get_basis . source"},{"id":32,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.gram_schmidt","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.gram_schmidt-Union{Tuple{ùîΩ}, Tuple{AbstractManifold{ùîΩ}, Any, ManifoldsBase.AbstractBasis{ùîΩ}}} where ùîΩ","content":" ManifoldsBase.gram_schmidt  ‚Äî  Method gram_schmidt(M::AbstractManifold{ùîΩ}, p, B::AbstractBasis{ùîΩ}) where {ùîΩ}\ngram_schmidt(M::AbstractManifold, p, V::AbstractVector) Compute an ONB in the tangent space at  p  on the [ AbstractManifold ](@ref}  M  from either an  AbstractBasis  basis ¬¥B¬¥ or a set of (at most)  manifold_dimension (M)  many vectors. Note that this method requires the manifold and basis to work on the same  AbstractNumbers ùîΩ , i.e. with real coefficients. The method always returns a basis, i.e. linearly dependent vectors are removed. Keyword arguments warn_linearly_dependent  ( false ) ‚Äì warn if the basis vectors are not linearly independent skip_linearly_dependent  ( false ) ‚Äì whether to just skip ( true ) a vector that is linearly dependent to the previous ones or to stop ( false , default) at that point return_incomplete_set  ( false ) ‚Äì throw an error if the resulting set of vectors is not a basis but contains less vectors further keyword arguments can be passed to set the accuracy of the independence test. Especially  atol  is raised slightly by default to  atol = 5*1e-16 . Return value When a set of vectors is orthonormalized a set of vectors is returned. When an  AbstractBasis  is orthonormalized, a  CachedBasis  is returned. source"},{"id":33,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.hat","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.hat-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.hat  ‚Äî  Method hat(M::AbstractManifold, p, X‚Å±) Given a basis  $e_i$  on the tangent space at a point  p  and tangent component vector  $X^i ‚àà ‚Ñù$ , compute the equivalent vector representation  $X=X^i e_i$ , where Einstein summation notation is used: \\[‚àß : X^i ‚Ü¶ X^i e_i\\] For array manifolds, this converts a vector representation of the tangent vector to an array representation. The  vee  map is the  hat  map's inverse. source"},{"id":34,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.number_of_coordinates","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.number_of_coordinates-Union{Tuple{ùîæ}, Tuple{AbstractManifold, ManifoldsBase.AbstractBasis{ùîæ}}} where ùîæ","content":" ManifoldsBase.number_of_coordinates  ‚Äî  Method number_of_coordinates(M::AbstractManifold, B::AbstractBasis)\nnumber_of_coordinates(M::AbstractManifold, ::ùîæ) Compute the number of coordinates in basis of field type  ùîæ  on a manifold  M . This also corresponds to the number of vectors represented by  B , or stored within  B  in case of a  CachedBasis . source"},{"id":35,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.number_system","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.number_system-Union{Tuple{ManifoldsBase.AbstractBasis{ùîΩ}}, Tuple{ùîΩ}} where ùîΩ","content":" ManifoldsBase.number_system  ‚Äî  Method number_system(::AbstractBasis) The number system for the vectors of the given basis. source"},{"id":36,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.requires_caching","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.requires_caching-Tuple{ManifoldsBase.AbstractBasis}","content":" ManifoldsBase.requires_caching  ‚Äî  Method requires_caching(B::AbstractBasis) Return whether basis  B  can be used in  get_vector  and  get_coordinates  without calling  get_basis  first. source"},{"id":37,"pagetitle":"Bases for tangent spaces","title":"ManifoldsBase.vee","ref":"/manifoldsbase/stable/bases/#ManifoldsBase.vee-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.vee  ‚Äî  Method vee(M::AbstractManifold, p, X) Given a basis  $e_i$  on the tangent space at a point  p  and tangent vector  X , compute the vector components  $X^i ‚àà ‚Ñù$ , such that  $X = X^i e_i$ , where Einstein summation notation is used: \\[\\vee : X^i e_i ‚Ü¶ X^i\\] For array manifolds, this converts an array representation of the tangent vector to a vector representation. The  hat  map is the  vee  map's inverse. source"},{"id":40,"pagetitle":"Decorating/Extending a Manifold","title":"A Decorator for manifolds","ref":"/manifoldsbase/stable/decorator/#A-Decorator-for-manifolds","content":" A Decorator for manifolds Several properties of a manifold are often implicitly assumed, for example the choice of the (Riemannian) metric, the group structure or the embedding. The latter shall serve as an example how to either implicitly or explicitly specify the embedding to avoid re-implementations and/or distinguish different embeddings."},{"id":41,"pagetitle":"Decorating/Extending a Manifold","title":"The abstract decorator","ref":"/manifoldsbase/stable/decorator/#The-abstract-decorator","content":" The abstract decorator When first implementing a manifold, it might be beneficial to dispatch certain computations to already existing manifolds. For an embedded manifold that is isometrically embedded this might be the  inner  the manifold inherits in each tangent space from its embedding. This means we would like to dispatch the default implementation of a function to some other manifold. We refer to this as implicit decoration, since one can not ‚Äúsee‚Äù explicitly that a certain manifold inherits this property. As an example consider the  Sphere . At each point the tangent space can be identified with a subspace of the tangent space in the embedding, the  Euclidean  manifold which the unit vectors of the sphere belong to. Thus every tangent space inherits its metric from the embedding. Since in the default implementation in  Manifolds.jl  points are represented by unit vectors and tangent vectors at a point as vectors orthogonal to that point, we can just dispatch the inner product to the embedding without having to re-implement this. The manifold using such an implicit dispatch just has to be a subtype of  AbstractDecoratorManifold ."},{"id":42,"pagetitle":"Decorating/Extending a Manifold","title":"Traits with an inheritance hierarchy","ref":"/manifoldsbase/stable/decorator/#Traits-with-an-inheritance-hierarchy","content":" Traits with an inheritance hierarchy The properties mentioned above might form a hierarchy. For embedded manifolds, again, we might have just a manifold whose points are represented in some embedding. If the manifold is even isometrically embedded, it is embedded but also inherits the Riemannian metric by restricting the metric from the embedding to the corresponding tangent space under consideration. But it also inherits the functions defined for the plain embedding, for example checking some conditions for the validity of points and vectors. If it is even a submanifold, also further functions are inherited like the  shortest_geodesic . We use a variation of  Tim Holy's Traits Trick  (THTT) which takes into account this nestedness of traits."},{"id":43,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.AbstractDecoratorManifold","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.AbstractDecoratorManifold","content":" ManifoldsBase.AbstractDecoratorManifold  ‚Äî  Type AbstractDecoratorManifold{ùîΩ} <: AbstractManifold{ùîΩ} Declare a manifold to be an abstract decorator. A manifold which is a subtype of is a  decorated manifold , i.e. has certain additional properties or delegates certain properties to other manifolds. Most prominently, a manifold might be an embedded manifold, i.e. points on a manifold  $\\mathcal M$  are represented by (some, maybe not all) points on another manifold  $\\mathcal N$ . Depending on the type of embedding, several functions are dedicated to the embedding. For example if the embedding is isometric, then the  inner  does not have to be implemented for  $\\mathcal M$  but can be automatically implemented by deligation to  $\\mathcal N$ . This is modelled by the  AbstractDecoratorManifold  and traits. These are mapped to functions, which determine the types of transparencies. source"},{"id":44,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.AbstractTrait","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.AbstractTrait","content":" ManifoldsBase.AbstractTrait  ‚Äî  Type AbstractTrait An abstract trait type to build a sequence of traits source"},{"id":45,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.EmptyTrait","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.EmptyTrait","content":" ManifoldsBase.EmptyTrait  ‚Äî  Type EmptyTrait <: AbstractTrait A Trait indicating that no feature is present. source"},{"id":46,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.IsExplicitDecorator","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.IsExplicitDecorator","content":" ManifoldsBase.IsExplicitDecorator  ‚Äî  Type IsExplicitDecorator <: AbstractTrait Specify that a certain type should dispatch per default to its  decorated_manifold . Note Any decorator  behind  this decorator might not have any effect, since the function dispatch is moved to its field at this point. Therefore this decorator should always be  last  in the  TraitList . source"},{"id":47,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.TraitList","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.TraitList","content":" ManifoldsBase.TraitList  ‚Äî  Type TraitList <: AbstractTrait Combine two traits into a combined trait.  Note that this introduces a preceedence. the first of the traits takes preceedence if a trait is implemented for both functions. Constructor TraitList(head::AbstractTrait, tail::AbstractTrait) source"},{"id":48,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.@next_trait_function","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.@next_trait_function-Tuple{Any, Any}","content":" ManifoldsBase.@next_trait_function  ‚Äî  Macro next_trait_function(trait_type, sig) Define a special trait-handling method for function indicated by  sig . It does not change the result but the presence of such additional methods may prevent method recursion limits in Julia's inference from being triggered. Some functions may work faster after adding methods generated by  next_trait_function . See the \"Trait recursion breaking\" section at the bottom of  src/decorator_trait.jl  file for an example of intended usage. source"},{"id":49,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.active_traits","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.active_traits-Tuple{Any, Vararg{Any}}","content":" ManifoldsBase.active_traits  ‚Äî  Method active_traits(f, args...) Return the list of traits applicable to the given call of function  f `. This function should be overloaded for specific function calls. source"},{"id":50,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.expand_trait","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.expand_trait-Tuple{AbstractTrait}","content":" ManifoldsBase.expand_trait  ‚Äî  Method expand_trait(t::AbstractTrait) Expand given trait into an ordered  TraitList  list of traits with their parent traits obtained using  parent_trait . source"},{"id":51,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.merge_traits","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.merge_traits-Tuple{}","content":" ManifoldsBase.merge_traits  ‚Äî  Method merge_traits(t1, t2, trest...) Merge two traits into a nested list of traits. Note that this takes trait preceedence into account, i.e.  t1  takes preceedence over  t2  is any operations. It always returns either ab  EmptyTrait  or a  TraitList . This means that for one argument it just returns the trait itself if it is list-like, or wraps the trait in a   single-element list otherwise, two arguments that are list-like, it merges them, two arguments of which only the first one is list-like and the second one is not,   it appends the second argument to the list, two arguments of which only the second one is list-like, it prepends the first one to   the list, two arguments of which none is list-like, it creates a two-element list. more than two arguments it recursively performs a left-assiciative recursive reduction   on arguments, that is for example  merge_traits(t1, t2, t3)  is equivalent to    merge_traits(merge_traits(t1, t2), t3) source"},{"id":52,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.next_trait","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.next_trait-Tuple{AbstractTrait}","content":" ManifoldsBase.next_trait  ‚Äî  Method next_trait(t::AbstractTrait) Return the next trait to consider, which by default is no following trait (i.e.  EmptyTrait ). Expecially for a a  TraitList  this function returns the (remaining) tail of the remaining traits. source"},{"id":53,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.parent_trait","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.parent_trait-Tuple{AbstractTrait}","content":" ManifoldsBase.parent_trait  ‚Äî  Method parent_trait(t::AbstractTrait) Return the parent trait for trait  t , that is the more general trait whose behaviour it inherits as a fallback. source The key part of the trait system is that it forms a list of traits, from the most specific one to the least specific one, and tries to find a specific implementation of a function for a trait in the least. This ensures that there are, by design, no ambiguities (caused by traits) in the method selection process. Trait resolution is driven by Julia's method dispatch and the compiler is sufficiently clever to quite reliably constant-propagate traits and inline method calls. The list of traits is browsed from the most specific one for implementation of a given function for that trait. If one is found, the implementation is called and it may internally call completely different function, breaking the trait dispatch chain. When no implementation for a trait is found, the next trait on the list is checked, until  EmptyTrait  is reached, which is conventionally the last trait to be considered, expected to have the most generic default implementation of a function If you want to continue with the following traits afterwards, use  s = next_trait (t)  of a  TraitList t  to continue working on the next trait in the list by calling the function with  s  as first argument."},{"id":54,"pagetitle":"Decorating/Extending a Manifold","title":"The Manifold decorator","ref":"/manifoldsbase/stable/decorator/#The-Manifold-decorator","content":" The Manifold decorator Based on the generic  TraitList  the following types, functions, and macros introduce the decorator trait which allows to decorate an arbitrary  <: AbstractDecoratorManifold  with further features."},{"id":55,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.IsEmbeddedManifold","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.IsEmbeddedManifold","content":" ManifoldsBase.IsEmbeddedManifold  ‚Äî  Type IsEmbeddedManifold <: AbstractTrait A trait to declare an  AbstractManifold  as an embedded manifold. source"},{"id":56,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.IsEmbeddedSubmanifold","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.IsEmbeddedSubmanifold","content":" ManifoldsBase.IsEmbeddedSubmanifold  ‚Äî  Type IsEmbeddedSubmanifold <: AbstractTrait A trait to determine whether an  AbstractDecoratorManifold M  is an embedded submanifold. It is a special case of the  IsIsometricEmbeddedManifold  trait, i.e. it has all properties of this trait. In this trait, additionally to the isometric embedded manifold, all retractions, inverse retractions, and vectors transports, especially  exp ,  log , and  parallel_transport_to  are passed to the embedding. source"},{"id":57,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.IsIsometricEmbeddedManifold","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.IsIsometricEmbeddedManifold","content":" ManifoldsBase.IsIsometricEmbeddedManifold  ‚Äî  Type IsIsometricManifoldEmbeddedManifold <: AbstractTrait A Trait to determine whether an  AbstractDecoratorManifold M  is an isometrically embedded manifold. It is a special case of the  IsEmbeddedManifold  trait, i.e. it has all properties of this trait. Here, additionally, netric related functions like  inner  and  norm  are passed to the embedding source"},{"id":58,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.decorated_manifold","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.decorated_manifold-Tuple{AbstractDecoratorManifold}","content":" ManifoldsBase.decorated_manifold  ‚Äî  Method decorated_manifold(M::AbstractDecoratorManifold) For a manifold  M  that is decorated with some properties, this function returns the manifold without that manifold, i.e. the manifold that  was decorated . source"},{"id":59,"pagetitle":"Decorating/Extending a Manifold","title":"ManifoldsBase.get_embedding","ref":"/manifoldsbase/stable/decorator/#ManifoldsBase.get_embedding-Tuple{AbstractDecoratorManifold, Any}","content":" ManifoldsBase.get_embedding  ‚Äî  Method get_embedding(M::AbstractDecoratorManifold)\nget_embedding(M::AbstractDecoratorManifold, p) Specify the embedding of a manifold that has abstract decorators. the embedding might depend on a point representation, where different point representations are distinguished as subtypes of  AbstractManifoldPoint . A unique or default representation might also just be an  AbstractArray . source For an example see the  (implicit) embedded manifold ."},{"id":62,"pagetitle":"Design principles","title":"Main Design Principles","ref":"/manifoldsbase/stable/design/#Design","content":" Main Design Principles The interface for a manifold is defined to be as generic as possible, such that applications can be implemented as independently as possible from an actual manifold. This way, algorithms like those from  Manopt.jl  can be implemented on  arbitrary  manifolds. The main design criteria for the interface are: Aims to also provide  efficient global state-free , both  in-place  and  out-of-place  computations whenever possible. Provide a high level interface that is easy to use. Therefore this interface has 3 main features, that we will explain using two (related) concepts, the  exponential map  that maps a tangent vector  $X$  at a point  $p$  to a point  $q$  or mathematically  $\\exp_p:T_p\\mathcal M \\to \\mathcal M$  and its generalization, a  retract ion  $\\operatorname{retr}_p$  with the same domain and range. You do not need to know their exact definition at this point, just that there is  one  exponential map on a Riemannian manifold, and several retractions, where one of them is the exponential map (called  ExponentialRetraction  for completeness). Every retraction has its own subtype of the  AbstractRetractionMethod  that uniquely defines it. The following three design patterns aim to fulfil the criteria from above, while also avoiding ambiguities in multiple dispatch using the  dispatch on one argument at a time  approach."},{"id":63,"pagetitle":"Design principles","title":"General order of parameters","ref":"/manifoldsbase/stable/design/#General-order-of-parameters","content":" General order of parameters Since the central element for functions on a manifold is the manifold itself, it should always be the first parameter, even for in-place functions. Then the classical parameters of a function (for example a point and a tangent vector for the retraction) follow and the final part are parameters to further dispatch on, which usually have their defaults. Besides this order the functions follow the scheme ‚Äúallocate early‚Äù, i.e. to switch to the mutating variant when reasonable, cf.  Mutating and allocating functions ."},{"id":64,"pagetitle":"Design principles","title":"A 3-Layer architecture for dispatch","ref":"/manifoldsbase/stable/design/#A-3-Layer-architecture-for-dispatch","content":" A 3-Layer architecture for dispatch The general architecture consists of three layers The high level interface for ease of use ‚Äì and to dispatch on other manifolds. The intermediate layer to dispatch on different parameters in the last section, e.g. type of retraction or vector transport. The lowest layer for specific manifolds to dispatch on different types of points and tangent vectors. Usually this layer with a specific manifold and no optional parameters. These three layers are described in more detail in the following. The main motivation to introduce these layers is, that it reduces method ambiguities. It also provides a good structure where to implement extensions to this interface."},{"id":65,"pagetitle":"Design principles","title":"Layer I: The high level interface and ease of use","ref":"/manifoldsbase/stable/design/#design-layer1","content":" Layer I: The high level interface and ease of use The highest layer for convenience of decorators. A usual scheme is, that a manifold might assume several things implicitly, for example the default implementation of the sphere  $\\mathbb S^n$  using unit vectors in  $\\mathbb R^{n+1}$ . The embedding can be explicitly used to avoid re-implementations ‚Äì the inner product can be ‚Äúpassed on‚Äù to its embedding. To do so, we ‚Äúdecorate‚Äù the manifold by making it an  AbstractDecoratorManifold  and activating the right traits see the tutorial  How to Implement a Manifold . The explicit case of the  EmbeddedManifold  can be used to distinguish different embeddings of a manifold, but also their dispatch (onto the manifold or its embedding, depending on the type of embedding) happens here. Note that all other parameters of a function should be as least typed as possible for all parameters besides the manifold. With respect to the  dispatch on one argument at a time  paradigm, this layer dispatches the  manifold first . We also stay as abstract as possible, for example on the  AbstractManifold  level if possible. If a function has optional positional arguments, (like  retract ) their default values might be filled/provided on this layer. This layer ends usually in calling the same functions like  retract  but prefixed with a  _  to enter  Layer II . Note Usually only functions from this layer are exported from the interface, since these are the ones one should use for generic implementations. If you implement your own manifold,  import  the necessary lower layer functions as needed."},{"id":66,"pagetitle":"Design principles","title":"Layer II: An internal dispatch interface for parameters","ref":"/manifoldsbase/stable/design/#design-layer2","content":" Layer II: An internal dispatch interface for parameters This layer is an interims layer to dispatch on the (optional/default) parameters of a function. For example the last parameter of retraction:  retract  determines the type (variant) to be used. The last function in the previous layer calls  _retract , which is an internal function. These parameters are usually the last parameters of a function. On this layer, e.g. for  _retract  only these last parameters should be typed, the manifold should stay at the  AbstractManifold  level. The layer dispatches on different functions per existing parameter type (and might pass this one further on, if it has fields). Function definitions on this layer should only be extended when introducing new such parameter types, for example when introducing a new type of a retraction. The functions from this layer should never be called directly, are hence also not exported and carry the  _  prefix. They should only be called as the final step in the previous layer. If the default parameters are not dispatched per type, using  _  might be skipped. The same holds for functions that do not have these parameters. When there is no dispatch for different types of the optional parameter (here  t ), the  _  might be skipped. One could hence see the last code line as a definition on Layer I that passes directly to Layer III, since there are not parameter to dispatch on. To close this section, let‚Äòs look at an example. The high level (or  Layer I ) definition of the retraction is given by retract!(M::AbstractManifold, q, p, X, m::AbstractRetractionMethod=default_retraction_method(M, typeof(p))) = _retract!(M, q, p, X, m) Note that the convenience function  retract(M, q, p, X, m)  first allocates a  q  before calling this function as well. This level now dispatches on different retraction types  m . It usually passes to specific functions implemented in  Layer III , here for example _retract!(M::AbstractManifold, q, p, X, m::Exponentialretraction) = exp(M, q, p, X)\n_retract!(M::AbstractManifold, q, p, X, m::PolarRetraction) = retract_polar(M, q, p, X) where the  ExponentialRetraction  is resolved by again calling a function on  Layer I  (to fill futher default values if these exist). The  PolarRetraction  is dispatched to  retract_polar! , a function on  Layer III . For further details and dispatches, see  retractions and inverse retractions  for an overview. Note The documentation should be attached to the high level functions, since this again fosters ease of use. If you implement a polar retraction, you should write a method of function  retract_polar!  but the doc string should be attached to  retract(::M, ::P, ::V, ::PolarRetraction)  for your types  ::M, ::P, ::V  of the manifold, points and vectors, respectively. To summarize, with respect to the  dispatch on one argument at a time  paradigm, this layer dispatches the (optional)  parameters second ."},{"id":67,"pagetitle":"Design principles","title":"Layer III: The base layer with focus on implementations","ref":"/manifoldsbase/stable/design/#design-layer3","content":" Layer III: The base layer with focus on implementations This lower level aims for the actual implementation of the function avoiding ambiguities. It should have as few as possible optional parameters and as concrete as possible types for these. This means the function name should be similar to its high level parent (for example  retract!  and  retract_polar!   above) The manifold type in method signature should always be as narrow as possible. The points/vectors should either be untyped (for the default representation or if there is only one implementation) or provide all type bounds (for second representations or when using  AbstractManifoldPoint  and  AbstractTangentVector , respectively). The first step that often happens on this level is memory allocation and calling the in-place function. If faster, it might also implement the function at hand itself. Usually functions from this layer are not exported, when they have an analogue on the first layer. For example the function  retract_polar! (M, q, p, X)  is not exported, since when using the interface one would use the  PolarRetraction  or to be precise call  retract! (M, q, p, X, PolarRetraction()) . When implementing your own manifold, you have to import functions like these anyway. To summarize, with respect to the  dispatch on one argument at a time  paradigm, this layer dispatches the  concrete manifold and point/vector types last ."},{"id":68,"pagetitle":"Design principles","title":"Mutating and allocating functions","ref":"/manifoldsbase/stable/design/#inplace-and-noninplace","content":" Mutating and allocating functions Every function, where this is applicable, should provide an in-place and an allocating variant. For example for the exponential map  exp(M, p, X)  returns a  new  point  q  where the result is computed in. On the other hand  exp!(M, q, p, X)  computes the result in place of  q , where the design of the implementation should keep in mind that also  exp!(M, p, p, X)  should correctly overwrite  p . The interface provides a way to determine the allocation type and a result to compute/allocate the resulting memory, such that the default implementation allocating functions, like  exp  is to allocate the resulting memory and call  exp! . Note It might be useful to provide two distinct implementations, for example when using AD schemes. The default is meant for ease of use (concerning implementation), since then one has to just implement the in-place variants. Non-mutating functions in  ManifoldsBase.jl  are typically implemented using in-place variants after a suitable allocation of memory. Not that this allocation usually takes place only on  Layer III  when dispatching on points. Both  Layer I  and  Layer II  are usually implemented for both variants in parallel."},{"id":69,"pagetitle":"Design principles","title":"Allocation of new points and vectors","ref":"/manifoldsbase/stable/design/#Allocation-of-new-points-and-vectors","content":" Allocation of new points and vectors The  allocate  function behaves like  similar  for simple representations of points and vectors (for example  Array{Float64} ). For more complex types, such as nested representations of  PowerManifold  (see  NestedPowerRepresentation ), checked types like  ValidationMPoint  and more it operates differently. While  similar  only concerns itself with the higher level of nested structures,  allocate  maps itself through all levels of nesting until a simple array of numbers is reached and then calls  similar . The difference can be most easily seen in the following example: julia> x = similar([[1.0], [2.0]])\n2-element Array{Array{Float64,1},1}:\n #undef\n #undef\n\njulia> y = allocate([[1.0], [2.0]])\n2-element Array{Array{Float64,1},1}:\n [6.90031725726027e-310]\n [6.9003678131654e-310]\n\njulia> x[1]\nERROR: UndefRefError: access to undefined reference\nStacktrace:\n [1] getindex(::Array{Array{Float64,1},1}, ::Int64) at ./array.jl:744\n [2] top-level scope at REPL[12]:1\n\njulia> y[1]\n1-element Array{Float64,1}:\n 6.90031725726027e-310 The function  allocate_result  allocates a correct return value. It takes into account the possibility that different arguments may have different numeric  number_eltype  types thorough the  allocate_result_type  function. The most prominent example of the usage of this function is the logarithmic function  log  when used with typed points. Lets assume on a manifold  M  the have points of type  P  and corresponding tangent vector types  V . then the logarithmic map has the signature log(::M, ::P, ::P) but the return type would be  $V$ , whose internal sizes (fields/arrays) will depend on the concrete type of one of the points. This is accomplished by implementing a method  allocate_result(::M, ::typeof(log), ::P, ::P)  that returns the concrete variable for the result. This way, even with specific types, one just has to implement  log!  and the one line for the allocation. Note This dispatch from the allocating to the in-place variant happens in Layer I (which changed in ManifoldsBase.jl 0.15), that is, functions like  exp  or  retract  allocate their result and call the in-place variant  exp!  and  retract!  afterwards, where the ladder passes down to layer III to reach  retract_polar! ."},{"id":72,"pagetitle":"Basic functions","title":"Functions on manifolds","ref":"/manifoldsbase/stable/functions/#Functions-on-manifolds","content":" Functions on manifolds This page collects several basic functions on manifolds."},{"id":73,"pagetitle":"Basic functions","title":"The exponential map, the logarithmic map, and geodesics","ref":"/manifoldsbase/stable/functions/#exp-and-log","content":" The exponential map, the logarithmic map, and geodesics Geodesics are the generalizations of a straight line to manifolds, i.e. their intrinsic acceleration is zero. Together with geodesics one also obtains the exponential map and its inverse, the logarithmic map. Informally speaking, the exponential map takes a vector (think of a direction and a length) at one point and returns another point, which lies towards this direction at distance of the specified length. The logarithmic map does the inverse, i.e. given two points, it tells which vector ‚Äúpoints towards‚Äù the other point."},{"id":74,"pagetitle":"Basic functions","title":"Base.exp","ref":"/manifoldsbase/stable/functions/#Base.exp-Tuple{AbstractManifold, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::AbstractManifold, p, X) Compute the exponential map of tangent vector  X  at point  p  from the manifold  AbstractManifold M , i.e. \\[\\exp_p X = Œ≥_{p,X}(1),\\] where  $Œ≥_{p,X}$  is the unique geodesic starting in  $Œ≥(0)=p$  such that  $\\dot Œ≥(0) = X$ . See also  shortest_geodesic ,  retract . source"},{"id":75,"pagetitle":"Basic functions","title":"Base.log","ref":"/manifoldsbase/stable/functions/#Base.log-Tuple{AbstractManifold, Any, Any}","content":" Base.log  ‚Äî  Method log(M::AbstractManifold, p, q) Compute the logarithmic map of point  q  at base point  p  on the  AbstractManifold M . The logarithmic map is the inverse of the  exp onential map. Note that the logarithmic map might not be globally defined. See also  inverse_retract . source"},{"id":76,"pagetitle":"Basic functions","title":"ManifoldsBase.exp!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.exp!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.exp!  ‚Äî  Method exp!(M::AbstractManifold, q, p, X) Compute the exponential map of tangent vector  X , optionally scaled by  t ,  at point  p  from the manifold  AbstractManifold M . The result is saved to  q . If you want to implement exponential map for your manifold, you should implement the in-place method with, that is  exp_fused!(M::MyManifold, q, p, X) . See also  exp . source"},{"id":77,"pagetitle":"Basic functions","title":"ManifoldsBase.exp_fused!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.exp_fused!-Tuple{AbstractManifold, Any, Any, Any, Number}","content":" ManifoldsBase.exp_fused!  ‚Äî  Method exp_fused!(M::AbstractManifold, q, p, X, t::Number) Compute the exponential map of tangent vector  X  scaled by  t  at point  p  in-place of  q . Compared to  exp! , this method provides the opportunity to avoid the allocation when computing  t*X . By default, this method performs this operation and passes to  exp! . source"},{"id":78,"pagetitle":"Basic functions","title":"ManifoldsBase.exp_fused","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.exp_fused-Tuple{AbstractManifold, Any, Any, Number}","content":" ManifoldsBase.exp_fused  ‚Äî  Method exp_fused(M::AbstractManifold, p, X, t::Number = 1) Compute the exponential map of tangent vector  X  scaled by  t  at point  p . Compared to  exp , this method provides the opportunity to avoid the allocation when computing  t*X . By default, this method allocates the resulting point  q  and passes to  exp_fused! . source"},{"id":79,"pagetitle":"Basic functions","title":"ManifoldsBase.geodesic!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.geodesic!-Tuple{AbstractManifold, Any, Any, Any, AbstractVector}","content":" ManifoldsBase.geodesic!  ‚Äî  Method geodesic!(M::AbstractManifold, Q, p, X, T::AbstractVector) -> AbstractVector Get the geodesic with initial point  p  and velocity  X  on the  AbstractManifold M . A geodesic is a curve of zero acceleration. That is for the curve  $Œ≥_{p,X}: I ‚Üí \\mathcal M$ , with  $Œ≥_{p,X}(0) = p$  and  $\\dot Œ≥_{p,X}(0) = X$  a geodesic further fulfills \\[‚àá_{\\dot Œ≥_{p,X}(t)} \\dot Œ≥_{p,X}(t) = 0,\\] i.e. the curve is acceleration free with respect to the Riemannian metric. This function evaluates the geodeic at time points  t  fom  T  in place of  Q . source"},{"id":80,"pagetitle":"Basic functions","title":"ManifoldsBase.geodesic!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.geodesic!-Tuple{AbstractManifold, Any, Any, Any, Real}","content":" ManifoldsBase.geodesic!  ‚Äî  Method geodesic!(M::AbstractManifold, q, p, X, t::Real) Get the geodesic with initial point  p  and velocity  X  on the  AbstractManifold M . A geodesic is a curve of zero acceleration. That is for the curve  $Œ≥_{p,X}: I ‚Üí \\mathcal M$ , with  $Œ≥_{p,X}(0) = p$  and  $\\dot Œ≥_{p,X}(0) = X$  a geodesic further fulfills \\[‚àá_{\\dot Œ≥_{p,X}(t)} \\dot Œ≥_{p,X}(t) = 0,\\] i.e. the curve is acceleration free with respect to the Riemannian metric. This function evaluates the geodeic at  t  in place of  q . source"},{"id":81,"pagetitle":"Basic functions","title":"ManifoldsBase.geodesic!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.geodesic!-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.geodesic!  ‚Äî  Method geodesic!(M::AbstractManifold, p, X) -> Function Get the geodesic with initial point  p  and velocity  X  on the  AbstractManifold M . A geodesic is a curve of zero acceleration. That is for the curve  $Œ≥_{p,X}: I ‚Üí \\mathcal M$ , with  $Œ≥_{p,X}(0) = p$  and  $\\dot Œ≥_{p,X}(0) = X$  a geodesic further fulfills \\[‚àá_{\\dot Œ≥_{p,X}(t)} \\dot Œ≥_{p,X}(t) = 0,\\] i.e. the curve is acceleration free with respect to the Riemannian metric. This yields that the curve has constant velocity and is locally distance-minimizing. This function returns a function  (q,t)  of (time)  t  that mutates  q `. source"},{"id":82,"pagetitle":"Basic functions","title":"ManifoldsBase.geodesic","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.geodesic-Tuple{AbstractManifold, Any, Any, AbstractVector}","content":" ManifoldsBase.geodesic  ‚Äî  Method geodesic(M::AbstractManifold, p, X, T::AbstractVector) -> AbstractVector Evaluate the geodesic  $Œ≥_{p,X}: I ‚Üí \\mathcal M$ , with  $Œ≥_{p,X}(0) = p$  and  $\\dot Œ≥_{p,X}(0) = X$  a geodesic further fulfills \\[‚àá_{\\dot Œ≥_{p,X}(t)} \\dot Œ≥_{p,X}(t) = 0,\\] at time points  t  from  T . source"},{"id":83,"pagetitle":"Basic functions","title":"ManifoldsBase.geodesic","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.geodesic-Tuple{AbstractManifold, Any, Any, Real}","content":" ManifoldsBase.geodesic  ‚Äî  Method geodesic(M::AbstractManifold, p, X, t::Real) Evaluate the geodesic  $Œ≥_{p,X}: I ‚Üí \\mathcal M$ , with  $Œ≥_{p,X}(0) = p$  and  $\\dot Œ≥_{p,X}(0) = X$  a geodesic further fulfills \\[‚àá_{\\dot Œ≥_{p,X}(t)} \\dot Œ≥_{p,X}(t) = 0,\\] at time  t . source"},{"id":84,"pagetitle":"Basic functions","title":"ManifoldsBase.geodesic","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.geodesic-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.geodesic  ‚Äî  Method geodesic(M::AbstractManifold, p, X) -> Function Get the geodesic with initial point  p  and velocity  X  on the  AbstractManifold M . A geodesic is a curve of zero acceleration. That is for the curve  $Œ≥_{p,X}: I ‚Üí \\mathcal M$ , with  $Œ≥_{p,X}(0) = p$  and  $\\dot Œ≥_{p,X}(0) = X$  a geodesic further fulfills \\[‚àá_{\\dot Œ≥_{p,X}(t)} \\dot Œ≥_{p,X}(t) = 0,\\] i.e. the curve is acceleration free with respect to the Riemannian metric. This yields, that the curve has constant velocity that is locally distance-minimizing. This function returns a function of (time)  t . source"},{"id":85,"pagetitle":"Basic functions","title":"ManifoldsBase.log!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.log!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.log!  ‚Äî  Method log!(M::AbstractManifold, X, p, q) Compute the logarithmic map of point  q  at base point  p  on the  AbstractManifold M . The result is saved to  X . The logarithmic map is the inverse of the  exp! onential map. Note that the logarithmic map might not be globally defined. see also  log  and  inverse_retract! , source"},{"id":86,"pagetitle":"Basic functions","title":"ManifoldsBase.shortest_geodesic!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.shortest_geodesic!-Tuple{AbstractManifold, Any, Any, Any, AbstractVector}","content":" ManifoldsBase.shortest_geodesic!  ‚Äî  Method shortest_geodesic!(M::AbstractManifold, R, p, q, T::AbstractVector) -> AbstractVector Evaluate a  geodesic $Œ≥_{p,q}(t)$  whose length is the shortest path between the points  p and  q , where  $Œ≥_{p,q}(0)=p$  and  $Œ≥_{p,q}(1)=q$  at all  t  from  T  in place of  R . When there are multiple shortest geodesics, a deterministic choice will be taken. source"},{"id":87,"pagetitle":"Basic functions","title":"ManifoldsBase.shortest_geodesic!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.shortest_geodesic!-Tuple{AbstractManifold, Any, Any, Any, Real}","content":" ManifoldsBase.shortest_geodesic!  ‚Äî  Method shortest_geodesic!(M::AabstractManifold, r, p, q, t::Real) Evaluate a  geodesic $Œ≥_{p,q}(t)$  whose length is the shortest path between the points  p and  q , where  $Œ≥_{p,q}(0)=p$  and  $Œ≥_{p,q}(1)=q$  at  t  in place of  r . When there are multiple shortest geodesics, a deterministic choice will be taken. source"},{"id":88,"pagetitle":"Basic functions","title":"ManifoldsBase.shortest_geodesic!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.shortest_geodesic!-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.shortest_geodesic!  ‚Äî  Method shortest_geodesic!(M::AbstractManifold, p, q) -> Function Get a  geodesic $Œ≥_{p,q}(t)$  whose length is the shortest path between the points  p and  q , where  $Œ≥_{p,q}(0)=p$  and  $Œ≥_{p,q}(1)=q$ . When there are multiple shortest geodesics, a deterministic choice will be returned. This function returns a function  (r,t) -> ...  of time  t  which works in place of  r . Further variants shortest_geodesic!(M::AabstractManifold, r, p, q, t::Real)\nshortest_geodesic!(M::AbstractManifold, R, p, q, T::AbstractVector) -> AbstractVector mutate (and return) the point  r  and the vector of points  R , respectively, returning the point at time  t  or points at times  t  in  T  along the shortest  geodesic . source"},{"id":89,"pagetitle":"Basic functions","title":"ManifoldsBase.shortest_geodesic","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.shortest_geodesic-Tuple{AbstractManifold, Any, Any, AbstractVector}","content":" ManifoldsBase.shortest_geodesic  ‚Äî  Method shortest_geodesic(M::AbstractManifold, p, q, T::AbstractVector) -> AbstractVector Evaluate a  geodesic $Œ≥_{p,q}(t)$  whose length is the shortest path between the points  p and  q , where  $Œ≥_{p,q}(0)=p$  and  $Œ≥_{p,q}(1)=q$  at time points  T . When there are multiple shortest geodesics, a deterministic choice will be returned. source"},{"id":90,"pagetitle":"Basic functions","title":"ManifoldsBase.shortest_geodesic","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.shortest_geodesic-Tuple{AbstractManifold, Any, Any, Real}","content":" ManifoldsBase.shortest_geodesic  ‚Äî  Method shortest_geodesic(M::AabstractManifold, p, q, t::Real) Evaluate a  geodesic $Œ≥_{p,q}(t)$  whose length is the shortest path between the points  p and  q , where  $Œ≥_{p,q}(0)=p$  and  $Œ≥_{p,q}(1)=q$  at time  t . When there are multiple shortest geodesics, a deterministic choice will be returned. source"},{"id":91,"pagetitle":"Basic functions","title":"ManifoldsBase.shortest_geodesic","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.shortest_geodesic-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.shortest_geodesic  ‚Äî  Method shortest_geodesic(M::AbstractManifold, p, q) -> Function Get a  geodesic $Œ≥_{p,q}(t)$  whose length is the shortest path between the points  p and  q , where  $Œ≥_{p,q}(0)=p$  and  $Œ≥_{p,q}(1)=q$ . When there are multiple shortest geodesics, a deterministic choice will be returned. This function returns a function of time, which may be a  Real  or an  AbstractVector . source"},{"id":92,"pagetitle":"Basic functions","title":"Parallel transport","ref":"/manifoldsbase/stable/functions/#subsec-parallel-transport","content":" Parallel transport While moving vectors from one base point to another is the identity in the Euclidean space ‚Äì¬†or in other words all tangent spaces (directions one can ‚Äúwalk‚Äù into) are the same. This is different on a manifold. If we have two points  $p,q ‚àà \\mathcal M$ , we take a  $c: [0,1] ‚Üí \\mathcal M$  connecting the two points, i.e.  $c(0) = p$  and  $c(1) = q$ . this could be a (or the) geodesic. If we further consider a vector field  $X: [0,1] ‚Üí T\\mathcal M$ , i.e. where  $X(t) ‚àà T_{c(t)}\\mathcal M$ . Then the vector field is called  parallel  if its covariant derivative  $\\frac{\\mathrm{D}}{\\mathrm{d}t}X(t) = 0$  for all  $t‚àà |0,1]$ . If we now impose a value for  $X=X(0) ‚àà T_p\\mathcal M$ , we obtain an ODE with an initial condition. The resulting value  $X(1) ‚àà T_q\\mathcal M$  is called the  parallel transport  of  X  along  $c$  or in case of a geodesic the _parallel transport of  X  from  p  to  q ."},{"id":93,"pagetitle":"Basic functions","title":"ManifoldsBase.parallel_transport_direction","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.parallel_transport_direction-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_direction  ‚Äî  Method parallel_transport_direction(M::AbstractManifold, p, X, d) Compute the parallel transport of  $X$  along the curve  $c(t) = Œ≥_{p,X}(t)$  to  $c(1)=q$ , where  $c(t)=Œ≥_{p,X}(t)$  is the the unique geodesic starting from  $Œ≥_{p,d}(0)=p$  into direction  $Ãá\\dot Œ≥_{p,d}(0)=d$ . By default this function calls  parallel_transport_to (M, p, X, q) , where  $q=\\exp_pX$ . source"},{"id":94,"pagetitle":"Basic functions","title":"ManifoldsBase.parallel_transport_to","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.parallel_transport_to-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::AbstractManifold, p, X, q) Compute the parallel transport of  $X$  along the curve  $c(t) = Œ≥_{p,q}(t)$ , i.e. the (assumed to be unique)  geodesic $Œ≥_{p,q}$  connecting  p  and  q . source"},{"id":95,"pagetitle":"Basic functions","title":"Further functions on manifolds","ref":"/manifoldsbase/stable/functions/#Further-functions-on-manifolds","content":" Further functions on manifolds"},{"id":96,"pagetitle":"Basic functions","title":"General functions provided by the interface","ref":"/manifoldsbase/stable/functions/#General-functions-provided-by-the-interface","content":" General functions provided by the interface"},{"id":97,"pagetitle":"Basic functions","title":"Base.angle","ref":"/manifoldsbase/stable/functions/#Base.angle-Tuple{AbstractManifold, Any, Any, Any}","content":" Base.angle  ‚Äî  Method angle(M::AbstractManifold, p, X, Y) Compute the angle between tangent vectors  X  and  Y  at point  p  from the  AbstractManifold M  with respect to the inner product from  inner . source"},{"id":98,"pagetitle":"Basic functions","title":"Base.copy","ref":"/manifoldsbase/stable/functions/#Base.copy-Tuple{AbstractManifold, Any, Any}","content":" Base.copy  ‚Äî  Method copy(M::AbstractManifold, p, X) Copy the value(s) from the tangent vector  X  at a point  p  on the  AbstractManifold M  into a new tangent vector. See  allocate_result  for the allocation of new point memory and  copyto!  for the copying. source"},{"id":99,"pagetitle":"Basic functions","title":"Base.copy","ref":"/manifoldsbase/stable/functions/#Base.copy-Tuple{AbstractManifold, Any}","content":" Base.copy  ‚Äî  Method copy(M::AbstractManifold, p) Copy the value(s) from the point  p  on the  AbstractManifold M  into a new point. See  allocate_result  for the allocation of new point memory and  copyto!  for the copying. source"},{"id":100,"pagetitle":"Basic functions","title":"Base.copyto!","ref":"/manifoldsbase/stable/functions/#Base.copyto!-Tuple{AbstractManifold, Any, Any, Any}","content":" Base.copyto!  ‚Äî  Method copyto!(M::AbstractManifold, Y, p, X) Copy the value(s) from  X  to  Y , where both are tangent vectors from the tangent space at  p  on the  AbstractManifold M . This function defaults to calling  copyto!(Y, X) , but it might be useful to overwrite the function at the level, where also information from  p  and  M  can be accessed. source"},{"id":101,"pagetitle":"Basic functions","title":"Base.copyto!","ref":"/manifoldsbase/stable/functions/#Base.copyto!-Tuple{AbstractManifold, Any, Any}","content":" Base.copyto!  ‚Äî  Method copyto!(M::AbstractManifold, q, p) Copy the value(s) from  p  to  q , where both are points on the  AbstractManifold M . This function defaults to calling  copyto!(q, p) , but it might be useful to overwrite the function at the level, where also information from  M  can be accessed. source"},{"id":102,"pagetitle":"Basic functions","title":"Base.isapprox","ref":"/manifoldsbase/stable/functions/#Base.isapprox-Tuple{AbstractManifold, Any, Any, Any}","content":" Base.isapprox  ‚Äî  Method isapprox(M::AbstractManifold, p, X, Y; error:Symbol=:none; kwargs...) Check if vectors  X  and  Y  tangent at  p  from  AbstractManifold M  are approximately equal. The optional positional argument can be used to get more information for the case that the result is false, if the concrete manifold provides such information. Currently the following are supported :error  - throws an error if  isapprox  evaluates to false, providing possibly a more detailed error. Note that this turns  isapprox  basically to an  @assert . :info  ‚Äì prints the information in an  @info :warn  ‚Äì prints the information in an  @warn :none  (default) ‚Äì the function just returns  true / false By default these informations are collected by calling  check_approx . Keyword arguments can be used to specify tolerances. source"},{"id":103,"pagetitle":"Basic functions","title":"Base.isapprox","ref":"/manifoldsbase/stable/functions/#Base.isapprox-Tuple{AbstractManifold, Any, Any}","content":" Base.isapprox  ‚Äî  Method isapprox(M::AbstractManifold, p, q; error::Symbol=:none, kwargs...) Check if points  p  and  q  from  AbstractManifold M  are approximately equal. The keyword argument can be used to get more information for the case that the result is false, if the concrete manifold provides such information. Currently the following are supported :error  - throws an error if  isapprox  evaluates to false, providing possibly a more detailed error. Note that this turns  isapprox  basically to an  @assert . :info  ‚Äì prints the information in an  @info :warn  ‚Äì prints the information in an  @warn :none  (default) ‚Äì the function just returns  true / false Keyword arguments can be used to specify tolerances. source"},{"id":104,"pagetitle":"Basic functions","title":"Base.rand","ref":"/manifoldsbase/stable/functions/#Base.rand-Tuple{AbstractManifold}","content":" Base.rand  ‚Äî  Method Random.rand(M::AbstractManifold, [d::Integer]; vector_at=nothing)\nRandom.rand(rng::AbstractRNG, M::AbstractManifold, [d::Integer]; vector_at=nothing) Generate a random point on manifold  M  (when  vector_at  is  nothing ) or a tangent vector at point  vector_at  (when it is not  nothing ). Optionally a random number generator  rng  to be used can be specified. An optional integer  d  indicates that a vector of  d  points or tangent vectors is to be generated. Note Usually a uniform distribution should be expected for compact manifolds and a Gaussian-like distribution for non-compact manifolds and tangent vectors, although it is not guaranteed. The distribution may change between releases. rand  methods for specific manifolds may take additional keyword arguments. source"},{"id":105,"pagetitle":"Basic functions","title":"LinearAlgebra.norm","ref":"/manifoldsbase/stable/functions/#LinearAlgebra.norm-Tuple{AbstractManifold, Any, Any}","content":" LinearAlgebra.norm  ‚Äî  Method norm(M::AbstractManifold, p, X) Compute the norm of tangent vector  X  at point  p  from a  AbstractManifold M . By default this is computed using  inner . source"},{"id":106,"pagetitle":"Basic functions","title":"ManifoldsBase.Weingarten!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.Weingarten!-Tuple{AbstractManifold, Vararg{Any, 4}}","content":" ManifoldsBase.Weingarten!  ‚Äî  Method Weingarten!(M, Y, p, X, V) Compute the Weingarten map  $\\mathcal W_p\\colon T_p\\mathcal M √ó N_p\\mathcal M \\to T_p\\mathcal M$  in place of  Y , see  Weingarten . source"},{"id":107,"pagetitle":"Basic functions","title":"ManifoldsBase.Weingarten","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.Weingarten-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Weingarten(M, p, X, V) Compute the Weingarten map  $\\mathcal W_p\\colon T_p\\mathcal M √ó N_p\\mathcal M \\to T_p\\mathcal M$ , where  $N_p\\mathcal M$  is the orthogonal complement of the tangent space  $T_p\\mathcal M$  of the embedded submanifold  $\\mathcal M$ , where we denote the embedding by  $\\mathcal E$ . The Weingarten map can be defined by restricting the differential of the orthogonal  project ion  $\\operatorname{proj}_{T_p\\mathcal M}\\colon T_p \\mathcal E \\to T_p\\mathcal M$  with respect to the base point  $p$ , i.e. defining \\[\\mathcal P_X := D_p\\operatorname{proj}_{T_p\\mathcal M}(Y)[X],\n\\qquad Y \\in T_p \\mathcal E, X \\in T_p\\mathcal M,\\] the Weingarten map can be written as  $\\mathcal W_p(X,V) = \\mathcal P_X(V)$ . The Weingarten map is named after  Julius Weingarten  (1836‚Äì1910). source"},{"id":108,"pagetitle":"Basic functions","title":"ManifoldsBase.allocate","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.allocate-Tuple{Any, Vararg{Any}}","content":" ManifoldsBase.allocate  ‚Äî  Method allocate(a)\nallocate(a, dims::Integer...)\nallocate(a, dims::Tuple)\nallocate(a, T::Type)\nallocate(a, T::Type, dims::Integer...)\nallocate(a, T::Type, dims::Tuple)\nallocate(M::AbstractManifold, a)\nallocate(M::AbstractManifold, a, dims::Integer...)\nallocate(M::AbstractManifold, a, dims::Tuple)\nallocate(M::AbstractManifold, a, T::Type)\nallocate(M::AbstractManifold, a, T::Type, dims::Integer...)\nallocate(M::AbstractManifold, a, T::Type, dims::Tuple) Allocate an object similar to  a . It is similar to function  similar , although instead of working only on the outermost layer of a nested structure, it maps recursively through outer layers and calls  similar  on the innermost array-like object only. Type  T  is the new number element type  number_eltype , if it is not given the element type of  a  is retained. The  dims  argument can be given for non-nested allocation and is forwarded to the function  similar . It's behavior can be overridden by a specific manifold, for example power manifold with nested replacing representation can decide that  allocate  for  Array{<:SArray}  returns another  Array{<:SArray}  instead of  Array{<:MArray} , as would be done by default. source"},{"id":109,"pagetitle":"Basic functions","title":"ManifoldsBase.allocate_on","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.allocate_on-Tuple{AbstractManifold}","content":" ManifoldsBase.allocate_on  ‚Äî  Method allocate_on(M::AbstractManifold, [T:::Type])\nallocate_on(M::AbstractManifold, F::FiberType, [T:::Type]) Allocate a new point on manifold  M  with optional type given by  T . Note that  T  is not number element type as in  allocate  but rather the type of the entire point to be returned. If  F  is provided, then an element of the corresponding fiber is allocated, assuming it is independent of the base point. To allocate a tangent vector, use `` Example julia> using ManifoldsBase\n\njulia> M = ManifoldsBase.DefaultManifold(4)\nDefaultManifold(4; field = ‚Ñù)\n\njulia> allocate_on(M)\n4-element Vector{Float64}:\n 0.0\n 0.0\n 0.0\n 0.0\n\njulia> allocate_on(M, Array{Float64})\n4-element Vector{Float64}:\n 0.0\n 0.0\n 0.0\n 0.0\n\njulia> allocate_on(M, TangentSpaceType())\n4-element Vector{Float64}:\n 0.0\n 0.0\n 0.0\n 0.0\n\njulia> allocate_on(M, TangentSpaceType(), Array{Float64})\n4-element Vector{Float64}:\n 0.0\n 0.0\n 0.0\n 0.0\n source"},{"id":110,"pagetitle":"Basic functions","title":"ManifoldsBase.base_manifold","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.base_manifold","content":" ManifoldsBase.base_manifold  ‚Äî  Function base_manifold(M::AbstractManifold, depth = Val(-1)) Return the internally stored  AbstractManifold  for decorated manifold  M  and the base manifold for vector bundles or power manifolds. The optional parameter  depth  can be used to remove only the first  depth  many decorators and return the  AbstractManifold  from that level, whether its decorated or not. Any negative value deactivates this depth limit. source"},{"id":111,"pagetitle":"Basic functions","title":"ManifoldsBase.default_type","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.default_type-Tuple{AbstractManifold, ManifoldsBase.FiberType}","content":" ManifoldsBase.default_type  ‚Äî  Method default_type(M::AbstractManifold, ft::FiberType) Get the default type of points from the fiber  ft  of the fiber bundle based on manifold  M . For example, call  default_type(MyManifold(), TangentSpaceType())  to get the default type of a tangent vector. source"},{"id":112,"pagetitle":"Basic functions","title":"ManifoldsBase.default_type","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.default_type-Tuple{AbstractManifold}","content":" ManifoldsBase.default_type  ‚Äî  Method default_type(M::AbstractManifold) Get the default type of points on manifold  M . source"},{"id":113,"pagetitle":"Basic functions","title":"ManifoldsBase.distance","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.distance-Tuple{AbstractManifold, Any, Any, AbstractInverseRetractionMethod}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::AbstractManifold, p, q, m::AbstractInverseRetractionMethod) Approximate distance between points  p  and  q  on manifold  M  using  AbstractInverseRetractionMethod m . source"},{"id":114,"pagetitle":"Basic functions","title":"ManifoldsBase.distance","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.distance-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::AbstractManifold, p, q) Shortest distance between the points  p  and  q  on the  AbstractManifold M , i.e. \\[d(p,q) = \\inf_{Œ≥} L(Œ≥),\\] where the infimum is over all piecewise smooth curves  $Œ≥: [a,b] \\to \\mathcal M$  connecting  $Œ≥(a)=p$  and  $Œ≥(b)=q$  and \\[L(Œ≥) = \\displaystyle\\int_{a}^{b} \\lVert \\dotŒ≥(t)\\rVert_{Œ≥(t)} \\mathrm{d}t\\] is the length of the curve  $Œ≥$ . If  $\\mathcal M$  is not connected, i.e. consists of several disjoint components, the distance between two points from different components should be  $‚àû$ . source"},{"id":115,"pagetitle":"Basic functions","title":"ManifoldsBase.embed!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.embed!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.embed!  ‚Äî  Method embed!(M::AbstractManifold, Y, p, X) Embed a tangent vector  X  at a point  p  on the  AbstractManifold M  into the ambient space and return the result in  Y . This method is only available for manifolds where implicitly an embedding or ambient space is given. Additionally,  embed!  includes changing data representation, if applicable, i.e. if the tangents on  M  are not represented in the same way as tangents on the embedding, the representation is changed accordingly. This is the case for example for Lie groups, when tangent vectors are represented in the Lie algebra. The embedded tangents are then in the tangent spaces of the embedded base points. The default is set in such a way that it assumes that the points on  M  are represented in their embedding (for example like the unit vectors in a space to represent the sphere) and hence embedding also for tangent vectors is the identity by default. See also:  EmbeddedManifold ,  project! source"},{"id":116,"pagetitle":"Basic functions","title":"ManifoldsBase.embed!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.embed!-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.embed!  ‚Äî  Method embed!(M::AbstractManifold, q, p) Embed point  p  from the  AbstractManifold M  into an ambient space. This method is only available for manifolds where implicitly an embedding or ambient space is given. Not implementing this function means, there is no proper embedding for your manifold. Additionally,  embed  might include changing data representation, if applicable, i.e. if points on  M  are not represented in the same way as their counterparts in the embedding, the representation is changed accordingly. The default is set in such a way that it assumes that the points on  M  are represented in their embedding (for example like the unit vectors in a space to represent the sphere) and hence embedding in the identity by default. If you have more than one embedding, see  EmbeddedManifold  for defining a second embedding. If your point  p  is already represented in some embedding, see  AbstractDecoratorManifold  how you can avoid reimplementing code from the embedded manifold See also:  EmbeddedManifold ,  project! source"},{"id":117,"pagetitle":"Basic functions","title":"ManifoldsBase.embed","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.embed-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.embed  ‚Äî  Method embed(M::AbstractManifold, p, X) Embed a tangent vector  X  at a point  p  on the  AbstractManifold M  into an ambient space. This method is only available for manifolds where implicitly an embedding or ambient space is given. Not implementing this function means, there is no proper embedding for your tangent space(s). Additionally,  embed  might include changing data representation, if applicable, i.e. if tangent vectors on  M  are not represented in the same way as their counterparts in the embedding, the representation is changed accordingly. The default is set in such a way that memory is allocated and  embed!(M, Y, p. X)  is called. If you have more than one embedding, see  EmbeddedManifold  for defining a second embedding. If your tangent vector  X  is already represented in some embedding, see  AbstractDecoratorManifold  how you can avoid reimplementing code from the embedded manifold See also:  EmbeddedManifold ,  project source"},{"id":118,"pagetitle":"Basic functions","title":"ManifoldsBase.embed","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.embed-Tuple{AbstractManifold, Any}","content":" ManifoldsBase.embed  ‚Äî  Method embed(M::AbstractManifold, p) Embed point  p  from the  AbstractManifold M  into the ambient space. This method is only available for manifolds where implicitly an embedding or ambient space is given. Additionally,  embed  includes changing data representation, if applicable, i.e. if the points on  M  are not represented in the same way as points on the embedding, the representation is changed accordingly. The default is set in such a way that memory is allocated and  embed!(M, q, p)  is called. See also:  EmbeddedManifold ,  project source"},{"id":119,"pagetitle":"Basic functions","title":"ManifoldsBase.embed_project","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.embed_project-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.embed_project  ‚Äî  Method embed_project(M::AbstractManifold, p, X) Embed vector  X  tangent at  p  from manifold  M  an project it back to tangent space at  p . For points from that tangent space this is identity but in case embedding is defined for tangent vectors from outside of it, this can serve as a way to for example remove numerical inaccuracies caused by some algorithms. source"},{"id":120,"pagetitle":"Basic functions","title":"ManifoldsBase.embed_project","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.embed_project-Tuple{AbstractManifold, Any}","content":" ManifoldsBase.embed_project  ‚Äî  Method embed_project(M::AbstractManifold, p) Embed  p  from manifold  M  an project it back to  M . For points from  M  this is identity but in case embedding is defined for points outside of  M , this can serve as a way to for example remove numerical inaccuracies caused by some algorithms. source"},{"id":121,"pagetitle":"Basic functions","title":"ManifoldsBase.has_components","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.has_components-Tuple{AbstractManifold}","content":" ManifoldsBase.has_components  ‚Äî  Method has_components(M::AbstractManifold) Return whether the  AbstractManifold (M)  consists of components, like the  PowerManifold  or the  ProductManifold , that one can iterate over. By default, this function returns  false . source"},{"id":122,"pagetitle":"Basic functions","title":"ManifoldsBase.injectivity_radius","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.injectivity_radius-Tuple{AbstractManifold}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::AbstractManifold) Infimum of the injectivity radii  injectivity_radius(M,p)  of all points  p  on the  AbstractManifold . injectivity_radius(M::AbstractManifold, p) Return the distance  $d$  such that  exp(M, p, X)  is injective for all tangent vectors shorter than  $d$  (i.e. has an inverse). injectivity_radius(M::AbstractManifold[, x], method::AbstractRetractionMethod)\ninjectivity_radius(M::AbstractManifold, x, method::AbstractRetractionMethod) Distance  $d$  such that  retract(M, p, X, method)  is injective for all tangent vectors shorter than  $d$  (i.e. has an inverse) for point  p  if provided or all manifold points otherwise. In order to dispatch on different retraction methods, please either implement  _injectivity_radius(M[, p], m::T)  for your retraction  R  or specifically  injectivity_radius_exp(M[, p])  for the exponential map. By default the variant with a point  p  assumes that the default (without  p ) can ve called as a lower bound. source"},{"id":123,"pagetitle":"Basic functions","title":"ManifoldsBase.inner","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.inner-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::AbstractManifold, p, X, Y) Compute the inner product of tangent vectors  X  and  Y  at point  p  from the  AbstractManifold M . source"},{"id":124,"pagetitle":"Basic functions","title":"ManifoldsBase.is_flat","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.is_flat-Tuple{AbstractManifold}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(M::AbstractManifold) Return true if the  AbstractManifold M  is flat, i.e. if its Riemann curvature tensor is everywhere zero. source"},{"id":125,"pagetitle":"Basic functions","title":"ManifoldsBase.is_point","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.is_point-Tuple{AbstractManifold, Any, Bool}","content":" ManifoldsBase.is_point  ‚Äî  Method is_point(M::AbstractManifold, p; error::Symbol = :none, kwargs...)\nis_point(M::AbstractManifold, p, throw_error::Bool; kwargs...) Return whether  p  is a valid point on the  AbstractManifold M . By default the function calls  check_point , which returns an  ErrorException  or  nothing . How to report a potential error can be set using the  error=  keyword :error           - throws an error if  p  is not a point :info            - displays the error message as an  @info :warn            - displays the error message as a  @warning :none  (default) ‚Äì the function just returns  true / false all other symbols are equivalent to  error=:none . The second signature is a shorthand, where the boolean is used for  error=:error  ( true ) and  error=:none  (default,  false ). This case ignores the  error=  keyword source"},{"id":126,"pagetitle":"Basic functions","title":"ManifoldsBase.is_vector","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.is_vector-Tuple{AbstractManifold, Any, Any, Bool, Bool}","content":" ManifoldsBase.is_vector  ‚Äî  Method is_vector(M::AbstractManifold, p, X, check_base_point::Bool=true; error::Symbol=:none, kwargs...)\nis_vector(M::AbstractManifold, p, X, check_base_point::Bool=true, throw_error::Boolean; kwargs...) Return whether  X  is a valid tangent vector at point  p  on the  AbstractManifold M . Returns either  true  or  false . If  check_base_point  is set to true, this function also (first) calls  is_point  on  p . Then, the function calls  check_vector  and checks whether the returned value is  nothing  or an error. How to report a potential error can be set using the  error=  keyword :error           - throws an error if  X  is not a tangent vector and/or  p  is not point ^  :info            - displays the error message as an  @info :warn            - displays the error message as a  @warn ing. :none            - (default) the function just returns  true / false all other symbols are equivalent to  error=:none The second signature is a shorthand, where  throw_error  is used for  error=:error  ( true ) and  error=:none  (default,  false ). This case ignores the  error=  keyword. source"},{"id":127,"pagetitle":"Basic functions","title":"ManifoldsBase.manifold_dimension","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.manifold_dimension-Tuple{AbstractManifold}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::AbstractManifold) The dimension  $n=\\dim_{\\mathcal M}$  of real space  $\\mathbb R^n$  to which the neighborhood of each point of the  AbstractManifold M  is homeomorphic. source"},{"id":128,"pagetitle":"Basic functions","title":"ManifoldsBase.mid_point!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.mid_point!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.mid_point!  ‚Äî  Method mid_point!(M::AbstractManifold, q, p1, p2) Calculate the middle between the two point  p1  and  p2  from manifold  M . By default uses  log , divides the vector by 2 and uses  exp! . Saves the result in  q . source"},{"id":129,"pagetitle":"Basic functions","title":"ManifoldsBase.mid_point","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.mid_point-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.mid_point  ‚Äî  Method mid_point(M::AbstractManifold, p1, p2) Calculate the middle between the two point  p1  and  p2  from manifold  M . By default uses  log , divides the vector by 2 and uses  exp . source"},{"id":130,"pagetitle":"Basic functions","title":"ManifoldsBase.number_eltype","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.number_eltype-Tuple{Any}","content":" ManifoldsBase.number_eltype  ‚Äî  Method number_eltype(x) Numeric element type of the a nested representation of a point or a vector. To be used in conjunction with  allocate  or  allocate_result . source"},{"id":131,"pagetitle":"Basic functions","title":"ManifoldsBase.representation_size","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.representation_size-Tuple{AbstractManifold}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::AbstractManifold) The size of an array representing a point on  AbstractManifold M . Returns  nothing  by default indicating that points are not represented using an  AbstractArray . source"},{"id":132,"pagetitle":"Basic functions","title":"ManifoldsBase.riemann_tensor","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.riemann_tensor-Tuple{AbstractManifold, Vararg{Any, 4}}","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(M::AbstractManifold, p, X, Y, Z) Compute the value of the Riemann tensor  $R(X_f,Y_f)Z_f$  at point  p , where  $X_f$ ,  $Y_f$  and  $Z_f$  are vector fields defined by parallel transport of, respectively,  X ,  Y  and  Z  to the desired point. All computations are performed using the connection associated to manifold  M . The formula reads  $R(X_f,Y_f)Z_f = \\nabla_X\\nabla_Y Z - \\nabla_Y\\nabla_X Z - \\nabla_{[X, Y]}Z$ , where  $[X, Y]$  is the Lie bracket of vector fields. Note that some authors define this quantity with inverse sign. source"},{"id":133,"pagetitle":"Basic functions","title":"ManifoldsBase.sectional_curvature","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.sectional_curvature-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.sectional_curvature  ‚Äî  Method sectional_curvature(M::AbstractManifold, p, X, Y) Compute the sectional curvature of a manifold  $\\mathcal M$  at a point  $p \\in \\mathcal M$  on two linearly independent tangent vectors at  $p$ . The formula reads \\[\n    \\kappa_p(X, Y) = \\frac{‚ü®R(X, Y, Y), X‚ü©_p}{\\lVert X \\rVert^2_p \\lVert Y \\rVert^2_p - ‚ü®X, Y‚ü©^2_p}\n\\] where  $R(X, Y, Y)$  is the  riemann_tensor  on  $\\mathcal M$ . Input M :   a manifold  $\\mathcal M$ p :   a point  $p \\in \\mathcal M$ X :   a tangent vector  $X \\in T_p \\mathcal M$ Y :   a tangent vector  $Y \\in T_p \\mathcal M$ source"},{"id":134,"pagetitle":"Basic functions","title":"ManifoldsBase.sectional_curvature_max","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.sectional_curvature_max-Tuple{AbstractManifold}","content":" ManifoldsBase.sectional_curvature_max  ‚Äî  Method sectional_curvature_max(M::AbstractManifold) Upper bound on sectional curvature of manifold  M . The formula reads \\[\\omega = \\operatorname{sup}_{p\\in\\mathcal M, X\\in T_p\\mathcal M, Y\\in T_p\\mathcal M, ‚ü®X, Y‚ü© ‚â† 0} \\kappa_p(X, Y)\\] source"},{"id":135,"pagetitle":"Basic functions","title":"ManifoldsBase.sectional_curvature_min","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.sectional_curvature_min-Tuple{AbstractManifold}","content":" ManifoldsBase.sectional_curvature_min  ‚Äî  Method sectional_curvature_min(M::AbstractManifold) Lower bound on sectional curvature of manifold  M . The formula reads \\[\\omega = \\operatorname{inf}_{p\\in\\mathcal M, X\\in T_p\\mathcal M, Y\\in T_p\\mathcal M, ‚ü®X, Y‚ü© ‚â† 0} \\kappa_p(X, Y)\\] source"},{"id":136,"pagetitle":"Basic functions","title":"ManifoldsBase.zero_vector!","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.zero_vector!-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.zero_vector!  ‚Äî  Method zero_vector!(M::AbstractManifold, X, p) Save to  X  the tangent vector from the tangent space  $T_p\\mathcal M$  at  p  that represents the zero vector, i.e. such that retracting  X  to the  AbstractManifold M  at  p  produces  p . source"},{"id":137,"pagetitle":"Basic functions","title":"ManifoldsBase.zero_vector","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.zero_vector-Tuple{AbstractManifold, Any}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(M::AbstractManifold, p) Return the tangent vector from the tangent space  $T_p\\mathcal M$  at  p  on the  AbstractManifold M , that represents the zero vector, i.e. such that a retraction at  p  produces  p . source"},{"id":138,"pagetitle":"Basic functions","title":"Internal functions","ref":"/manifoldsbase/stable/functions/#Internal-functions","content":" Internal functions While you should always add your documentation to functions from the last section, some of the functions dispatch onto functions on  layer III . These are the ones you usually implement for your manifold ‚Äì unless there is no lower level function called, like for the  manifold_dimension ."},{"id":139,"pagetitle":"Basic functions","title":"Base.convert","ref":"/manifoldsbase/stable/functions/#Base.convert-Tuple{Type, AbstractManifold, Any, Any}","content":" Base.convert  ‚Äî  Method convert(T::Type, M::AbstractManifold, p, X) Convert vector  X  tangent at point  p  from manifold  M  to type  T . source"},{"id":140,"pagetitle":"Basic functions","title":"Base.convert","ref":"/manifoldsbase/stable/functions/#Base.convert-Tuple{Type, AbstractManifold, Any}","content":" Base.convert  ‚Äî  Method convert(T::Type, M::AbstractManifold, p) Convert point  p  from manifold  M  to type  T . source"},{"id":141,"pagetitle":"Basic functions","title":"ManifoldsBase._isapprox","ref":"/manifoldsbase/stable/functions/#ManifoldsBase._isapprox-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase._isapprox  ‚Äî  Method _isapprox(M::AbstractManifold, p, X, Y; kwargs...) An internal function for testing whether tangent vectors  X  and  Y  from tangent space at point  p  from manifold  M  are approximately equal. Returns either  true  or  false  and does not support errors like  isapprox . For more details see documentation of  check_approx . source"},{"id":142,"pagetitle":"Basic functions","title":"ManifoldsBase._isapprox","ref":"/manifoldsbase/stable/functions/#ManifoldsBase._isapprox-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase._isapprox  ‚Äî  Method _isapprox(M::AbstractManifold, p, q; kwargs...) An internal function for testing whether points  p  and  q  from manifold  M  are approximately equal. Returns either  true  or  false  and does not support errors like  isapprox . For more details see documentation of  check_approx . source"},{"id":143,"pagetitle":"Basic functions","title":"ManifoldsBase._pick_basic_allocation_argument","ref":"/manifoldsbase/stable/functions/#ManifoldsBase._pick_basic_allocation_argument-Tuple{AbstractManifold, Any, Vararg{Any}}","content":" ManifoldsBase._pick_basic_allocation_argument  ‚Äî  Method _pick_basic_allocation_argument(::AbstractManifold, f, x...) Pick which one of elements of  x  should be used as a basis for allocation in the  allocate_result(M::AbstractManifold, f, x...)  method. This can be specialized to, for example, skip  Identity  arguments in Manifolds.jl group-related functions. source"},{"id":144,"pagetitle":"Basic functions","title":"ManifoldsBase.allocate_result","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.allocate_result-Tuple{AbstractManifold, Any, Vararg{Any}}","content":" ManifoldsBase.allocate_result  ‚Äî  Method allocate_result(M::AbstractManifold, f, x...) Allocate an array for the result of function  f  on  AbstractManifold M  and arguments  x...  for implementing the non-modifying operation using the modifying operation. Usefulness of passing a function is demonstrated by methods that allocate results of musical isomorphisms. source"},{"id":145,"pagetitle":"Basic functions","title":"ManifoldsBase.allocate_result_type","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.allocate_result_type-Union{Tuple{TF}, Tuple{N}, Tuple{AbstractManifold, TF, NTuple{N, Any}}} where {N, TF}","content":" ManifoldsBase.allocate_result_type  ‚Äî  Method allocate_result_type(M::AbstractManifold, f, args::NTuple{N,Any}) where N Return type of element of the array that will represent the result of function  f  and the  AbstractManifold M  on given arguments  args  (passed as a tuple). source"},{"id":146,"pagetitle":"Basic functions","title":"ManifoldsBase.are_linearly_independent","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.are_linearly_independent-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.are_linearly_independent  ‚Äî  Method are_linearly_independent(M::AbstractManifold, p, X, Y) Check is vectors  X ,  Y  tangent at  p  to  M  are linearly independent. source"},{"id":147,"pagetitle":"Basic functions","title":"ManifoldsBase.check_approx","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.check_approx-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.check_approx  ‚Äî  Method check_approx(M::AbstractManifold, p, q; kwargs...)\ncheck_approx(M::AbstractManifold, p, X, Y; kwargs...) Check whether two elements are approximately equal, either  p ,  q  on the  AbstractManifold  or the two tangent vectors  X ,  Y  in the tangent space at  p  are approximately the same. The keyword arguments  kwargs  can be used to set tolerances, similar to Julia's  isapprox . This function might use  isapprox  from Julia internally and is similar to  isapprox , with the difference that is returns an  ApproximatelyError  if the two elements are not approximately equal, containting a more detailed description/reason. If the two elements are approximalely equal, this method returns  nothing . This method is an internal function and is called by  isapprox  whenever the user specifies an  error=  keyword therein.  _isapprox  is another related internal function. It is supposed to provide a fast true/false decision whether points or vectors are equal or not, while  check_approx  also provides a textual explanation. If no additional explanation is needed, a manifold may just implement a method of  _isapprox , while it should also implement  check_approx  if a more detailed explanation could be helpful. source"},{"id":148,"pagetitle":"Basic functions","title":"ManifoldsBase.check_point","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.check_point-Tuple{AbstractManifold, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::AbstractManifold, p; kwargs...) -> Union{Nothing,String} Return  nothing  when  p  is a point on the  AbstractManifold M . Otherwise, return an error with description why the point does not belong to manifold  M . By default,  check_point  returns  nothing , i.e. if no checks are implemented, the assumption is to be optimistic for a point not deriving from the  AbstractManifoldPoint  type. source"},{"id":149,"pagetitle":"Basic functions","title":"ManifoldsBase.check_size","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.check_size-Tuple{AbstractManifold, Any}","content":" ManifoldsBase.check_size  ‚Äî  Method check_size(M::AbstractManifold, p)\ncheck_size(M::AbstractManifold, p, X) Check whether  p  has the right  representation_size  for a  AbstractManifold M . Additionally if a tangent vector is given, both  p  and  X  are checked to be of corresponding correct representation sizes for points and tangent vectors on  M . By default,  check_size  returns  nothing , i.e. if no checks are implemented, the assumption is to be optimistic. source"},{"id":150,"pagetitle":"Basic functions","title":"ManifoldsBase.check_vector","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.check_vector-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::AbstractManifold, p, X; kwargs...) -> Union{Nothing,String} Check whether  X  is a valid tangent vector in the tangent space of  p  on the  AbstractManifold M . An implementation does not have to validate the point  p . If it is not a tangent vector, an error string should be returned. By default,  check_vector  returns  nothing , i.e. if no checks are implemented, the assumption is to be optimistic for tangent vectors not deriving from the  AbstractTangentVector  type. source"},{"id":151,"pagetitle":"Basic functions","title":"ManifoldsBase.size_to_tuple","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.size_to_tuple-Union{Tuple{Type{S}}, Tuple{S}} where S<:Tuple","content":" ManifoldsBase.size_to_tuple  ‚Äî  Method size_to_tuple(::Type{S}) where S<:Tuple Converts a size given by  Tuple{N, M, ...}  into a tuple  (N, M, ...) . source"},{"id":152,"pagetitle":"Basic functions","title":"ManifoldsBase.tangent_vector_type","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.tangent_vector_type-Tuple{AbstractManifold, Type}","content":" ManifoldsBase.tangent_vector_type  ‚Äî  Method tangent_vector_type(::AbstractManifold, point_type::Type) Change  point_type  that is a type of points on manifold  M  to matching type for representing tangent vectors. source"},{"id":153,"pagetitle":"Basic functions","title":"Approximation Methods","ref":"/manifoldsbase/stable/functions/#Approximation-Methods","content":" Approximation Methods"},{"id":154,"pagetitle":"Basic functions","title":"ManifoldsBase.AbstractApproximationMethod","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.AbstractApproximationMethod","content":" ManifoldsBase.AbstractApproximationMethod  ‚Äî  Type AbstractApproximationMethod Abstract type for defining estimation methods on manifolds. source"},{"id":155,"pagetitle":"Basic functions","title":"ManifoldsBase.CyclicProximalPointEstimation","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.CyclicProximalPointEstimation","content":" ManifoldsBase.CyclicProximalPointEstimation  ‚Äî  Type CyclicProximalPointEstimation <: AbstractApproximationMethod Method for estimation using the cyclic proximal point technique, which is based on  proximal maps . source"},{"id":156,"pagetitle":"Basic functions","title":"ManifoldsBase.EfficientEstimator","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.EfficientEstimator","content":" ManifoldsBase.EfficientEstimator  ‚Äî  Type EfficientEstimator <: AbstractApproximationMethod Method for estimation in the best possible sense, see  Efficiency (Statictsics)  for more details. This can for example be used when computing the usual mean on an Euclidean space, which is the best estimator. source"},{"id":157,"pagetitle":"Basic functions","title":"ManifoldsBase.ExtrinsicEstimation","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.ExtrinsicEstimation","content":" ManifoldsBase.ExtrinsicEstimation  ‚Äî  Type ExtrinsicEstimation{T} <: AbstractApproximationMethod Method for estimation in the ambient space with a method of type  T  and projecting the result back to the manifold. source"},{"id":158,"pagetitle":"Basic functions","title":"ManifoldsBase.GeodesicInterpolation","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.GeodesicInterpolation","content":" ManifoldsBase.GeodesicInterpolation  ‚Äî  Type GeodesicInterpolation <: AbstractApproximationMethod Method for estimation based on geodesic interpolation. source"},{"id":159,"pagetitle":"Basic functions","title":"ManifoldsBase.GeodesicInterpolationWithinRadius","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.GeodesicInterpolationWithinRadius","content":" ManifoldsBase.GeodesicInterpolationWithinRadius  ‚Äî  Type GeodesicInterpolationWithinRadius{T} <: AbstractApproximationMethod Method for estimation based on geodesic interpolation that is restricted to some  radius Constructor GeodesicInterpolationWithinRadius(radius::Real) source"},{"id":160,"pagetitle":"Basic functions","title":"ManifoldsBase.GradientDescentEstimation","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.GradientDescentEstimation","content":" ManifoldsBase.GradientDescentEstimation  ‚Äî  Type GradientDescentEstimation <: AbstractApproximationMethod Method for estimation using  gradient descent . source"},{"id":161,"pagetitle":"Basic functions","title":"ManifoldsBase.WeiszfeldEstimation","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.WeiszfeldEstimation","content":" ManifoldsBase.WeiszfeldEstimation  ‚Äî  Type WeiszfeldEstimation <: AbstractApproximationMethod Method for estimation using the Weiszfeld algorithm, compare for example the computation of the  Geometric median . source"},{"id":162,"pagetitle":"Basic functions","title":"ManifoldsBase.default_approximation_method","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.default_approximation_method-Tuple{AbstractManifold, Any}","content":" ManifoldsBase.default_approximation_method  ‚Äî  Method default_approximation_method(M::AbstractManifold, f)\ndefault_approximation_method(M::AbtractManifold, f, T) Specify a default estimation method for an  AbstractManifold  and a specific function  f  and optionally as well a type  T  to distinguish different (point or vector) representations on  M . By default, all functions  f  call the signature for just a manifold. The exceptional functions are: retract  and  retract!  which fall back to  default_retraction_method inverse_retract  and  inverse_retract!  which fall back to  default_inverse_retraction_method any of the vector transport mehods fall back to  default_vector_transport_method source"},{"id":163,"pagetitle":"Basic functions","title":"Error Messages","ref":"/manifoldsbase/stable/functions/#Error-Messages","content":" Error Messages This interface introduces a small set of own error messages."},{"id":164,"pagetitle":"Basic functions","title":"ManifoldsBase.AbstractManifoldDomainError","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.AbstractManifoldDomainError","content":" ManifoldsBase.AbstractManifoldDomainError  ‚Äî  Type AbstractManifoldDomainError <: Exception An absytract Case for Errors when checking validity of points/vectors on mainfolds source"},{"id":165,"pagetitle":"Basic functions","title":"ManifoldsBase.ApproximatelyError","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.ApproximatelyError","content":" ManifoldsBase.ApproximatelyError  ‚Äî  Type ApproximatelyError{V,S} <: Exception Store an error that occurs when two data structures, e.g. points or tangent vectors. Fields val  amount the two approximate elements are apart ‚Äì is set to  NaN  if this is not known msg  a message providing more detail about the performed test and why it failed. Constructors ApproximatelyError(val::V, msg::S) where {V,S} Generate an Error with value  val  and message  msg . ApproximatelyError(msg::S) where {S} Generate a message without a value (using  val=NaN  internally) and message  msg . source"},{"id":166,"pagetitle":"Basic functions","title":"ManifoldsBase.ComponentManifoldError","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.ComponentManifoldError","content":" ManifoldsBase.ComponentManifoldError  ‚Äî  Type CompnentError{I,E} <: Exception Store an error that occured in a component, where the additional  index  is stored. Fields index::I  index where the error occured` error::E  error that occured. source"},{"id":167,"pagetitle":"Basic functions","title":"ManifoldsBase.CompositeManifoldError","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.CompositeManifoldError","content":" ManifoldsBase.CompositeManifoldError  ‚Äî  Type CompositeManifoldError{T} <: Exception A composite type to collect a set of errors that occured. Mainly used in conjunction with  ComponentManifoldError  to store a set of errors that occured. Fields errors  a  Vector  of  <:Exceptions . source"},{"id":168,"pagetitle":"Basic functions","title":"ManifoldsBase.ManifoldDomainError","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.ManifoldDomainError","content":" ManifoldsBase.ManifoldDomainError  ‚Äî  Type ManifoldDomainError{<:Exception} <: Exception An error to represent a nested (Domain) error on a manifold, for example if a point or tangent vector is invalid because its representation in some embedding is already invalid. source"},{"id":169,"pagetitle":"Basic functions","title":"ManifoldsBase.OutOfInjectivityRadiusError","ref":"/manifoldsbase/stable/functions/#ManifoldsBase.OutOfInjectivityRadiusError","content":" ManifoldsBase.OutOfInjectivityRadiusError  ‚Äî  Type OutOfInjectivityRadiusError An error thrown when a function (for example  log arithmic map or  inverse_retract ) is given arguments outside of its  injectivity_radius . source"},{"id":172,"pagetitle":"Manifolds","title":"Manifolds","ref":"/manifoldsbase/stable/manifolds/#Manifolds","content":" Manifolds While the interface  ManifoldsBase.jl  does not cover concrete manifolds, it provides a few helpers to build or create manifolds based on existing manifolds"},{"id":173,"pagetitle":"Manifolds","title":"A default manifold","ref":"/manifoldsbase/stable/manifolds/#A-default-manifold","content":" A default manifold DefaultManifold  is a simplified version of  Euclidean  and demonstrates a basic interface implementation. It can be used to perform simple tests. Since when using  Manifolds.jl  the  Euclidean  is available, the  DefaultManifold  itself is not exported."},{"id":174,"pagetitle":"Manifolds","title":"ManifoldsBase.DefaultManifold","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.DefaultManifold","content":" ManifoldsBase.DefaultManifold  ‚Äî  Type DefaultManifold <: AbstractManifold This default manifold illustrates the main features of the interface and provides a skeleton to build one's own manifold. It is a simplified/shortened variant of  Euclidean  from  Manifolds.jl . This manifold further illustrates how to type your manifold points and tangent vectors. Note that the interface does not require this, but it might be handy in debugging and educative situations to verify correctness of involved variables. Constructor DefaultManifold(n::Int...; field = ‚Ñù, parameter::Symbol = :field) Arguments: n : shape of array representing points on the manifold. field : field over which the manifold is defined. Either  ‚Ñù ,  ‚ÑÇ  or  ‚Ñç . parameter : whether a type parameter should be used to store  n . By default size is stored in a field. Value can either be  :field  or  :type . source"},{"id":175,"pagetitle":"Manifolds","title":"Embedded manifold","ref":"/manifoldsbase/stable/manifolds/#sec-embedded-manifold","content":" Embedded manifold The embedded manifold is a manifold  $\\mathcal M$  which is modelled  explicitly  specifying its embedding  $\\mathcal N$  in which the points and tangent vectors are represented. Most prominently  is_point  and  is_vector  of an embedded manifold are implemented to check whether the point is a valid point in the embedding. This can of course still be extended by further tests.  ManifoldsBase.jl  provides two possibilities of easily introducing this in order to dispatch some functions to the embedding."},{"id":176,"pagetitle":"Manifolds","title":"Implicit case: the IsEmbeddedManifold Trait","ref":"/manifoldsbase/stable/manifolds/#subsec-implicit-embedded","content":" Implicit case: the  IsEmbeddedManifold  Trait For the implicit case, your manifold has to be a subtype of the  AbstractDecoratorManifold . Adding a method to the  active_traits  function for a manifold that returns an  AbstractTrait IsEmbeddedManifold , makes that manifold an embedded manifold. You just have to also define  get_embedding  so that appropriate functions are passed on to that embedding. This is the implicit case, since the manifold type itself does not carry any information about the embedding, just the trait and the function definition do."},{"id":177,"pagetitle":"Manifolds","title":"Explicit case: the EmbeddedManifold","ref":"/manifoldsbase/stable/manifolds/#subsec-explicit-embedded","content":" Explicit case: the  EmbeddedManifold The  EmbeddedManifold  itself is an  AbstractDecoratorManifold  so it is a case of the implicit embedding itself, but internally stores both the original manifold and the embedding. They are also parameters of the type. This way, an additional embedding of one manifold in another can be modelled. That is, if the manifold is implemented using the implicit embedding approach from before but can also be implemented using a  different  embedding, then this method should be chosen, since you can dispatch functions that you want to implement in this embedding then on the type which explicitly has the manifold and its embedding as parameters. Hence this case should be used for any further embedding after the first or if the default implementation works without an embedding and the alternative needs one."},{"id":178,"pagetitle":"Manifolds","title":"ManifoldsBase.EmbeddedManifold","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.EmbeddedManifold","content":" ManifoldsBase.EmbeddedManifold  ‚Äî  Type EmbeddedManifold{ùîΩ, MT <: AbstractManifold, NT <: AbstractManifold} <: AbstractDecoratorManifold{ùîΩ} A type to represent an explicit embedding of a  AbstractManifold M  of type  MT  embedded into a manifold  N  of type  NT . By default, an embedded manifold is set to be embedded, but neither isometrically embedded nor a submanifold. Note This type is not required if a manifold  M  is to be embedded in one specific manifold  N .  One can then just implement  embed!  and  project! . You can further pass functions to the embedding, for example, when it is an isometric embedding, by using an  AbstractDecoratorManifold . Only for a second ‚Äìmaybe considered non-default‚Äì embedding, this type should be considered in order to dispatch on different embed and project methods for different embeddings  N . Fields manifold  the manifold that is an embedded manifold embedding  a second manifold, the first one is embedded into Constructor EmbeddedManifold(M, N) Generate the  EmbeddedManifold  of the  AbstractManifold M  into the  AbstractManifold N . source"},{"id":179,"pagetitle":"Manifolds","title":"ManifoldsBase.decorated_manifold","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.decorated_manifold-Tuple{EmbeddedManifold}","content":" ManifoldsBase.decorated_manifold  ‚Äî  Method decorated_manifold(M::EmbeddedManifold, d::Val{N} = Val(-1)) Return the manifold of  M  that is decorated with its embedding. For this specific type the internally stored enhanced manifold  M.manifold  is returned. See also  base_manifold , where this is used to (potentially) completely undecorate the manifold. source"},{"id":180,"pagetitle":"Manifolds","title":"ManifoldsBase.get_embedding","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.get_embedding-Tuple{EmbeddedManifold}","content":" ManifoldsBase.get_embedding  ‚Äî  Method get_embedding(M::EmbeddedManifold) Return the embedding  AbstractManifold N  of  M , if it exists. source"},{"id":181,"pagetitle":"Manifolds","title":"Metrics","ref":"/manifoldsbase/stable/manifolds/#Metrics","content":" Metrics Most metric-related functionality is currently defined in  Manifolds.jl  but a few basic types are defined here."},{"id":182,"pagetitle":"Manifolds","title":"ManifoldsBase.AbstractMetric","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.AbstractMetric","content":" ManifoldsBase.AbstractMetric  ‚Äî  Type AbstractMetric Abstract type for the pseudo-Riemannian metric tensor  $g$ , a family of smoothly varying inner products on the tangent space. See  inner . Functor (metric::Metric)(M::AbstractManifold)\n(metric::Metric)(M::MetricManifold) Generate the  MetricManifold  that wraps the manifold  M  with given  metric . This works for both a variable containing the metric as well as a subtype  T<:AbstractMetric , where a zero parameter constructor  T()  is availabe. If  M  is already a metric manifold, the inner manifold with the new  metric  is returned. source"},{"id":183,"pagetitle":"Manifolds","title":"ManifoldsBase.EuclideanMetric","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.EuclideanMetric","content":" ManifoldsBase.EuclideanMetric  ‚Äî  Type EuclideanMetric <: RiemannianMetric A general type for any manifold that employs the Euclidean Metric, for example the  Euclidean  manifold itself, or the  Sphere , where every tangent space (as a plane in the embedding) uses this metric (in the embedding). Since the metric is independent of the field type, this metric is also used for the Hermitian metrics, i.e. metrics that are analogous to the  EuclideanMetric  but where the field type of the manifold is  ‚ÑÇ . This metric is the default metric for example for the  Euclidean  manifold. source"},{"id":184,"pagetitle":"Manifolds","title":"ManifoldsBase.RiemannianMetric","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.RiemannianMetric","content":" ManifoldsBase.RiemannianMetric  ‚Äî  Type RiemannianMetric <: AbstractMetric Abstract type for Riemannian metrics, a family of positive definite inner products. The positive definite property means that for  $X  ‚àà T_p \\mathcal M$ , the inner product  $g(X, X) > 0$  whenever  $X$  is not the zero vector. source"},{"id":185,"pagetitle":"Manifolds","title":"ManifoldsBase.change_metric!","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.change_metric!-Tuple{AbstractManifold, Any, ManifoldsBase.AbstractMetric, Any, Any}","content":" ManifoldsBase.change_metric!  ‚Äî  Method change_metric!(M::AbstractcManifold, Y, G2::AbstractMetric, p, X) Compute the  change_metric  in place of  Y . source"},{"id":186,"pagetitle":"Manifolds","title":"ManifoldsBase.change_metric","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.change_metric-Tuple{AbstractManifold, ManifoldsBase.AbstractMetric, Any, Any}","content":" ManifoldsBase.change_metric  ‚Äî  Method change_metric(M::AbstractcManifold, G2::AbstractMetric, p, X) On the  AbstractManifold M  with implicitly given metric  $g_1$  and a second  AbstractMetric $g_2$  this function performs a change of metric in the sense that it returns the tangent vector  $Z=BX$  such that the linear map  $B$  fulfills \\[g_2(Y_1,Y_2) = g_1(BY_1,BY_2) \\quad \\text{for all } Y_1, Y_2 ‚àà T_p\\mathcal M.\\] source"},{"id":187,"pagetitle":"Manifolds","title":"ManifoldsBase.change_representer!","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.change_representer!-Tuple{AbstractManifold, Any, ManifoldsBase.AbstractMetric, Any, Any}","content":" ManifoldsBase.change_representer!  ‚Äî  Method change_representer!(M::AbstractcManifold, Y, G2::AbstractMetric, p, X) Compute the  change_metric  in place of  Y . source"},{"id":188,"pagetitle":"Manifolds","title":"ManifoldsBase.change_representer","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.change_representer-Tuple{AbstractManifold, ManifoldsBase.AbstractMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(M::AbstractManifold, G2::AbstractMetric, p, X) Convert the representer  X  of a linear function (in other words a cotangent vector at  p ) in the tangent space at  p  on the  AbstractManifold M  given with respect to the  AbstractMetric G2  into the representer with respect to the (implicit) metric of  M . In order to convert  X  into the representer with respect to the (implicitly given) metric  $g_1$  of  M , we have to find the conversion function  $c: T_p\\mathcal M \\to T_p\\mathcal M$  such that \\[    g_2(X,Y) = g_1(c(X),Y)\\] source"},{"id":189,"pagetitle":"Manifolds","title":"A manifold for validation","ref":"/manifoldsbase/stable/manifolds/#A-manifold-for-validation","content":" A manifold for validation ValidationManifold  is a simple decorator using the  AbstractDecoratorManifold  that ‚Äúdecorates‚Äù a manifold with tests that all involved points and vectors are valid for the wrapped manifold. For example involved input and output paratemers are checked before and after running a function, repectively. This is done by calling  is_point  or  is_vector  whenever applicable."},{"id":190,"pagetitle":"Manifolds","title":"ManifoldsBase.ValidationCotangentVector","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.ValidationCotangentVector","content":" ManifoldsBase.ValidationCotangentVector  ‚Äî  Type ValidationCotangentVector = ValidationFibreVector{CotangentSpaceType} Represent a cotangent vector to a point on an  ValidationManifold , i.e. on a manifold where data can be represented by arrays. The array is stored internally and semantically. This distinguished the value from  ValidationMPoint s vectors of other types. source"},{"id":191,"pagetitle":"Manifolds","title":"ManifoldsBase.ValidationFibreVector","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.ValidationFibreVector","content":" ManifoldsBase.ValidationFibreVector  ‚Äî  Type ValidationFibreVector{TType<:VectorSpaceType,V,P} <: AbstractFibreVector{TType} Represent a tangent vector to a point on an  ValidationManifold . The original vector of the manifold is stored internally. The corresponding base point of the fibre can be stored as well. The  TType  indicates the type of fibre, for example  TangentSpaceType  or  CotangentSpaceType . Fields value::V : the internally stored vector on the fibre point::P : the point the vector is associated with Constructor     ValidationFibreVector{TType}(value, point=nothing) source"},{"id":192,"pagetitle":"Manifolds","title":"ManifoldsBase.ValidationMPoint","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.ValidationMPoint","content":" ManifoldsBase.ValidationMPoint  ‚Äî  Type ValidationMPoint{P} <: AbstractManifoldPoint Represent a point on an  ValidationManifold . The point is stored internally. Fields value::P : the internally stored point on a manifold Constructor     ValidationMPoint(value) Create a point on the manifold with the value  value . source"},{"id":193,"pagetitle":"Manifolds","title":"ManifoldsBase.ValidationManifold","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.ValidationManifold","content":" ManifoldsBase.ValidationManifold  ‚Äî  Type ValidationManifold{ùîΩ,M<:AbstractManifold{ùîΩ}} <: AbstractDecoratorManifold{ùîΩ} A manifold to add tests to input and output values of functions defined in the interface. Additionally the points and tangent vectors can also be encapsulated, cf.  ValidationMPoint ,  ValidationTangentVector , and  ValidationCotangentVector . These types can be used to see where some data is assumed to be from, when working on manifolds where both points and tangent vectors are represented as (plain) arrays. Using the  ignore_contexts  keyword allows to specify a single  Symbol  or a vector of  Symbols  Of which contexts to ignore. Current contexts are :All : disable all checks :Point : checks for points :Vector : checks for vectors :Output : checks for output :Input : checks for input variables Using the  ignore_functions  keyword (dictionary) allows to disable/ignore certain checks within single functions for this manifold. The  key  of the dictionary has to be the  Function  to exclude something in. The  value  is either a single symbol or a vector of symbols with the same meaning as the  ignore_contexts  keyword, but limited to this function Examples exp => :All  disables  all  checks in the  exp  function exp => :Point  excludes point checks in the  exp  function exp => [:Point, :Vector]  excludes point and vector checks in the  exp  function This manifold is a decorator for a manifold, i.e. it decorates a  AbstractManifold M  with types points, vectors, and covectors. Fields manifold::M : The manifold to be decorated mode::Symbol : The mode to be used for error handling, either  :error  or  :warn ignore_contexts::AbstractVector{Symbol} : store contexts to be ignored of validation. ignore_functions::Dict{<:Function,<:Union{Symbol,<:AbstractVector{Symbol}} : store contexts to be ignored with in a function or its mutating variant. Constructors ValidationManifold(M::AbstractManifold; kwargs...) Generate the Validation manifold ValidationManifold(M::AbstractManifold, V::ValidationManifold; kwargs...) Generate the Validation manifold for  M  with the default values of  V . Keyword arguments error::Symbol=:error : specify how errors in the validation should be reported. this is passed to  is_point  and  is_vector  as the  error  keyword argument. Available values are  :error ,  :warn ,  :info , and  :none . Every other value is treated as  :none . store_base_point::Bool=false : specify whether or not to store the point  p  a tangent or cotangent vector is associated with. This can be useful for debugging purposes. ignore_contexts = Vector{Symbol}()  a vector to indicate which validation contexts should not be performed. ignore_functions=Dict{Function,Union{Symbol,Vector{Symbol}}}()  a dictionary to disable certain contexts within functions. The key here is the non-mutating function variant (if it exists). The contexts are thre same as in  ignore_contexts . source"},{"id":194,"pagetitle":"Manifolds","title":"ManifoldsBase.ValidationTangentVector","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.ValidationTangentVector","content":" ManifoldsBase.ValidationTangentVector  ‚Äî  Type ValidationTangentVector = ValidationFibreVector{TangentSpaceType} Represent a tangent vector to a point on an  ValidationManifold , i.e. on a manifold where data can be represented by arrays. The array is stored internally and semantically. This distinguished the value from  ValidationMPoint s vectors of other types. source"},{"id":195,"pagetitle":"Manifolds","title":"ManifoldsBase._msg","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase._msg-Tuple{ValidationManifold, Any}","content":" ManifoldsBase._msg  ‚Äî  Method _msg(str; error=:None, within::Union{Nothing,<:Function} = nothing,\ncontext::Union{NTuple{N,Symbol} where N} = NTuple{0,Symbol}()) issue a message  str  according to the mode  mode  (as  @error ,  @warn ,  @info ). source"},{"id":196,"pagetitle":"Manifolds","title":"ManifoldsBase._vMc","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase._vMc","content":" ManifoldsBase._vMc  ‚Äî  Function _vMc(M::ValidationManifold, f::Function, context::Symbol)\n_vMc(M::ValidationManifold, f::Function, context::NTuple{N,Symbol}) where {N} Return whether a check should be performed within  f  and the  context ( s ) provided. This function returns false and hence indicates not to check, when (one of the)  context ( s ) is in the ignore list for  f  within  ignore_functions (one of the)  context ( s ) is in the  ignore_contexts  list Otherwise the test is active. !!! Note    This function is internal and used very often, co it has a very short name;      _vMc  stands for \" ValidationManifold  check\". source"},{"id":197,"pagetitle":"Manifolds","title":"ManifoldsBase.internal_value","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.internal_value-Tuple{Any}","content":" ManifoldsBase.internal_value  ‚Äî  Method internal_value(p) Return the internal value of an  ValidationMPoint ,  ValidationTangentVector , or  ValidationCotangentVector  if the value  p  is encapsulated as such. Return  p  if it is already an a (plain) value on a manifold. source"},{"id":198,"pagetitle":"Manifolds","title":"ManifoldsBase.is_point","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.is_point-Tuple{ValidationManifold, Any}","content":" ManifoldsBase.is_point  ‚Äî  Method is_point(M::ValidationManifold, p; kwargs...) Perform  is_point  on a  ValidationManifold , where two additional keywords can be used within=nothing  to specify a function from within which this call was issued context::NTuple{N,Symbol} where N=()  to specify one or more contexts, this call was issued in. The context  :Point  is added before checking whether the test should be performed all other keywords are passed on. source"},{"id":199,"pagetitle":"Manifolds","title":"ManifoldsBase.is_vector","ref":"/manifoldsbase/stable/manifolds/#ManifoldsBase.is_vector","content":" ManifoldsBase.is_vector  ‚Äî  Function is_vector(M::ValidationManifold, p, X, cbp=true; kwargs...) perform  is_vector  on a  ValidationManifold , where two additional keywords can be used within=nothing  to specify a function from within which this call was issued context::NTuple{N,Symbol} where N=()  to specify one or more contexts, this call was issued in. The context  :Point  is added before checking whether the test should be performed all other keywords are passed on. source"},{"id":202,"pagetitle":"Meta-Manifolds","title":"Meta Manifolds","ref":"/manifoldsbase/stable/metamanifolds/#Meta-Manifolds","content":" Meta Manifolds While the interface does not provide concrete manifolds itself, it does provide several manifolds that can be build based on a given  AbstractManifold  instance."},{"id":203,"pagetitle":"Meta-Manifolds","title":"(Abstract) power manifold","ref":"/manifoldsbase/stable/metamanifolds/#sec-power-manifold","content":" (Abstract) power manifold A power manifold is constructed like higher dimensional vector spaces are formed from the real line, just that for every point  $p = (p_1,\\ldots,p_n) ‚àà \\mathcal M^n$  on the power manifold  $\\mathcal M^n$  the entries of  $p$  are points  $p_1,\\ldots,p_n ‚àà \\mathcal M$  on some manifold  $\\mathcal M$ . Note that  $n$  can also be replaced by multiple values, such that  $p$  is not a vector but a matrix or a multi-index array of points."},{"id":204,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.AbstractPowerManifold","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.AbstractPowerManifold","content":" ManifoldsBase.AbstractPowerManifold  ‚Äî  Type AbstractPowerManifold{ùîΩ,M,TPR} <: AbstractManifold{ùîΩ} An abstract  AbstractManifold  to represent manifolds that are build as powers of another  AbstractManifold M  with representation type  TPR , a subtype of  AbstractPowerRepresentation . source"},{"id":205,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.AbstractPowerRepresentation","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.AbstractPowerRepresentation","content":" ManifoldsBase.AbstractPowerRepresentation  ‚Äî  Type AbstractPowerRepresentation An abstract representation type of points and tangent vectors on a power manifold. source"},{"id":206,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.NestedPowerRepresentation","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.NestedPowerRepresentation","content":" ManifoldsBase.NestedPowerRepresentation  ‚Äî  Type NestedPowerRepresentation Representation of points and tangent vectors on a power manifold using arrays of size equal to  TSize  of a  PowerManifold . Each element of such array stores a single point or tangent vector. For modifying operations, each element of the outer array is modified in-place, differently than in  NestedReplacingPowerRepresentation . source"},{"id":207,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.NestedReplacingPowerRepresentation","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.NestedReplacingPowerRepresentation","content":" ManifoldsBase.NestedReplacingPowerRepresentation  ‚Äî  Type NestedReplacingPowerRepresentation Representation of points and tangent vectors on a power manifold using arrays of size equal to  TSize  of a  PowerManifold . Each element of such array stores a single point or tangent vector. For modifying operations, each element of the outer array is replaced using non-modifying operations, differently than for  NestedReplacingPowerRepresentation . source"},{"id":208,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.PowerBasisData","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.PowerBasisData","content":" ManifoldsBase.PowerBasisData  ‚Äî  Type PowerBasisData{TB<:AbstractArray} Data storage for an array of basis data. source"},{"id":209,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.PowerManifold","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.PowerManifold","content":" ManifoldsBase.PowerManifold  ‚Äî  Type PowerManifold{ùîΩ,TM<:AbstractManifold,TSize,TPR<:AbstractPowerRepresentation} <: AbstractPowerManifold{ùîΩ,TM} The power manifold  $\\mathcal M^{n_1√ó n_2 √ó ‚Ä¶ √ó n_d}$  with power geometry.   TSize  defines the number of elements along each axis, either statically using   TypeParameter  or storing it in a field. For example, a manifold-valued time series would be represented by a power manifold with  $d$  equal to 1 and  $n_1$  equal to the number of samples. A manifold-valued image (for example in diffusion tensor imaging) would be represented by a two-axis power manifold ( $d=2$ ) with  $n_1$  and  $n_2$  equal to width and height of the image. While the size of the manifold is static, points on the power manifold would not be represented by statically-sized arrays. Constructor PowerManifold(M::PowerManifold, N_1, N_2, ..., N_d; parameter::Symbol=:field)\nPowerManifold(M::AbstractManifold, NestedPowerRepresentation(), N_1, N_2, ..., N_d; parameter::Symbol=:field)\nM^(N_1, N_2, ..., N_d) Generate the power manifold  $M^{N_1 √ó N_2 √ó ‚Ä¶ √ó N_d}$ . By default, a  PowerManifold  is expanded further, i.e. for  M=PowerManifold(N, 3) PowerManifold(M, 2)  is equivalent to  PowerManifold(N, 3, 2) . Points are then 3√ó2 matrices of points on  N . Providing a  NestedPowerRepresentation  as the second argument to the constructor can be used to nest manifold, i.e.  PowerManifold(M, NestedPowerRepresentation(), 2)  represents vectors of length 2 whose elements are vectors of length 3 of points on N in a nested array representation. The third signature  M^(...)  is equivalent to the first one, and hence either yields a combination of power manifolds to  one  larger power manifold, or a power manifold with the default representation. Since there is no default  AbstractPowerRepresentation  within this interface, the  ^  operator is only available for  PowerManifold s and concatenates dimensions. parameter : whether a type parameter should be used to store  n . By default size is stored in a field. Value can either be  :field  or  :type . source"},{"id":210,"pagetitle":"Meta-Manifolds","title":"Base.copyto!","ref":"/manifoldsbase/stable/metamanifolds/#Base.copyto!-Tuple{AbstractPowerManifold{ùîΩ, <:AbstractManifold{ùîΩ}, NestedPowerRepresentation} where ùîΩ, Any, Any, Any}","content":" Base.copyto!  ‚Äî  Method copyto!(M::PowerManifoldNested, Y, p, X) Copy the values elementwise, i.e. call  copyto!(M.manifold, B, a, A)  for all elements  A ,  a  and  B  of  X ,  p , and  Y , respectively. source"},{"id":211,"pagetitle":"Meta-Manifolds","title":"Base.copyto!","ref":"/manifoldsbase/stable/metamanifolds/#Base.copyto!-Tuple{AbstractPowerManifold{ùîΩ, <:AbstractManifold{ùîΩ}, NestedPowerRepresentation} where ùîΩ, Any, Any}","content":" Base.copyto!  ‚Äî  Method copyto!(M::PowerManifoldNested, q, p) Copy the values elementwise, i.e. call  copyto!(M.manifold, b, a)  for all elements  a  and  b  of  p  and  q , respectively. source"},{"id":212,"pagetitle":"Meta-Manifolds","title":"Base.exp","ref":"/manifoldsbase/stable/metamanifolds/#Base.exp-Tuple{AbstractPowerManifold, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::AbstractPowerManifold, p, X) Compute the exponential map from  p  in direction  X  on the  AbstractPowerManifold M , which can be computed using the base manifolds exponential map element-wise. source"},{"id":213,"pagetitle":"Meta-Manifolds","title":"Base.fill!","ref":"/manifoldsbase/stable/metamanifolds/#Base.fill!-Tuple{Any, Any, AbstractPowerManifold{ùîΩ, <:AbstractManifold{ùîΩ}, NestedReplacingPowerRepresentation} where ùîΩ}","content":" Base.fill!  ‚Äî  Method fill!(P, p, M::AbstractPowerManifold) Fill a point  P  on the  AbstractPowerManifold M , setting every entry to  p . Note while usually the manifold is the first argument in all functions in  ManifoldsBase.jl , we follow the signature of  fill! , where the power manifold serves are the size information. source"},{"id":214,"pagetitle":"Meta-Manifolds","title":"Base.fill","ref":"/manifoldsbase/stable/metamanifolds/#Base.fill-Tuple{Any, AbstractPowerManifold}","content":" Base.fill  ‚Äî  Method fill(p, M::AbstractPowerManifold) Create a point on the  AbstractPowerManifold M , where every entry is set to the point  p . Note while usually the manifold is a first argument in all functions in  ManifoldsBase.jl , we follow the signature of  fill , where the power manifold serves are the size information. source"},{"id":215,"pagetitle":"Meta-Manifolds","title":"Base.getindex","ref":"/manifoldsbase/stable/metamanifolds/#Base.getindex-Tuple{AbstractArray, AbstractPowerManifold, Vararg{Union{Colon, Integer, AbstractVector}}}","content":" Base.getindex  ‚Äî  Method getindex(p, M::AbstractPowerManifold, i::Union{Integer,Colon,AbstractVector}...)\np[M::AbstractPowerManifold, i...] Access the element(s) at index  [i...]  of a point  p  on an  AbstractPowerManifold M  by linear or multidimensional indexing. See also  Array Indexing  in Julia. source"},{"id":216,"pagetitle":"Meta-Manifolds","title":"Base.getindex","ref":"/manifoldsbase/stable/metamanifolds/#Base.getindex-Union{Tuple{ùîΩ}, Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType, <:AbstractPowerManifold}, Vararg{Union{Colon, Integer, AbstractVector}}}} where ùîΩ","content":" Base.getindex  ‚Äî  Method getindex(M::TangentSpace{ùîΩ, AbstractPowerManifold}, i...)\nTpM[i...] Access the  i th manifold component from an  AbstractPowerManifold s' tangent space  TpM . source"},{"id":217,"pagetitle":"Meta-Manifolds","title":"Base.log","ref":"/manifoldsbase/stable/metamanifolds/#Base.log-Tuple{AbstractPowerManifold, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::AbstractPowerManifold, p, q) Compute the logarithmic map from  p  to  q  on the  AbstractPowerManifold M , which can be computed using the base manifolds logarithmic map elementwise. source"},{"id":218,"pagetitle":"Meta-Manifolds","title":"Base.setindex!","ref":"/manifoldsbase/stable/metamanifolds/#Base.setindex!-Tuple{AbstractArray, Any, AbstractPowerManifold, Vararg{Union{Colon, Integer, AbstractVector}}}","content":" Base.setindex!  ‚Äî  Method setindex!(q, p, M::AbstractPowerManifold, i::Union{Integer,Colon,AbstractVector}...)\nq[M::AbstractPowerManifold, i...] = p Set the element(s) at index  [i...]  of a point  q  on an  AbstractPowerManifold M  by linear or multidimensional indexing to  q . See also  Array Indexing  in Julia. source"},{"id":219,"pagetitle":"Meta-Manifolds","title":"Base.view","ref":"/manifoldsbase/stable/metamanifolds/#Base.view-Tuple{AbstractArray, AbstractPowerManifold{ùîΩ, <:AbstractManifold{ùîΩ}, NestedPowerRepresentation} where ùîΩ, Vararg{Union{Colon, Integer, AbstractVector}}}","content":" Base.view  ‚Äî  Method view(p, M::PowerManifoldNested, i::Union{Integer,Colon,AbstractVector}...) Get the view of the element(s) at index  [i...]  of a point  p  on an  AbstractPowerManifold M  by linear or multidimensional indexing. source"},{"id":220,"pagetitle":"Meta-Manifolds","title":"LinearAlgebra.norm","ref":"/manifoldsbase/stable/metamanifolds/#LinearAlgebra.norm","content":" LinearAlgebra.norm  ‚Äî  Function norm(M::AbstractPowerManifold, p, X, r::Real=2) Compute the norm of  X  from the tangent space of  p  on an  AbstractPowerManifold M , i.e. from the element wise norms  r -norm is computed, where the default  r=2  yields the Frobenius norm is computed. source"},{"id":221,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.Weingarten","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.Weingarten-Tuple{AbstractPowerManifold, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Y = Weingarten(M::AbstractPowerManifold, p, X, V)\nWeingarten!(M::AbstractPowerManifold, Y, p, X, V) Since the metric decouples, also the computation of the Weingarten map  $\\mathcal W_p$  can be computed elementwise on the single elements of the  PowerManifold M . source"},{"id":222,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase._allocate_access_nested","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase._allocate_access_nested-Tuple{AbstractPowerManifold{ùîΩ, <:AbstractManifold{ùîΩ}, NestedPowerRepresentation} where ùîΩ, Any, Any}","content":" ManifoldsBase._allocate_access_nested  ‚Äî  Method _allocate_access_nested(M::PowerManifoldNested, y, i) Helper function for  allocate_result  on  PowerManifoldNested . In allocation  y  can be a number in which case  _access_nested  wouldn't work. source"},{"id":223,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase._parameter_symbol","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase._parameter_symbol-Tuple{PowerManifold}","content":" ManifoldsBase._parameter_symbol  ‚Äî  Method _parameter_symbol(M::PowerManifold) Return  :field  if size of  PowerManifold M  is stored in a field and  :type  if in a  TypeParameter . source"},{"id":224,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.change_metric","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.change_metric-Tuple{AbstractPowerManifold, ManifoldsBase.AbstractMetric, Any, Any}","content":" ManifoldsBase.change_metric  ‚Äî  Method change_metric(M::AbstractPowerManifold, ::AbstractMetric, p, X) Since the metric on a power manifold decouples, the change of metric can be done elementwise. source"},{"id":225,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.change_representer","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.change_representer-Tuple{AbstractPowerManifold, ManifoldsBase.AbstractMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(M::AbstractPowerManifold, ::AbstractMetric, p, X) Since the metric on a power manifold decouples, the change of a representer can be done elementwise source"},{"id":226,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.check_point","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.check_point-Tuple{AbstractPowerManifold, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::AbstractPowerManifold, p; kwargs...) Check whether  p  is a valid point on an  AbstractPowerManifold M , i.e. each element of  p  has to be a valid point on the base manifold. If  p  is not a point on  M  a  CompositeManifoldError  consisting of all error messages of the components, for which the tests fail is returned. The tolerance for the last test can be set using the  kwargs... . source"},{"id":227,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.check_power_size","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.check_power_size-Tuple{AbstractPowerManifold, Any}","content":" ManifoldsBase.check_power_size  ‚Äî  Method check_power_size(M, p)\ncheck_power_size(M, p, X) Check whether  p has the right size to represent points on M`` generically, i.e. just checking the overall sizes, not the individual ones per manifold. source"},{"id":228,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.check_vector","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.check_vector-Tuple{AbstractPowerManifold, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::AbstractPowerManifold, p, X; kwargs... ) Check whether  X  is a tangent vector to  p  an the  AbstractPowerManifold M , i.e. atfer  check_point (M, p) , and all projections to base manifolds must be respective tangent vectors. If  X  is not a tangent vector to  p  on  M  a  CompositeManifoldError  consisting of all error messages of the components, for which the tests fail is returned. The tolerance for the last test can be set using the  kwargs... . source"},{"id":229,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.default_inverse_retraction_method","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.default_inverse_retraction_method-Tuple{PowerManifold}","content":" ManifoldsBase.default_inverse_retraction_method  ‚Äî  Method default_inverse_retraction_method(M::PowerManifold) Use the default inverse retraction method of the internal  M.manifold  also in defaults of functions defined for the power manifold, meaning that this is used elementwise. source"},{"id":230,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.default_retraction_method","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.default_retraction_method-Tuple{PowerManifold}","content":" ManifoldsBase.default_retraction_method  ‚Äî  Method default_retraction_method(M::PowerManifold) Use the default retraction method of the internal  M.manifold  also in defaults of functions defined for the power manifold, meaning that this is used elementwise. source"},{"id":231,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.default_vector_transport_method","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.default_vector_transport_method-Tuple{PowerManifold}","content":" ManifoldsBase.default_vector_transport_method  ‚Äî  Method default_vector_transport_method(M::PowerManifold) Use the default vector transport method of the internal  M.manifold  also in defaults of functions defined for the power manifold, meaning that this is used elementwise. source"},{"id":232,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.distance","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.distance","content":" ManifoldsBase.distance  ‚Äî  Function distance(M::AbstractPowerManifold, p, q, r::Real=2)\ndistance(M::AbstractPowerManifold, p, q, m::AbstractInverseRetractionMethod=LogarithmicInverseRetraction(), r::Real=2) Compute the distance between  q  and  p  on an  AbstractPowerManifold . First, the componentwise distances are computed using the Riemannian distance function on  M.manifold . These can be approximated using the  norm  of an  AbstractInverseRetractionMethod m . This yields an array of distance values. Second, we compute the  r -norm on this array of distances. This is also the only place, there the  r  is used. source"},{"id":233,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.distance","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.distance-Tuple{AbstractPowerManifold, Any, Any, Real}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::AbstractPowerManifold, p, q, r::Real=2)\ndistance(M::AbstractPowerManifold, p, q, m::AbstractInverseRetractionMethod=LogarithmicInverseRetraction(), r::Real=2) Compute the distance between  q  and  p  on an  AbstractPowerManifold . First, the componentwise distances are computed using the Riemannian distance function on  M.manifold . These can be approximated using the  norm  of an  AbstractInverseRetractionMethod m . This yields an array of distance values. Second, we compute the  r -norm on this array of distances. This is also the only place, there the  r  is used. source"},{"id":234,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.get_component","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.get_component-Tuple{AbstractPowerManifold, Any, Vararg{Any}}","content":" ManifoldsBase.get_component  ‚Äî  Method get_component(M::AbstractPowerManifold, p, idx...) Get the component of a point  p  on an  AbstractPowerManifold M  at index  idx . source"},{"id":235,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.has_components","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.has_components-Tuple{AbstractPowerManifold}","content":" ManifoldsBase.has_components  ‚Äî  Method has_components(::AbstractPowerManifold) Return  true , since points on an  AbstractPowerManifold  consist of components. source"},{"id":236,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.injectivity_radius","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.injectivity_radius-Tuple{AbstractPowerManifold, Any}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::AbstractPowerManifold[, p]) the injectivity radius on an  AbstractPowerManifold  is for the global case equal to the one of its base manifold. For a given point  p  it's equal to the minimum of all radii in the array entries. source"},{"id":237,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.inner","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.inner-Tuple{AbstractPowerManifold, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::AbstractPowerManifold, p, X, Y) Compute the inner product of  X  and  Y  from the tangent space at  p  on an  AbstractPowerManifold M , i.e. for each arrays entry the tangent vector entries from  X  and  Y  are in the tangent space of the corresponding element from  p . The inner product is then the sum of the elementwise inner products. source"},{"id":238,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.inverse_retract","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.inverse_retract-Tuple{AbstractPowerManifold, Vararg{Any}}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::AbstractPowerManifold, p, q, m::AbstractInverseRetractionMethod) Compute the inverse retraction from  p  with respect to  q  on an  AbstractPowerManifold M  using an  AbstractInverseRetractionMethod . Then this method is performed elementwise, so the inverse retraction method has to be one that is available on the base  AbstractManifold . source"},{"id":239,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.is_flat","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.is_flat-Tuple{AbstractPowerManifold}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(M::AbstractPowerManifold) Return true if  AbstractPowerManifold  is flat. It is flat if and only if the wrapped manifold is flat. source"},{"id":240,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.manifold_dimension","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.manifold_dimension-Tuple{PowerManifold}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::PowerManifold) Returns the manifold-dimension of an  PowerManifold M $=\\mathcal N = (\\mathcal M)^{n_1,‚Ä¶,n_d}$ , i.e. with  $n=(n_1,‚Ä¶,n_d)$  the array size of the power manifold and  $d_{\\mathcal M}$  the dimension of the base manifold  $\\mathcal M$ , the manifold is of dimension \\[\\dim(\\mathcal N) = \\dim(\\mathcal M)\\prod_{i=1}^d n_i = n_1n_2‚ãÖ‚Ä¶‚ãÖ n_d \\dim(\\mathcal M).\\] source"},{"id":241,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.power_dimensions","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.power_dimensions-Tuple{PowerManifold}","content":" ManifoldsBase.power_dimensions  ‚Äî  Method power_dimensions(M::PowerManifold) return the power of  M , source"},{"id":242,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.project","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.project-Tuple{AbstractPowerManifold, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::AbstractPowerManifold, p, X) Project the point  X  onto the tangent space at  p  on the  AbstractPowerManifold M  by projecting all components. source"},{"id":243,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.project","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.project-Tuple{AbstractPowerManifold, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::AbstractPowerManifold, p) Project the point  p  from the embedding onto the  AbstractPowerManifold M  by projecting all components. source"},{"id":244,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.retract","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.retract-Tuple{AbstractPowerManifold, Vararg{Any}}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::AbstractPowerManifold, p, X, method::AbstractRetractionMethod) Compute the retraction from  p  with tangent vector  X  on an  AbstractPowerManifold M  using a  AbstractRetractionMethod . Then this method is performed elementwise, so the retraction method has to be one that is available on the base  AbstractManifold . source"},{"id":245,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.riemann_tensor","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.riemann_tensor-Tuple{AbstractPowerManifold, Vararg{Any, 4}}","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(M::AbstractPowerManifold, p, X, Y, Z) Compute the Riemann tensor at point from  p  with tangent vectors  X ,  Y  and  Z  on the  AbstractPowerManifold M . source"},{"id":246,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.sectional_curvature","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.sectional_curvature-Tuple{AbstractPowerManifold, Any, Any, Any}","content":" ManifoldsBase.sectional_curvature  ‚Äî  Method sectional_curvature(M::AbstractPowerManifold, p, X, Y) Compute the sectional curvature of a power manifold manifold  $\\mathcal M$  at a point  $p \\in \\mathcal M$  on two linearly independent tangent vectors at  $p$ . It may be 0 for  if projections of  X  and  Y  on subspaces corresponding to component manifolds are not linearly independent. source"},{"id":247,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.sectional_curvature_max","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.sectional_curvature_max-Tuple{AbstractPowerManifold}","content":" ManifoldsBase.sectional_curvature_max  ‚Äî  Method sectional_curvature_max(M::AbstractPowerManifold) Upper bound on sectional curvature of  AbstractPowerManifold M . It is the maximum of sectional curvature of the wrapped manifold and 0 in case there are two or more component manifolds, as the sectional curvature corresponding to the plane spanned by vectors  (X_1, 0, ... 0)  and  (0, X_2, 0, ..., 0)  is 0. source"},{"id":248,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.sectional_curvature_min","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.sectional_curvature_min-Tuple{AbstractPowerManifold}","content":" ManifoldsBase.sectional_curvature_min  ‚Äî  Method sectional_curvature_min(M::AbstractPowerManifold) Lower bound on sectional curvature of  AbstractPowerManifold M . It is the minimum of sectional curvature of the wrapped manifold and 0 in case there are two or more component manifolds, as the sectional curvature corresponding to the plane spanned by vectors  (X_1, 0, ... 0)  and  (0, X_2, 0, ..., 0)  is 0. source"},{"id":249,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.set_component!","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.set_component!-Tuple{AbstractPowerManifold, Any, Any, Vararg{Any}}","content":" ManifoldsBase.set_component!  ‚Äî  Method set_component!(M::AbstractPowerManifold, q, p, idx...) Set the component of a point  q  on an  AbstractPowerManifold M  at index  idx  to  p , which itself is a point on the  AbstractManifold  the power manifold is build on. source"},{"id":250,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.vector_transport_to","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.vector_transport_to-Tuple{AbstractPowerManifold, Any, Any, Any, AbstractVectorTransportMethod}","content":" ManifoldsBase.vector_transport_to  ‚Äî  Method vector_transport_to(M::AbstractPowerManifold, p, X, q, method::AbstractVectorTransportMethod) Compute the vector transport the tangent vector  X at  p  to  q  on the  PowerManifold M  using an  AbstractVectorTransportMethod m . This method is performed elementwise, i.e. the method  m  has to be implemented on the base manifold. source"},{"id":251,"pagetitle":"Meta-Manifolds","title":"Product Manifold","ref":"/manifoldsbase/stable/metamanifolds/#ProductManifold","content":" Product Manifold"},{"id":252,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.InverseProductRetraction","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.InverseProductRetraction","content":" ManifoldsBase.InverseProductRetraction  ‚Äî  Type InverseProductRetraction(retractions::AbstractInverseRetractionMethod...) Product inverse retraction of  inverse retractions . Works on  ProductManifold . source"},{"id":253,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.ProductBasisData","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.ProductBasisData","content":" ManifoldsBase.ProductBasisData  ‚Äî  Type ProductBasisData A typed tuple to store tuples of data of stored/precomputed bases for a  ProductManifold . source"},{"id":254,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.ProductManifold","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.ProductManifold","content":" ManifoldsBase.ProductManifold  ‚Äî  Type ProductManifold{ùîΩ,TM<:Tuple} <: AbstractManifold{ùîΩ} Product manifold  $M_1 √ó M_2 √ó ‚Ä¶  √ó M_n$  with product geometry. Constructor ProductManifold(M_1, M_2, ..., M_n) generates the product manifold  $M_1 √ó M_2 √ó ‚Ä¶ √ó M_n$ . Alternatively, the same manifold can be contructed using the  √ó  operator:  M_1 √ó M_2 √ó M_3 . source"},{"id":255,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.ProductMetric","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.ProductMetric","content":" ManifoldsBase.ProductMetric  ‚Äî  Type ProductMetric <: AbstractMetric A type to represent the product of metrics for a  ProductManifold . source"},{"id":256,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.ProductRetraction","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.ProductRetraction","content":" ManifoldsBase.ProductRetraction  ‚Äî  Type ProductRetraction(retractions::AbstractRetractionMethod...) Product retraction of  retractions . Works on  ProductManifold . source"},{"id":257,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.ProductVectorTransport","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.ProductVectorTransport","content":" ManifoldsBase.ProductVectorTransport  ‚Äî  Type ProductVectorTransport(methods::AbstractVectorTransportMethod...) Product vector transport type of  methods . Works on  ProductManifold . source"},{"id":258,"pagetitle":"Meta-Manifolds","title":"Base.exp","ref":"/manifoldsbase/stable/metamanifolds/#Base.exp-Tuple{ProductManifold, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::ProductManifold, p, X) compute the exponential map from  p  in the direction of  X  on the  ProductManifold M , which is the elementwise exponential map on the internal manifolds that build  M . source"},{"id":259,"pagetitle":"Meta-Manifolds","title":"Base.getindex","ref":"/manifoldsbase/stable/metamanifolds/#Base.getindex-Tuple{ProductManifold, Integer}","content":" Base.getindex  ‚Äî  Method getindex(M::ProductManifold, i)\nM[i] access the  i th manifold component from the  ProductManifold M . source"},{"id":260,"pagetitle":"Meta-Manifolds","title":"Base.getindex","ref":"/manifoldsbase/stable/metamanifolds/#Base.getindex-Union{Tuple{ùîΩ}, Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType, <:ProductManifold}, Integer}} where ùîΩ","content":" Base.getindex  ‚Äî  Method getindex(M::TangentSpace{ùîΩ,<:ProductManifold}, i::Integer)\nTpM[i] Access the  i th manifold component from a  ProductManifold s' tangent space  TpM . source"},{"id":261,"pagetitle":"Meta-Manifolds","title":"Base.log","ref":"/manifoldsbase/stable/metamanifolds/#Base.log-Tuple{ProductManifold, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::ProductManifold, p, q) Compute the logarithmic map from  p  to  q  on the  ProductManifold M , which can be computed using the logarithmic maps of the manifolds elementwise. source"},{"id":262,"pagetitle":"Meta-Manifolds","title":"LinearAlgebra.cross","ref":"/manifoldsbase/stable/metamanifolds/#LinearAlgebra.cross-Tuple{Vararg{AbstractInverseRetractionMethod}}","content":" LinearAlgebra.cross  ‚Äî  Method √ó(m, n)\ncross(m, n)\ncross(m1, m2, m3,...) Return the  InverseProductRetraction  For two or more  AbstractInverseRetractionMethod s, where for the case that one of them is a  InverseProductRetraction  itself, the other is either prepended (if  r  is a product) or appenden (if  s ) is. If both  InverseProductRetraction s, they are combined into one keeping the order. source"},{"id":263,"pagetitle":"Meta-Manifolds","title":"LinearAlgebra.cross","ref":"/manifoldsbase/stable/metamanifolds/#LinearAlgebra.cross-Tuple{Vararg{AbstractManifold}}","content":" LinearAlgebra.cross  ‚Äî  Method √ó(M, N)\ncross(M, N)\ncross(M1, M2, M3,...) Return the  ProductManifold  For two  AbstractManifold s  M  and  N , where for the case that one of them is a  ProductManifold  itself, the other is either prepended (if  N  is a product) or appenden (if  M ) is. If both are product manifold, they are combined into one product manifold, keeping the order. For the case that more than one is a product manifold of these is build with the same approach as above source"},{"id":264,"pagetitle":"Meta-Manifolds","title":"LinearAlgebra.cross","ref":"/manifoldsbase/stable/metamanifolds/#LinearAlgebra.cross-Tuple{Vararg{AbstractRetractionMethod}}","content":" LinearAlgebra.cross  ‚Äî  Method √ó(m, n)\ncross(m, n)\ncross(m1, m2, m3,...) Return the  ProductRetraction  For two or more  AbstractRetractionMethod s, where for the case that one of them is a  ProductRetraction  itself, the other is either prepended (if  m  is a product) or appenden (if  n ) is. If both  ProductRetraction s, they are combined into one keeping the order. source"},{"id":265,"pagetitle":"Meta-Manifolds","title":"LinearAlgebra.cross","ref":"/manifoldsbase/stable/metamanifolds/#LinearAlgebra.cross-Tuple{Vararg{AbstractVectorTransportMethod}}","content":" LinearAlgebra.cross  ‚Äî  Method √ó(m, n)\ncross(m, n)\ncross(m1, m2, m3,...) Return the  ProductVectorTransport  For two or more  AbstractVectorTransportMethod s, where for the case that one of them is a  ProductVectorTransport  itself, the other is either prepended (if  r  is a product) or appenden (if  s ) is. If both  ProductVectorTransport s, they are combined into one keeping the order. source"},{"id":266,"pagetitle":"Meta-Manifolds","title":"LinearAlgebra.norm","ref":"/manifoldsbase/stable/metamanifolds/#LinearAlgebra.norm","content":" LinearAlgebra.norm  ‚Äî  Function norm(M::ProductManifold, p, X, r::Real=2) Compute the ( r -)norm of  X  from the tangent space of  p  on the  ProductManifold , i.e. from the element wise norms the 2-norm is computed. source"},{"id":267,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.Weingarten","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.Weingarten-Tuple{ProductManifold, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Y = Weingarten(M::ProductManifold, p, X, V)\nWeingarten!(M::ProductManifold, Y, p, X, V) Since the metric decouples, also the computation of the Weingarten map  $\\mathcal W_p$  can be computed elementwise on the single elements of the  ProductManifold M . source"},{"id":268,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.change_metric","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.change_metric-Tuple{ProductManifold, ManifoldsBase.AbstractMetric, Any, Any}","content":" ManifoldsBase.change_metric  ‚Äî  Method change_metric(M::ProductManifold, ::AbstractMetric, p, X) Since the metric on a product manifold decouples, the change of metric can be done elementwise. source"},{"id":269,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.change_representer","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.change_representer-Tuple{ProductManifold, ManifoldsBase.AbstractMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(M::ProductManifold, ::AbstractMetric, p, X) Since the metric on a product manifold decouples, the change of a representer can be done elementwise source"},{"id":270,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.check_point","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.check_point-Tuple{ProductManifold, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::ProductManifold, p; kwargs...) Check whether  p  is a valid point on the  ProductManifold M . If  p  is not a point on  M  a  CompositeManifoldError .consisting of all error messages of the components, for which the tests fail is returned. The tolerance for the last test can be set using the  kwargs... . source"},{"id":271,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.check_size","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.check_size-Tuple{ProductManifold, Any}","content":" ManifoldsBase.check_size  ‚Äî  Method check_size(M::ProductManifold, p; kwargs...) Check whether  p  is of valid size on the  ProductManifold M . If  p  has components of wrong size a  CompositeManifoldError .consisting of all error messages of the components, for which the tests fail is returned. The tolerance for the last test can be set using the  kwargs... . source"},{"id":272,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.check_vector","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.check_vector-Tuple{ProductManifold, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::ProductManifold, p, X; kwargs... ) Check whether  X  is a tangent vector to  p  on the  ProductManifold M , i.e. all projections to base manifolds must be respective tangent vectors. If  X  is not a tangent vector to  p  on  M  a  CompositeManifoldError .consisting of all error messages of the components, for which the tests fail is returned. The tolerance for the last test can be set using the  kwargs... . source"},{"id":273,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.distance","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.distance","content":" ManifoldsBase.distance  ‚Äî  Function distance(M::ProductManifold, p, q, r::Real=2)\ndistance(M::ProductManifold, p, q, m::AbstractInverseRetractionMethod=LogarithmicInverseRetraction(), r::Real=2) Compute the distance between  q  and  p  on an  ProductManifold . First, the componentwise distances are computed. These can be approximated using the  norm  of an  AbstractInverseRetractionMethod m . Then, the  r -norm of the tuple of these elements is computed. source"},{"id":274,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.distance","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.distance","content":" ManifoldsBase.distance  ‚Äî  Function distance(M::ProductManifold, p, q, r::Real=2)\ndistance(M::ProductManifold, p, q, m::AbstractInverseRetractionMethod=LogarithmicInverseRetraction(), r::Real=2) Compute the distance between  q  and  p  on an  ProductManifold . First, the componentwise distances are computed. These can be approximated using the  norm  of an  AbstractInverseRetractionMethod m . Then, the  r -norm of the tuple of these elements is computed. source"},{"id":275,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.get_component","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.get_component-Tuple{ProductManifold, Any, Any}","content":" ManifoldsBase.get_component  ‚Äî  Method get_component(M::ProductManifold, p, i) Get the  i th component of a point  p  on a  ProductManifold M . source"},{"id":276,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.has_components","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.has_components-Tuple{ProductManifold}","content":" ManifoldsBase.has_components  ‚Äî  Method has_components(::ProductManifold) Return  true  since points on an  ProductManifold  consist of components. source"},{"id":277,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.injectivity_radius","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.injectivity_radius-Tuple{ProductManifold, Vararg{Any}}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::ProductManifold)\ninjectivity_radius(M::ProductManifold, x) Compute the injectivity radius on the  ProductManifold , which is the minimum of the factor manifolds. source"},{"id":278,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.inner","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.inner-Tuple{ProductManifold, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::ProductManifold, p, X, Y) compute the inner product of two tangent vectors  X ,  Y  from the tangent space at  p  on the  ProductManifold M , which is just the sum of the internal manifolds that build  M . source"},{"id":279,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.inverse_retract","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.inverse_retract-Tuple{ProductManifold, Any, Any, Any, AbstractInverseRetractionMethod}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::ProductManifold, p, q, m::AbstractInverseRetractionMethod) Compute the inverse retraction from  p  with respect to  q  on the  ProductManifold M  using an  AbstractInverseRetractionMethod , which is used on each manifold of the product. source"},{"id":280,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.inverse_retract","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.inverse_retract-Tuple{ProductManifold, Any, Any, Any, InverseProductRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::ProductManifold, p, q, m::InverseProductRetraction) Compute the inverse retraction from  p  with respect to  q  on the  ProductManifold M  using an  InverseProductRetraction , which by default encapsulates a inverse retraction for each manifold of the product. Then this method is performed elementwise, so the encapsulated inverse retraction methods have to be available per factor. source"},{"id":281,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.is_flat","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.is_flat-Tuple{ProductManifold}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::ProductManifold) Return true if and only if all component manifolds of  ProductManifold M  are flat. source"},{"id":282,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.manifold_dimension","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.manifold_dimension-Tuple{ProductManifold}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::ProductManifold) Return the manifold dimension of the  ProductManifold , which is the sum of the manifold dimensions the product is made of. source"},{"id":283,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.number_of_components","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.number_of_components-Union{Tuple{ProductManifold{ùîΩ, <:NTuple{N, Any}}}, Tuple{N}, Tuple{ùîΩ}} where {ùîΩ, N}","content":" ManifoldsBase.number_of_components  ‚Äî  Method number_of_components(M::ProductManifold{<:NTuple{N,Any}}) where {N} Calculate the number of manifolds multiplied in the given  ProductManifold M . source"},{"id":284,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.retract","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.retract-Tuple{ProductManifold, Any, Any, AbstractRetractionMethod}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::ProductManifold, p, X, m::AbstractRetractionMethod) Compute the retraction from  p  with tangent vector  X  on the  ProductManifold M  using the  AbstractRetractionMethod m  on every manifold. source"},{"id":285,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.retract","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.retract-Tuple{ProductManifold, Any, Any, ProductRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::ProductManifold, p, X, m::ProductRetraction) Compute the retraction from  p  with tangent vector  X  on the  ProductManifold M  using an  ProductRetraction , which by default encapsulates retractions of the base manifolds. Then this method is performed elementwise, so the encapsulated retractions method has to be one that is available on the manifolds. source"},{"id":286,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.riemann_tensor","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.riemann_tensor-Tuple{ProductManifold, Vararg{Any, 4}}","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(M::ProductManifold, p, X, Y, Z) Compute the Riemann tensor at point from  p  with tangent vectors  X ,  Y  and  Z  on the  ProductManifold M . source"},{"id":287,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.sectional_curvature","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.sectional_curvature-Tuple{ProductManifold, Any, Any, Any}","content":" ManifoldsBase.sectional_curvature  ‚Äî  Method sectional_curvature(M::ProductManifold, p, X, Y) Compute the sectional curvature of a manifold  $\\mathcal M$  at a point  $p \\in \\mathcal M$  on two linearly independent tangent vectors at  $p$ . It may be 0 for a product of non-flat manifolds if projections of  X  and  Y  on subspaces corresponding to component manifolds are not linearly independent. source"},{"id":288,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.sectional_curvature_max","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.sectional_curvature_max-Tuple{ProductManifold}","content":" ManifoldsBase.sectional_curvature_max  ‚Äî  Method sectional_curvature_max(M::ProductManifold) Upper bound on sectional curvature of  ProductManifold M . It is the maximum of sectional curvatures of component manifolds and 0 in case there are two or more component manifolds, as the sectional curvature corresponding to the plane spanned by vectors  (X_1, 0)  and  (0, X_2)  is 0. source"},{"id":289,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.sectional_curvature_min","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.sectional_curvature_min-Tuple{ProductManifold}","content":" ManifoldsBase.sectional_curvature_min  ‚Äî  Method sectional_curvature_min(M::ProductManifold) Lower bound on sectional curvature of  ProductManifold M . It is the minimum of sectional curvatures of component manifolds and 0 in case there are two or more component manifolds, as the sectional curvature corresponding to the plane spanned by vectors  (X_1, 0)  and  (0, X_2)  is 0. source"},{"id":290,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.select_from_tuple","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.select_from_tuple-Union{Tuple{P}, Tuple{N}, Tuple{NTuple{N, Any}, Val{P}}} where {N, P}","content":" ManifoldsBase.select_from_tuple  ‚Äî  Method select_from_tuple(t::NTuple{N, Any}, positions::Val{P}) Selects elements of tuple  t  at positions specified by the second argument. For example  select_from_tuple((\"a\", \"b\", \"c\"), Val((3, 1, 1)))  returns  (\"c\", \"a\", \"a\") . source"},{"id":291,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.set_component!","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.set_component!-Tuple{ProductManifold, Any, Any, Any}","content":" ManifoldsBase.set_component!  ‚Äî  Method set_component!(M::ProductManifold, q, p, i) Set the  i th component of a point  q  on a  ProductManifold M  to  p , where  p  is a point on the  AbstractManifold   this factor of the product manifold consists of. source"},{"id":292,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.submanifold","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.submanifold-Tuple{ProductManifold, Integer}","content":" ManifoldsBase.submanifold  ‚Äî  Method submanifold(M::ProductManifold, i::Integer) Extract the  i th factor of the product manifold  M . source"},{"id":293,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.submanifold","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.submanifold-Tuple{ProductManifold, Val}","content":" ManifoldsBase.submanifold  ‚Äî  Method submanifold(M::ProductManifold, i::Val)\nsubmanifold(M::ProductManifold, i::AbstractVector) Extract the factor of the product manifold  M  indicated by indices in  i . For example, for  i  equal to  Val((1, 3))  the product manifold constructed from the first and the third factor is returned. The version with  AbstractVector  is not type-stable, for better preformance use  Val . source"},{"id":294,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.submanifold_component","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.submanifold_component-Tuple","content":" ManifoldsBase.submanifold_component  ‚Äî  Method submanifold_component(M::AbstractManifold, p, i::Integer)\nsubmanifold_component(M::AbstractManifold, p, ::Val{i}) where {i}\nsubmanifold_component(p, i::Integer)\nsubmanifold_component(p, ::Val{i}) where {i} Project the product array  p  on  M  to its  i th component. A new array is returned. source"},{"id":295,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.submanifold_components","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.submanifold_components-Tuple","content":" ManifoldsBase.submanifold_components  ‚Äî  Method submanifold_components(M::AbstractManifold, p)\nsubmanifold_components(p) Get the projected components of  p  on the submanifolds of  M . The components are returned in a Tuple. source"},{"id":296,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.vector_transport_to","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.vector_transport_to-Tuple{ProductManifold, Any, Any, Any, AbstractVectorTransportMethod}","content":" ManifoldsBase.vector_transport_to  ‚Äî  Method vector_transport_to(M::ProductManifold, p, X, q, m::AbstractVectorTransportMethod) Compute the vector transport the tangent vector  X  at  p  to  q  on the  ProductManifold M  using an  AbstractVectorTransportMethod m  on each manifold. source"},{"id":297,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.vector_transport_to","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.vector_transport_to-Tuple{ProductManifold, Any, Any, Any, ProductVectorTransport}","content":" ManifoldsBase.vector_transport_to  ‚Äî  Method vector_transport_to(M::ProductManifold, p, X, q, m::ProductVectorTransport) Compute the vector transport the tangent vector  X  at  p  to  q  on the  ProductManifold M  using a  ProductVectorTransport m . source"},{"id":298,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.ziptuples","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.ziptuples-Union{Tuple{M}, Tuple{N}, Tuple{NTuple{N, Any}, NTuple{M, Any}}} where {N, M}","content":" ManifoldsBase.ziptuples  ‚Äî  Method ziptuples(a, b[, c[, d[, e]]]) Zips tuples  a ,  b , and remaining in a fast, type-stable way. If they have different lengths, the result is trimmed to the length of the shorter tuple. source"},{"id":299,"pagetitle":"Meta-Manifolds","title":"Fiber","ref":"/manifoldsbase/stable/metamanifolds/#Fiber","content":" Fiber"},{"id":300,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.Fiber","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.Fiber","content":" ManifoldsBase.Fiber  ‚Äî  Type Fiber{ùîΩ,TFiber<:FiberType,TM<:AbstractManifold,TX} <: AbstractManifold{ùîΩ} A fiber of a fiber bundle at a point  p  on the manifold. This fiber itself is also a  manifold . For vector fibers it's by default flat and hence isometric to the  Euclidean  manifold. Fields manifold     ‚Äì base space of the fiber bundle point        ‚Äì a point  $p$  from the base space; the fiber corresponds to the preimage                 by bundle projection  $\\pi^{-1}(\\{p\\})$ . Constructor Fiber(M::AbstractManifold, p, fiber_type::FiberType) A fiber of type  fiber_type  at point  p  from the manifold  manifold . source"},{"id":301,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.FiberType","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.FiberType","content":" ManifoldsBase.FiberType  ‚Äî  Type abstract type FiberType end An abstract type for fiber types that can be used within  Fiber . source"},{"id":302,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.VectorSpaceFiber","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.VectorSpaceFiber","content":" ManifoldsBase.VectorSpaceFiber  ‚Äî  Type VectorSpaceFiber{ùîΩ,M,TSpaceType} = Fiber{ùîΩ,TSpaceType,M}\n    where {ùîΩ,M<:AbstractManifold,TSpaceType<:VectorSpaceType} Alias for a  Fiber  when the fiber is a vector space. source"},{"id":303,"pagetitle":"Meta-Manifolds","title":"Tangent Space","ref":"/manifoldsbase/stable/metamanifolds/#Tangent-Space","content":" Tangent Space"},{"id":304,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.CotangentSpace","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.CotangentSpace","content":" ManifoldsBase.CotangentSpace  ‚Äî  Type CotangentSpace{ùîΩ,M} = Fiber{ùîΩ,CotangentSpaceType,M} where {ùîΩ,M<:AbstractManifold} A manifold for the Cotangent space  $T^*_p\\mathcal M$  at a point  $p\\in\\mathcal M$ . This is modelled as an alias for  VectorSpaceFiber  corresponding to  CotangentSpaceType . Constructor CotangentSpace(M::AbstractManifold, p) Return the manifold (vector space) representing the cotangent space  $T^*_p\\mathcal M$  at point  p ,  $p\\in\\mathcal M$ . source"},{"id":305,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.TangentSpace","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.TangentSpace","content":" ManifoldsBase.TangentSpace  ‚Äî  Type TangentSpace{ùîΩ,M} = Fiber{ùîΩ,TangentSpaceType,M} where {ùîΩ,M<:AbstractManifold} A manifold for the tangent space  $T_p\\mathcal M$  at a point  $p\\in\\mathcal M$ . This is modelled as an alias for  VectorSpaceFiber  corresponding to  TangentSpaceType . Constructor TangentSpace(M::AbstractManifold, p) Return the manifold (vector space) representing the tangent space  $T_p\\mathcal M$  at point  p ,  $p\\in\\mathcal M$ . source"},{"id":306,"pagetitle":"Meta-Manifolds","title":"Base.exp","ref":"/manifoldsbase/stable/metamanifolds/#Base.exp-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ, Any, Any}","content":" Base.exp  ‚Äî  Method exp(TpM::TangentSpace, X, V) Exponential map of tangent vectors  X  from  TpM  and a direction  V , which is also from the  TangentSpace TpM  since we identify the tangent space of  TpM  with  TpM . The exponential map then simplifies to the sum  X+V . source"},{"id":307,"pagetitle":"Meta-Manifolds","title":"Base.log","ref":"/manifoldsbase/stable/metamanifolds/#Base.log-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(TpM::TangentSpace, X, Y) Logarithmic map on the  TangentSpace TpM , calculated as the difference of tangent vectors  q  and  p  from  TpM . source"},{"id":308,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.Weingarten","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.Weingarten-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Y = Weingarten(TpM::TangentSpace, X, V, A)\nWeingarten!(TpM::TangentSpace, Y, p, X, V) Compute the Weingarten map  $\\mathcal W_X$  at  X  on the  TangentSpace TpM  with respect to the tangent vector  $V \\in T_p\\mathcal M$  and the normal vector  $A \\in N_p\\mathcal M$ . Since this a flat space by itself, the result is always the zero tangent vector. source"},{"id":309,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.base_point","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.base_point-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ}","content":" ManifoldsBase.base_point  ‚Äî  Method base_point(TpM::TangentSpace) Return the base point of the  TangentSpace . source"},{"id":310,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.distance","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.distance-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::TangentSpace, X, Y) Distance between vectors  X  and  Y  from the  TangentSpace TpM . It is calculated as the  norm  (induced by the metric on  TpM ) of their difference. source"},{"id":311,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.injectivity_radius","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.injectivity_radius-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(TpM::TangentSpace) Return the injectivity radius on the  TangentSpace TpM , which is  $‚àû$ . source"},{"id":312,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.inner","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.inner-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::TangentSpace, X, V, W) For any  $X ‚àà T_p\\mathcal M$  we identify the tangent space  $T_X(T_p\\mathcal M)$  with  $T_p\\mathcal M$  again. Hence an inner product of  $V,W$  is just the inner product of the tangent space itself.  $‚ü®V,W‚ü©_X = ‚ü®V,W‚ü©_p$ . source"},{"id":313,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.is_flat","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.is_flat-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::TangentSpace) The  TangentSpace  is a flat manifold, so this returns  true . source"},{"id":314,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.manifold_dimension","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.manifold_dimension-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(TpM::TangentSpace) Return the dimension of the  TangentSpace $T_p\\mathcal M$  at  $p‚àà\\mathcal M$ , which is the same as the dimension of the manifold  $\\mathcal M$ . source"},{"id":315,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.parallel_transport_to","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.parallel_transport_to-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(::TangentSpace, X, V, Y) Transport the tangent vector  $Z ‚àà T_X(T_p\\mathcal M)$  from  X  to  Y . Since we identify  $T_X(T_p\\mathcal M) = T_p\\mathcal M$  and the tangent space is a vector space, parallel transport simplifies to the identity, so this function yields  $V$  as a result. source"},{"id":316,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.project","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.project-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(TpM::TangentSpace, X, V) Project the vector  V  from the embedding of the tangent space  TpM  (identified with  $T_X(T_p\\mathcal M)$ ), that is project the vector  V  onto the tangent space at  TpM.point . source"},{"id":317,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.project","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.project-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(TpM::TangentSpace, X) Project the point  X  from embedding of the  TangentSpace TpM  onto  TpM . source"},{"id":318,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.zero_vector","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.zero_vector-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ, Vararg{Any}}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(TpM::TangentSpace, X) Zero tangent vector at point  X  from the  TangentSpace TpM , that is the zero tangent vector at point  TpM.point , since we identify the tangent space  $T_X(T_p\\mathcal M)$  with  $T_p\\mathcal M$ . source"},{"id":319,"pagetitle":"Meta-Manifolds","title":"ManifoldsBase.zero_vector","ref":"/manifoldsbase/stable/metamanifolds/#ManifoldsBase.zero_vector-Tuple{ManifoldsBase.Fiber{ùîΩ, TangentSpaceType} where ùîΩ}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(TpM::TangentSpace) Zero tangent vector in the  TangentSpace TpM , that is the zero tangent vector at point  TpM.point . source"},{"id":322,"pagetitle":"Numerical Verification","title":"Numerical Verification","ref":"/manifoldsbase/stable/numerical_verification/#Numerical-Verification","content":" Numerical Verification"},{"id":323,"pagetitle":"Numerical Verification","title":"ManifoldsBase.check_inverse_retraction","ref":"/manifoldsbase/stable/numerical_verification/#ManifoldsBase.check_inverse_retraction","content":" ManifoldsBase.check_inverse_retraction  ‚Äî  Function check_inverse_retraction(\n    M::AbstractManifold,\n    inverse_rectraction_method::AbstractInverseRetractionMethod,\n    p=rand(M),\n    X=rand(M; vector_at=p);\n    #\n    exactness_tol::Real = 1e-12,\n    io::Union{IO,Nothing} = nothing,\n    limits::Tuple = (-8.0, 0.0),\n    log_range::AbstractVector = range(limits[1], limits[2]; length=N),\n    N::Int = 101,\n    name::String = \"inverse retraction\",\n    plot::Bool = false,\n    second_order::Bool = true\n    slope_tol::Real = 0.1,\n    error::Symbol = :none,\n    window = nothing,\n) Check numerically wether the inverse retraction  inverse_retraction_method  is correct. This requires the  exp  and  norm  functions to be implemented for the  AbstractManifold M . This implements a method similar to [ Bou23 , Section 4.8 or Section 6.8]. Note that if the errors are below the given tolerance and the method is exact, no plot is generated, Keyword arguments exactness_tol :     if all errors are below this tolerance, the inverse retraction is considered to be exact io :                provide an  IO  to print the result to limits :            specify the limits in the  log_range , that is the exponent for the range log_range :         specify the range of points (in log scale) to sample the length of the tangent vector  X N :                 number of points to verify within the  log_range  default range  $[10^{-8},10^{0}]$ name :              name to display in the plot plot :              whether to plot the result (see  plot_slope ) The plot is in log-log-scale. This is returned and can then also be saved. second_order :      check whether the retraction is of second order. if set to  false , first order is checked. slope_tol :         tolerance for the slope (global) of the approximation error :             specify how to report errors:  :none ,  :info ,  :warn , or  :error  are available window :            specify window sizes within the  log_range  that are used for the slope estimation. the default is, to use all window sizes  2:N . source"},{"id":324,"pagetitle":"Numerical Verification","title":"ManifoldsBase.check_retraction","ref":"/manifoldsbase/stable/numerical_verification/#ManifoldsBase.check_retraction","content":" ManifoldsBase.check_retraction  ‚Äî  Function check_retraction(\n    M::AbstractManifold,\n    rectraction_method::AbstractRetractionMethod,\n    p=rand(M),\n    X=rand(M; vector_at=p);\n    #\n    exactness_tol::Real = 1e-12,\n    io::Union{IO,Nothing} = nothing,\n    limits::Tuple = (-8.0, 0.0),\n    log_range::AbstractVector = range(limits[1], limits[2]; length=N),\n    N::Int = 101,\n    name::String = \"retraction\",\n    plot::Bool = false,\n    second_order::Bool = true\n    slope_tol::Real = 0.1,\n    error::Symbol = :none,\n    window = nothing,\n) Check numerically wether the retraction  vector_transport_to  is correct, by selecting a set of points  $q_i = \\exp_p (t_i X)$  where  $t$  takes all values from  log_range , to then compare  parallel_transport_to  to the  vector_transport_method  applied to the vector  Y . This requires the  exp ,  parallel_transport_to  and  norm  function to be implemented for the  AbstractManifold M . This implements a method similar to [ Bou23 , Section 4.8 or Section 6.8]. Note that if the errors are below the given tolerance and the method is exact, no plot is generated, Keyword arguments exactness_tol :     if all errors are below this tolerance, the retraction is considered to be exact io :                provide an  IO  to print the result to limits :            specify the limits in the  log_range , that is the exponent for the range log_range :         specify the range of points (in log scale) to sample the length of the tangent vector  X N :                 number of points to verify within the  log_range  default range  $[10^{-8},10^{0}]$ name :              name to display in the plot plot :              whether to plot the result (if  Plots.jl  is loaded). The plot is in log-log-scale. This is returned and can then also be saved. second_order :      check whether the retraction is of second order. if set to  false , first order is checked. slope_tol :         tolerance for the slope (global) of the approximation error :             specify how to report errors:  :none ,  :info ,  :warn , or  :error  are available window :            specify window sizes within the  log_range  that are used for the slope estimation. the default is, to use all window sizes  2:N . source"},{"id":325,"pagetitle":"Numerical Verification","title":"ManifoldsBase.check_vector_transport","ref":"/manifoldsbase/stable/numerical_verification/#ManifoldsBase.check_vector_transport","content":" ManifoldsBase.check_vector_transport  ‚Äî  Function check_vector_transport(\n    M::AbstractManifold,\n    vector_transport_method::AbstractVectorTransportMethod,\n    p=rand(M),\n    X=rand(M; vector_at=p),\n    Y=rand(M; vector_at=p);\n    #\n    exactness_tol::Real = 1e-12,\n    io::Union{IO,Nothing} = nothing,\n    limits::Tuple = (-8.0, 0.0),\n    log_range::AbstractVector = range(limits[1], limits[2]; length=N),\n    N::Int = 101,\n    name::String = \"inverse retraction\",\n    plot::Bool = false,\n    second_order::Bool = true\n    slope_tol::Real = 0.1,\n    error::Symbol = :none,\n    window = nothing,\n) Check numerically wether the retraction  vector_transport_to  is correct, by selecting a set of points  $q_i = \\exp_p (t_i X)$  where  $t$  takes all values from  log_range , to then compare  parallel_transport_to  to the  vector_transport_method  applied to the vector  Y . This requires the  exp ,  parallel_transport_to  and  norm  function to be implemented for the  AbstractManifold M . This implements a method similar to [ Bou23 , Section 4.8 or Section 6.8]. Note that if the errors are below the given tolerance and the method is exact, no plot is generated, Keyword arguments exactness_tol :     if all errors are below this tolerance, the differential is considered to be exact io :                provide an  IO  to print the result to limits :            specify the limits in the  log_range , that is the exponent for the range log_range :         specify the range of points (in log scale) to sample the differential line N :                 number of points to verify within the  log_range  default range  $[10^{-8},10^{0}]$ name :              name to display in the plot plot :              whether to plot the result (if  Plots.jl  is loaded). The plot is in log-log-scale. This is returned and can then also be saved. second_order :      check whether the retraction is of second order. if set to  false , first order is checked. slope_tol :         tolerance for the slope (global) of the approximation error :             specify how to report errors:  :none ,  :info ,  :warn , or  :error  are available window :            specify window sizes within the  log_range  that are used for the slope estimation. the default is, to use all window sizes  2:N . source"},{"id":326,"pagetitle":"Numerical Verification","title":"Internal functions","ref":"/manifoldsbase/stable/numerical_verification/#Internal-functions","content":" Internal functions The following functions split the check into several parts, for example looking for the best fitting window and finding out the best slope, or plotting the slope."},{"id":327,"pagetitle":"Numerical Verification","title":"ManifoldsBase.find_best_slope_window","ref":"/manifoldsbase/stable/numerical_verification/#ManifoldsBase.find_best_slope_window","content":" ManifoldsBase.find_best_slope_window  ‚Äî  Function (a, b, i, j) = find_best_slope_window(X, Y, window=nothing; slope::Real=2.0, slope_tol::Real=0.1) Check data X,Y for the largest contiguous interval (window) with a regression line fitting ‚Äúbest‚Äù. Among all intervals with a slope within  slope_tol  to  slope  the longest one is taken. If no such interval exists, the one with the slope closest to  slope  is taken. If the window is set to  nothing  (default), all window sizes  2,...,length(X)  are checked. You can also specify a window size or an array of window sizes. For each window size, all its translates in the data is checked. For all these (shifted) windows the regression line is computed (with  a,b  in  a + t*b ) and the best line is computed. From the best line the following data is returned a ,  b  specifying the regression line  a + t*b i ,  j  determining the window, i.e the regression line stems from data  X[i], ..., X[j] Note This function has to be implemented using some statistics package. loading  Statistics.jl  provides a default implementation. source"},{"id":328,"pagetitle":"Numerical Verification","title":"ManifoldsBase.plot_slope","ref":"/manifoldsbase/stable/numerical_verification/#ManifoldsBase.plot_slope-Tuple{Any, Any}","content":" ManifoldsBase.plot_slope  ‚Äî  Method plot_slope(x, y;\n    slope=2,\n    line_base=0,\n    a=0,\n    b=2.0,\n    i=1,\n    j=length(x)\n) Plot the result from the verification functions on data  x,y  with two comparison lines line_base  + t slope   as the global slope(s) the plot could have a  +  b*t  on the interval [ x[i] ,  x[j] ] for some (best fitting) comparison slope Note This function has to be implemented for a certain plotting package. loading  Plots.jl  provides a default implementation. source"},{"id":331,"pagetitle":"Projections","title":"Projections","ref":"/manifoldsbase/stable/projections/#Projections","content":" Projections A manifold might be embedded in some space. Often this is implicitly assumed, for example the complex  Circle  is embedded in the complex plane. Let‚Äòs keep the circle in mind in the following as a simple example. For the general case of explicitly stating an embedding and/or to distinguish several, different embeddings, see  Embedded Manifolds  below. To make this a little more concrete, let‚Äòs assume we have a manifold  $\\mathcal M$  which is embedded in some manifold  $\\mathcal N$  and the image  $i(\\mathcal M)$  of the embedding function  $i$  is a closed set (with respect to the topology on  $\\mathcal N$ ). Then we can do two kinds of projections. To make this concrete in an example for the Circle  $\\mathcal M=\\mathcal C := \\{ p ‚àà ‚ÑÇ¬†| |p| = 1\\}$  the embedding can be chosen to be the manifold  $\\mathcal N = ‚ÑÇ$  and due to our representation of  $\\mathcal C$  as complex numbers already, we have  $i(p) = p$ , that is the identity as the embedding function. The first projection we can consider is for a given a point  $p‚àà\\mathcal N$  in the embedding we can look for the closest point on the manifold  $\\mathcal M$ , i.e. \\[  \\operatorname*{arg\\,min}_{q‚àà \\mathcal M}\\ d_{\\mathcal N}(i(q),p)\\] And this resulting  $q$  we call the projection of  $p$  onto the manifold  $\\mathcal M$ . The second projection we can look at is for a given a point  $p‚àà\\mathcal M$  and a vector in  $X‚àà T_{i(p)}\\mathcal N$  in the embedding, where we can similarly look for the closest tangent vector  $Y‚àà T_p\\mathcal M$ , which we have to embed itself before itself. Embedding a tangent vector is usually the same as using the pushforward  $\\mathrm{d}i_p$  of the embedding (at  $p$ ). We obtain \\[  \\operatorname*{arg\\,min}_{Y‚àà T_p\\mathcal M}\\ \\bigl\\lVert \\mathrm{d}i(p)[Y] - X \\bigr\\rVert_{i(p)}\\] And we call the resulting  $Y$  the projection of  $X$  onto the tangent space  $T_p\\mathcal M$  at  $p$ . Let‚Äòs look at the little more concrete example of the complex circle again. Here, the closest point of  $p ‚àà ‚ÑÇ$  is just the projection onto the circle, or in other words  $q = \\frac{p}{\\lvert p \\rvert}$ , as long as  $p\\neq 0$ . For  $p=0$  the projection is not defined. A tangent space  $T_p\\mathcal C$  in the embedding is the line through the origin that is orthogonal to a point  $p‚àà\\mathcal C$ . This can be better visualized by looking at  $p+T_p\\mathcal C$  which is actually the line tangent to  $p$  on the unit circle. Note that this shift does not change the resulting projection relative to the origin of the tangent space. Here the projection can be computed as the classical projection onto the line, i.e.   $Y = X - ‚ü®X,p‚ü©X$ . Both projections onto  $\\mathcal C$  and onto  $T_p\\mathcal C$  are illustrated in the following figure. The functions provided in this interface are the following."},{"id":332,"pagetitle":"Projections","title":"ManifoldsBase.project!","ref":"/manifoldsbase/stable/projections/#ManifoldsBase.project!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.project!  ‚Äî  Method project!(M::AbstractManifold, Y, p, X) Project ambient space representation of a vector  X  to a tangent vector at point  p  on the  AbstractManifold M . The result is saved in vector  Y . This method is only available for manifolds where implicitly an embedding or ambient space is given. Additionally,  project!  includes changing data representation, if applicable, i.e. if the tangents on  M  are not represented in the same way as points on the embedding, the representation is changed accordingly. This is the case for example for Lie groups, when tangent vectors are represented in the Lie algebra. after projection the change to the Lie algebra is perfomed, too. See also:  EmbeddedManifold ,  embed! source"},{"id":333,"pagetitle":"Projections","title":"ManifoldsBase.project!","ref":"/manifoldsbase/stable/projections/#ManifoldsBase.project!-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.project!  ‚Äî  Method project!(M::AbstractManifold, q, p) Project point  p  from the ambient space onto the  AbstractManifold M . The result is storedin  q . This method is only available for manifolds where implicitly an embedding or ambient space is given. Additionally, the projection includes changing data representation, if applicable, i.e. if the points on  M  are not represented in the same array data, the data is changed accordingly. See also:  EmbeddedManifold ,  embed! source"},{"id":334,"pagetitle":"Projections","title":"ManifoldsBase.project","ref":"/manifoldsbase/stable/projections/#ManifoldsBase.project-Tuple{AbstractManifold, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::AbstractManifold, p, X) Project ambient space representation of a vector  X  to a tangent vector at point  p  on the  AbstractManifold M . This method is only available for manifolds where implicitly an embedding or ambient space is given. Additionally,  project  includes changing data representation, if applicable, i.e. if the tangents on  M  are not represented in the same way as points on the embedding, the representation is changed accordingly. This is the case for example for Lie groups, when tangent vectors are represented in the Lie algebra. after projection the change to the Lie algebra is perfomed, too. See also:  EmbeddedManifold ,  embed source"},{"id":335,"pagetitle":"Projections","title":"ManifoldsBase.project","ref":"/manifoldsbase/stable/projections/#ManifoldsBase.project-Tuple{AbstractManifold, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::AbstractManifold, p) Project point  p  from the ambient space of the  AbstractManifold M  to  M . This method is only available for manifolds where implicitly an embedding or ambient space is given. Additionally, the projection includes changing data representation, if applicable, i.e. if the points on  M  are not represented in the same array data, the data is changed accordingly. See also:  EmbeddedManifold ,  embed source"},{"id":338,"pagetitle":"References","title":"Literature","ref":"/manifoldsbase/stable/references/#Literature","content":" Literature [AMS08] P.-A.¬†Absil, R.¬†Mahony and R.¬†Sepulchre.  Optimization Algorithms on Matrix Manifolds  (Princeton University Press, 2008), available online at  press.princeton.edu/chapters/absil/ . [Bou23] N.¬†Boumal.  An Introduction to Optimization on Smooth Manifolds . First¬†Edition ( Cambridge University Press, 2023 ). [EPS72] J.¬†Ehlers, F.¬†A.¬†Pirani and A.¬†Schild.  Republication of: The geometry of free fall and light propagation .  General¬†Relativity¬†and¬†Gravitation  44 , 1587‚Äì1609  (1972). [LP13] M.¬†Lorenzi and X.¬†Pennec.  Efficient Parallel Transport of Deformations in Time Series of Images: From Schild's to Pole Ladder .  Journal¬†of¬†Mathematical¬†Imaging¬†and¬†Vision  50 , 5‚Äì17  (2013),  arXiv:00870489 . [MF12] P.¬†Muralidharan and P.¬†T.¬†Fletcher.  Sasaki metrics for analysis of longitudinal data on manifolds . In:  2012 IEEE Conference on Computer Vision and Pattern Recognition  (2012). [Pen18] X.¬†Pennec.  Parallel Transport with Pole Ladder: a Third Order Scheme in Affine Connection Spaces which is Exact in Affine Symmetric Spaces.  arXiv¬†Preprint (2018),  arXiv:1805.11436 . [SI13] H.¬†Sato and T.¬†Iwai.  A new,  globally convergent Riemannian conjugate gradient method .  Optimization  64 , 1011‚Äì1031  (2013),  arXiv:1302.0125 ."},{"id":341,"pagetitle":"Retractions","title":"Retractions and inverse Retractions","ref":"/manifoldsbase/stable/retractions/#sec-retractions","content":" Retractions and inverse Retractions The  exponential and logarithmic map  might be too expensive to evaluate or not be available in a very stable numerical way on certain manifolds  $\\mathcal M$ . Retractions provide a possibly cheap, fast and stable alternative. A  retraction $\\operatorname{retr}_p: T_p\\mathcal M ‚Üí \\mathcal M$  is a smooth map that fulfils (for all  $p‚àà\\mathcal M$ ) that $\\operatorname{retr}_p(0) = p$ $D\\operatorname{retr}_p(0): T_p\\mathcal M \\to T_p\\mathcal M$  is the identity map, i.e.  $D\\operatorname{retr}_p(0)[X]=X$  holds for all  $X‚àà T_p\\mathcal M$ , where  $D\\operatorname{retr}_p$  denotes the differential of the retraction. A retraction  $\\operatorname{retr}_p$  can be interpreted as a first order approximation to the exponential map  $\\exp_p$ . The retraction is called of second order if for all  $X$  the curves  $c(t) = R_p(tX)$  have a zero acceleration at  $t=0$ , i.e.  $c''(0) = 0$ . The following figure compares the exponential map  exp (M, p, X)  on the  Circle (‚ÑÇ)  (or  Sphere (1)  embedded in  $‚Ñù^2$  with one possible retraction, the one based on projections. Note especially that  $\\operatorname{dist}(p,q)=\\lVert X\\rVert_p$  while this is not the case for the result  $\\operatorname{retr}_p(X) = q'$ . Similar to the  exp onential map the  retract ion might not be globally invertible, but locally it is. So locally one can define the inverse retraction  $\\operatorname{retr}_p^{-1}\\colon \\mathcal M \\to T_p\\mathcal M$ , which can be seen as a first order approximation to the  log arithmic map. Within the  ManifoldsBase.jl  interface the inverse retraction is called  inverse_retract . The general interface looks as follows."},{"id":342,"pagetitle":"Retractions","title":"ManifoldsBase.default_inverse_retraction_method","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.default_inverse_retraction_method-Tuple{AbstractManifold}","content":" ManifoldsBase.default_inverse_retraction_method  ‚Äî  Method default_inverse_retraction_method(M::AbstractManifold)\ndefault_inverse_retraction_method(M::AbstractManifold, ::Type{T}) where {T} The  AbstractInverseRetractionMethod  that is used when calling  inverse_retract  without specifying the inverse retraction method. By default, this is the  LogarithmicInverseRetraction . This method can also be specified more precisely with a point type  T , for the case that on a  M  there are two different representations of points, which provide different inverse retraction methods. source"},{"id":343,"pagetitle":"Retractions","title":"ManifoldsBase.default_retraction_method","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.default_retraction_method-Tuple{AbstractManifold}","content":" ManifoldsBase.default_retraction_method  ‚Äî  Method default_retraction_method(M::AbstractManifold)\ndefault_retraction_method(M::AbstractManifold, ::Type{T}) where {T} The  AbstractRetractionMethod  that is used when calling  retract  without specifying the retraction method. By default, this is the  ExponentialRetraction . This method can also be specified more precisely with a point type  T , for the case that on a  M  there are two different representations of points, which provide different retraction methods. source"},{"id":344,"pagetitle":"Retractions","title":"ManifoldsBase.inverse_retract","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.inverse_retract","content":" ManifoldsBase.inverse_retract  ‚Äî  Function inverse_retract(M::AbstractManifold, p, q)\ninverse_retract(M::AbstractManifold, p, q, method::AbstractInverseRetractionMethod Compute the inverse retraction, a cheaper, approximate version of the  log arithmic map), of points  p  and  q  on the  AbstractManifold M . Inverse retraction method can be specified by the last argument, defaulting to  default_inverse_retraction_method (M) . For available inverse retractions on certain manifolds see the documentation on the corresponding manifold. See also  retract . source"},{"id":345,"pagetitle":"Retractions","title":"ManifoldsBase.inverse_retract!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.inverse_retract!","content":" ManifoldsBase.inverse_retract!  ‚Äî  Function inverse_retract!(M::AbstractManifold, X, p, q[, method::AbstractInverseRetractionMethod]) Compute the inverse retraction, a cheaper, approximate version of the  log arithmic map), of points  p  and  q  on the  AbstractManifold M . Result is saved to  X . Inverse retraction method can be specified by the last argument, defaulting to  default_inverse_retraction_method (M) . See the documentation of respective manifolds for available methods. See also  retract! . source"},{"id":346,"pagetitle":"Retractions","title":"ManifoldsBase.retract","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract","content":" ManifoldsBase.retract  ‚Äî  Function retract(M::AbstractManifold, p, X, method::AbstractRetractionMethod=default_retraction_method(M, typeof(p)))\nretract!(M::AbstractManifold, q, p, X, method::AbstractRetractionMethod=default_retraction_method(M, typeof(p))) Compute a retraction, an approximate version of the  exp onential map, from  p  into direction  X , scaled by  t , on the  AbstractManifold M . This can be computed in-place of  q . A retraction  $\\operatorname{retr}_p: T_p\\mathcal M ‚Üí \\mathcal M$  is a smooth map that fulfils $\\operatorname{retr}_p(0) = p$ $D\\operatorname{retr}_p(0): T_p\\mathcal M ‚Üí T_p\\mathcal M$  is the identity map, i.e.  $D\\operatorname{retr}_p(0)[X]=X$  holds for all  $X‚àà T_p\\mathcal M$ , where  $D\\operatorname{retr}_p$  denotes the differential of the retraction The retraction is called of second order if for all  $X$  the curves  $c(t) = R_p(tX)$  have a zero acceleration at  $t=0$ , i.e.  $c''(0) = 0$ . Retraction method can be specified by the last argument, defaulting to  default_retraction_method (M) . For further available retractions see the documentation of respective manifolds. Locally, the retraction is invertible. For the inverse operation, see  inverse_retract . source"},{"id":347,"pagetitle":"Retractions","title":"ManifoldsBase.retract!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract!","content":" ManifoldsBase.retract!  ‚Äî  Function retract(M::AbstractManifold, p, X, method::AbstractRetractionMethod=default_retraction_method(M, typeof(p)))\nretract!(M::AbstractManifold, q, p, X, method::AbstractRetractionMethod=default_retraction_method(M, typeof(p))) Compute a retraction, an approximate version of the  exp onential map, from  p  into direction  X , scaled by  t , on the  AbstractManifold M . This can be computed in-place of  q . A retraction  $\\operatorname{retr}_p: T_p\\mathcal M ‚Üí \\mathcal M$  is a smooth map that fulfils $\\operatorname{retr}_p(0) = p$ $D\\operatorname{retr}_p(0): T_p\\mathcal M ‚Üí T_p\\mathcal M$  is the identity map, i.e.  $D\\operatorname{retr}_p(0)[X]=X$  holds for all  $X‚àà T_p\\mathcal M$ , where  $D\\operatorname{retr}_p$  denotes the differential of the retraction The retraction is called of second order if for all  $X$  the curves  $c(t) = R_p(tX)$  have a zero acceleration at  $t=0$ , i.e.  $c''(0) = 0$ . Retraction method can be specified by the last argument, defaulting to  default_retraction_method (M) . For further available retractions see the documentation of respective manifolds. Locally, the retraction is invertible. For the inverse operation, see  inverse_retract . source"},{"id":348,"pagetitle":"Retractions","title":"Types of Retractions","ref":"/manifoldsbase/stable/retractions/#Types-of-Retractions","content":" Types of Retractions To distinguish different types of retractions, the last argument of the retraction as well as its inverse specifies a type. The following ones are available."},{"id":349,"pagetitle":"Retractions","title":"ManifoldsBase.AbstractInverseRetractionMethod","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.AbstractInverseRetractionMethod","content":" ManifoldsBase.AbstractInverseRetractionMethod  ‚Äî  Type AbstractInverseRetractionMethod <: AbstractApproximationMethod Abstract type for methods for inverting a retraction (see  inverse_retract ). source"},{"id":350,"pagetitle":"Retractions","title":"ManifoldsBase.AbstractRetractionMethod","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.AbstractRetractionMethod","content":" ManifoldsBase.AbstractRetractionMethod  ‚Äî  Type AbstractRetractionMethod <: AbstractApproximationMethod Abstract type for methods for  retract ing a tangent vector to a manifold. source"},{"id":351,"pagetitle":"Retractions","title":"ManifoldsBase.ApproximateInverseRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.ApproximateInverseRetraction","content":" ManifoldsBase.ApproximateInverseRetraction  ‚Äî  Type ApproximateInverseRetraction <: AbstractInverseRetractionMethod An abstract type for representing approximate inverse retraction methods. source"},{"id":352,"pagetitle":"Retractions","title":"ManifoldsBase.ApproximateRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.ApproximateRetraction","content":" ManifoldsBase.ApproximateRetraction  ‚Äî  Type ApproximateRetraction <: AbstractRetractionMethod An abstract type for representing approximate retraction methods. source"},{"id":353,"pagetitle":"Retractions","title":"ManifoldsBase.CayleyInverseRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.CayleyInverseRetraction","content":" ManifoldsBase.CayleyInverseRetraction  ‚Äî  Type CayleyInverseRetraction <: AbstractInverseRetractionMethod A retraction based on the Cayley transform, which is realized by using the  PadeRetraction {1} . Technical Note Though you would call e.g.  inverse_retract (M, p, q, CayleyInverseRetraction()) , to implement an inverse caley retraction, define  inverse_retract_cayley! (M, X, p, q)  for your manifold  M . By default both these functions fall back to calling a  PadeInverseRetraction (1) . source"},{"id":354,"pagetitle":"Retractions","title":"ManifoldsBase.CayleyRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.CayleyRetraction","content":" ManifoldsBase.CayleyRetraction  ‚Äî  Type CayleyRetraction <: AbstractRetractionMethod A retraction based on the Cayley transform, which is realized by using the  PadeRetraction {1} . Technical Note Though you would call e.g.  retract (M, p, X, CayleyRetraction()) , to implement a caley retraction, define  retract_cayley! (M, q, p, X, t)  for your manifold  M . By default both these functions fall back to calling a  PadeRetraction (1) . source"},{"id":355,"pagetitle":"Retractions","title":"ManifoldsBase.EmbeddedInverseRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.EmbeddedInverseRetraction","content":" ManifoldsBase.EmbeddedInverseRetraction  ‚Äî  Type EmbeddedInverseRetraction{T<:AbstractInverseRetractionMethod} <: AbstractInverseRetractionMethod Compute an inverse retraction by using the inverse retraction of type  T  in the embedding and projecting the result Constructor EmbeddedInverseRetraction(r::AbstractInverseRetractionMethod) Generate the inverse retraction with inverse retraction  r  to use in the embedding. source"},{"id":356,"pagetitle":"Retractions","title":"ManifoldsBase.EmbeddedRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.EmbeddedRetraction","content":" ManifoldsBase.EmbeddedRetraction  ‚Äî  Type EmbeddedRetraction{T<:AbstractRetractionMethod} <: AbstractRetractionMethod Compute a retraction by using the retraction of type  T  in the embedding and projecting the result. Constructor EmbeddedRetraction(r::AbstractRetractionMethod) Generate the retraction with retraction  r  to use in the embedding. source"},{"id":357,"pagetitle":"Retractions","title":"ManifoldsBase.ExponentialRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.ExponentialRetraction","content":" ManifoldsBase.ExponentialRetraction  ‚Äî  Type ExponentialRetraction <: AbstractRetractionMethod Retraction using the exponential map. source"},{"id":358,"pagetitle":"Retractions","title":"ManifoldsBase.InverseRetractionWithKeywords","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.InverseRetractionWithKeywords","content":" ManifoldsBase.InverseRetractionWithKeywords  ‚Äî  Type InverseRetractionWithKeywords{R<:AbstractRetractionMethod,K} <: AbstractInverseRetractionMethod Since inverse retractions might have keywords, this type is a way to set them as an own type to be used as a specific inverse retraction. Another reason for this type is that we dispatch on the inverse retraction first and only the last layer would be implemented with keywords, so this way they can be passed down. Fields inverse_retraction  the inverse retraction that is decorated with keywords kwargs  the keyword arguments Note that you can nest this type. Then the most outer specification of a keyword is used. Constructor InverseRetractionWithKeywords(m::T; kwargs...) where {T <: AbstractInverseRetractionMethod} Specify the subtype  T <: AbstractInverseRetractionMethod  to have keywords  kwargs... . source"},{"id":359,"pagetitle":"Retractions","title":"ManifoldsBase.LogarithmicInverseRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.LogarithmicInverseRetraction","content":" ManifoldsBase.LogarithmicInverseRetraction  ‚Äî  Type LogarithmicInverseRetraction <: AbstractInverseRetractionMethod Inverse retraction using the  log arithmic map. source"},{"id":360,"pagetitle":"Retractions","title":"ManifoldsBase.NLSolveInverseRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.NLSolveInverseRetraction","content":" ManifoldsBase.NLSolveInverseRetraction  ‚Äî  Type NLSolveInverseRetraction{T<:AbstractRetractionMethod,TV,TK} <:\n    ApproximateInverseRetraction An inverse retraction method for approximating the inverse of a retraction using  NLsolve . Constructor NLSolveInverseRetraction(\n    method::AbstractRetractionMethod[, X0];\n    project_tangent=false,\n    project_point=false,\n    nlsolve_kwargs...,\n) Constructs an approximate inverse retraction for the retraction  method  with initial guess  X0 , defaulting to the zero vector. If  project_tangent  is  true , then the tangent vector is projected before the retraction using  project . If  project_point  is  true , then the resulting point is projected after the retraction.  nlsolve_kwargs  are keyword arguments passed to  NLsolve.nlsolve . source"},{"id":361,"pagetitle":"Retractions","title":"ManifoldsBase.ODEExponentialRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.ODEExponentialRetraction","content":" ManifoldsBase.ODEExponentialRetraction  ‚Äî  Type ODEExponentialRetraction{T<:AbstractRetractionMethod, B<:AbstractBasis} <: AbstractRetractionMethod Approximate the exponential map on the manifold by evaluating the ODE descripting the geodesic at 1, assuming the default connection of the given manifold by solving the ordinary differential equation \\[\\frac{d^2}{dt^2} p^k + Œì^k_{ij} \\frac{d}{dt} p_i \\frac{d}{dt} p_j = 0,\\] where  $Œì^k_{ij}$  are the Christoffel symbols of the second kind, and the Einstein summation convention is assumed. Constructor ODEExponentialRetraction(\n    r::AbstractRetractionMethod,\n    b::AbstractBasis=DefaultOrthogonalBasis(),\n) Generate the retraction with a retraction to use internally (for some approaches) and a basis for the tangent space(s). source"},{"id":362,"pagetitle":"Retractions","title":"ManifoldsBase.PadeInverseRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.PadeInverseRetraction","content":" ManifoldsBase.PadeInverseRetraction  ‚Äî  Type PadeInverseRetraction{m} <: AbstractInverseRetractionMethod An inverse retraction based on the Pad√© approximation of order  $m$  for the retraction. Technical Note Though you would call e.g.  inverse_retract (M, p, q, PadeInverseRetraction(m)) , to implement an inverse Pad√© retraction, define  inverse_retract_pade! (M, X, p, q, m)  for your manifold  M . source"},{"id":363,"pagetitle":"Retractions","title":"ManifoldsBase.PadeRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.PadeRetraction","content":" ManifoldsBase.PadeRetraction  ‚Äî  Type PadeRetraction{m} <: AbstractRetractionMethod A retraction based on the Pad√© approximation of order  $m$ Constructor PadeRetraction(m::Int) Technical Note Though you would call e.g.  retract (M, p, X, PadeRetraction(m)) , to implement a Pad√© retraction, define  retract_pade! (M, q, p, X, t, m)  for your manifold  M . source"},{"id":364,"pagetitle":"Retractions","title":"ManifoldsBase.PolarInverseRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.PolarInverseRetraction","content":" ManifoldsBase.PolarInverseRetraction  ‚Äî  Type PolarInverseRetraction <: AbstractInverseRetractionMethod Inverse retractions that are based on a singular value decomposition of the matrix / matrices for point and tangent vector on a  AbstractManifold Technical Note Though you would call e.g.  inverse_retract (M, p, q, PolarInverseRetraction()) , to implement an inverse polar retraction, define  inverse_retract_polar! (M, X, p, q)  for your manifold  M . source"},{"id":365,"pagetitle":"Retractions","title":"ManifoldsBase.PolarRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.PolarRetraction","content":" ManifoldsBase.PolarRetraction  ‚Äî  Type PolarRetraction <: AbstractRetractionMethod Retractions that are based on singular value decompositions of the matrix / matrices for point and tangent vectors. Technical Note Though you would call e.g.  retract (M, p, X, PolarRetraction()) , to implement a polar retraction, define  retract_polar! (M, q, p, X, t)  for your manifold  M . source"},{"id":366,"pagetitle":"Retractions","title":"ManifoldsBase.ProjectionInverseRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.ProjectionInverseRetraction","content":" ManifoldsBase.ProjectionInverseRetraction  ‚Äî  Type ProjectionInverseRetraction <: AbstractInverseRetractionMethod Inverse retractions that are based on a projection (or its inversion). Technical Note Though you would call e.g.  inverse_retract (M, p, q, ProjectionInverseRetraction()) , to implement an inverse projection retraction, define  inverse_retract_project! (M, X, p, q)  for your manifold  M . source"},{"id":367,"pagetitle":"Retractions","title":"ManifoldsBase.ProjectionRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.ProjectionRetraction","content":" ManifoldsBase.ProjectionRetraction  ‚Äî  Type ProjectionRetraction <: AbstractRetractionMethod Retractions that are based on projection and usually addition in the embedding. Technical Note Though you would call e.g.  retract (M, p, X, ProjectionRetraction()) , to implement a projection retraction, define  retract_project! (M, q, p, X, t)  for your manifold  M . source"},{"id":368,"pagetitle":"Retractions","title":"ManifoldsBase.QRInverseRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.QRInverseRetraction","content":" ManifoldsBase.QRInverseRetraction  ‚Äî  Type QRInverseRetraction <: AbstractInverseRetractionMethod Inverse retractions that are based on a QR decomposition of the matrix / matrices for point and tangent vector on a  AbstractManifold Technical Note Though you would call e.g.  inverse_retract (M, p, q, QRInverseRetraction()) , to implement an inverse QR retraction, define  inverse_retract_qr! (M, X, p, q)  for your manifold  M . source"},{"id":369,"pagetitle":"Retractions","title":"ManifoldsBase.QRRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.QRRetraction","content":" ManifoldsBase.QRRetraction  ‚Äî  Type QRRetraction <: AbstractRetractionMethod Retractions that are based on a QR decomposition of the matrix / matrices for point and tangent vector on a  AbstractManifold Technical Note Though you would call e.g.  retract (M, p, X, QRRetraction()) , to implement a QR retraction, define  retract_qr! (M, q, p, X, t)  for your manifold  M . source"},{"id":370,"pagetitle":"Retractions","title":"ManifoldsBase.RetractionWithKeywords","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.RetractionWithKeywords","content":" ManifoldsBase.RetractionWithKeywords  ‚Äî  Type RetractionWithKeywords{R<:AbstractRetractionMethod,K} <: AbstractRetractionMethod Since retractions might have keywords, this type is a way to set them as an own type to be used as a specific retraction. Another reason for this type is that we dispatch on the retraction first and only the last layer would be implemented with keywords, so this way they can be passed down. Fields retraction  the retraction that is decorated with keywords kwargs  the keyword arguments Note that you can nest this type. Then the most outer specification of a keyword is used. Constructor RetractionWithKeywords(m::T; kwargs...) where {T <: AbstractRetractionMethod} Specify the subtype  T <: AbstractRetractionMethod  to have keywords  kwargs... . source"},{"id":371,"pagetitle":"Retractions","title":"ManifoldsBase.SasakiRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.SasakiRetraction","content":" ManifoldsBase.SasakiRetraction  ‚Äî  Type struct SasakiRetraction <: AbstractRetractionMethod end Exponential map on  TangentBundle  computed via Euler integration as described in [ MF12 ]. The system of equations for  $\\gamma : ‚Ñù \\to T\\mathcal M$  such that  $Œ≥(1) = \\exp_{p,X}(X_M, X_F)$  and  $Œ≥(0)=(p, X)$  reads \\[\\dot{Œ≥}(t) = (\\dot{p}(t), \\dot{X}(t)) = (R(X(t), \\dot{X}(t))\\dot{p}(t), 0)\\] where  $R$  is the Riemann curvature tensor (see  riemann_tensor ). Constructor SasakiRetraction(L::Int) In this constructor  L  is the number of integration steps. source"},{"id":372,"pagetitle":"Retractions","title":"ManifoldsBase.SoftmaxInverseRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.SoftmaxInverseRetraction","content":" ManifoldsBase.SoftmaxInverseRetraction  ‚Äî  Type SoftmaxInverseRetraction <: AbstractInverseRetractionMethod Describes an inverse retraction that is based on the softmax function. Technical Note Though you would call e.g.  inverse_retract (M, p, q, SoftmaxInverseRetraction()) , to implement an inverse softmax retraction, define  inverse_retract_softmax! (M, X, p, q)  for your manifold  M . source"},{"id":373,"pagetitle":"Retractions","title":"ManifoldsBase.SoftmaxRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.SoftmaxRetraction","content":" ManifoldsBase.SoftmaxRetraction  ‚Äî  Type SoftmaxRetraction <: AbstractRetractionMethod Describes a retraction that is based on the softmax function. Technical Note Though you would call e.g.  retract (M, p, X, SoftmaxRetraction()) , to implement a softmax retraction, define  retract_softmax! (M, q, p, X, t)  for your manifold  M . source"},{"id":374,"pagetitle":"Retractions","title":"ManifoldsBase.StabilizedRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.StabilizedRetraction","content":" ManifoldsBase.StabilizedRetraction  ‚Äî  Type StabilizedRetraction <: AbstractRetractionMethod A retraction wraps another retraction and projects the resulting point onto the manifold for numerical stability. Constructor StabilizedRetraction(::AbstractRetractionMethod=ExponentialRetraction()) source"},{"id":375,"pagetitle":"Retractions","title":"ManifoldsBase.ShootingInverseRetraction","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.ShootingInverseRetraction","content":" ManifoldsBase.ShootingInverseRetraction  ‚Äî  Type ShootingInverseRetraction <: ApproximateInverseRetraction Approximating the inverse of a retraction using the shooting method. This implementation of the shooting method works by using another inverse retraction to form the first guess of the vector. This guess is updated by shooting the vector, guessing the vector pointing from the shooting result to the target point, and transporting this vector update back to the initial point on a discretized grid. This process is repeated until the norm of the vector update falls below a specified tolerance or the maximum number of iterations is reached. Fields retraction::AbstractRetractionMethod : The retraction whose inverse is approximated. initial_inverse_retraction::AbstractInverseRetractionMethod : The inverse retraction used   to form the initial guess of the vector. vector_transport::AbstractVectorTransportMethod : The vector transport used to transport   the initial guess of the vector. num_transport_points::Int : The number of discretization points used for vector   transport in the shooting method. 2 is the minimum number of points, including just the   endpoints. tolerance::Real : The tolerance for the shooting method. max_iterations::Int : The maximum number of iterations for the shooting method. source"},{"id":376,"pagetitle":"Retractions","title":"The functions on layer 3","ref":"/manifoldsbase/stable/retractions/#The-functions-on-layer-3","content":" The functions on layer 3 While you should always add your documentation to  retract  or  retract!  when implementing new manifolds, the actual implementation happens on the following functions on  layer III ."},{"id":377,"pagetitle":"Retractions","title":"ManifoldsBase.inverse_retract_cayley!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.inverse_retract_cayley!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.inverse_retract_cayley!  ‚Äî  Method inverse_retract_cayley!(M::AbstractManifold, X, p, q) Compute the in-place variant of the  CayleyInverseRetraction , which by default calls the first order [ PadeInverseRetraction ¬ß(@ref). source"},{"id":378,"pagetitle":"Retractions","title":"ManifoldsBase.inverse_retract_embedded!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.inverse_retract_embedded!-Tuple{AbstractManifold, Any, Any, Any, AbstractInverseRetractionMethod}","content":" ManifoldsBase.inverse_retract_embedded!  ‚Äî  Method inverse_retract_embedded!(M::AbstractManifold, X, p, q, m::AbstractInverseRetractionMethod) Compute the in-place variant of the  EmbeddedInverseRetraction  using the  AbstractInverseRetractionMethod m  in the embedding (see  get_embedding ) and projecting the result back. source"},{"id":379,"pagetitle":"Retractions","title":"ManifoldsBase.inverse_retract_nlsolve!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.inverse_retract_nlsolve!-Tuple{AbstractManifold, Any, Any, Any, NLSolveInverseRetraction}","content":" ManifoldsBase.inverse_retract_nlsolve!  ‚Äî  Method inverse_retract_nlsolve!(M::AbstractManifold, X, p, q, m::NLSolveInverseRetraction) Compute the in-place variant of the  NLSolveInverseRetraction m . source"},{"id":380,"pagetitle":"Retractions","title":"ManifoldsBase.inverse_retract_pade!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.inverse_retract_pade!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.inverse_retract_pade!  ‚Äî  Method inverse_retract_pade!(M::AbstractManifold, p, q, n) Compute the in-place variant of the  PadeInverseRetraction (n) , source"},{"id":381,"pagetitle":"Retractions","title":"ManifoldsBase.inverse_retract_polar!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.inverse_retract_polar!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.inverse_retract_polar!  ‚Äî  Method inverse_retract_polar!(M::AbstractManifold, X, p, q) Compute the in-place variant of the  PolarInverseRetraction . source"},{"id":382,"pagetitle":"Retractions","title":"ManifoldsBase.inverse_retract_project!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.inverse_retract_project!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.inverse_retract_project!  ‚Äî  Method inverse_retract_project!(M::AbstractManifold, X, p, q) Compute the in-place variant of the  ProjectionInverseRetraction . source"},{"id":383,"pagetitle":"Retractions","title":"ManifoldsBase.inverse_retract_qr!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.inverse_retract_qr!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.inverse_retract_qr!  ‚Äî  Method inverse_retract_qr!(M::AbstractManifold, X, p, q) Compute the in-place variant of the  QRInverseRetraction . source"},{"id":384,"pagetitle":"Retractions","title":"ManifoldsBase.inverse_retract_softmax!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.inverse_retract_softmax!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.inverse_retract_softmax!  ‚Äî  Method inverse_retract_softmax!(M::AbstractManifold, X, p, q) Compute the in-place variant of the  SoftmaxInverseRetraction . source"},{"id":385,"pagetitle":"Retractions","title":"ManifoldsBase.retract_cayley!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract_cayley!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.retract_cayley!  ‚Äî  Method retract_cayley!(M::AbstractManifold, q, p, X) Compute the in-place variant of the  CayleyRetraction , which by default falls back to calling the first order  PadeRetraction . source"},{"id":386,"pagetitle":"Retractions","title":"ManifoldsBase.retract_embedded!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract_embedded!-Tuple{AbstractManifold, Any, Any, Any, AbstractRetractionMethod}","content":" ManifoldsBase.retract_embedded!  ‚Äî  Method retract_embedded!(M::AbstractManifold, q, p, X, m::AbstractRetractionMethod) Compute the in-place variant of the  EmbeddedRetraction  using the  AbstractRetractionMethod m  in the embedding (see  get_embedding ) and projecting the result back. source"},{"id":387,"pagetitle":"Retractions","title":"ManifoldsBase.retract_embedded_fused!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract_embedded_fused!-Tuple{AbstractManifold, Any, Any, Any, Number, AbstractRetractionMethod}","content":" ManifoldsBase.retract_embedded_fused!  ‚Äî  Method retract_embedded_fused!(M::AbstractManifold, q, p, X, t::Number, m::AbstractRetractionMethod) Compute the scaled variant of  retract_embedded! . source"},{"id":388,"pagetitle":"Retractions","title":"ManifoldsBase.retract_exp_ode!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract_exp_ode!-Tuple{AbstractManifold, Any, Any, Any, AbstractRetractionMethod, ManifoldsBase.AbstractBasis}","content":" ManifoldsBase.retract_exp_ode!  ‚Äî  Method retract_exp_ode!(M::AbstractManifold, q, p, X, m::AbstractRetractionMethod, B::AbstractBasis) Compute the in-place variant of the  ODEExponentialRetraction (m, B) . source"},{"id":389,"pagetitle":"Retractions","title":"ManifoldsBase.retract_fused","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract_fused","content":" ManifoldsBase.retract_fused  ‚Äî  Function retract_fused(M::AbstractManifold, p, X, t::Number, method::AbstractRetractionMethod=default_retraction_method(M, typeof(p)))\nretract_fused!(M::AbstractManifold, q, p, X, t::Number, method::AbstractRetractionMethod=default_retraction_method(M, typeof(p))) A variant of  retract  that performs retraction on the vector  X  scaled by  t . This can be faster in some cases compared to multiplying  X  by  t , especially when performing this for multiple values of  t . This can be computed in-place of  q . By default, this falls back to calling  retract  with  t*X . Technical Note This fallback is happening on the in-place variant in  Layer 3 . Hence implementing this performant variant requires to implement the corresponding third layer fused function, like for example  retract_polar_fused! . The ‚Äúnon-fused‚Äù variant always also has to be implemented, but can then be just spefied to fallback to the fused variant. for example retract_polar!(M, q, p, X) = retract_polar_fused!(M, q, p, X, one(eltype(p))) source"},{"id":390,"pagetitle":"Retractions","title":"ManifoldsBase.retract_fused!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract_fused!","content":" ManifoldsBase.retract_fused!  ‚Äî  Function retract_fused(M::AbstractManifold, p, X, t::Number, method::AbstractRetractionMethod=default_retraction_method(M, typeof(p)))\nretract_fused!(M::AbstractManifold, q, p, X, t::Number, method::AbstractRetractionMethod=default_retraction_method(M, typeof(p))) A variant of  retract  that performs retraction on the vector  X  scaled by  t . This can be faster in some cases compared to multiplying  X  by  t , especially when performing this for multiple values of  t . This can be computed in-place of  q . By default, this falls back to calling  retract  with  t*X . Technical Note This fallback is happening on the in-place variant in  Layer 3 . Hence implementing this performant variant requires to implement the corresponding third layer fused function, like for example  retract_polar_fused! . The ‚Äúnon-fused‚Äù variant always also has to be implemented, but can then be just spefied to fallback to the fused variant. for example retract_polar!(M, q, p, X) = retract_polar_fused!(M, q, p, X, one(eltype(p))) source"},{"id":391,"pagetitle":"Retractions","title":"ManifoldsBase.retract_pade!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract_pade!-Tuple{AbstractManifold, Any, Any, Any, PadeRetraction}","content":" ManifoldsBase.retract_pade!  ‚Äî  Method retract_pade!(M::AbstractManifold, q, p, X, m::PadeRetraction) Compute the in-place variant of the  PadeRetraction m . source"},{"id":392,"pagetitle":"Retractions","title":"ManifoldsBase.retract_polar!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract_polar!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.retract_polar!  ‚Äî  Method retract_polar!(M::AbstractManifold, q, p, X) Compute the in-place variant of the  PolarRetraction . source"},{"id":393,"pagetitle":"Retractions","title":"ManifoldsBase.retract_project!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract_project!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.retract_project!  ‚Äî  Method retract_project!(M::AbstractManifold, q, p, X) Compute the in-place variant of the  ProjectionRetraction . source"},{"id":394,"pagetitle":"Retractions","title":"ManifoldsBase.retract_project_fused!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract_project_fused!-Tuple{AbstractManifold, Any, Any, Any, Number}","content":" ManifoldsBase.retract_project_fused!  ‚Äî  Method retract_project_fused!(M::AbstractManifold, q, p, X, t::Number) Compute the in-place variant of the  ProjectionRetraction . source"},{"id":395,"pagetitle":"Retractions","title":"ManifoldsBase.retract_qr!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract_qr!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.retract_qr!  ‚Äî  Method retract_qr!(M::AbstractManifold, q, p, X) Compute the in-place variant of the  QRRetraction . source"},{"id":396,"pagetitle":"Retractions","title":"ManifoldsBase.retract_sasaki!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract_sasaki!-Tuple{AbstractManifold, Any, Any, Any, SasakiRetraction}","content":" ManifoldsBase.retract_sasaki!  ‚Äî  Method retract_sasaki!(M::AbstractManifold, q, p, X, m::SasakiRetraction) Compute the in-place variant of the  SasakiRetraction m . source"},{"id":397,"pagetitle":"Retractions","title":"ManifoldsBase.retract_softmax!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.retract_softmax!-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldsBase.retract_softmax!  ‚Äî  Method retract_softmax!(M::AbstractManifold, q, p, X) Compute the in-place variant of the  SoftmaxRetraction . source"},{"id":398,"pagetitle":"Retractions","title":"ManifoldsBase.inverse_retract_shooting!","ref":"/manifoldsbase/stable/retractions/#ManifoldsBase.inverse_retract_shooting!-Tuple{AbstractManifold, Any, Any, Any, ShootingInverseRetraction}","content":" ManifoldsBase.inverse_retract_shooting!  ‚Äî  Method inverse_retract_shooting!(M::AbstractManifold, X, p, q, m::ShootingInverseRetraction) Approximate the inverse of a retraction using the shooting method. source"},{"id":401,"pagetitle":"How to define a manifold","title":"How to Implement a Manifold","ref":"/manifoldsbase/stable/tutorials/implement-a-manifold/#How-to-Implement-a-Manifold","content":" How to Implement a Manifold This tutorial illustrates, how to implement your very first manifold. We start from the very beginning and cover the basic ideas of the interface provided by  ManifoldsBase.jl  interface."},{"id":402,"pagetitle":"How to define a manifold","title":"Preliminaries","ref":"/manifoldsbase/stable/tutorials/implement-a-manifold/#Preliminaries","content":" Preliminaries We will use a simple example in this tutorial, since the main focus here is to illustrate how to define a manifold. We will use the sphere of radius  $r$  embedded in  $\\mathbb R^{d+1}$ , i.e.¬†all vectors of length  $r$ . Formally we define \\[\\mathbb S_r^d :=\n\\bigl\\{\n    p \\in \\mathbb R^{d+1}\n    \\big|\n    \\lVert p \\rVert = r\n\\bigr\\}\\] When defining a Riemannian manifold mathematically, there is several things to keep in mind, for example the metric imposed on the tangent spaces. For this interface we assume these things to be given implicitly for a first implementation, but they can be made more precise when necessary. The only thing we have to be aware of for now is the  number_system , i.e.¬†whether our manifold is a real-valued or a complex-valued manifold. The abstract type all manifolds inherit from, the  AbstractManifold {ùîΩ}  has this number system as a parameter. The usual parameter we will use are the  RealNumbers () , which have a short hand in  ManifoldsBase.jl , namely  ‚Ñù . The second one are the  ComplexNumbers () , or  ‚ÑÇ  for short. using LinearAlgebra, ManifoldsBase\nusing ManifoldsBase: ‚Ñù"},{"id":403,"pagetitle":"How to define a manifold","title":"Defining a manifold","ref":"/manifoldsbase/stable/tutorials/implement-a-manifold/#Defining-a-manifold","content":" Defining a manifold A manifold itself is a  struct  that is a subtype of  AbstractManifold  and should contain. We usually recommend to also document your new manifold. Since the  Sphere  is already a name used within  Manifolds.jl , let‚Äôs use a slightly more specific name. We define \"\"\"\n    ScaledSphere <: AbstractManifold{‚Ñù}\n\nDefine a sphere of fixed radius\n\n# Fields\n\n* `dimension` dimension of the sphere\n* `radius` the radius of the sphere\n\n# Constructor\n\n    ScaledSphere(dimension,radius=1.0)\n\nInitialize the manifold to a certain `dimension` and `radius`,\nwhich by default is set to `1.0`\n\"\"\"\nstruct ScaledSphere <: AbstractManifold{‚Ñù}\n    dimension::Int\n    radius::Float64\nend And we can directly use this manifold and set M = ScaledSphere(2,1.5) ScaledSphere(2, 1.5)"},{"id":404,"pagetitle":"How to define a manifold","title":"Functions I: Manifold properties","ref":"/manifoldsbase/stable/tutorials/implement-a-manifold/#Functions-I:-Manifold-properties","content":" Functions I: Manifold properties While the interface provides a lot of possible functions to define for your manifold, you only need to define those that are necessary for your implementation. If you are using other packages depending on  ManifoldsBase.jl  you will often just get a ‚ÄúMethod not defined‚Äù and sometimes an ambiguity error indicating that a function is missing that is required for a certain task. We can first start with a technical function which internally is often used. Any of our points or tangent vectors are represented as a  $(d+1)$ -dimensional vector. This is internally often used when allocating memory, see  representation_size . It returns a tuple representing the size of arrays for valid points. We define import ManifoldsBase: representation_size\nrepresentation_size(M::ScaledSphere) = (M.dimension+1,) Similarly, we can implement the function returning the dimension of the manifold, cf.¬† manifold_dimension  as import ManifoldsBase: manifold_dimension\nmanifold_dimension(M::ScaledSphere) = M.dimension and we can now easily use them to access the dimension of the manifold manifold_dimension(M) 2"},{"id":405,"pagetitle":"How to define a manifold","title":"Functions II: Verifying Points and tangent vectors","ref":"/manifoldsbase/stable/tutorials/implement-a-manifold/#Functions-II:-Verifying-Points-and-tangent-vectors","content":" Functions II: Verifying Points and tangent vectors The first two functions we want to define are those to check points and tangent vectors for our manifold. Let‚Äôs first clarify what the tangent space looks like. The directions ‚Äúwe can walk into‚Äù from a point  $p\\in \\mathbb S_r^d$  are all  $X$  that are orthogonal to  $p$ , which is the plane/vector space tangent to the sphere. Formally \\[T_p\\mathbb S_r^d :=\n\\bigl\\{\n    X \\in \\mathbb R^{d+1}\n    \\big|\n    \\langle p, X \\rangle = 0\n\\bigr\\}, \\qquad p \\in \\mathbb S_r^d\\] to verify either  p  or  X  one uses  is_point (M,p)  and  is_vector (M, p, X)  respectively. Since both involve some automatic options and possibilities, for example whether to throw an error or just return false, both mention that the actual functions to implement are  check_point  and  check_vector , which both do not throw but  return  an error if something is wrong. In principle we would have to check two properties, namely that the size of  p  is of correct size  M.dimension+1  and that its norm is  M.radius . Luckily, by defining  representation_size  the first check is automatically done already ‚Äì¬†actually for both points and vectors. We define import ManifoldsBase: check_point\nfunction check_point(M::ScaledSphere, p; kwargs...)\n    if !isapprox(norm(p), M.radius; kwargs...)\n        return DomainError(norm(p), \"The norm of $p is not $(M.radius).\")\n    end\n    return nothing\nend And we can directly test the function. To see all 3 failing ones, we switch from errors to warnings in the check is_point(M, [1.5, 0.0], error=:warn) # wrong size\nis_point(M, [1.0, 0.0, 0.0], error=:warn) # wrong norm\nis_point(M, [1.5, 0.0, 0.0], error=:warn) # on the manifold, returns true ‚îå Warning: DomainError with (2,):\n‚îÇ The point [1.5, 0.0] can not belong to the manifold ScaledSphere(2, 1.5), since its size (2,) is not equal to the manifolds representation size ((3,)).\n‚îî @ ManifoldsBase ~/work/ManifoldsBase.jl/ManifoldsBase.jl/src/ManifoldsBase.jl:805\n‚îå Warning: DomainError with 1.0:\n‚îÇ The norm of [1.0, 0.0, 0.0] is not 1.5.\n‚îî @ ManifoldsBase ~/work/ManifoldsBase.jl/ManifoldsBase.jl/src/ManifoldsBase.jl:818\n\ntrue similarly for vectors, we just have to implement the orthogonality check. import ManifoldsBase: check_vector\nfunction check_vector(M::ScaledSphere, p, X; kwargs...)\n    if !isapprox(dot(p,X), 0.0; kwargs...)\n        return DomainError(\n            dot(p,X),\n            \"The tangent vector $X is not orthogonal to $p.\"\n        )\n    end\n    return nothing\nend and again, the high level interface can be used to display warnings for vectors not fulfilling the criterion, but we can also activate a check for the point using the last positional argument is_vector(M, [1.5, 0.0, 0.0], [0.0, 1.0]; error=:warn) # wrong size\nis_vector(M, [1.5, 0.0, 0.0], [1.0, 1.0, 0.0]; error=:warn) # not orthogonal norm\nis_vector(M, [1.0, 0.0, 0.0], [0.0, 1.0, 0.0], true; error=:warn) # point not valid\nis_vector(M, [1.5, 0.0, 0.0], [0.0, 1.0, 0.0], true; error=:warn) # returns true ‚îå Warning: DomainError with (2,):\n‚îÇ The tangent vector [0.0, 1.0] can not belong to the manifold ScaledSphere(2, 1.5), since its size (2,) is not equal to the manifodls representation size ((3,)).\n‚îî @ ManifoldsBase ~/work/ManifoldsBase.jl/ManifoldsBase.jl/src/ManifoldsBase.jl:889\n‚îå Warning: DomainError with 1.5:\n‚îÇ The tangent vector [1.0, 1.0, 0.0] is not orthogonal to [1.5, 0.0, 0.0].\n‚îî @ ManifoldsBase ~/work/ManifoldsBase.jl/ManifoldsBase.jl/src/ManifoldsBase.jl:902\n‚îå Warning: DomainError with 1.0:\n‚îÇ The norm of [1.0, 0.0, 0.0] is not 1.5.\n‚îî @ ManifoldsBase ~/work/ManifoldsBase.jl/ManifoldsBase.jl/src/ManifoldsBase.jl:818\n\ntrue"},{"id":406,"pagetitle":"How to define a manifold","title":"Functions on Manifolds III: The exponential map and a retraction.","ref":"/manifoldsbase/stable/tutorials/implement-a-manifold/#Functions-on-Manifolds-III:-The-exponential-map-and-a-retraction.","content":" Functions on Manifolds III: The exponential map and a retraction. For the final group of functions, we want to implement the  exp onential map and a  retract ion. Both are ways to ‚Äúmove around‚Äù on the manifold, that is, given a point  $p$  and a tangent vector indicating a ‚Äúwalking direction‚Äù, the two functions we define will return a new point  $q$  on the manifold. For functions that compute a new point or tangent vector,  ManifoldsBase.jl  always provides two variants: One that allocates new memory, and one that allows to provide the memory the result should be returned in¬†to spare memory allocations. Let‚Äôs first take a look at what the exponential map is defined like. We follow the shortest curves, that is great arcs, on the sphere. Formally we have \\[\\exp_p X =\n\\cos\\Bigl(\\frac{1}{r}\\lVert X \\rVert\\Bigr)p +\n\\sin\\Bigl(\\frac{1}{r}\\lVert X \\rVert\\Bigr)\\frac{X}{\\lVert X \\rVert}.\\] In fact, from the two functions above,  exp (M, p, X) , and  exp! (M, q, p, X)  that works in place of  q , we only have to implement the second. The first one,  exp  by default falls back to allocating memory and calling the second. So  exp  should only be defined, if there is a special reason for. Furthermore, we usually do not verify/check inputs to spare time. If a user feels insecure, they could for example use the  ValidationManifold  wrapper which adds explicit checks of inputs and outputs. We define import ManifoldsBase: exp!\nfunction exp!(M::ScaledSphere, q, p, X)\n    nX = norm(X)\n    if nX == 0\n        q .= p\n    else\n        q .= cos(nX/M.radius)*p + M.radius*sin(nX/M.radius) .* (1/nX) .* X\n    end\n    return q\nend and we can directly test our function starting in the north pole and ‚Äúwalking down‚Äù to the equator q = exp(M, [0.0, 0.0, 1.5], [0.75œÄ, 0.0, 0.0]) 3-element Vector{Float64}:\n 1.5\n 0.0\n 9.184850993605148e-17 but we also get the other variants for free, for example q2 = zero(q)\nexp!(M, q2, [0.0, 0.0, 1.5], [0.75œÄ, 0.0, 0.0])\nq2 3-element Vector{Float64}:\n 1.5\n 0.0\n 9.184850993605148e-17 or the one that shortens or elongates the path by a factor, for example, if we walk twice the distance, we reach the opposite point ManifoldsBase.exp_fused!(M, q2, [0.0, 0.0, 1.5], [0.75œÄ, 0.0, 0.0], 2.0)\nq2 3-element Vector{Float64}:\n  1.8369701987210297e-16\n  0.0\n -1.5 Of course we can easily check that both points we computed still lie on the sphere all([is_point(M, q), is_point(M, q2)]) true Since the exponential map might in general be expensive, we can do a similar implementation with the  ProjectionRetraction . Here, we really have to take into account, that the interface is  designed with 3 levels  in mind: While the actual function we would call in the end is  retract(M, p, X, ProjectionRetraction())  (or its  !  variant), we actually have to implement  retract_project!(M, q, p, X, t)  for technical details, that are a bit beyond this introductory tutorial. In short this split avoids ambiguity errors for decorators of the manifolds. We define function ManifoldsBase.retract_project!(M::ScaledSphere, q, p, X)\n    q .= (p + X) .* (M.radius/norm(p + X))\n    return q\nend And to test also this function, and avoiding allocations at the same time, we call retract!(M, q, [0.0, 0.0, 1.5], [0.75œÄ, 0.0, 0.0], ProjectionRetraction()) 3-element Vector{Float64}:\n 1.2653454121031529\n 0.0\n 0.8055439082194726 Finally, there is  default_retraction_method  to specify which is the default retraction to use. By default this is default_retraction_method(M) ExponentialRetraction() But we can easily specify this for our manifold as well, for example defining import ManifoldsBase: default_retraction_method\ndefault_retraction_method(::ScaledSphere) = ProjectionRetraction() default_retraction_method (generic function with 6 methods) Then default_retraction_method(M) ProjectionRetraction() and retract without a method specified would always fall back to using the projection retraction instead of the exponential map. Note that for compatibility there is the  AbstractRetractionMethod  called  ExponentialRetraction  which makes  retract  fall back to calling  exp ."},{"id":407,"pagetitle":"How to define a manifold","title":"Technical Details","ref":"/manifoldsbase/stable/tutorials/implement-a-manifold/#Technical-Details","content":" Technical Details This notebook was rendered with the following environment Pkg.status() Status `~/work/ManifoldsBase.jl/ManifoldsBase.jl/tutorials/Project.toml`\n  [7073ff75] IJulia v1.27.0\n  [3362f125] ManifoldsBase v1.2.0 `~/work/ManifoldsBase.jl/ManifoldsBase.jl`\n  [91a5bcdd] Plots v1.40.13"},{"id":410,"pagetitle":"An abstract manifold","title":"The Manifold interface","ref":"/manifoldsbase/stable/types/#The-Manifold-interface","content":" The Manifold interface"},{"id":411,"pagetitle":"An abstract manifold","title":"The AbstractManifold","ref":"/manifoldsbase/stable/types/#The-AbstractManifold","content":" The  AbstractManifold The main type is the  AbstractManifold . It represents the manifold per se. Throughout the documentation of  ManifoldsBase.jl  we might use the  Euclidean Space  and the  Sphere  (both implemented in  Manifolds.jl ) as easy examples to illustrate properties and features of this interface on concrete examples."},{"id":412,"pagetitle":"An abstract manifold","title":"ManifoldsBase.AbstractManifold","ref":"/manifoldsbase/stable/types/#ManifoldsBase.AbstractManifold","content":" ManifoldsBase.AbstractManifold  ‚Äî  Type AbstractManifold{ùîΩ} A type to represent a (Riemannian) manifold. The  AbstractManifold  is a central type of this interface. It allows to distinguish different implementations of functions like the  exp onential and  log arithmic map for different manifolds. Usually, the manifold is the first parameter in any of these functions within  ManifoldsBase.jl . Based on these, say ‚Äúelementary‚Äù functions, as the two mentioned above, more general functions are built, for example the  shortest_geodesic  and the  geodesic . These should only be overwritten (reimplemented) if for a certain manifold specific, more efficient implementations are possible, that do not just call the elementary functions. The [ AbstractManifold ] is parametrized by  AbstractNumbers  to distinguish for example real (‚Ñù) and complex (‚ÑÇ) manifolds. For subtypes the preferred order of parameters is: size and simple value parameters, followed by the  AbstractNumbers field , followed by data type parameters, which might depend on the abstract number field type. source which should store information about the manifold, for example parameters inherent to the manifold."},{"id":413,"pagetitle":"An abstract manifold","title":"Points on a manifold","ref":"/manifoldsbase/stable/types/#Points-on-a-manifold","content":" Points on a manifold Points do not necessarily have to be typed. Usually one can just use any type. When a manifold has multiple representations, these should be distinguished by point and vector types."},{"id":414,"pagetitle":"An abstract manifold","title":"ManifoldsBase.AbstractManifoldPoint","ref":"/manifoldsbase/stable/types/#ManifoldsBase.AbstractManifoldPoint","content":" ManifoldsBase.AbstractManifoldPoint  ‚Äî  Type AbstractManifoldPoint Type for a point on a manifold. While an  AbstractManifold  does not necessarily require this type, for example when it is implemented for  Vector s or  Matrix  type elements, this type can be used either for more complicated representations, semantic verification, or when dispatching on different representations of points on a manifold. Since semantic verification and different representations usually might still only store a matrix internally, it is possible to use  @manifold_element_forwards  and  @default_manifold_fallbacks  to reduce implementation overhead. source Converting points between different representations can be performed using the  convert  function with either two or three arguments ( convert(T, M, p)  or  convert(T, p) ). For some manifolds providing  M  may be necessary. The first variant falls back to the second variant."},{"id":415,"pagetitle":"An abstract manifold","title":"Tangent and Cotangent spaces","ref":"/manifoldsbase/stable/types/#Tangent-and-Cotangent-spaces","content":" Tangent and Cotangent spaces"},{"id":416,"pagetitle":"An abstract manifold","title":"ManifoldsBase.AbstractCotangentVector","ref":"/manifoldsbase/stable/types/#ManifoldsBase.AbstractCotangentVector","content":" ManifoldsBase.AbstractCotangentVector  ‚Äî  Type AbstractCotangentVector = AbstractFibreVector{CotangentSpaceType} Type for a cotangent vector of a manifold. While a  AbstractManifold  does not necessarily require this type, for example when it is implemented for  Vector s or  Matrix  type elements, this type can be used for more complicated representations, semantic verification, or even dispatch for different representations of cotangent vectors and their types on a manifold. source"},{"id":417,"pagetitle":"An abstract manifold","title":"ManifoldsBase.AbstractFibreVector","ref":"/manifoldsbase/stable/types/#ManifoldsBase.AbstractFibreVector","content":" ManifoldsBase.AbstractFibreVector  ‚Äî  Type AbstractFibreVector{TType<:VectorSpaceType} Type for a vector from a vector space (fibre of a vector bundle) of type  TType  of a manifold. While a  AbstractManifold  does not necessarily require this type, for example when it is implemented for  Vector s or  Matrix  type elements, this type can be used for more complicated representations, semantic verification, or even dispatch for different representations of tangent vectors and their types on a manifold. You may use macro  @manifold_vector_forwards  to introduce commonly used method definitions for your subtype of  AbstractFibreVector . source"},{"id":418,"pagetitle":"An abstract manifold","title":"ManifoldsBase.AbstractTangentVector","ref":"/manifoldsbase/stable/types/#ManifoldsBase.AbstractTangentVector","content":" ManifoldsBase.AbstractTangentVector  ‚Äî  Type AbstractTangentVector = AbstractFibreVector{TangentSpaceType} Type for a tangent vector of a manifold. While a  AbstractManifold  does not necessarily require this type, for example when it is implemented for  Vector s or  Matrix  type elements, this type can be used for more complicated representations, semantic verification, or even dispatch for different representations of tangent vectors and their types on a manifold. source"},{"id":419,"pagetitle":"An abstract manifold","title":"ManifoldsBase.FVector","ref":"/manifoldsbase/stable/types/#ManifoldsBase.FVector","content":" ManifoldsBase.FVector  ‚Äî  Type FVector(type::VectorSpaceType, data, basis::AbstractBasis) Decorator indicating that the vector  data  contains coordinates of a vector from a fiber of a vector bundle of type  type .  basis  is an object describing the basis of that space in which the coordinates are given. Conversion between  FVector  representation and the default representation of an object (for example a tangent vector) for a manifold should be done using  get_coordinates  and  get_vector . Examples julia> using Manifolds\n\njulia> M = Sphere(2)\nSphere(2, ‚Ñù)\n\njulia> p = [1.0, 0.0, 0.0]\n3-element Vector{Float64}:\n 1.0\n 0.0\n 0.0\n\njulia> X = [0.0, 2.0, -1.0]\n3-element Vector{Float64}:\n  0.0\n  2.0\n -1.0\n\njulia> B = DefaultOrthonormalBasis()\nDefaultOrthonormalBasis(‚Ñù)\n\njulia> fX = TFVector(get_coordinates(M, p, X, B), B)\nTFVector([2.0, -1.0], DefaultOrthonormalBasis(‚Ñù))\n\njulia> X_back = get_vector(M, p, fX.data, fX.basis)\n3-element Vector{Float64}:\n -0.0\n  2.0\n -1.0 source"},{"id":420,"pagetitle":"An abstract manifold","title":"ManifoldsBase.vector_space_dimension","ref":"/manifoldsbase/stable/types/#ManifoldsBase.vector_space_dimension-Tuple{AbstractManifold, ManifoldsBase.VectorSpaceType}","content":" ManifoldsBase.vector_space_dimension  ‚Äî  Method vector_space_dimension(M::AbstractManifold, V::VectorSpaceType) Dimension of the vector space of type  V  on manifold  M . source This interface also covers a large variety how to  model bases in tangent spaces . Converting tangent vectors between different representations can be performed using the  convert  function with either three or four arguments ( convert(T, M, p, X)  or  convert(T, p, X) ). For some manifolds providing  M  may be necessary. The first variant falls back to the second variant."},{"id":421,"pagetitle":"An abstract manifold","title":"Macros for automatic forwards for simple points/tangent vectors","ref":"/manifoldsbase/stable/types/#Macros-for-automatic-forwards-for-simple-points/tangent-vectors","content":" Macros for automatic forwards for simple points/tangent vectors When distinguishing different representations of points or tangent vectors on one manifold, it might happen that both a subtype of  AbstractManifoldPoint  and a subtype of  AbstractTangentVector  are just encapsulating a value This is taken into account by the following macros, that forward several actions just to this field. Most prominently vector operations for the tangent vectors. If there is still a default case, a macro sets this type to be equivalent to calling the manifold functions just with the types field that carries the value."},{"id":422,"pagetitle":"An abstract manifold","title":"ManifoldsBase.@default_manifold_fallbacks","ref":"/manifoldsbase/stable/types/#ManifoldsBase.@default_manifold_fallbacks-Tuple{Any, Any, Any, Symbol, Symbol}","content":" ManifoldsBase.@default_manifold_fallbacks  ‚Äî  Macro default_manifold_fallbacks(TM, TP, TV, pfield::Symbol, vfield::Symbol) Introduce default fallbacks for all basic functions on manifolds, for manifold of type  TM , points of type  TP , tangent vectors of type  TV , with forwarding to fields  pfield  and  vfield  for point and tangent vector functions, respectively. source"},{"id":423,"pagetitle":"An abstract manifold","title":"ManifoldsBase.@manifold_element_forwards","ref":"/manifoldsbase/stable/types/#ManifoldsBase.@manifold_element_forwards-Tuple{Any, Symbol}","content":" ManifoldsBase.@manifold_element_forwards  ‚Äî  Macro manifold_element_forwards(T, field::Symbol)\nmanifold_element_forwards(T, Twhere, field::Symbol) Introduce basic fallbacks for type  T  (which can be a subtype of  Twhere ) that represents points or vectors for a manifold. Fallbacks will work by forwarding to the field passed in  field ` List of forwarded functions: allocate , copy , copyto! , number_eltype  (only for values, not the type itself), similar , size , == . source"},{"id":424,"pagetitle":"An abstract manifold","title":"ManifoldsBase.@manifold_vector_forwards","ref":"/manifoldsbase/stable/types/#ManifoldsBase.@manifold_vector_forwards-Tuple{Any, Symbol}","content":" ManifoldsBase.@manifold_vector_forwards  ‚Äî  Macro manifold_vector_forwards(T, field::Symbol)\nmanifold_vector_forwards(T, Twhere, field::Symbol) Introduce basic fallbacks for type  T  that represents vectors from a vector bundle for a manifold.  Twhere  is put into  where  clause of each method. Fallbacks work by forwarding to field passed as  field . List of forwarded functions: basic arithmetic ( * ,  / ,  \\ ,  + ,  - ), all things from  @manifold_element_forwards , broadcasting support. example @eval @manifold_vector_forwards ValidationFibreVector{TType} TType value source"},{"id":425,"pagetitle":"An abstract manifold","title":"Number Systems","ref":"/manifoldsbase/stable/types/#number-system","content":" Number Systems The  AbstractManifold  has one parameter to distinguish the number system a manifold is based on."},{"id":426,"pagetitle":"An abstract manifold","title":"ManifoldsBase.AbstractNumbers","ref":"/manifoldsbase/stable/types/#ManifoldsBase.AbstractNumbers","content":" ManifoldsBase.AbstractNumbers  ‚Äî  Type AbstractNumbers An abstract type to represent the number system on which a manifold is built. This provides concrete number types for dispatch. The two most common number types are the fields  RealNumbers  ( ‚Ñù  for short) and  ComplexNumbers  ( ‚ÑÇ ). source"},{"id":427,"pagetitle":"An abstract manifold","title":"ManifoldsBase.ComplexNumbers","ref":"/manifoldsbase/stable/types/#ManifoldsBase.ComplexNumbers","content":" ManifoldsBase.ComplexNumbers  ‚Äî  Type ComplexNumbers <: AbstractNumbers\n‚ÑÇ = ComplexNumbers() The field of complex numbers. source"},{"id":428,"pagetitle":"An abstract manifold","title":"ManifoldsBase.QuaternionNumbers","ref":"/manifoldsbase/stable/types/#ManifoldsBase.QuaternionNumbers","content":" ManifoldsBase.QuaternionNumbers  ‚Äî  Type QuaternionNumbers <: AbstractNumbers\n‚Ñç = QuaternionNumbers() The division algebra of quaternions. source"},{"id":429,"pagetitle":"An abstract manifold","title":"ManifoldsBase.RealNumbers","ref":"/manifoldsbase/stable/types/#ManifoldsBase.RealNumbers","content":" ManifoldsBase.RealNumbers  ‚Äî  Type RealNumbers <: AbstractNumbers\n‚Ñù = RealNumbers() The field of real numbers. source"},{"id":430,"pagetitle":"An abstract manifold","title":"ManifoldsBase._unify_number_systems","ref":"/manifoldsbase/stable/types/#ManifoldsBase._unify_number_systems-Tuple{ManifoldsBase.AbstractNumbers, Vararg{ManifoldsBase.AbstractNumbers}}","content":" ManifoldsBase._unify_number_systems  ‚Äî  Method _unify_number_systems(ùîΩs::AbstractNumbers...) Compute a number system that includes all given number systems (as sub-systems) and is closed under addition and multiplication. source"},{"id":431,"pagetitle":"An abstract manifold","title":"ManifoldsBase.number_system","ref":"/manifoldsbase/stable/types/#ManifoldsBase.number_system-Union{Tuple{AbstractManifold{ùîΩ}}, Tuple{ùîΩ}} where ùîΩ","content":" ManifoldsBase.number_system  ‚Äî  Method number_system(M::AbstractManifold{ùîΩ}) Return the number system the manifold  M  is based on, i.e. the parameter  ùîΩ . source"},{"id":432,"pagetitle":"An abstract manifold","title":"ManifoldsBase.real_dimension","ref":"/manifoldsbase/stable/types/#ManifoldsBase.real_dimension-Tuple{ManifoldsBase.AbstractNumbers}","content":" ManifoldsBase.real_dimension  ‚Äî  Method real_dimension(ùîΩ::AbstractNumbers) Return the real dimension  $\\dim_‚Ñù ùîΩ$  of the  AbstractNumbers  system  ùîΩ . The real dimension is the dimension of a real vector space with which a number in  ùîΩ  can be identified. For example,  ComplexNumbers  have a real dimension of 2, and  QuaternionNumbers  have a real dimension of 4. source"},{"id":433,"pagetitle":"An abstract manifold","title":"Type Parameter","ref":"/manifoldsbase/stable/types/#type-parameter","content":" Type Parameter Concrete  AbstractManifold s usually correspond to families of manifolds that are parameterized by some numbers, for example determining their  manifold_dimension . Those numbers can either be stored in a field or as a type parameter of the structure. The  TypeParameter  offers the flexibility to have this parameter either as type parameter or a field."},{"id":434,"pagetitle":"An abstract manifold","title":"ManifoldsBase.TypeParameter","ref":"/manifoldsbase/stable/types/#ManifoldsBase.TypeParameter","content":" ManifoldsBase.TypeParameter  ‚Äî  Type TypeParameter{T} Represents numeric parameters of a manifold type as type parameters, allowing for static specialization of methods. source"},{"id":435,"pagetitle":"An abstract manifold","title":"ManifoldsBase.wrap_type_parameter","ref":"/manifoldsbase/stable/types/#ManifoldsBase.wrap_type_parameter","content":" ManifoldsBase.wrap_type_parameter  ‚Äî  Function wrap_type_parameter(parameter::Symbol, data) Wrap  data  in  TypeParameter  if  parameter  is  :type  or return  data  unchanged if  parameter  is  :field . Intended for use in manifold constructors, see  DefaultManifold  for an example. source"},{"id":438,"pagetitle":"Vector transports","title":"Vector transport","ref":"/manifoldsbase/stable/vector_transports/#Vector-transport","content":" Vector transport Similar to the  exponential and logarithmic map  also the  parallel transport  might be costly to compute, especially when there is no closed form solution known and it has to be approximated with numerical methods. Similar to the  retraction and its inverse , the generalisation of the parallel transport can be phrased as follows A  vector transport  is a way to transport a vector between two tangent spaces. Let  $p,q ‚àà \\mathcal M$  be given,  $c$  the curve along which we want to transport (cf.  parallel transport , for example a geodesic or curve given by a retraction. We can specify the geodesic or curve a retraction realises for example by a direction  $d$ . More precisely using [ AMS08 ], Def. 8.1.1, a vector transport  $T_{p,d}: T_p\\mathcal M \\to T_q\\mathcal M$ ,  $p‚àà \\mathcal M$ ,  $Y‚àà T_p\\mathcal M$  is a smooth mapping associated to a retraction  $\\operatorname{retr}_p(Y) = q$  such that (associated retraction)  $\\mathcal T_{p,d}X ‚àà T_q\\mathcal M$  if and only if  $q = \\operatorname{retr}_p(d)$ , (consistency)  $\\mathcal T_{p,0_p}X = X$  for all  $X‚ààT_p\\mathcal M$ , (linearity)  $\\mathcal T_{p,d}(Œ±X+Œ≤Y) = \\mathcal Œ±T_{p,d}X + \\mathcal Œ≤T_{p,d}Y$  for all  $Œ±, Œ≤ ‚àà ùîΩ$ , hold. Currently the following methods for vector transport are defined in  ManifoldsBase.jl ."},{"id":439,"pagetitle":"Vector transports","title":"ManifoldsBase.default_vector_transport_method","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.default_vector_transport_method-Tuple{AbstractManifold}","content":" ManifoldsBase.default_vector_transport_method  ‚Äî  Method default_vector_transport_method(M::AbstractManifold)\ndefault_vector_transport_method(M::AbstractManifold, ::Type{T}) where {T} The  AbstractVectorTransportMethod  that is used when calling  vector_transport_to  or  vector_transport_direction  without specifying the vector transport method. By default, this is  ParallelTransport . This method can also be specified more precisely with a point type  T , for the case that on a  M  there are two different representations of points, which provide different vector transport methods. source"},{"id":440,"pagetitle":"Vector transports","title":"ManifoldsBase.vector_transport_direction","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.vector_transport_direction","content":" ManifoldsBase.vector_transport_direction  ‚Äî  Function vector_transport_direction(M::AbstractManifold, p, X, d)\nvector_transport_direction(M::AbstractManifold, p, X, d, m::AbstractVectorTransportMethod) Given an  AbstractManifold $\\mathcal M$  the vector transport is a generalization of the  parallel_transport_direction  that identifies vectors from different tangent spaces. More precisely using [ AMS08 ], Def. 8.1.1, a vector transport  $T_{p,d}: T_p\\mathcal M \\to T_q\\mathcal M$ ,  $p‚àà \\mathcal M$ ,  $Y‚àà T_p\\mathcal M$  is a smooth mapping associated to a retraction  $\\operatorname{retr}_p(Y) = q$  such that (associated retraction)  $\\mathcal T_{p,d}X ‚àà T_q\\mathcal M$  if and only if  $q = \\operatorname{retr}_p(d)$ . (consistency)  $\\mathcal T_{p,0_p}X = X$  for all  $X‚ààT_p\\mathcal M$ (linearity)  $\\mathcal T_{p,d}(Œ±X+Œ≤Y) = Œ±\\mathcal T_{p,d}X + Œ≤\\mathcal T_{p,d}Y$ For the  AbstractVectorTransportMethod  we might even omit the third point. The  AbstractLinearVectorTransportMethod s are linear. Input Parameters M  a manifold p  indicating the tangent space of X  the tangent vector to be transported d  indicating a transport direction (and distance through its length) m  an  AbstractVectorTransportMethod , by default  default_vector_transport_method , so usually  ParallelTransport Usually this method requires a  AbstractRetractionMethod  as well. By default this is assumed to be the  default_retraction_method  or implicitly given (and documented) for a vector transport. To explicitly distinguish different retractions for a vector transport, see  VectorTransportDirection . Instead of spcifying a start direction  d  one can equivalently also specify a target tanget space  $T_q\\mathcal M$ , see  vector_transport_to . By default  vector_transport_direction  falls back to using  vector_transport_to , using the  default_retraction_method  on  M . source"},{"id":441,"pagetitle":"Vector transports","title":"ManifoldsBase.vector_transport_direction!","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.vector_transport_direction!","content":" ManifoldsBase.vector_transport_direction!  ‚Äî  Function vector_transport_direction!(M::AbstractManifold, Y, p, X, d)\nvector_transport_direction!(M::AbstractManifold, Y, p, X, d, m::AbstractVectorTransportMethod) Transport a vector  X  from the tangent space at a point  p  on the  AbstractManifold M  in the direction indicated by the tangent vector  d  at  p . By default,  retract  and  vector_transport_to!  are used with the  m  and  r , which default to  default_vector_transport_method (M)  and  default_retraction_method (M) , respectively. The result is saved to  Y . See  vector_transport_direction  for more details. source"},{"id":442,"pagetitle":"Vector transports","title":"ManifoldsBase.vector_transport_to","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.vector_transport_to","content":" ManifoldsBase.vector_transport_to  ‚Äî  Function vector_transport_to(M::AbstractManifold, p, X, q)\nvector_transport_to(M::AbstractManifold, p, X, q, m::AbstractVectorTransportMethod)\nvector_transport_to(M::AbstractManifold, p, X, q, m::AbstractVectorTransportMethod) Transport a vector  X  from the tangent space at a point  p  on the  AbstractManifold M  along a curve implicitly given by an  AbstractRetractionMethod  associated to  m . By default  m  is the  default_vector_transport_method (M) . To explicitly specify a (different) retraction to the implicitly assumeed retraction, see  VectorTransportTo . Note that some vector transport methods might also carry their own retraction they are associated to, like the   DifferentiatedRetractionVectorTransport  and some are even independent of the retraction, for example the  ProjectionTransport . This method is equivalent to using  $d = \\operatorname{retr}^{-1}_p(q)$  in  vector_transport_direction (M, p, X, q, m, r) , where you can find the formal definition. This is the fallback for  VectorTransportTo . source"},{"id":443,"pagetitle":"Vector transports","title":"ManifoldsBase.vector_transport_to!","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.vector_transport_to!","content":" ManifoldsBase.vector_transport_to!  ‚Äî  Function vector_transport_to!(M::AbstractManifold, Y, p, X, q)\nvector_transport_to!(M::AbstractManifold, Y, p, X, q, m::AbstractVectorTransportMethod) Transport a vector  X  from the tangent space at a point  p  on the  AbstractManifold M  to  q  using the  AbstractVectorTransportMethod m  and the  AbstractRetractionMethod r . The result is computed in  Y . See  vector_transport_to  for more details. source"},{"id":444,"pagetitle":"Vector transports","title":"Types of vector transports","ref":"/manifoldsbase/stable/vector_transports/#Types-of-vector-transports","content":" Types of vector transports To distinguish different types of vector transport we introduce the  AbstractVectorTransportMethod . The following concrete types are available."},{"id":445,"pagetitle":"Vector transports","title":"ManifoldsBase.AbstractLinearVectorTransportMethod","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.AbstractLinearVectorTransportMethod","content":" ManifoldsBase.AbstractLinearVectorTransportMethod  ‚Äî  Type AbstractLinearVectorTransportMethod <: AbstractVectorTransportMethod Abstract type for linear methods for transporting vectors, that is transport of a linear combination of vectors is a linear combination of transported vectors. source"},{"id":446,"pagetitle":"Vector transports","title":"ManifoldsBase.AbstractVectorTransportMethod","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.AbstractVectorTransportMethod","content":" ManifoldsBase.AbstractVectorTransportMethod  ‚Äî  Type AbstractVectorTransportMethod <: AbstractApproximationMethod Abstract type for methods for transporting vectors. Such vector transports are not necessarily linear. See also AbstractLinearVectorTransportMethod source"},{"id":447,"pagetitle":"Vector transports","title":"ManifoldsBase.DifferentiatedRetractionVectorTransport","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.DifferentiatedRetractionVectorTransport","content":" ManifoldsBase.DifferentiatedRetractionVectorTransport  ‚Äî  Type DifferentiatedRetractionVectorTransport{R<:AbstractRetractionMethod} <:\n    AbstractVectorTransportMethod A type to specify a vector transport that is given by differentiating a retraction. This can be introduced in two ways. Let  $\\mathcal M$  be a Riemannian manifold,  $p‚àà\\mathcal M$  a point, and  $X,Y‚àà T_p\\mathcal M$  denote two tangent vectors at  $p$ . Given a retraction (cf.  AbstractRetractionMethod )  $\\operatorname{retr}$ , the vector transport of  X  in direction  Y  (cf.  vector_transport_direction ) by differentiation this retraction, is given by \\[\\mathcal T^{\\operatorname{retr}}_{p,Y}X\n= D_Y\\operatorname{retr}_p(Y)[X]\n= \\frac{\\mathrm{d}}{\\mathrm{d}t}\\operatorname{retr}_p(Y+tX)\\Bigr|_{t=0}.\\] see [ AMS08 ], Section 8.1.2 for more details. This can be phrased similarly as a  vector_transport_to  by introducing  $q=\\operatorname{retr}_pX$  and defining \\[\\mathcal T^{\\operatorname{retr}}_{q \\gets p}X = \\mathcal T^{\\operatorname{retr}}_{p,Y}X\\] which in practice usually requires the  inverse_retract  to exists in order to compute  $Y = \\operatorname{retr}_p^{-1}q$ . Constructor DifferentiatedRetractionVectorTransport(m::AbstractRetractionMethod) source"},{"id":448,"pagetitle":"Vector transports","title":"ManifoldsBase.EmbeddedVectorTransport","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.EmbeddedVectorTransport","content":" ManifoldsBase.EmbeddedVectorTransport  ‚Äî  Type EmbeddedVectorTransport{T<:AbstractVectorTransportMethod} <: AbstractVectorTransportMethod Compute a vector transport by using the vector transport of type  T  in the embedding and projecting the result. Constructor EmbeddedVectorTransport(vt::AbstractVectorTransportMethod) Generate the vector transport with vector transport  vt  to use in the embedding. source"},{"id":449,"pagetitle":"Vector transports","title":"ManifoldsBase.ParallelTransport","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.ParallelTransport","content":" ManifoldsBase.ParallelTransport  ‚Äî  Type ParallelTransport <: AbstractVectorTransportMethod Compute the vector transport by parallel transport, see  parallel_transport_to source"},{"id":450,"pagetitle":"Vector transports","title":"ManifoldsBase.PoleLadderTransport","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.PoleLadderTransport","content":" ManifoldsBase.PoleLadderTransport  ‚Äî  Type PoleLadderTransport <: AbstractVectorTransportMethod Specify to use  pole_ladder  as vector transport method within  vector_transport_to  or  vector_transport_direction , i.e. Let  $X‚àà T_p\\mathcal M$  be a tangent vector at  $p‚àà\\mathcal M$  and  $q‚àà\\mathcal M$  the point to transport to. Then  $x = \\exp_pX$  is used to call  y = pole_ladder (M, p, x, q)  and the resulting vector is obtained by computing  $Y = -\\log_qy$ . The  PoleLadderTransport  posesses two advantages compared to  SchildsLadderTransport : it is cheaper to evaluate, if you want to transport several vectors, since the mid point  $c$  then stays unchanged. while both methods are exact if the curvature is zero, pole ladder is even exact in symmetric Riemannian manifolds [ Pen18 ] The pole ladder was was proposed in [ LP13 ]. Its name stems from the fact that it resembles a pole ladder when applied to a sequence of points usccessively. Constructor PoleLadderTransport(\n    retraction = ExponentialRetraction(),\n    inverse_retraction = LogarithmicInverseRetraction(),\n) Construct the classical pole ladder that employs exp and log, i.e. as proposed in[ LP13 ]. For an even cheaper transport the inner operations can be changed to an  AbstractRetractionMethod retraction  and an  AbstractInverseRetractionMethod inverse_retraction , respectively. source"},{"id":451,"pagetitle":"Vector transports","title":"ManifoldsBase.ProjectionTransport","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.ProjectionTransport","content":" ManifoldsBase.ProjectionTransport  ‚Äî  Type ProjectionTransport <: AbstractVectorTransportMethod Specify to use projection onto tangent space as vector transport method within  vector_transport_to  or  vector_transport_direction . See  project  for details. source"},{"id":452,"pagetitle":"Vector transports","title":"ManifoldsBase.ScaledVectorTransport","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.ScaledVectorTransport","content":" ManifoldsBase.ScaledVectorTransport  ‚Äî  Type ScaledVectorTransport{T} <: AbstractVectorTransportMethod Introduce a scaled variant of any  AbstractVectorTransportMethod T , as introduced in [ SI13 ] for some  $X‚àà T_p\\mathcal M$  as \\[    \\mathcal T^{\\mathrm{S}}(X) = \\frac{\\lVert X\\rVert_p}{\\lVert \\mathcal T(X)\\rVert_q}\\mathcal T(X).\\] Note that the resulting point  q  has to be known, i.e. for  vector_transport_direction  the curve or more precisely its end point has to be known (via an exponential map or a retraction). Therefore a default implementation is only provided for the  vector_transport_to Constructor ScaledVectorTransport(m::AbstractVectorTransportMethod) source"},{"id":453,"pagetitle":"Vector transports","title":"ManifoldsBase.SchildsLadderTransport","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.SchildsLadderTransport","content":" ManifoldsBase.SchildsLadderTransport  ‚Äî  Type SchildsLadderTransport <: AbstractVectorTransportMethod Specify to use  schilds_ladder  as vector transport method within  vector_transport_to  or  vector_transport_direction , i.e. Let  $X‚àà T_p\\mathcal M$  be a tangent vector at  $p‚àà\\mathcal M$  and  $q‚àà\\mathcal M$  the point to transport to. Then \\[P^{\\mathrm{S}}_{q\\gets p}(X) =\n    \\log_q\\bigl( \\operatorname{retr}_p ( 2\\operatorname{retr}_p^{-1}c ) \\bigr),\\] where  $c$  is the mid point between  $q$  and  $d=\\exp_pX$ . This method employs the internal function  schilds_ladder (M, p, d, q)  that avoids leaving the manifold. The name stems from the image of this paralleltogram in a repeated application yielding the image of a ladder. The approximation was proposed in [ EPS72 ]. Constructor SchildsLadderTransport(\n    retraction = ExponentialRetraction(),\n    inverse_retraction = LogarithmicInverseRetraction(),\n) Construct the classical Schilds ladder that employs exp and log, i.e. as proposed in [ EPS72 ]. For an even cheaper transport these inner operations can be changed to an  AbstractRetractionMethod retraction  and an  AbstractInverseRetractionMethod inverse_retraction , respectively. source"},{"id":454,"pagetitle":"Vector transports","title":"ManifoldsBase.VectorTransportDirection","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.VectorTransportDirection","content":" ManifoldsBase.VectorTransportDirection  ‚Äî  Type VectorTransportDirection{VM<:AbstractVectorTransportMethod,RM<:AbstractRetractionMethod}\n    <: AbstractVectorTransportMethod Specify a  vector_transport_direction  using a  AbstractVectorTransportMethod  with explicitly using the  AbstractRetractionMethod  to determine the point in the specified direction where to transsport to. Note that you only need this for the non-default (non-implicit) second retraction method associated to a vector transport, i.e. when a first implementation assumed an implicit associated retraction. source"},{"id":455,"pagetitle":"Vector transports","title":"ManifoldsBase.VectorTransportTo","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.VectorTransportTo","content":" ManifoldsBase.VectorTransportTo  ‚Äî  Type VectorTransportTo{VM<:AbstractVectorTransportMethod,RM<:AbstractRetractionMethod}\n    <: AbstractVectorTransportMethod Specify a  vector_transport_to  using a  AbstractVectorTransportMethod  with explicitly using the  AbstractInverseRetractionMethod  to determine the direction that transports from  in  p to  q . Note that you only need this for the non-default (non-implicit) second retraction method associated to a vector transport, i.e. when a first implementation assumed an implicit associated retraction. source"},{"id":456,"pagetitle":"Vector transports","title":"ManifoldsBase.VectorTransportWithKeywords","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.VectorTransportWithKeywords","content":" ManifoldsBase.VectorTransportWithKeywords  ‚Äî  Type VectorTransportWithKeywords{V<:AbstractVectorTransportMethod, K} <: AbstractVectorTransportMethod Since vector transports might have keywords, this type is a way to set them as an own type to be used as a specific vector transport. Another reason for this type is that we dispatch on the vector transport first and only the last layer would be implemented with keywords, so this way they can be passed down. Fields vector_transport  the vector transport that is decorated with keywords kwargs  the keyword arguments Note that you can nest this type. Then the most outer specification of a keyword is used. Constructor VectorTransportWithKeywords(m::T; kwargs...) where {T <: AbstractVectorTransportMethod} Specify the subtype  T <: AbstractVectorTransportMethod  to have keywords  kwargs... . source"},{"id":457,"pagetitle":"Vector transports","title":"Functions to implement (on Layer III)","ref":"/manifoldsbase/stable/vector_transports/#Functions-to-implement-(on-Layer-III)","content":" Functions to implement (on Layer III) While you should always add your documentation to the first layer vector transport methods above when implementing new manifolds, the actual implementation happens on the following functions on  layer III ."},{"id":458,"pagetitle":"Vector transports","title":"ManifoldsBase.pole_ladder","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.pole_ladder","content":" ManifoldsBase.pole_ladder  ‚Äî  Function pole_ladder(\n    M,\n    p,\n    d,\n    q,\n    c = mid_point(M, p, q);\n    retraction=default_retraction_method(M, typeof(p)),\n    inverse_retraction=default_inverse_retraction_method(M, typeof(p))\n) Compute an inner step of the pole ladder, that can be used as a  vector_transport_to . Let  $c = \\gamma_{p,q}(\\frac{1}{2})$  mid point between  p  and  q , then the pole ladder is given by \\[    \\operatorname{Pl}(p,d,q) = \\operatorname{retr}_d (2\\operatorname{retr}_d^{-1}c)\\] Where the classical pole ladder employs  $\\operatorname{retr}_d=\\exp_d$  and  $\\operatorname{retr}_d^{-1}=\\log_d$  but for an even cheaper transport these can be set to different  AbstractRetractionMethod  and  AbstractInverseRetractionMethod . When you have  $X=log_pd$  and  $Y = -\\log_q \\operatorname{Pl}(p,d,q)$ , you will obtain the  PoleLadderTransport . When performing multiple steps, this method avoids the switching to the tangent space. Keep in mind that after  $n$  successive steps the tangent vector reads  $Y_n = (-1)^n\\log_q \\operatorname{Pl}(p_{n-1},d_{n-1},p_n)$ . It is cheaper to evaluate than  schilds_ladder , sinc if you want to form multiple ladder steps between  p  and  q , but with different  d , there is just one evaluation of a geodesic each., since the center  c  can be reused. source"},{"id":459,"pagetitle":"Vector transports","title":"ManifoldsBase.pole_ladder!","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.pole_ladder!","content":" ManifoldsBase.pole_ladder!  ‚Äî  Function pole_ladder(\n    M,\n    pl,\n    p,\n    d,\n    q,\n    c = mid_point(M, p, q),\n    X = allocate_result_type(M, log, d, c);\n    retraction = default_retraction_method(M, typeof(p)),\n    inverse_retraction = default_inverse_retraction_method(M, typeof(p)),\n) Compute the  pole_ladder , i.e. the result is saved in  pl .  X  is used for storing intermediate inverse retraction. source"},{"id":460,"pagetitle":"Vector transports","title":"ManifoldsBase.schilds_ladder","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.schilds_ladder","content":" ManifoldsBase.schilds_ladder  ‚Äî  Function schilds_ladder(\n    M,\n    p,\n    d,\n    q,\n    c = mid_point(M, q, d);\n    retraction = default_retraction_method(M, typeof(p)),\n    inverse_retraction = default_inverse_retraction_method(M, typeof(p)),\n) Perform an inner step of schilds ladder, which can be used as a  vector_transport_to , see  SchildsLadderTransport . Let  $c = \\gamma_{q,d}(\\frac{1}{2})$  denote the mid point on the shortest geodesic connecting  $q$  and the point  $d$ . Then Schild's ladder reads as \\[\\operatorname{Sl}(p,d,q) = \\operatorname{retr}_p( 2\\operatorname{retr}_p^{-1} c)\\] Where the classical Schilds ladder employs  $\\operatorname{retr}_d=\\exp_d$  and  $\\operatorname{retr}_d^{-1}=\\log_d$  but for an even cheaper transport these can be set to different  AbstractRetractionMethod  and  AbstractInverseRetractionMethod . In consistency with  pole_ladder  you can change the way the mid point is computed using the optional parameter  c , but note that here it's the mid point between  q  and  d . When you have  $X=log_pd$  and  $Y = \\log_q \\operatorname{Sl}(p,d,q)$ , you will obtain the  PoleLadderTransport . Then the approximation to the transported vector is given by  $\\log_q\\operatorname{Sl}(p,d,q)$ . When performing multiple steps, this method avoidsd the switching to the tangent space. Hence after  $n$  successive steps the tangent vector reads  $Y_n = \\log_q \\operatorname{Pl}(p_{n-1},d_{n-1},p_n)$ . source"},{"id":461,"pagetitle":"Vector transports","title":"ManifoldsBase.schilds_ladder!","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.schilds_ladder!","content":" ManifoldsBase.schilds_ladder!  ‚Äî  Function schilds_ladder!(\n    M,\n    sl\n    p,\n    d,\n    q,\n    c = mid_point(M, q, d),\n    X = allocate_result_type(M, log, d, c);\n    retraction = default_retraction_method(M, typeof(p)),\n    inverse_retraction = default_inverse_retraction_method(M, typeof(p)),\n) Compute  schilds_ladder  and return the value in the parameter  sl . If the required mid point  c  was computed before, it can be passed using  c , and the allocation of new memory can be avoided providing a tangent vector  X  for the interims result. source"},{"id":462,"pagetitle":"Vector transports","title":"ManifoldsBase.vector_transport_direction_diff!","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.vector_transport_direction_diff!-NTuple{6, Any}","content":" ManifoldsBase.vector_transport_direction_diff!  ‚Äî  Method vector_transport_direction_diff!(M::AbstractManifold, Y, p, X, d, m::AbstractRetractionMethod) Compute the vector transport of  X  from  $T_p\\mathcal M$  into the direction  d  using the differential of the  AbstractRetractionMethod m  in place of  Y . source"},{"id":463,"pagetitle":"Vector transports","title":"ManifoldsBase.vector_transport_direction_embedded!","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.vector_transport_direction_embedded!-Tuple{AbstractManifold, Any, Any, Any, Any, AbstractVectorTransportMethod}","content":" ManifoldsBase.vector_transport_direction_embedded!  ‚Äî  Method vector_transport_direction_embedded!(M::AbstractManifold, Y, p, X, d, m::AbstractVectorTransportMethod) Compute the vector transport of  X  from  $T_p\\mathcal M$  into the direction  d  using the  AbstractRetractionMethod m  in the embedding. The default implementataion requires one allocation for the points and tangent vectors in the embedding and the resulting point, but the final projection is performed in place of  Y source"},{"id":464,"pagetitle":"Vector transports","title":"ManifoldsBase.vector_transport_to_diff!","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.vector_transport_to_diff!-Tuple{AbstractManifold, Vararg{Any, 5}}","content":" ManifoldsBase.vector_transport_to_diff!  ‚Äî  Method vector_transport_to_diff(M::AbstractManifold, p, X, q, r) Compute a vector transport by using a  DifferentiatedRetractionVectorTransport r  in place of  Y . source"},{"id":465,"pagetitle":"Vector transports","title":"ManifoldsBase.vector_transport_to_embedded!","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.vector_transport_to_embedded!-Tuple{AbstractManifold, Vararg{Any, 5}}","content":" ManifoldsBase.vector_transport_to_embedded!  ‚Äî  Method vector_transport_to_embedded!(M::AbstractManifold, Y, p, X, q, m::AbstractRetractionMethod) Compute the vector transport of  X  from  $T_p\\mathcal M$  to the point  q  using the  of the  AbstractRetractionMethod m  in th embedding. The default implementataion requires one allocation for the points and tangent vectors in the embedding and the resulting point, but the final projection is performed in place of  Y source"},{"id":466,"pagetitle":"Vector transports","title":"ManifoldsBase.vector_transport_to_project!","ref":"/manifoldsbase/stable/vector_transports/#ManifoldsBase.vector_transport_to_project!-Tuple{AbstractManifold, Vararg{Any, 4}}","content":" ManifoldsBase.vector_transport_to_project!  ‚Äî  Method vector_transport_to_project!(M::AbstractManifold, Y, p, X, q) Compute a vector transport by projecting  $X\\in T_p\\mathcal M$  onto the tangent space  $T_q\\mathcal M$  at  $q$  in place of  Y . source"},{"id":469,"pagetitle":"Home","title":"Manifolds","ref":"/manifolds/stable/#Manifolds","content":" Manifolds"},{"id":470,"pagetitle":"Home","title":"Manifolds.Manifolds","ref":"/manifolds/stable/#Manifolds.Manifolds","content":" Manifolds.Manifolds  ‚Äî  Module Manifolds.jl  provides a library of manifolds aiming for an easy-to-use and fast implementation. source The implemented manifolds are accompanied by their mathematical formulae. The manifolds are implemented using the interface for manifolds given in  ManifoldsBase.jl . You can use that interface to implement your own software on manifolds, such that all manifolds based on that interface can be used within your code. For more information, see the  About  section."},{"id":471,"pagetitle":"Home","title":"Getting started","ref":"/manifolds/stable/#Getting-started","content":" Getting started To install the package just type using Pkg; Pkg.add(\"Manifolds\") Then you can directly start, for example to stop half way from the north pole on the  Sphere  to a point on the the equator, you can generate the  shortest_geodesic . It internally employs  log  and  exp . using Manifolds\nM = Sphere(2)\nŒ≥ = shortest_geodesic(M, [0., 0., 1.], [0., 1., 0.])\nŒ≥(0.5) 3-element Vector{Float64}:\n 0.0\n 0.7071067811865475\n 0.7071067811865476"},{"id":472,"pagetitle":"Home","title":"Citation","ref":"/manifolds/stable/#Citation","content":" Citation If you use  Manifolds.jl  in your work, please cite the following @online{2106.08777,\n    Author = {Seth D. Axen and Mateusz Baran and Ronny Bergmann and Krzysztof Rzecki},\n    Title = {Manifolds.jl: An Extensible Julia Framework for Data Analysis on Manifolds},\n    Year = {2021},\n    Eprint = {2106.08777},\n    Eprinttype = {arXiv},\n} To refer to a specific version, it is recommended to cite, for example, @software{manifoldsjl-zenodo-mostrecent,\n  Author = {Seth D. Axen and Mateusz Baran and Ronny Bergmann},\n  Title = {Manifolds.jl},\n  Doi = {10.5281/ZENODO.4292129},\n  Url = {https://zenodo.org/record/4292129},\n  Publisher = {Zenodo},\n  Year = {2021},\n  Copyright = {MIT License}\n} for the most recent version or a corresponding version specific DOI, see  the list of all versions . Note that both citations are in  BibLaTeX  format."},{"id":475,"pagetitle":"Atlases and charts","title":"Atlases and charts","ref":"/manifolds/stable/features/atlases/#atlases_and_charts","content":" Atlases and charts Atlases on an  $n$ -dimensional manifold  $\\mathcal M$ are collections of charts  $\\mathcal A = \\{(U_i, œÜ_i) \\colon i \\in I\\}$ , where  $I$  is a (finite or infinite) index family, such that  $U_i \\subseteq \\mathcal M$  is an open set and each chart  $œÜ_i: U_i ‚Üí ‚Ñù^n$  is a homeomorphism. This means, that  $œÜ_i$  is bijective ‚Äì sometimes also called one-to-one and onto - and continuous, and its inverse  $œÜ_i^{-1}$  is continuous as well. The inverse  $œÜ_i^{-1}$  is called (local) parametrization. The resulting  parameters $a=œÜ(p)$  of  $p$  (with respect to the chart  $œÜ$ ) are in the literature also called ‚Äú(local) coordinates‚Äù. To distinguish the parameter  $a$  from   get_coordinates  in a basis, we use the terminology parameter in this package. For an atlas  $\\mathcal A$  we further require that \\[\\displaystyle\\bigcup_{i\\in I} U_i = \\mathcal M.\\] We say that  $œÜ_i$  is a chart about  $p$ , if  $p\\in U_i$ . An atlas provides a connection between a manifold and the Euclidean space  $‚Ñù^n$ , since locally, a chart about  $p$  can be used to identify its neighborhood (as long as you stay in  $U_i$ ) with a subset of a Euclidean space. Most manifolds we consider are smooth, i.e. any change of charts  $œÜ_i \\circ œÜ_j^{-1}: ‚Ñù^n ‚Üí ‚Ñù^n$ , where  $i,j\\in I$ , is a smooth function. These changes of charts are also called transition maps. Most operations on manifolds in  Manifolds.jl  avoid operating in a chart through appropriate embeddings and formulas derived for particular manifolds, though atlases provide the most general way of working with manifolds. Compared to these approaches, using an atlas is often more technical and time-consuming. They are extensively used in metric-related functions on  MetricManifold s. Atlases are represented by objects of subtypes of  AbstractAtlas . There are no type restrictions for indices of charts in atlases. Operations using atlases and charts are available through the following functions: get_chart_index  can be used to select an appropriate chart for the neighborhood of a given point  $p$ . This function should work deterministically, i.e. for a fixed  $p$  always return the same chart. get_parameters  converts a point to its parameters with respect to the chart in a chart. get_point  converts parameters (local coordinates) in a chart to the point that corresponds to them. induced_basis  returns a basis of a given vector space at a point induced by a chart  $œÜ$ . transition_map  converts coordinates of a point between two charts, e.g. computes  $œÜ_i\\circ œÜ_j^{-1}: ‚Ñù^n ‚Üí ‚Ñù^n$ ,  $i,j\\in I$ . While an atlas could store charts as explicit functions, it is favourable, that the [ get_parameters ] actually implements a chart  $œÜ$ ,  get_point  its inverse, the prametrization  $œÜ^{-1}$ ."},{"id":476,"pagetitle":"Atlases and charts","title":"Manifolds.AbstractAtlas","ref":"/manifolds/stable/features/atlases/#Manifolds.AbstractAtlas","content":" Manifolds.AbstractAtlas  ‚Äî  Type AbstractAtlas{ùîΩ} An abstract class for atlases whith charts that have values in the vector space  ùîΩ‚Åø  for some value of  n .  ùîΩ  is a number system determined by an  AbstractNumbers  object. source"},{"id":477,"pagetitle":"Atlases and charts","title":"Manifolds.InducedBasis","ref":"/manifolds/stable/features/atlases/#Manifolds.InducedBasis","content":" Manifolds.InducedBasis  ‚Äî  Type InducedBasis(vs::VectorSpaceType, A::AbstractAtlas, i) The basis induced by chart with index  i  from an  AbstractAtlas A  of vector space of type  vs . For the  vs  a  TangentSpace  this works as  follows: Let  $n$  denote the dimension of the manifold  $\\mathcal M$ . Let the parameter  $a=œÜ_i(p) ‚àà \\mathbb R^n$  and  $j‚àà\\{1,‚Ä¶,n\\}$ . We can look at the  $j$ th parameter curve  $b_j(t) = a + te_j$ , where  $e_j$  denotes the  $j$ th unit vector. Using the parametrisation we obtain a curve  $c_j(t) = œÜ_i^{-1}(b_j(t))$  which fulfills  $c(0) = p$ . Now taking the derivative(s) with respect to  $t$  (and evaluate at  $t=0$ ), we obtain a tangent vector for each  $j$  corresponding to an equivalence class of curves (having the same derivative) as \\[X_j = [c_j] = \\frac{\\mathrm{d}}{\\mathrm{d}t} c_i(t) \\Bigl|_{t=0}\\] and the set  $\\{X_1,\\ldots,X_n\\}$  is the chart-induced basis of  $T_p\\mathcal M$ . See also VectorSpaceType ,  AbstractBasis source"},{"id":478,"pagetitle":"Atlases and charts","title":"Manifolds.RetractionAtlas","ref":"/manifolds/stable/features/atlases/#Manifolds.RetractionAtlas","content":" Manifolds.RetractionAtlas  ‚Äî  Type RetractionAtlas{\n    ùîΩ,\n    TRetr<:AbstractRetractionMethod,\n    TInvRetr<:AbstractInverseRetractionMethod,\n    TBasis<:AbstractBasis,\n} <: AbstractAtlas{ùîΩ} An atlas indexed by points on a manifold,  $\\mathcal M = I$  and parameters (local coordinates) are given in  $T_p\\mathcal M$ . This means that a chart  $œÜ_p = \\mathrm{cord}\\circ\\mathrm{retr}_p^{-1}$  is only locally defined (around  $p$ ), where  $\\mathrm{cord}$  is the decomposition of the tangent vector into coordinates with respect to the given basis of the tangent space, cf.  get_coordinates . The parametrization is given by  $œÜ_p^{-1}=\\mathrm{retr}_p\\circ\\mathrm{vec}$ , where  $\\mathrm{vec}$  turns the basis coordinates into a tangent vector, cf.  get_vector . In short: The coordinates with respect to a basis are used together with a retraction as a parametrization. See also AbstractAtlas ,  AbstractInverseRetractionMethod ,  AbstractRetractionMethod ,  AbstractBasis source"},{"id":479,"pagetitle":"Atlases and charts","title":"LinearAlgebra.norm","ref":"/manifolds/stable/features/atlases/#LinearAlgebra.norm-Tuple{AbstractManifold, AbstractAtlas, Any, Any, Any}","content":" LinearAlgebra.norm  ‚Äî  Method norm(M::AbstractManifold, A::AbstractAtlas, i, a, Xc) Calculate norm on manifold  M  at point with parameters  a  in chart  i  of an  AbstractAtlas A  of vector with coefficients  Xc  in induced basis. source"},{"id":480,"pagetitle":"Atlases and charts","title":"Manifolds.affine_connection!","ref":"/manifolds/stable/features/atlases/#Manifolds.affine_connection!-Tuple{AbstractManifold, Any, AbstractAtlas, Vararg{Any, 4}}","content":" Manifolds.affine_connection!  ‚Äî  Method affine_connection!(M::AbstractManifold, Zc, A::AbstractAtlas, i, a, Xc, Yc) Calculate affine connection on manifold  M  at point with parameters  a  in chart  i  of an an  AbstractAtlas A  of vectors with coefficients  Zc  and  Yc  in induced basis and save the result in  Zc . source"},{"id":481,"pagetitle":"Atlases and charts","title":"Manifolds.affine_connection","ref":"/manifolds/stable/features/atlases/#Manifolds.affine_connection-Tuple{AbstractManifold, Vararg{Any, 5}}","content":" Manifolds.affine_connection  ‚Äî  Method affine_connection(M::AbstractManifold, A::AbstractAtlas, i, a, Xc, Yc) Calculate affine connection on manifold  M  at point with parameters  a  in chart  i  of  AbstractAtlas A  of vectors with coefficients  Xc  and  Yc  in induced basis. source"},{"id":482,"pagetitle":"Atlases and charts","title":"Manifolds.check_chart_switch","ref":"/manifolds/stable/features/atlases/#Manifolds.check_chart_switch-Tuple{AbstractManifold, AbstractAtlas, Any, Any}","content":" Manifolds.check_chart_switch  ‚Äî  Method check_chart_switch(M::AbstractManifold, A::AbstractAtlas, i, a) Determine whether chart should be switched when an operation in chart  i  from an  AbstractAtlas A  reaches parameters  a  in that chart. By default  false  is returned. source"},{"id":483,"pagetitle":"Atlases and charts","title":"Manifolds.get_chart_index","ref":"/manifolds/stable/features/atlases/#Manifolds.get_chart_index-Tuple{AbstractManifold, AbstractAtlas, Any, Any}","content":" Manifolds.get_chart_index  ‚Äî  Method get_chart_index(M::AbstractManifold, A::AbstractAtlas, i, a) Select a chart from an  AbstractAtlas A  for manifold  M  that is suitable for representing the neighborhood of point with parametrization  a  in chart  i . This selection should be deterministic, although different charts may be selected for arbitrarily close but distinct points. See also get_default_atlas source"},{"id":484,"pagetitle":"Atlases and charts","title":"Manifolds.get_chart_index","ref":"/manifolds/stable/features/atlases/#Manifolds.get_chart_index-Tuple{AbstractManifold, AbstractAtlas, Any}","content":" Manifolds.get_chart_index  ‚Äî  Method get_chart_index(M::AbstractManifold, A::AbstractAtlas, p) Select a chart from an  AbstractAtlas A  for manifold  M  that is suitable for representing the neighborhood of point  p . This selection should be deterministic, although different charts may be selected for arbitrarily close but distinct points. See also get_default_atlas source"},{"id":485,"pagetitle":"Atlases and charts","title":"Manifolds.get_default_atlas","ref":"/manifolds/stable/features/atlases/#Manifolds.get_default_atlas-Tuple{AbstractManifold}","content":" Manifolds.get_default_atlas  ‚Äî  Method get_default_atlas(::AbstractManifold) Determine the default real-valued atlas for the given manifold. source"},{"id":486,"pagetitle":"Atlases and charts","title":"Manifolds.get_parameters","ref":"/manifolds/stable/features/atlases/#Manifolds.get_parameters-Tuple{AbstractManifold, AbstractAtlas, Any, Any}","content":" Manifolds.get_parameters  ‚Äî  Method get_parameters(M::AbstractManifold, A::AbstractAtlas, i, p) Calculate parameters (local coordinates) of point  p  on manifold  M  in chart from an  AbstractAtlas A  at index  i . This function is hence an implementation of the chart  $œÜ_i(p), i\\in I$ . The parameters are in the number system determined by  A . If the point  $p\\notin U_i$  is not in the domain of the chart, this method should throw an error. See also get_point ,  get_chart_index source"},{"id":487,"pagetitle":"Atlases and charts","title":"Manifolds.get_point","ref":"/manifolds/stable/features/atlases/#Manifolds.get_point-Tuple{AbstractManifold, AbstractAtlas, Any, Any}","content":" Manifolds.get_point  ‚Äî  Method get_point(M::AbstractManifold, A::AbstractAtlas, i, a) Calculate point at parameters (local coordinates)  a  on manifold  M  in chart from an  AbstractAtlas A  at index  i . This function is hence an implementation of the inverse  $œÜ_i^{-1}(a), i\\in I$  of a chart, also called a parametrization. See also get_parameters ,  get_chart_index source"},{"id":488,"pagetitle":"Atlases and charts","title":"Manifolds.induced_basis","ref":"/manifolds/stable/features/atlases/#Manifolds.induced_basis-Tuple{AbstractManifold, AbstractAtlas, Any, VectorSpaceType}","content":" Manifolds.induced_basis  ‚Äî  Method induced_basis(M::AbstractManifold, A::AbstractAtlas, i, p, VST::VectorSpaceType) Basis of vector space of type  VST  at point  p  from manifold  M  induced by chart ( A ,  i ). See also VectorSpaceType ,  AbstractAtlas source"},{"id":489,"pagetitle":"Atlases and charts","title":"Manifolds.induced_basis","ref":"/manifolds/stable/features/atlases/#Manifolds.induced_basis-Union{Tuple{ùîΩ}, Tuple{AbstractManifold{ùîΩ}, AbstractAtlas, Any}, Tuple{AbstractManifold{ùîΩ}, AbstractAtlas, Any, VectorSpaceType}} where ùîΩ","content":" Manifolds.induced_basis  ‚Äî  Method induced_basis(::AbstractManifold, A::AbstractAtlas, i, VST::VectorSpaceType = TangentSpaceType()) Get the basis induced by chart with index  i  from an  AbstractAtlas A  of vector space of type  vs . Returns an object of type  InducedBasis . See also VectorSpaceType ,  AbstractBasis source"},{"id":490,"pagetitle":"Atlases and charts","title":"Manifolds.inverse_chart_injectivity_radius","ref":"/manifolds/stable/features/atlases/#Manifolds.inverse_chart_injectivity_radius-Tuple{AbstractManifold, AbstractAtlas, Any}","content":" Manifolds.inverse_chart_injectivity_radius  ‚Äî  Method inverse_chart_injectivity_radius(M::AbstractManifold, A::AbstractAtlas, i) Injectivity radius of  get_point  for chart  i  from an  AbstractAtlas A  of a manifold  M . source"},{"id":491,"pagetitle":"Atlases and charts","title":"Manifolds.local_metric","ref":"/manifolds/stable/features/atlases/#Manifolds.local_metric-Tuple{AbstractManifold, Any, InducedBasis}","content":" Manifolds.local_metric  ‚Äî  Method local_metric(M::AbstractManifold, p, B::InducedBasis) Compute the local metric tensor for vectors expressed in terms of coordinates in basis  B  on manifold  M . The point  p  is not checked. source"},{"id":492,"pagetitle":"Atlases and charts","title":"Manifolds.transition_map","ref":"/manifolds/stable/features/atlases/#Manifolds.transition_map-Tuple{AbstractManifold, AbstractAtlas, Any, AbstractAtlas, Any, Any}","content":" Manifolds.transition_map  ‚Äî  Method transition_map(M::AbstractManifold, A_from::AbstractAtlas, i_from, A_to::AbstractAtlas, i_to, a)\ntransition_map(M::AbstractManifold, A::AbstractAtlas, i_from, i_to, a) Given coordinates  a  in chart  (A_from, i_from)  of a point on manifold  M , returns coordinates of that point in chart  (A_to, i_to) . If  A_from  and  A_to  are equal,  A_to  can be omitted. Mathematically this function is the transition map or change of charts, but it might even be between two atlases  $A_{\\text{from}} = \\{(U_i,œÜ_i)\\}_{i\\in I}$  and  $A_{\\text{to}} = \\{(V_j,\\psi_j)\\}_{j\\in J}$ , and hence  $I, J$  are their index sets. We have  $i_{\\text{from}}\\in I$ ,  $i_{\\text{to}}\\in J$ . This method then computes \\[\\bigl(\\psi_{i_{\\text{to}}}\\circ œÜ_{i_{\\text{from}}}^{-1}\\bigr)(a)\\] Note that, similarly to  get_parameters , this method should fail the same way if  $V_{i_{\\text{to}}}\\cap U_{i_{\\text{from}}}=\\emptyset$ . See also AbstractAtlas ,  get_parameters ,  get_point source"},{"id":493,"pagetitle":"Atlases and charts","title":"Manifolds.transition_map_diff!","ref":"/manifolds/stable/features/atlases/#Manifolds.transition_map_diff!-Tuple{AbstractManifold, Any, AbstractAtlas, Vararg{Any, 4}}","content":" Manifolds.transition_map_diff!  ‚Äî  Method transition_map_diff!(M::AbstractManifold, c_out, A::AbstractAtlas, i_from, a, c, i_to) Compute  transition_map_diff  on given arguments and save the result in  c_out . source"},{"id":494,"pagetitle":"Atlases and charts","title":"Manifolds.transition_map_diff","ref":"/manifolds/stable/features/atlases/#Manifolds.transition_map_diff-Tuple{AbstractManifold, AbstractAtlas, Vararg{Any, 4}}","content":" Manifolds.transition_map_diff  ‚Äî  Method transition_map_diff(M::AbstractManifold, A::AbstractAtlas, i_from, a, c, i_to) Compute differential of transition map from chart  i_from  to chart  i_to  from an  AbstractAtlas A  on manifold  M  at point with parameters  a  on tangent vector with coordinates  c  in the induced basis. source"},{"id":495,"pagetitle":"Atlases and charts","title":"ManifoldsBase.inner","ref":"/manifolds/stable/features/atlases/#ManifoldsBase.inner-Tuple{AbstractManifold, AbstractAtlas, Vararg{Any, 4}}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::AbstractManifold, A::AbstractAtlas, i, a, Xc, Yc) Calculate inner product on manifold  M  at point with parameters  a  in chart  i  of an atlas  A  of vectors with coefficients  Xc  and  Yc  in induced basis. source"},{"id":496,"pagetitle":"Atlases and charts","title":"Cotangent space and musical isomorphisms","ref":"/manifolds/stable/features/atlases/#Cotangent-space-and-musical-isomorphisms","content":" Cotangent space and musical isomorphisms Related to atlases, there is also support for the cotangent space and coefficients of cotangent vectors in bases of the cotangent space. Functions  sharp  and  flat  implement musical isomorphisms for arbitrary vector bundles."},{"id":497,"pagetitle":"Atlases and charts","title":"Manifolds.RieszRepresenterCotangentVector","ref":"/manifolds/stable/features/atlases/#Manifolds.RieszRepresenterCotangentVector","content":" Manifolds.RieszRepresenterCotangentVector  ‚Äî  Type RieszRepresenterCotangentVector(M::AbstractManifold, p, X) Cotangent vector in Riesz representer form on manifold  M  at point  p  with Riesz representer  X . source"},{"id":498,"pagetitle":"Atlases and charts","title":"Manifolds.flat","ref":"/manifolds/stable/features/atlases/#Manifolds.flat-Tuple{AbstractManifold, Any, Any}","content":" Manifolds.flat  ‚Äî  Method flat(M::AbstractManifold, p, X) Compute the flat isomorphism (one of the musical isomorphisms) of tangent vector  X  from the vector space of type  M  at point  p  from the underlying  AbstractManifold . The function can be used for example to transform vectors from the tangent bundle to vectors from the cotangent bundle  $‚ô≠ : T\\mathcal M ‚Üí T^{*}\\mathcal M$ source"},{"id":499,"pagetitle":"Atlases and charts","title":"Manifolds.sharp","ref":"/manifolds/stable/features/atlases/#Manifolds.sharp-Tuple{AbstractManifold, Any, Any}","content":" Manifolds.sharp  ‚Äî  Method sharp(M::AbstractManifold, p, Œæ) Compute the sharp isomorphism (one of the musical isomorphisms) of vector  Œæ  from the vector space  M  at point  p  from the underlying  AbstractManifold . The function can be used for example to transform vectors from the cotangent bundle to vectors from the tangent bundle  $‚ôØ : T^{*}\\mathcal M ‚Üí T\\mathcal M$ source"},{"id":500,"pagetitle":"Atlases and charts","title":"Computations in charts","ref":"/manifolds/stable/features/atlases/#Computations-in-charts","content":" Computations in charts"},{"id":501,"pagetitle":"Atlases and charts","title":"Manifolds.IntegratorTerminatorNearChartBoundary","ref":"/manifolds/stable/features/atlases/#Manifolds.IntegratorTerminatorNearChartBoundary","content":" Manifolds.IntegratorTerminatorNearChartBoundary  ‚Äî  Type IntegratorTerminatorNearChartBoundary{TKwargs} An object for determining the point at which integration of a differential equation in a chart on a manifold should be terminated for the purpose of switching a chart. The value stored in  check_chart_switch_kwargs  will be passed as keyword arguments to   check_chart_switch . By default an empty tuple is stored. source"},{"id":502,"pagetitle":"Atlases and charts","title":"Manifolds.estimate_distance_from_bvp","ref":"/manifolds/stable/features/atlases/#Manifolds.estimate_distance_from_bvp","content":" Manifolds.estimate_distance_from_bvp  ‚Äî  Function estimate_distance_from_bvp(\n    M::AbstractManifold,\n    a1,\n    a2,\n    A::AbstractAtlas,\n    i;\n    solver=MIRK4(),\n    dt=0.05,\n    kwargs...,\n) Estimate distance between points on  AbstractManifold  M with parameters  a1  and  a2  in chart  i  of  AbstractAtlas A  using solver  solver , employing  solve_chart_log_bvp  to solve the geodesic BVP. source"},{"id":503,"pagetitle":"Atlases and charts","title":"Manifolds.solve_chart_exp_ode","ref":"/manifolds/stable/features/atlases/#Manifolds.solve_chart_exp_ode","content":" Manifolds.solve_chart_exp_ode  ‚Äî  Function solve_chart_exp_ode(\n    M::AbstractManifold,\n    a,\n    Xc,\n    A::AbstractAtlas,\n    i0;\n    solver=AutoVern9(Rodas5()),\n    final_time=1.0,\n    check_chart_switch_kwargs=NamedTuple(),\n    kwargs...,\n) Solve geodesic ODE on a manifold  M  from point of coordinates  a  in chart  i0  from an  AbstractAtlas A  in direction of coordinates  Xc  in the induced basis. source"},{"id":504,"pagetitle":"Atlases and charts","title":"Manifolds.solve_chart_log_bvp","ref":"/manifolds/stable/features/atlases/#Manifolds.solve_chart_log_bvp","content":" Manifolds.solve_chart_log_bvp  ‚Äî  Function solve_chart_log_bvp(\n    M::AbstractManifold,\n    a1,\n    a2,\n    A::AbstractAtlas,\n    i;\n    solver=MIRK4(),\n    dt::Real=0.05,\n    kwargs...,\n) Solve the BVP corresponding to geodesic calculation on  AbstractManifold  M, between points with parameters  a1  and  a2  in a chart  i  of an  AbstractAtlas A  using solver  solver . Geodesic Œ≥ is sampled at time interval  dt , with Œ≥(0) = a1 and Œ≥(1) = a2. source"},{"id":505,"pagetitle":"Atlases and charts","title":"Manifolds.solve_chart_parallel_transport_ode","ref":"/manifolds/stable/features/atlases/#Manifolds.solve_chart_parallel_transport_ode","content":" Manifolds.solve_chart_parallel_transport_ode  ‚Äî  Function solve_chart_parallel_transport_ode(\n    M::AbstractManifold,\n    a,\n    Xc,\n    A::AbstractAtlas,\n    i0,\n    Yc;\n    solver=AutoVern9(Rodas5()),\n    check_chart_switch_kwargs=NamedTuple(),\n    final_time=1.0,\n    kwargs...,\n) Parallel transport vector with coordinates  Yc  along geodesic on a manifold  M  from point of coordinates  a  in a chart  i0  from an  AbstractAtlas A  in direction of coordinates  Xc  in the induced basis. source"},{"id":508,"pagetitle":"Differentiation","title":"Differentiation","ref":"/manifolds/stable/features/differentiation/#Differentiation","content":" Differentiation Documentation for  Manifolds.jl 's methods and types for finite differences and automatic differentiation."},{"id":509,"pagetitle":"Differentiation","title":"Differentiation backends","ref":"/manifolds/stable/features/differentiation/#Differentiation-backends","content":" Differentiation backends Further differentiation backends and features are available in  ManifoldDiff.jl ."},{"id":510,"pagetitle":"Differentiation","title":"FiniteDifferenes.jl","ref":"/manifolds/stable/features/differentiation/#FiniteDifferenes.jl","content":" FiniteDifferenes.jl"},{"id":511,"pagetitle":"Differentiation","title":"Riemannian differentiation backends","ref":"/manifolds/stable/features/differentiation/#Riemannian-differentiation-backends","content":" Riemannian differentiation backends"},{"id":514,"pagetitle":"Distributions","title":"Distributions","ref":"/manifolds/stable/features/distributions/#Distributions","content":" Distributions The following functions and types provide support for manifold-valued and tangent space-valued distributions:"},{"id":517,"pagetitle":"Group actions","title":"Group actions","ref":"/manifolds/stable/features/group_actions/#Group-actions","content":" Group actions Group actions represent actions of a given group on a specified manifold. The following operations are available: action_side : whether action acts from the  LeftSide  or  RightSide  (not to be confused with action direction). apply : performs given action of an element of the group on an object of compatible type. apply_diff : differential of  apply  with respect to the object it acts upon. direction : tells whether a given action is  LeftAction ,  RightAction . inverse_apply : performs given action of the inverse of an element of the group on an object of compatible type. By default inverts the element and calls  apply  but it may be have a faster implementation for some actions. inverse_apply_diff : counterpart of  apply_diff  for  inverse_apply . optimal_alignment : determine the element of a group that, when it acts upon a point, produces the element closest to another given point in the metric of the G-manifold. Furthermore, group operation action features the following: translate : an operation that performs either ( LeftAction ) on the  LeftSide  or ( RightAction ) on the  RightSide  translation, or actions by inverses of elements ( RightAction  on the  LeftSide  and  LeftAction  on the  RightSide ). This is by default performed by calling  compose  with appropriate order of arguments. This function is separated from  compose  mostly to easily represent its differential,  translate_diff . translate_diff : differential of  translate  with respect to the point being translated. adjoint_action : adjoint action of a given element of a Lie group on an element of its Lie algebra. lie_bracket : Lie bracket of two vectors from a Lie algebra corresponding to a given group. The following group actions are available: Group operation action  GroupOperationAction  that describes action of a group on itself. RotationAction , that is action of  SpecialOrthogonal  group on different manifolds. TranslationAction , which is the action of  TranslationGroup  group on different manifolds."},{"id":518,"pagetitle":"Group actions","title":"Manifolds.AbstractGroupAction","ref":"/manifolds/stable/features/group_actions/#Manifolds.AbstractGroupAction","content":" Manifolds.AbstractGroupAction  ‚Äî  Type AbstractGroupAction{AD<:ActionDirection} An abstract group action on a manifold.  ActionDirection AD  indicates whether it is a left or right action. source"},{"id":519,"pagetitle":"Group actions","title":"Manifolds.adjoint_apply_diff_group","ref":"/manifolds/stable/features/group_actions/#Manifolds.adjoint_apply_diff_group-Tuple{AbstractGroupAction, Any, Any, Any}","content":" Manifolds.adjoint_apply_diff_group  ‚Äî  Method adjoint_apply_diff_group(A::AbstractGroupAction, a, X, p) Pullback with respect to group element of group action  A . \\[(\\mathrm{d}œÑ^{p,*}) : T_{œÑ_{a} p} \\mathcal M ‚Üí T_{a} \\mathcal G\\] source"},{"id":520,"pagetitle":"Group actions","title":"Manifolds.apply!","ref":"/manifolds/stable/features/group_actions/#Manifolds.apply!-Tuple{AbstractGroupAction, Any, Any, Any}","content":" Manifolds.apply!  ‚Äî  Method apply!(A::AbstractGroupAction, q, a, p) Apply action  a  to the point  p  with the rule specified by  A . The result is saved in  q . source"},{"id":521,"pagetitle":"Group actions","title":"Manifolds.apply","ref":"/manifolds/stable/features/group_actions/#Manifolds.apply-Tuple{AbstractGroupAction, Any, Any}","content":" Manifolds.apply  ‚Äî  Method apply(A::AbstractGroupAction, a, p) Apply action  a  to the point  p  using map  $œÑ_a$ , specified by  A . Unless otherwise specified, the right action is defined in terms of the left action: \\[\\mathrm{R}_a = \\mathrm{L}_{a^{-1}}\\] source"},{"id":522,"pagetitle":"Group actions","title":"Manifolds.apply_diff","ref":"/manifolds/stable/features/group_actions/#Manifolds.apply_diff-Tuple{AbstractGroupAction, Any, Any, Any}","content":" Manifolds.apply_diff  ‚Äî  Method apply_diff(A::AbstractGroupAction, a, p, X) For point  $p ‚àà \\mathcal M$  and tangent vector  $X ‚àà T_p \\mathcal M$ , compute the action on  $X$  of the differential of the action of  $a ‚àà \\mathcal{G}$ , specified by rule  A . Written as  $(\\mathrm{d}œÑ_a)_p$ , with the specified left or right convention, the differential transports vectors \\[(\\mathrm{d}œÑ_a)_p : T_p \\mathcal M ‚Üí T_{œÑ_a p} \\mathcal M\\] source"},{"id":523,"pagetitle":"Group actions","title":"Manifolds.apply_diff_group","ref":"/manifolds/stable/features/group_actions/#Manifolds.apply_diff_group-Tuple{AbstractGroupAction, Any, Any, Any}","content":" Manifolds.apply_diff_group  ‚Äî  Method apply_diff_group(A::AbstractGroupAction, a, X, p) Compute the value of differential of action  AbstractGroupAction A  on vector  X , where element  a  is acting on  p , with respect to the group element. Let  $\\mathcal G$  be the group acting on manifold  $\\mathcal M$  by the action  A . The action is of element  $g ‚àà \\mathcal G$  on a point  $p ‚àà \\mathcal M$ . The differential transforms vector  X  from the tangent space at  a ‚àà \\mathcal G ,  $X ‚àà T_a \\mathcal G$  into a tangent space of the manifold  $\\mathcal M$ . When action on element  p  is written as  $\\mathrm{d}œÑ^p$ , with the specified left or right convention, the differential transforms vectors \\[(\\mathrm{d}œÑ^p) : T_{a} \\mathcal G ‚Üí T_{œÑ_a p} \\mathcal M\\] See also apply ,  apply_diff source"},{"id":524,"pagetitle":"Group actions","title":"Manifolds.base_group","ref":"/manifolds/stable/features/group_actions/#Manifolds.base_group-Tuple{AbstractGroupAction}","content":" Manifolds.base_group  ‚Äî  Method base_group(A::AbstractGroupAction) The group that acts in  AbstractGroupAction A . source"},{"id":525,"pagetitle":"Group actions","title":"Manifolds.center_of_orbit","ref":"/manifolds/stable/features/group_actions/#Manifolds.center_of_orbit","content":" Manifolds.center_of_orbit  ‚Äî  Function center_of_orbit(\n    A::AbstractGroupAction,\n    pts,\n    p,\n    mean_method::AbstractApproximationMethod = GradientDescentEstimation(),\n) Calculate an action element  $a$  of action  A  that is the mean element of the orbit of  p  with respect to given set of points  pts . The  mean  is calculated using the method  mean_method . The orbit of  $p$  with respect to the action of a group  $\\mathcal{G}$  is the set \\[O = \\{ œÑ_a p : a ‚àà \\mathcal{G} \\}.\\] This function is useful for computing means on quotients of manifolds by a Lie group action. source"},{"id":526,"pagetitle":"Group actions","title":"Manifolds.direction","ref":"/manifolds/stable/features/group_actions/#Manifolds.direction-Union{Tuple{AbstractGroupAction{AD}}, Tuple{AD}} where AD","content":" Manifolds.direction  ‚Äî  Method direction(::AbstractGroupAction{AD}) -> AD Get the direction of the action: either  LeftAction  or  RightAction . source"},{"id":527,"pagetitle":"Group actions","title":"Manifolds.group_manifold","ref":"/manifolds/stable/features/group_actions/#Manifolds.group_manifold-Tuple{AbstractGroupAction}","content":" Manifolds.group_manifold  ‚Äî  Method group_manifold(A::AbstractGroupAction) The manifold the action  A  acts upon. source"},{"id":528,"pagetitle":"Group actions","title":"Manifolds.inverse_apply!","ref":"/manifolds/stable/features/group_actions/#Manifolds.inverse_apply!-Tuple{AbstractGroupAction, Any, Any, Any}","content":" Manifolds.inverse_apply!  ‚Äî  Method inverse_apply!(A::AbstractGroupAction, q, a, p) Apply inverse of action  a  to the point  p  with the rule specified by  A . The result is saved in  q . source"},{"id":529,"pagetitle":"Group actions","title":"Manifolds.inverse_apply","ref":"/manifolds/stable/features/group_actions/#Manifolds.inverse_apply-Tuple{AbstractGroupAction, Any, Any}","content":" Manifolds.inverse_apply  ‚Äî  Method inverse_apply(A::AbstractGroupAction, a, p) Apply inverse of action  a  to the point  p . The action is specified by  A . source"},{"id":530,"pagetitle":"Group actions","title":"Manifolds.inverse_apply_diff","ref":"/manifolds/stable/features/group_actions/#Manifolds.inverse_apply_diff-Tuple{AbstractGroupAction, Any, Any, Any}","content":" Manifolds.inverse_apply_diff  ‚Äî  Method inverse_apply_diff(A::AbstractGroupAction, a, p, X) For group point  $p ‚àà \\mathcal M$  and tangent vector  $X ‚àà T_p \\mathcal M$ , compute the action on  $X$  of the differential of the inverse action of  $a ‚àà \\mathcal{G}$ , specified by rule  A . Written as  $(\\mathrm{d}œÑ_a^{-1})_p$ , with the specified left or right convention, the differential transports vectors. \\[(\\mathrm{d}œÑ_a^{-1})_p : T_p \\mathcal M ‚Üí T_{œÑ_a^{-1} p} \\mathcal M\\] source"},{"id":531,"pagetitle":"Group actions","title":"Manifolds.optimal_alignment!","ref":"/manifolds/stable/features/group_actions/#Manifolds.optimal_alignment!-Tuple{AbstractGroupAction, Any, Any, Any}","content":" Manifolds.optimal_alignment!  ‚Äî  Method optimal_alignment!(A::AbstractGroupAction, x, p, q) Calculate an action element of action  A  that acts upon  p  to produce the element closest to  q . The result is written to  x . source"},{"id":532,"pagetitle":"Group actions","title":"Manifolds.optimal_alignment","ref":"/manifolds/stable/features/group_actions/#Manifolds.optimal_alignment-Tuple{AbstractGroupAction, Any, Any}","content":" Manifolds.optimal_alignment  ‚Äî  Method optimal_alignment(A::AbstractGroupAction, p, q) Calculate an action element  $a$  of action  A  that acts upon  p  to produce the element closest to  q  in the metric of the G-manifold: \\[\\arg\\min_{a ‚àà \\mathcal{G}} d_{\\mathcal M}(œÑ_a p, q)\\] where  $\\mathcal{G}$  is the group that acts on the G-manifold  $\\mathcal M$ . source"},{"id":533,"pagetitle":"Group actions","title":"Group operation action","ref":"/manifolds/stable/features/group_actions/#Group-operation-action","content":" Group operation action"},{"id":534,"pagetitle":"Group actions","title":"Manifolds.GroupOperationAction","ref":"/manifolds/stable/features/group_actions/#Manifolds.GroupOperationAction","content":" Manifolds.GroupOperationAction  ‚Äî  Type GroupOperationAction{AD<:ActionDirection,AS<:GroupActionSide,G<:AbstractDecoratorManifold} <: AbstractGroupAction{AD} Action of a group upon itself via left or right translation, either from left or right side. An element  p  of the group can act upon another another element by either: left action from the left side:  $L_p: q ‚Ü¶ p \\circ q$ , right action from the left side:  $L'_p: q ‚Ü¶ p^{-1} \\circ q$ , right action from the right side:  $R_p: q ‚Ü¶ q \\circ p$ , left action from the right side:  $R'_p: q ‚Ü¶ q \\circ p^{-1}$ . Constructor GroupOperationAction(group::AbstractDecoratorManifold, AD::ActionDirectionAndSide = LeftForwardAction()) source"},{"id":535,"pagetitle":"Group actions","title":"Manifolds.action_side","ref":"/manifolds/stable/features/group_actions/#Manifolds.action_side-Union{Tuple{GroupOperationAction{AD, AS}}, Tuple{AS}, Tuple{AD}} where {AD<:ActionDirection, AS<:Manifolds.GroupActionSide}","content":" Manifolds.action_side  ‚Äî  Method action_side(A::GroupOperationAction) Return whether  GroupOperationAction A  acts on the  LeftSide  or  RightSide . source"},{"id":536,"pagetitle":"Group actions","title":"Manifolds.apply_diff_group","ref":"/manifolds/stable/features/group_actions/#Manifolds.apply_diff_group-Tuple{GroupOperationAction, Any, Any, Any}","content":" Manifolds.apply_diff_group  ‚Äî  Method apply_diff_group(A::GroupOperationAction, a, X, p) Compute differential of  GroupOperationAction A  with respect to group element at tangent vector  X : \\[(\\mathrm{d}œÑ^p) : T_{a} \\mathcal G ‚Üí T_{œÑ_a p} \\mathcal G\\] There are four cases: left action from the left side:  $L_a: p ‚Ü¶ a \\circ p$ , where \\[(\\mathrm{d}L_a) : T_{a} \\mathcal G ‚Üí T_{a \\circ p} \\mathcal G.\\] right action from the left side:  $L'_a: p ‚Ü¶ a^{-1} \\circ p$ , where \\[(\\mathrm{d}L'_a) : T_{a} \\mathcal G ‚Üí T_{a^{-1} \\circ p} \\mathcal G.\\] right action from the right side:  $R_a: p ‚Ü¶ p \\circ a$ , where \\[(\\mathrm{d}R_a) : T_{a} \\mathcal G ‚Üí T_{p \\circ a} \\mathcal G.\\] left action from the right side:  $R'_a: p ‚Ü¶ p \\circ a^{-1}$ , where \\[(\\mathrm{d}R'_a) : T_{a} \\mathcal G ‚Üí T_{p \\circ a^{-1}} \\mathcal G.\\] source"},{"id":537,"pagetitle":"Group actions","title":"Rotation action","ref":"/manifolds/stable/features/group_actions/#Rotation-action","content":" Rotation action"},{"id":538,"pagetitle":"Group actions","title":"Manifolds.ColumnwiseMultiplicationAction","ref":"/manifolds/stable/features/group_actions/#Manifolds.ColumnwiseMultiplicationAction","content":" Manifolds.ColumnwiseMultiplicationAction  ‚Äî  Type ColumnwiseMultiplicationAction{\n    TAD<:ActionDirection,\n    TM<:AbstractManifold,\n    TO<:MatrixGroup,\n} <: AbstractGroupAction{TAD} Action of the (special) unitary or orthogonal group  GeneralUnitaryMultiplicationGroup  or  GeneralLinear  group of type  On  columns of points on a matrix manifold  M . Constructor ColumnwiseMultiplicationAction(\n    M::AbstractManifold,\n    On::MatrixGroup,\n    AD::ActionDirection = LeftAction(),\n) source"},{"id":539,"pagetitle":"Group actions","title":"Manifolds.ComplexPlanarRotation","ref":"/manifolds/stable/features/group_actions/#Manifolds.ComplexPlanarRotation","content":" Manifolds.ComplexPlanarRotation  ‚Äî  Type ComplexPlanarRotation() Action of the circle group  CircleGroup  on  $‚Ñù^2$  by left multiplication. source"},{"id":540,"pagetitle":"Group actions","title":"Manifolds.QuaternionRotation","ref":"/manifolds/stable/features/group_actions/#Manifolds.QuaternionRotation","content":" Manifolds.QuaternionRotation  ‚Äî  Type QuaternionRotation Action of the unit quaternion group  Unitary (1, ‚Ñç)  on  $‚Ñù^3$ . source"},{"id":541,"pagetitle":"Group actions","title":"Manifolds.RotationAction","ref":"/manifolds/stable/features/group_actions/#Manifolds.RotationAction","content":" Manifolds.RotationAction  ‚Äî  Type RotationAction(\n    M::AbstractManifold,\n    SOn::SpecialOrthogonal,\n    AD::ActionDirection = LeftAction(),\n) Space of actions of the  SpecialOrthogonal  group  $\\mathrm{SO}(n)$  on a Euclidean-like manifold  M  of dimension  n . source"},{"id":542,"pagetitle":"Group actions","title":"Manifolds.RotationAroundAxisAction","ref":"/manifolds/stable/features/group_actions/#Manifolds.RotationAroundAxisAction","content":" Manifolds.RotationAroundAxisAction  ‚Äî  Type RotationAroundAxisAction(axis::AbstractVector) Space of actions of the circle group  RealCircleGroup  on  $‚Ñù^3$  around given  axis . source"},{"id":543,"pagetitle":"Group actions","title":"Manifolds.RowwiseMultiplicationAction","ref":"/manifolds/stable/features/group_actions/#Manifolds.RowwiseMultiplicationAction","content":" Manifolds.RowwiseMultiplicationAction  ‚Äî  Type RowwiseMultiplicationAction{\n    TAD<:ActionDirection,\n    TM<:AbstractManifold,\n    TO<:GeneralUnitaryMultiplicationGroup,\n} <: AbstractGroupAction{TAD} Action of the (special) unitary or orthogonal group  GeneralUnitaryMultiplicationGroup  of type  On  columns of points on a matrix manifold  M . Constructor RowwiseMultiplicationAction(\n    M::AbstractManifold,\n    On::GeneralUnitaryMultiplicationGroup,\n    AD::ActionDirection = LeftAction(),\n) source"},{"id":544,"pagetitle":"Group actions","title":"Manifolds.apply","ref":"/manifolds/stable/features/group_actions/#Manifolds.apply-Tuple{ComplexPlanarRotation, Complex, Any}","content":" Manifolds.apply  ‚Äî  Method apply(A::ComplexPlanarRotation, g::Complex, p) Rotate point  p  from  Euclidean(2)  manifold by the group element  g . The formula reads \\[p_{rot} =  \\begin{bmatrix}\n\\cos(Œ∏) & \\sin(Œ∏)\\\\\n-\\sin(Œ∏) & \\cos(Œ∏)\n\\end{bmatrix} p,\\] where  Œ∏  is the argument of complex number  g . source"},{"id":545,"pagetitle":"Group actions","title":"Manifolds.apply","ref":"/manifolds/stable/features/group_actions/#Manifolds.apply-Tuple{Manifolds.RotationAroundAxisAction, Any, Any}","content":" Manifolds.apply  ‚Äî  Method apply(A::RotationAroundAxisAction, Œ∏, p) Rotate point  p  from  Euclidean(3)  manifold around axis  A.axis  by angle  Œ∏ . The formula reads \\[p_{rot} = (\\cos(Œ∏))p + (k√óp) \\sin(Œ∏) + k (k‚ãÖp) (1-\\cos(Œ∏)),\\] where  $k$  is the vector  A.axis  and  ‚ãÖ  is the dot product. source"},{"id":546,"pagetitle":"Group actions","title":"Manifolds.apply","ref":"/manifolds/stable/features/group_actions/#Manifolds.apply-Tuple{QuaternionRotation, Quaternions.Quaternion, StaticArraysCore.SVector}","content":" Manifolds.apply  ‚Äî  Method apply(A::QuaternionRotation, g::Quaternion, p) Rotate point  p  from  Euclidean (3)  manifold through conjugation by the group element  g . The formula reads \\[(0, p_{rot,x}, p_{rot,y}, p_{rot,z}) = g ‚ãÖ (0, p_x, p_y, p_z) ‚ãÖ g^{\\mathrm{H}}\\] where  $(0, p_x, p_y, p_z)$  is quaternion with non-real coefficients from encoding the point  p  and  $g^{\\mathrm{H}}$  is quaternion conjugate of  $g$ . source"},{"id":547,"pagetitle":"Group actions","title":"Manifolds.optimal_alignment","ref":"/manifolds/stable/features/group_actions/#Manifolds.optimal_alignment-Tuple{Manifolds.ColumnwiseMultiplicationAction{LeftAction, <:AbstractManifold, <:Manifolds.GeneralUnitaryMultiplicationGroup}, Any, Any}","content":" Manifolds.optimal_alignment  ‚Äî  Method optimal_alignment(A::LeftColumnwiseMultiplicationAction, p, q) Compute optimal alignment for the left  ColumnwiseMultiplicationAction , i.e. the group element  $O^{*}$  that, when it acts on  p , returns the point closest to  q . Details of computation are described in Section 2.2.1 of [ SK16 ]. The formula reads \\[O^{*} = \\begin{cases}\nUV^T & \\text{if } \\operatorname{det}(p q^{\\mathrm{T}}) \\geq 0\\\\\nU K V^{\\mathrm{T}} & \\text{otherwise}\n\\end{cases}\\] where  $U \\Sigma V^{\\mathrm{T}}$  is the SVD decomposition of  $p q^{\\mathrm{T}}$  and  $K$  is the unit diagonal matrix with the last element on the diagonal replaced with -1. source"},{"id":548,"pagetitle":"Group actions","title":"Manifolds.quaternion_rotation_matrix","ref":"/manifolds/stable/features/group_actions/#Manifolds.quaternion_rotation_matrix-Tuple{Quaternions.Quaternion}","content":" Manifolds.quaternion_rotation_matrix  ‚Äî  Method quaternion_rotation_matrix(g::Quaternions.Quaternion) Compute rotation matrix for  RotationAction  corresponding to  QuaternionRotation  by  g . See https://www.songho.ca/opengl/gl_quaternion.html for details. source"},{"id":549,"pagetitle":"Group actions","title":"Translation action","ref":"/manifolds/stable/features/group_actions/#Translation-action","content":" Translation action"},{"id":550,"pagetitle":"Group actions","title":"Manifolds.TranslationAction","ref":"/manifolds/stable/features/group_actions/#Manifolds.TranslationAction","content":" Manifolds.TranslationAction  ‚Äî  Type TranslationAction(\n    M::AbstractManifold,\n    Rn::TranslationGroup,\n    AD::ActionDirection = LeftAction(),\n) Space of actions of the  TranslationGroup $\\mathrm{T}(n)$  on a Euclidean-like manifold  M . The left and right actions are equivalent. source"},{"id":551,"pagetitle":"Group actions","title":"Rotation-translation action (special Euclidean)","ref":"/manifolds/stable/features/group_actions/#Rotation-translation-action-(special-Euclidean)","content":" Rotation-translation action (special Euclidean)"},{"id":552,"pagetitle":"Group actions","title":"Manifolds.ColumnwiseSpecialEuclideanAction","ref":"/manifolds/stable/features/group_actions/#Manifolds.ColumnwiseSpecialEuclideanAction","content":" Manifolds.ColumnwiseSpecialEuclideanAction  ‚Äî  Type ColumnwiseSpecialEuclideanAction{\n    TM<:AbstractManifold,\n    TSE<:SpecialEuclidean,\n    TAD<:ActionDirection,\n} <: AbstractGroupAction{TAD} Action of the special Euclidean group  SpecialEuclidean  of type  SE  columns of points on a matrix manifold  M . Constructor ColumnwiseSpecialEuclideanAction(\n    M::AbstractManifold,\n    SE::SpecialEuclidean,\n    AD::ActionDirection = LeftAction(),\n) source"},{"id":553,"pagetitle":"Group actions","title":"Manifolds.RotationTranslationAction","ref":"/manifolds/stable/features/group_actions/#Manifolds.RotationTranslationAction","content":" Manifolds.RotationTranslationAction  ‚Äî  Type RotationTranslationAction(\n    M::AbstractManifold,\n    SOn::SpecialEuclidean,\n    AD::ActionDirection = LeftAction(),\n) Space of actions of the  SpecialEuclidean  group  $\\mathrm{SE}(n)$  on a Euclidean-like manifold  M  of dimension  n . Left actions corresponds to active transformations while right actions can be identified with passive transformations for a particular choice of a basis. source"},{"id":554,"pagetitle":"Group actions","title":"Manifolds.RotationTranslationActionOnVector","ref":"/manifolds/stable/features/group_actions/#Manifolds.RotationTranslationActionOnVector","content":" Manifolds.RotationTranslationActionOnVector  ‚Äî  Type RotationTranslationActionOnVector{TAD,ùîΩ,TE,TSE} Alias for  RotationTranslationAction  where the manifold  M  is  Euclidean  or  TranslationGroup  with size of type  TE , and  SpecialEuclidean  group has size type  TSE . source"},{"id":555,"pagetitle":"Group actions","title":"Manifolds.apply_diff_group","ref":"/manifolds/stable/features/group_actions/#Manifolds.apply_diff_group-Tuple{RotationTranslationActionOnVector{LeftAction, ùîΩ, TE} where {ùîΩ, TE}, Identity{Manifolds.SemidirectProductOperation{RotationAction{LeftAction, TranslationGroup{N, ‚Ñù}, SpecialOrthogonal{N}}}} where N, Any, Any}","content":" Manifolds.apply_diff_group  ‚Äî  Method apply_diff_group(\n    ::RotationTranslationActionOnVector{LeftAction},\n    ::SpecialEuclideanIdentity,\n    X,\n    p,\n) Compute differential of  apply  on left  RotationTranslationActionOnVector ,  with respect to  a  at identity, i.e. left-multiply point  p  by  X.x[2] . source"},{"id":558,"pagetitle":"Integration","title":"Integration","ref":"/manifolds/stable/features/integration/#Integration","content":" Integration"},{"id":559,"pagetitle":"Integration","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/features/integration/#Manifolds.manifold_volume-Tuple{AbstractManifold}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(M::AbstractManifold) Volume of manifold  M  defined through integration of Riemannian volume element in a chart. Note that for many manifolds there is no universal agreement over the exact ranges over which the integration should happen. For details see [ BST03 ]. source"},{"id":560,"pagetitle":"Integration","title":"Manifolds.volume_density","ref":"/manifolds/stable/features/integration/#Manifolds.volume_density-Tuple{AbstractManifold, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(M::AbstractManifold, p, X) Volume density function of manifold  M , i.e. determinant of the differential of exponential map  exp(M, p, X) . Determinant can be understood as computed in a basis, from the matrix of the linear operator said differential corresponds to. Details are available in Section 4.1 of [ CLLD22 ]. Note that volume density is well-defined only for  X  for which  exp(M, p, X)  is injective. source"},{"id":563,"pagetitle":"Statistics","title":"Statistics","ref":"/manifolds/stable/features/statistics/#Statistics","content":" Statistics"},{"id":564,"pagetitle":"Statistics","title":"Statistics.cov","ref":"/manifolds/stable/features/statistics/#Statistics.cov-Tuple{AbstractManifold, AbstractVector}","content":" Statistics.cov  ‚Äî  Method Statistics.cov(\n    M::AbstractManifold,\n    x::AbstractVector;\n    basis::AbstractBasis=DefaultOrthonormalBasis(),\n    tangent_space_covariance_estimator::CovarianceEstimator=SimpleCovariance(;\n        corrected=true,\n    ),\n    mean_estimation_method::AbstractApproximationMethod=GradientDescentEstimation(),\n    inverse_retraction_method::AbstractInverseRetractionMethod=default_inverse_retraction_method(\n        M, eltype(x),\n    ),\n) Estimate the covariance matrix of a set of points  x  on manifold  M . Since the covariance matrix on a manifold is a rank 2 tensor, the function returns its coefficients in basis induced by the given tangent space basis. See Section 5 of [ Pen06 ] for details. The mean is calculated using the specified  mean_estimation_method  using [mean](@ref Statistics.mean(::AbstractManifold, ::AbstractVector, ::AbstractApproximationMethod), and tangent vectors at this mean are calculated using the provided  inverse_retraction_method . Finally, the covariance matrix in the tangent plane is estimated using the Euclidean space  estimator  tangent_space_covariance_estimator . The type  CovarianceEstimator  is defined  in  StatsBase.jl   and examples of covariance estimation methods can be found in   CovarianceEstimation.jl . source"},{"id":565,"pagetitle":"Statistics","title":"Statistics.mean!","ref":"/manifolds/stable/features/statistics/#Statistics.mean!-Tuple{AbstractManifold, Vararg{Any}}","content":" Statistics.mean!  ‚Äî  Method mean!(M::AbstractManifold, y, x::AbstractVector[, w::AbstractWeights]; kwargs...)\nmean!(\n    M::AbstractManifold,\n    y,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method::AbstractApproximationMethod;\n    kwargs...,\n) Compute the  mean  in-place in  y . source"},{"id":566,"pagetitle":"Statistics","title":"Statistics.mean","ref":"/manifolds/stable/features/statistics/#Statistics.mean-Tuple{AbstractManifold, AbstractVector, AbstractVector, ExtrinsicEstimation}","content":" Statistics.mean  ‚Äî  Method mean(\n    M::AbstractManifold,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method::ExtrinsicEstimation;\n    kwargs...,\n) Estimate the Riemannian center of mass of  x  using  ExtrinsicEstimation , i.e. by computing the mean in the embedding and projecting the result back. See  mean  for a description of the remaining  kwargs . source"},{"id":567,"pagetitle":"Statistics","title":"Statistics.mean","ref":"/manifolds/stable/features/statistics/#Statistics.mean-Tuple{AbstractManifold, AbstractVector, AbstractVector, GeodesicInterpolationWithinRadius}","content":" Statistics.mean  ‚Äî  Method mean(\n    M::AbstractManifold,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method::GeodesicInterpolationWithinRadius;\n    kwargs...,\n) Estimate the Riemannian center of mass of  x  using  GeodesicInterpolationWithinRadius . See  mean  for a description of  kwargs . source"},{"id":568,"pagetitle":"Statistics","title":"Statistics.mean","ref":"/manifolds/stable/features/statistics/#Statistics.mean-Tuple{AbstractManifold, AbstractVector, AbstractVector, GeodesicInterpolation}","content":" Statistics.mean  ‚Äî  Method mean(\n    M::AbstractManifold,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method::GeodesicInterpolation;\n    shuffle_rng=nothing,\n    retraction::AbstractRetractionMethod = default_retraction_method(M, eltype(x)),\n    inverse_retraction::AbstractInverseRetractionMethod = default_inverse_retraction_method(M, eltype(x)),\n    kwargs...,\n) Estimate the Riemannian center of mass of  x  in an online fashion using repeated weighted geodesic interpolation. See  GeodesicInterpolation  for details. If  shuffle_rng  is provided, it is used to shuffle the order in which the points are considered for computing the mean. Optionally, pass  retraction  and  inverse_retraction  method types to specify the (inverse) retraction. source"},{"id":569,"pagetitle":"Statistics","title":"Statistics.mean","ref":"/manifolds/stable/features/statistics/#Statistics.mean-Tuple{AbstractManifold, Vararg{Any}}","content":" Statistics.mean  ‚Äî  Method mean(M::AbstractManifold, x::AbstractVector[, w::AbstractWeights]; kwargs...) Compute the (optionally weighted) Riemannian center of mass also known as Karcher mean of the vector  x  of points on the  AbstractManifold M , defined as the point that satisfies the minimizer \\[\\operatorname{argmin}_{y ‚àà \\mathcal M} \\frac{1}{2 \\sum_{i=1}^n w_i} \\sum_{i=1}^n w_i\\mathrm{d}_{\\mathcal M}^2(y,x_i),\\] where  $\\mathrm{d}_{\\mathcal M}$  denotes the Riemannian  distance . In the general case, the  GradientDescentEstimation  is used to compute the mean.     mean(         M::AbstractManifold,         x::AbstractVector,         [w::AbstractWeights,]         method::AbstractApproximationMethod=default approximation method(M, mean);         kwargs...,     ) Compute the mean using the specified  method . mean(\n    M::AbstractManifold,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method::GradientDescentEstimation;\n    p0=x[1],\n    stop_iter=100,\n    retraction::AbstractRetractionMethod = default_retraction_method(M),\n    inverse_retraction::AbstractInverseRetractionMethod = default_retraction_method(M, eltype(x)),\n    kwargs...,\n) Compute the mean using the gradient descent scheme  GradientDescentEstimation . Optionally, provide  p0 , the starting point (by default set to the first data point).  stop_iter  denotes the maximal number of iterations to perform and the  kwargs...  are passed to  isapprox  to stop, when the minimal change between two iterates is small. For more stopping criteria check the  Manopt.jl  package and use a solver therefrom. Optionally, pass  retraction  and  inverse_retraction  method types to specify the (inverse) retraction. The Theory stems from [ Kar77 ] and is also described in [ PA12 ] as the exponential barycenter. The algorithm is further described in[ ATV13 ]. source"},{"id":570,"pagetitle":"Statistics","title":"Statistics.median!","ref":"/manifolds/stable/features/statistics/#Statistics.median!-Tuple{AbstractManifold, Vararg{Any}}","content":" Statistics.median!  ‚Äî  Method median!(M::AbstractManifold, y, x::AbstractVector[, w::AbstractWeights]; kwargs...)\nmedian!(\n    M::AbstractManifold,\n    y,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method::AbstractApproximationMethod;\n    kwargs...,\n) computes the  median  in-place in  y . source"},{"id":571,"pagetitle":"Statistics","title":"Statistics.median","ref":"/manifolds/stable/features/statistics/#Statistics.median-Tuple{AbstractManifold, AbstractVector, AbstractVector, CyclicProximalPointEstimation}","content":" Statistics.median  ‚Äî  Method median(\n    M::AbstractManifold,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method::CyclicProximalPointEstimation;\n    p0=x[1],\n    stop_iter=1000000,\n    retraction::AbstractRetractionMethod = default_retraction_method(M, eltype(x),),\n    inverse_retraction::AbstractInverseRetractionMethod = default_inverse_retraction_method(M, eltype(x),),\n    kwargs...,\n) Compute the median using  CyclicProximalPointEstimation . Optionally, provide  p0 , the starting point (by default set to the first data point).  stop_iter  denotes the maximal number of iterations to perform and the  kwargs...  are passed to  isapprox  to stop, when the minimal change between two iterates is small. For more stopping criteria check the  Manopt.jl  package and use a solver therefrom. Optionally, pass  retraction  and  inverse_retraction  method types to specify the (inverse) retraction. The algorithm is further described in [ Bac14 ]. source"},{"id":572,"pagetitle":"Statistics","title":"Statistics.median","ref":"/manifolds/stable/features/statistics/#Statistics.median-Tuple{AbstractManifold, AbstractVector, AbstractVector, ExtrinsicEstimation}","content":" Statistics.median  ‚Äî  Method median(\n    M::AbstractManifold,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method::ExtrinsicEstimation;\n    kwargs...,\n) Estimate the median of  x  using  ExtrinsicEstimation , i.e. by computing the median in the embedding and projecting the result back. See  median  for a description of  kwargs . source"},{"id":573,"pagetitle":"Statistics","title":"Statistics.median","ref":"/manifolds/stable/features/statistics/#Statistics.median-Tuple{AbstractManifold, AbstractVector, AbstractVector, WeiszfeldEstimation}","content":" Statistics.median  ‚Äî  Method median(\n    M::AbstractManifold,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method::WeiszfeldEstimation;\n    Œ± = 1.0,\n    p0=x[1],\n    stop_iter=2000,\n    retraction::AbstractRetractionMethod = default_retraction_method(M, eltype(x)),\n    inverse_retraction::AbstractInverseRetractionMethod = default_inverse_retraction_method(M, eltype(x)),\n    kwargs...,\n) Compute the median using  WeiszfeldEstimation . Optionally, provide  p0 , the starting point (by default set to the first data point).  stop_iter  denotes the maximal number of iterations to perform and the  kwargs...  are passed to  isapprox  to stop, when the minimal change between two iterates is small. For more stopping criteria check the  Manopt.jl  package and use a solver therefrom. The parameter  $Œ±\\in (0,2]$  is a step size. The algorithm is further described in [ FVJ08 ], especially the update rule in Eq. (6), i.e. Let  $q_{k}$  denote the current iterate,  $n$  the number of points  $x_1,\\ldots,x_n$ , and \\[I_k = \\bigl\\{ i \\in \\{1,\\ldots,n\\} \\big| x_i \\neq q_k \\bigr\\}\\] all indices of points that are not equal to the current iterate. Then the update reads  $q_{k+1} = \\exp_{q_k}(Œ±X)$ , where \\[X = \\frac{1}{s}\\sum_{i\\in I_k} \\frac{w_i}{d_{\\mathcal M}(q_k,x_i)}\\log_{q_k}x_i\n\\quad\n\\text{ with }\n\\quad\ns = \\sum_{i\\in I_k} \\frac{w_i}{d_{\\mathcal M}(q_k,x_i)},\\] and where  $\\mathrm{d}_{\\mathcal M}$  denotes the Riemannian  distance . Optionally, pass  retraction  and  inverse_retraction  method types to specify the (inverse) retraction, which by default use the exponential and logarithmic map, respectively. source"},{"id":574,"pagetitle":"Statistics","title":"Statistics.median","ref":"/manifolds/stable/features/statistics/#Statistics.median-Tuple{AbstractManifold, Vararg{Any}}","content":" Statistics.median  ‚Äî  Method median(M::AbstractManifold, x::AbstractVector[, w::AbstractWeights]; kwargs...)\nmedian(\n    M::AbstractManifold,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method::AbstractApproximationMethod;\n    kwargs...,\n) Compute the (optionally weighted) Riemannian median of the vector  x  of points on the  AbstractManifold M , defined as the point that satisfies the minimizer \\[\\operatorname{argmin}_{y ‚àà \\mathcal M} \\frac{1}{\\sum_{i=1}^n w_i} \\sum_{i=1}^n w_i\\mathrm{d}_{\\mathcal M}(y,x_i),\\] where  $\\mathrm{d}_{\\mathcal M}$  denotes the Riemannian  distance . This function is nonsmooth (i.e nondifferentiable). In the general case, the  CyclicProximalPointEstimation  is used to compute the median. However, this default may be overloaded for specific manifolds. Compute the median using the specified  method . source"},{"id":575,"pagetitle":"Statistics","title":"Statistics.std","ref":"/manifolds/stable/features/statistics/#Statistics.std-Tuple{AbstractManifold, Vararg{Any}}","content":" Statistics.std  ‚Äî  Method std(M, x, m=mean(M, x); corrected=true, kwargs...)\nstd(M, x, w::AbstractWeights, m=mean(M, x, w); corrected=false, kwargs...) compute the optionally weighted standard deviation of a  Vector x  of  n  data points on the  AbstractManifold M , i.e. \\[\\sqrt{\\frac{1}{c} \\sum_{i=1}^n w_i d_{\\mathcal M}^2 (x_i,m)},\\] where  c  is a correction term, see  Statistics.std . The mean of  x  can be specified as  m , and the corrected variance can be activated by setting  corrected=true . source"},{"id":576,"pagetitle":"Statistics","title":"Statistics.var","ref":"/manifolds/stable/features/statistics/#Statistics.var-Tuple{AbstractManifold, Any}","content":" Statistics.var  ‚Äî  Method var(M, x, m=mean(M, x); corrected=true)\nvar(M, x, w::AbstractWeights, m=mean(M, x, w); corrected=false) compute the (optionally weighted) variance of a  Vector x  of  n  data points on the  AbstractManifold M , i.e. \\[\\frac{1}{c} \\sum_{i=1}^n w_i d_{\\mathcal M}^2 (x_i,m),\\] where  c  is a correction term, see  Statistics.var . The mean of  x  can be specified as  m , and the corrected variance can be activated by setting  corrected=true . All further  kwargs...  are passed to the computation of the mean (if that is not provided). source"},{"id":577,"pagetitle":"Statistics","title":"StatsBase.kurtosis","ref":"/manifolds/stable/features/statistics/#StatsBase.kurtosis-Tuple{AbstractManifold, AbstractVector, StatsBase.AbstractWeights}","content":" StatsBase.kurtosis  ‚Äî  Method kurtosis(M::AbstractManifold, x::AbstractVector, k::Int[, w::AbstractWeights], m=mean(M, x[, w])) Compute the excess kurtosis of points in  x  on manifold  M . Optionally provide weights  w  and/or a precomputed  mean m . source"},{"id":578,"pagetitle":"Statistics","title":"StatsBase.mean_and_std","ref":"/manifolds/stable/features/statistics/#StatsBase.mean_and_std-Tuple{AbstractManifold, Vararg{Any}}","content":" StatsBase.mean_and_std  ‚Äî  Method mean_and_std(M::AbstractManifold, x::AbstractVector[, w::AbstractWeights]; kwargs...) -> (mean, std) Compute the  mean  and the standard deviation  std  simultaneously. mean_and_std(\n    M::AbstractManifold,\n    x::AbstractVector\n    [w::AbstractWeights,]\n    method::AbstractApproximationMethod;\n    kwargs...,\n) -> (mean, var) Use the  method  for simultaneously computing the mean and standard deviation. To use a mean-specific method, call  mean  and then  std . source"},{"id":579,"pagetitle":"Statistics","title":"StatsBase.mean_and_var","ref":"/manifolds/stable/features/statistics/#StatsBase.mean_and_var-Tuple{AbstractManifold, AbstractVector, StatsBase.AbstractWeights, GeodesicInterpolationWithinRadius}","content":" StatsBase.mean_and_var  ‚Äî  Method mean_and_var(\n    M::AbstractManifold,\n    x::AbstractVector\n    [w::AbstractWeights,]\n    method::GeodesicInterpolationWithinRadius;\n    kwargs...,\n) -> (mean, var) Use repeated weighted geodesic interpolation to estimate the mean. Simultaneously, use a Welford-like recursion to estimate the variance. See  GeodesicInterpolationWithinRadius  and  mean_and_var  for more information. source"},{"id":580,"pagetitle":"Statistics","title":"StatsBase.mean_and_var","ref":"/manifolds/stable/features/statistics/#StatsBase.mean_and_var-Tuple{AbstractManifold, AbstractVector, StatsBase.AbstractWeights, GeodesicInterpolation}","content":" StatsBase.mean_and_var  ‚Äî  Method mean_and_var(\n    M::AbstractManifold,\n    x::AbstractVector\n    [w::AbstractWeights,]\n    method::GeodesicInterpolation;\n    shuffle_rng::Union{AbstractRNG,Nothing} = nothing,\n    retraction::AbstractRetractionMethod = default_retraction_method(M, eltype(x)),\n    inverse_retraction::AbstractInverseRetractionMethod = default_inverse_retraction_method(M, eltype(x)),\n    kwargs...,\n) -> (mean, var) Use the repeated weighted geodesic interpolation to estimate the mean. Simultaneously, use a Welford-like recursion to estimate the variance. If  shuffle_rng  is provided, it is used to shuffle the order in which the points are considered. Optionally, pass  retraction  and  inverse_retraction  method types to specify the (inverse) retraction. See  GeodesicInterpolation  for details on the geodesic interpolation method. Note The Welford algorithm for the variance is experimental and is not guaranteed to give accurate results except on  Euclidean . source"},{"id":581,"pagetitle":"Statistics","title":"StatsBase.mean_and_var","ref":"/manifolds/stable/features/statistics/#StatsBase.mean_and_var-Tuple{AbstractManifold, Vararg{Any}}","content":" StatsBase.mean_and_var  ‚Äî  Method mean_and_var(M::AbstractManifold, x::AbstractVector[, w::AbstractWeights]; kwargs...) -> (mean, var) Compute the  mean  and the  var iance simultaneously. See those functions for a description of the arguments. mean_and_var(\n    M::AbstractManifold,\n    x::AbstractVector\n    [w::AbstractWeights,]\n    method::AbstractApproximationMethod;\n    kwargs...,\n) -> (mean, var) Use the  method  for simultaneously computing the mean and variance. To use a mean-specific method, call  mean  and then  var . source"},{"id":582,"pagetitle":"Statistics","title":"StatsBase.moment","ref":"/manifolds/stable/features/statistics/#StatsBase.moment","content":" StatsBase.moment  ‚Äî  Function moment(M::AbstractManifold, x::AbstractVector, k::Int[, w::AbstractWeights], m=mean(M, x[, w])) Compute the  k th central moment of points in  x  on manifold  M . Optionally provide weights  w  and/or a precomputed  mean . source"},{"id":583,"pagetitle":"Statistics","title":"StatsBase.skewness","ref":"/manifolds/stable/features/statistics/#StatsBase.skewness-Tuple{AbstractManifold, AbstractVector, StatsBase.AbstractWeights}","content":" StatsBase.skewness  ‚Äî  Method skewness(M::AbstractManifold, x::AbstractVector, k::Int[, w::AbstractWeights], m=mean(M, x[, w])) Compute the standardized skewness of points in  x  on manifold  M . Optionally provide weights  w  and/or a precomputed  mean m . source"},{"id":584,"pagetitle":"Statistics","title":"Literature","ref":"/manifolds/stable/features/statistics/#Literature","content":" Literature [ATV13] B.¬†Afsari, R.¬†Tron and R.¬†Vidal.  On the Convergence of Gradient Descent for Finding the Riemannian Center of Mass .  SIAM¬†Journal¬†on¬†Control¬†and¬†Optimization  51 , 2230‚Äì2260  (2013),  arXiv:1201.0925 . [Bac14] M.¬†Baƒç√°k.  Computing medians and means in Hadamard spaces .  SIAM¬†Journal¬†on¬†Optimization  24 , 1542‚Äì1566  (2014),  arXiv:1210.2145 . [FVJ08] P.¬†T.¬†Fletcher, S.¬†Venkatasubramanian and S.¬†Joshi.  Robust statistics on Riemannian manifolds via the geometric median . In:  2008 IEEE Conference on Computer Vision and Pattern Recognition  (2008). [Kar77] H.¬†Karcher.  Riemannian center of mass and mollifier smoothing .  Communications¬†on¬†Pure¬†and¬†Applied¬†Mathematics  30 , 509‚Äì541  (1977). [Pen06] X.¬†Pennec.  Intrinsic Statistics on Riemannian Manifolds: Basic Tools for Geometric Measurements .  Journal¬†of¬†Mathematical¬†Imaging¬†and¬†Vision  25 , 127‚Äì154  (2006). [PA12] X.¬†Pennec and V.¬†Arsigny.  Exponential Barycenters of the Canonical Cartan Connection and Invariant Means on Lie Groups . In:  Matrix Information Geometry  (Springer, Berlin, Heidelberg, 2012); pp.¬†123‚Äì166,  arXiv:00699361 ."},{"id":587,"pagetitle":"Testing","title":"Testing","ref":"/manifolds/stable/features/testing/#Testing","content":" Testing Documentation for testing utilities for  Manifolds.jl . The function  test_manifold  can be used to verify that your manifold correctly implements the  Manifolds.jl  interface. Similarly  test_group  and  test_action  can be used to verify implementation of groups and group actions."},{"id":588,"pagetitle":"Testing","title":"Manifolds.test_action","ref":"/manifolds/stable/features/testing/#Manifolds.test_action","content":" Manifolds.test_action  ‚Äî  Function test_action(\n    A::AbstractGroupAction,\n    a_pts::AbstractVector,\n    m_pts::AbstractVector,\n    X_pts = [];\n    atol = 1e-10,\n    atol_ident_compose = 0,\n    test_optimal_alignment = false,\n    test_mutating_group=true,\n    test_mutating_action=true,\n    test_diff = false,\n    test_switch_direction = true,\n) Tests general properties of the action  A , given at least three different points that lie on it (contained in  a_pts ) and three different point that lie on the manifold it acts upon (contained in  m_pts ). Arguments atol_ident_compose = 0 : absolute tolerance for the test that composition with identity doesn't change the group element. source"},{"id":589,"pagetitle":"Testing","title":"Manifolds.test_group","ref":"/manifolds/stable/features/testing/#Manifolds.test_group","content":" Manifolds.test_group  ‚Äî  Function test_group(\n    G,\n    g_pts::AbstractVector,\n    X_pts::AbstractVector=[],\n    Xe_pts::AbstractVector=[];\n    atol::Real=1e-10,\n    test_mutating::Bool=true,\n    test_exp_lie_log::Bool=true,\n    test_diff::Bool=false,\n    test_invariance::Bool=false,\n    test_lie_bracket::Bool=false,\n    test_adjoint_action::Bool=false,\n    test_inv_diff::Bool=false,\n    test_adjoint_inv_diff::Bool=false,\n    test_apply_diff_group::Bool=false,\n    diff_convs = [(), (LeftForwardAction(),), (RightBackwardAction(),)],\n) Tests general properties of the group  G , given at least three different points elements of it (contained in  g_pts ). Optionally, specify  test_diff  to test differentials of translation, using  X_pts , which must contain at least one tangent vector at  g_pts[1] , and the direction conventions specified in  diff_convs .  Xe_pts  should contain tangent vectors at identity for testing Lie algebra operations. If the group is equipped with an invariant metric,  test_invariance  indicates that the invariance should be checked for the provided points. source"},{"id":590,"pagetitle":"Testing","title":"Manifolds.test_manifold","ref":"/manifolds/stable/features/testing/#Manifolds.test_manifold","content":" Manifolds.test_manifold  ‚Äî  Function test_manifold(\n    M::AbstractManifold,\n    pts::AbstractVector;\n    args,\n) Test general properties of manifold  M , given at least three different points that lie on it (contained in  pts ). Arguments basis_has_specialized_diagonalizing_get = false : if true, assumes that    DiagonalizingOrthonormalBasis  given in  basis_types  has    get_coordinates  and  get_vector  that work without caching. basis_types_to_from = () : basis types that will be tested based on    get_coordinates  and  get_vector . basis_types_vecs = ()  : basis types that will be tested based on  get_vectors default_inverse_retraction_method = ManifoldsBase.LogarithmicInverseRetraction() :   default method for inverse retractions ( log . default_retraction_method = ManifoldsBase.ExponentialRetraction() : default method for   retractions ( exp ). exp_log_atol_multiplier = 0 : change absolute tolerance of exp/log tests   (0 use default, i.e. deactivate atol and use rtol). exp_log_rtol_multiplier = 1 : change the relative tolerance of exp/log tests   (1 use default). This is deactivated if the  exp_log_atol_multiplier  is nonzero. expected_dimension_type = Integer : expected type of value returned by    manifold_dimension . inverse_retraction_methods = [] : inverse retraction methods that will be tested. is_mutating = true : whether mutating variants of functions should be tested. is_point_atol_multiplier = 0 : determines atol of  is_point  checks. is_tangent_atol_multiplier = 0 : determines atol of  is_vector  checks. has_get_embedding = false : whether the manifold has a specialized    get_embedding (M, p)  method (to test mutating  embed! ). This is experimental. mid_point12 = test_exp_log ? shortest_geodesic(M, pts[1], pts[2], 0.5) : nothing : if not  nothing , then check   that  mid_point(M, pts[1], pts[2])  is approximately equal to  mid_point12 . This is   by default set to  nothing  if  text_exp_log  is set to false. point_distributions = []  : point distributions to test. rand_tvector_atol_multiplier = 0  : chage absolute tolerance in testing random vectors   (0 use default, i.e. deactivate atol and use rtol) random tangent vectors are tangent   vectors. retraction_atol_multiplier = 0 : change absolute tolerance of (inverse) retraction tests   (0 use default, i.e. deactivate atol and use rtol). retraction_rtol_multiplier = 1 : change the relative tolerance of (inverse) retraction   tests (1 use default). This is deactivated if the  exp_log_atol_multiplier  is nonzero. retraction_methods = [] : retraction methods that will be tested. test_atlases = [] : Vector or tuple of atlases that should be tested. test_exp_log = true : if true, check that  exp  is the inverse of  log . test_injectivity_radius = true : whether implementation of  injectivity_radius    should be tested. test_inplace = false  : if true check if inplace variants work if they are activated,  e.g. check that  exp!(M, p, p, X)  work if  test_exp_log = true .  This in general requires  is_mutating  to be true. test_is_tangent : if true check that the  default_inverse_retraction_method    actually returns valid tangent vectors. test_musical_isomorphisms = false  : test musical isomorphisms. test_mutating_rand = false  : test the mutating random function for points on manifolds. test_project_point = false : test projections onto the manifold. test_project_tangent = false  : test projections on tangent spaces. test_representation_size = true  : test representation size of points/tvectprs. test_tangent_vector_broadcasting = true  : test boradcasting operators on TangentSpace. test_vector_spaces = true  : test Vector bundle of this manifold. test_default_vector_transport = false  : test the default vector transport (usually  parallel transport). test_vee_hat = false : test  vee  and  hat  functions. tvector_distributions = []  : tangent vector distributions to test. vector_transport_methods = [] : vector transport methods that should be tested. vector_transport_inverse_retractions = [default_inverse_retraction_method for _ in 1:length(vector_transport_methods)] ` inverse retractions to use with the vector transport method (especially the differentiated ones) vector_transport_to = [ true for _ in 1:length(vector_transport_methods)] : whether  to check the  to  variant of vector transport vector_transport_direction = [ true for _ in 1:length(vector_transport_methods)] : whether  to check the  direction  variant of vector transport source"},{"id":591,"pagetitle":"Testing","title":"Manifolds.find_eps","ref":"/manifolds/stable/features/testing/#Manifolds.find_eps","content":" Manifolds.find_eps  ‚Äî  Function find_eps(x...) Find an appropriate tolerance for given points or tangent vectors, or their types. source"},{"id":592,"pagetitle":"Testing","title":"Manifolds.test_parallel_transport","ref":"/manifolds/stable/features/testing/#Manifolds.test_parallel_transport","content":" Manifolds.test_parallel_transport  ‚Äî  Function test_parallel_transport(M,P; to=true, direction=true) Generic tests for parallel transport on  M given at least two pointsin  P . The single functions to transport  to  (a point) or (in a)  direction  are sub-tests that can be activated by the keywords arguments source"},{"id":595,"pagetitle":"Utilities","title":"Utilities","ref":"/manifolds/stable/features/utilities/#Utilities","content":" Utilities"},{"id":596,"pagetitle":"Utilities","title":"Ease of notation","ref":"/manifolds/stable/features/utilities/#Ease-of-notation","content":" Ease of notation The following terms introduce a nicer notation for some operations, for example using the ‚àà operator,  $p ‚àà \\mathcal M$  to determine whether  $p$  is a point on the  AbstractManifold $\\mathcal M$ ."},{"id":597,"pagetitle":"Utilities","title":"Base.in","ref":"/manifolds/stable/features/utilities/#Base.in","content":" Base.in  ‚Äî  Function Base.in(p, M::AbstractManifold; kwargs...)\np ‚àà M Check, whether a point  p  is a valid point (i.e. in) a  AbstractManifold M . This method employs  is_point  deactivating the error throwing option. source Base.in(p, TpM::TangentSpace; kwargs...)\nX ‚àà TangentSpace(M, p) Check whether  X  is a tangent vector from (in) the tangent space  $T_p\\mathcal M$ , i.e. the  TangentSpace  at  p  on the  AbstractManifold M . This method uses  is_vector  deactivating the error throw option. source"},{"id":598,"pagetitle":"Utilities","title":"Public documentation","ref":"/manifolds/stable/features/utilities/#Public-documentation","content":" Public documentation"},{"id":599,"pagetitle":"Utilities","title":"Manifolds.sectional_curvature_matrix","ref":"/manifolds/stable/features/utilities/#Manifolds.sectional_curvature_matrix","content":" Manifolds.sectional_curvature_matrix  ‚Äî  Function sectional_curvature_matrix(M::AbstractManifold, p, B::AbstractBasis) Compute the matrix of sectional curvatures of manifold  M  at point  p . Entry  (i, j)  corresponds to sectional curvature of the surface spanned by vectors  i   and  j  from basis  B . source"},{"id":600,"pagetitle":"Utilities","title":"Specific exception types","ref":"/manifolds/stable/features/utilities/#Specific-exception-types","content":" Specific exception types For some manifolds it is useful to keep an extra index, at which point on the manifold, the error occurred as well as to collect all errors that occurred on a manifold. This page contains the manifold-specific error messages this package introduces."},{"id":603,"pagetitle":"Centered matrices","title":"Centered matrices","ref":"/manifolds/stable/manifolds/centeredmatrices/#Centered-matrices","content":" Centered matrices"},{"id":604,"pagetitle":"Centered matrices","title":"Manifolds.CenteredMatrices","ref":"/manifolds/stable/manifolds/centeredmatrices/#Manifolds.CenteredMatrices","content":" Manifolds.CenteredMatrices  ‚Äî  Type CenteredMatrices{T,ùîΩ} <: AbstractDecoratorManifold{ùîΩ} The manifold of  $m√ón$  real-valued or complex-valued matrices whose columns sum to zero, i.e. \\[\\bigl\\{ p ‚àà ùîΩ^{m√ón}\\ \\big|\\ [1 ‚Ä¶ 1] * p = [0 ‚Ä¶ 0] \\bigr\\},\\] where  $ùîΩ ‚àà \\{‚Ñù,‚ÑÇ\\}$ . Constructor CenteredMatrices(m, n[, field=‚Ñù]; parameter::Symbol=:type) Generate the manifold of  m -by- n  ( field -valued) matrices whose columns sum to zero. parameter : whether a type parameter should be used to store  m  and  n . By default size is stored in type. Value can either be  :field  or  :type . source"},{"id":605,"pagetitle":"Centered matrices","title":"ManifoldsBase.Weingarten","ref":"/manifolds/stable/manifolds/centeredmatrices/#ManifoldsBase.Weingarten-Tuple{CenteredMatrices, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Y = Weingarten(M::CenteredMatrices, p, X, V)\nWeingarten!(M::CenteredMatrices, Y, p, X, V) Compute the Weingarten map  $\\mathcal W_p$  at  p  on the  CenteredMatrices M  with respect to the tangent vector  $X \\in T_p\\mathcal M$  and the normal vector  $V \\in N_p\\mathcal M$ . Since this a flat space by itself, the result is always the zero tangent vector. source"},{"id":606,"pagetitle":"Centered matrices","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/centeredmatrices/#ManifoldsBase.check_point-Union{Tuple{T}, Tuple{CenteredMatrices, T}} where T","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::CenteredMatrices, p; kwargs...) Check whether the matrix is a valid point on the  CenteredMatrices M , i.e. is an  m -by- n  matrix whose columns sum to zero. The tolerance for the column sums of  p  can be set using  kwargs... . source"},{"id":607,"pagetitle":"Centered matrices","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/centeredmatrices/#ManifoldsBase.check_vector-Union{Tuple{T}, Tuple{CenteredMatrices, Any, T}} where T","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::CenteredMatrices, p, X; kwargs... ) Check whether  X  is a tangent vector to manifold point  p  on the  CenteredMatrices M , i.e. that  X  is a matrix of size  (m, n)  whose columns sum to zero and its values are from the correct  AbstractNumbers . The tolerance for the column sums of  p  and  X  can be set using  kwargs... . source"},{"id":608,"pagetitle":"Centered matrices","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/centeredmatrices/#ManifoldsBase.is_flat-Tuple{CenteredMatrices}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::CenteredMatrices) Return true.  CenteredMatrices  is a flat manifold. source"},{"id":609,"pagetitle":"Centered matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/centeredmatrices/#ManifoldsBase.manifold_dimension-Union{Tuple{CenteredMatrices{<:Any, ùîΩ}}, Tuple{ùîΩ}} where ùîΩ","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::CenteredMatrices) Return the manifold dimension of the  CenteredMatrices m -by- n  matrix  M  over the number system  ùîΩ , i.e. \\[\\dim(\\mathcal M) = (m*n - n) \\dim_‚Ñù ùîΩ,\\] where  $\\dim_‚Ñù ùîΩ$  is the  real_dimension  of  ùîΩ . source"},{"id":610,"pagetitle":"Centered matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/centeredmatrices/#ManifoldsBase.project-Tuple{CenteredMatrices, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::CenteredMatrices, p, X) Project the matrix  X  onto the tangent space at  p  on the  CenteredMatrices M , i.e. \\[\\operatorname{proj}_p(X) = X - \\begin{bmatrix}\n1\\\\\n‚ãÆ\\\\\n1\n\\end{bmatrix} * [c_1 \\dots c_n],\\] where  $c_i = \\frac{1}{m}\\sum_{j=1}^m x_{j,i}$   for  $i = 1, \\dots, n$ . source"},{"id":611,"pagetitle":"Centered matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/centeredmatrices/#ManifoldsBase.project-Tuple{CenteredMatrices, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::CenteredMatrices, p) Projects  p  from the embedding onto the  CenteredMatrices M , i.e. \\[\\operatorname{proj}_{\\mathcal M}(p) = p - \\begin{bmatrix}\n1\\\\\n‚ãÆ\\\\\n1\n\\end{bmatrix} * [c_1 \\dots c_n],\\] where  $c_i = \\frac{1}{m}\\sum_{j=1}^m p_{j,i}$  for  $i = 1, \\dots, n$ . source"},{"id":614,"pagetitle":"Cholesky space","title":"Cholesky space","ref":"/manifolds/stable/manifolds/choleskyspace/#Cholesky-space","content":" Cholesky space The Cholesky space is a Riemannian manifold on the lower triangular matrices. Its metric is based on the cholesky decomposition. The  CholeskySpace  is used to define the  LogCholeskyMetric  on the manifold of   SymmetricPositiveDefinite  matrices."},{"id":615,"pagetitle":"Cholesky space","title":"Manifolds.CholeskySpace","ref":"/manifolds/stable/manifolds/choleskyspace/#Manifolds.CholeskySpace","content":" Manifolds.CholeskySpace  ‚Äî  Type CholeskySpace{T} <: AbstractManifold{‚Ñù} The manifold of lower triangular matrices with positive diagonal and a metric based on the Cholesky decomposition. The formulae for this manifold are for example summarized in Table 1 of [ Lin19 ]. Constructor CholeskySpace(n; parameter::Symbol=:type) Generate the manifold of  $n√ón$  lower triangular matrices with positive diagonal. source"},{"id":616,"pagetitle":"Cholesky space","title":"Base.exp","ref":"/manifolds/stable/manifolds/choleskyspace/#Base.exp-Tuple{CholeskySpace, Vararg{Any}}","content":" Base.exp  ‚Äî  Method exp(M::CholeskySpace, p, X) Compute the exponential map on the  CholeskySpace M  emanating from the lower triangular matrix with positive diagonal  p  towards the lower triangular matrix  X  The formula reads \\[\\exp_p X = ‚åä p ‚åã + ‚åä X ‚åã + \\operatorname{diag}(p)\n\\operatorname{diag}(p)\\exp\\bigl( \\operatorname{diag}(X)\\operatorname{diag}(p)^{-1}\\bigr),\\] where  $‚åä‚ãÖ‚åã$  denotes the strictly lower triangular matrix, and  $\\operatorname{diag}$  extracts the diagonal matrix. source"},{"id":617,"pagetitle":"Cholesky space","title":"Base.log","ref":"/manifolds/stable/manifolds/choleskyspace/#Base.log-Tuple{LinearAlgebra.Cholesky, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::CholeskySpace, X, p, q) Compute the logarithmic map on the  CholeskySpace M  for the geodesic emanating from the lower triangular matrix with positive diagonal  p  towards  q . The formula reads \\[\\log_p q = ‚åä p ‚åã - ‚åä q ‚åã + \\operatorname{diag}(p)\\log\\bigl(\\operatorname{diag}(q)\\operatorname{diag}(p)^{-1}\\bigr),\\] where  $‚åä‚ãÖ‚åã$  denotes the strictly lower triangular matrix, and  $\\operatorname{diag}$  extracts the diagonal matrix. source"},{"id":618,"pagetitle":"Cholesky space","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/choleskyspace/#ManifoldsBase.check_point-Union{Tuple{T}, Tuple{CholeskySpace, T}} where T","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::CholeskySpace, p; kwargs...) Check whether the matrix  p  lies on the  CholeskySpace M , i.e. it's size fits the manifold, it is a lower triangular matrix and has positive entries on the diagonal. The tolerance for the tests can be set using the  kwargs... . source"},{"id":619,"pagetitle":"Cholesky space","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/choleskyspace/#ManifoldsBase.check_vector-Tuple{CholeskySpace, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::CholeskySpace, p, X; kwargs... ) Check whether  v  is a tangent vector to  p  on the  CholeskySpace M , i.e. after  check_point (M,p) ,  X  has to have the same dimension as  p  and a symmetric matrix. The tolerance for the tests can be set using the  kwargs... . source"},{"id":620,"pagetitle":"Cholesky space","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/choleskyspace/#ManifoldsBase.distance-Tuple{CholeskySpace, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::CholeskySpace, p, q) Compute the Riemannian distance on the  CholeskySpace M  between two matrices  p ,  q  that are lower triangular with positive diagonal. The formula reads \\[d_{\\mathcal M}(p,q) = \\sqrt{\\sum_{i>j} (p_{ij}-q_{ij})^2 +\n\\sum_{j=1}^m (\\log p_{jj} - \\log q_{jj})^2\n}\\] source"},{"id":621,"pagetitle":"Cholesky space","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/choleskyspace/#ManifoldsBase.inner-Tuple{CholeskySpace, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::CholeskySpace, p, X, Y) Compute the inner product on the  CholeskySpace M  at the lower triangular matrix with positive diagonal  p  and the two tangent vectors  X , Y , i.e they are both lower triangular matrices with arbitrary diagonal. The formula reads \\[g_p(X,Y) = \\sum_{i>j} X_{ij}Y_{ij} + \\sum_{j=1}^m X_{ii}Y_{ii}p_{ii}^{-2}\\] source"},{"id":622,"pagetitle":"Cholesky space","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/choleskyspace/#ManifoldsBase.is_flat-Tuple{CholeskySpace}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::CholeskySpace) Return true.  CholeskySpace  is a flat manifold. See Proposition 8 of [ Lin19 ]. source"},{"id":623,"pagetitle":"Cholesky space","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/choleskyspace/#ManifoldsBase.manifold_dimension-Tuple{CholeskySpace}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::CholeskySpace) Return the manifold dimension for the  CholeskySpace M , i.e. \\[    \\dim(\\mathcal M) = \\frac{N(N+1)}{2}.\\] source"},{"id":624,"pagetitle":"Cholesky space","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/choleskyspace/#ManifoldsBase.parallel_transport_to-Tuple{CholeskySpace, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::CholeskySpace, p, X, q) Parallely transport the tangent vector  X  at  p  along the geodesic to  q  on the  CholeskySpace  manifold  M . The formula reads \\[\\mathcal P_{q‚Üêp}(X) = ‚åä X ‚åã\n+ \\operatorname{diag}(q)\\operatorname{diag}(p)^{-1}\\operatorname{diag}(X),\\] where  $‚åä‚ãÖ‚åã$  denotes the strictly lower triangular matrix, and  $\\operatorname{diag}$  extracts the diagonal matrix. source"},{"id":625,"pagetitle":"Cholesky space","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/choleskyspace/#ManifoldsBase.representation_size-Tuple{CholeskySpace}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::CholeskySpace) Return the representation size for the  CholeskySpace {N} M , i.e.  (N,N) . source"},{"id":626,"pagetitle":"Cholesky space","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/choleskyspace/#ManifoldsBase.zero_vector-Tuple{CholeskySpace, Vararg{Any}}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(M::CholeskySpace, p) Return the zero tangent vector on the  CholeskySpace M  at  p . source"},{"id":627,"pagetitle":"Cholesky space","title":"Literature","ref":"/manifolds/stable/manifolds/choleskyspace/#Literature","content":" Literature"},{"id":630,"pagetitle":"Circle","title":"Circle","ref":"/manifolds/stable/manifolds/circle/#Circle","content":" Circle"},{"id":631,"pagetitle":"Circle","title":"Manifolds.Circle","ref":"/manifolds/stable/manifolds/circle/#Manifolds.Circle","content":" Manifolds.Circle  ‚Äî  Type Circle{ùîΩ} <: AbstractManifold{ùîΩ} The circle  $ùïä^1$  is a manifold here represented by real-valued points in  $[-œÄ,œÄ)$  or complex-valued points  $z ‚àà ‚ÑÇ$  of absolute value  $\\lvert z\\rvert = 1$ . Constructor Circle(ùîΩ=‚Ñù) Generate the  ‚Ñù -valued Circle represented by angles, which alternatively can be set to use the  AbstractNumbers ùîΩ=‚ÑÇ  to obtain the circle represented by  ‚ÑÇ -valued circle of unit numbers. source"},{"id":632,"pagetitle":"Circle","title":"Base.exp","ref":"/manifolds/stable/manifolds/circle/#Base.exp-Tuple{Circle, Vararg{Any}}","content":" Base.exp  ‚Äî  Method exp(M::Circle, p, X) Compute the exponential map on the  Circle . \\[\\exp_p X = (p+X)_{2œÄ},\\] where  $(‚ãÖ)_{2œÄ}$  is the (symmetric) remainder with respect to division by  $2œÄ$ , i.e. in  $[-œÄ,œÄ)$ . For the complex-valued case, the same formula as for the  Sphere $ùïä^1$  is applied to values in the complex plane. source"},{"id":633,"pagetitle":"Circle","title":"Base.log","ref":"/manifolds/stable/manifolds/circle/#Base.log-Tuple{Circle, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::Circle, p, q) Compute the logarithmic map on the  Circle M . \\[\\log_p q = (q-p)_{2œÄ},\\] where  $(‚ãÖ)_{2œÄ}$  is the (symmetric) remainder with respect to division by  $2œÄ$ , i.e. in  $[-œÄ,œÄ)$ . For the complex-valued case, the same formula as for the  Sphere $ùïä^1$  is applied to values in the complex plane. source"},{"id":634,"pagetitle":"Circle","title":"Base.rand","ref":"/manifolds/stable/manifolds/circle/#Base.rand-Tuple{Circle}","content":" Base.rand  ‚Äî  Method Random.rand(M::Circle{‚Ñù}; vector_at = nothing, œÉ::Real=1.0) If  vector_at  is  nothing , return a random point on the  Circle $\\mathbb S^1$  by picking a random element from  $[-\\pi,\\pi)$  uniformly. If  vector_at  is not  nothing , return a random tangent vector from the tangent space of the point  vector_at  on the  Circle  by using a normal distribution with mean 0 and standard deviation  œÉ . source"},{"id":635,"pagetitle":"Circle","title":"Manifolds.complex_dot","ref":"/manifolds/stable/manifolds/circle/#Manifolds.complex_dot-Tuple{Any, Any}","content":" Manifolds.complex_dot  ‚Äî  Method complex_dot(a, b) Compute the inner product of two (complex) numbers with in the complex plane. source"},{"id":636,"pagetitle":"Circle","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/circle/#Manifolds.manifold_volume-Tuple{Circle}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(M::Circle) Return the volume of the  Circle M , i.e.  $2œÄ$ . source"},{"id":637,"pagetitle":"Circle","title":"Manifolds.sym_rem","ref":"/manifolds/stable/manifolds/circle/#Manifolds.sym_rem-Union{Tuple{N}, Tuple{N, Any}} where N<:Number","content":" Manifolds.sym_rem  ‚Äî  Method sym_rem(x,[T=œÄ]) Compute symmetric remainder of  x  with respect to the interall 2* T , i.e.  (x+T)%2T , where the default for  T  is  $œÄ$ source"},{"id":638,"pagetitle":"Circle","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/circle/#Manifolds.volume_density-Tuple{Circle, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(::Circle, p, X) Return volume density of  Circle , i.e. 1. source"},{"id":639,"pagetitle":"Circle","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.check_point-Tuple{Circle, Vararg{Any}}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::Circle, p) Check whether  p  is a point on the  Circle M . For the real-valued case,  p  is an angle and hence it checks that  $p ‚àà [-œÄ,œÄ)$ . for the complex-valued case, it is a unit number,  $p ‚àà ‚ÑÇ$  with  $\\lvert p \\rvert = 1$ . source"},{"id":640,"pagetitle":"Circle","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.check_vector-Tuple{Circle{‚Ñù}, Vararg{Any}}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::Circle, p, X; kwargs...) Check whether  X  is a tangent vector in the tangent space of  p  on the  Circle M . For the real-valued case represented by angles, all  X  are valid, since the tangent space is the whole real line. For the complex-valued case  X  has to lie on the line parallel to the tangent line at  p  in the complex plane, i.e. their inner product has to be zero. source"},{"id":641,"pagetitle":"Circle","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.distance-Tuple{Circle, Vararg{Any}}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::Circle, p, q) Compute the distance on the  Circle M , which is the absolute value of the symmetric remainder of  p  and  q  for the real-valued case and the angle between both complex numbers in the Gaussian plane for the complex-valued case. source"},{"id":642,"pagetitle":"Circle","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.embed-Tuple{Circle, Any, Any}","content":" ManifoldsBase.embed  ‚Äî  Method embed(M::Circle, p, X) Embed a tangent vector  X  at  p  on  Circle M  in the ambient space. It returns  X . source"},{"id":643,"pagetitle":"Circle","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.embed-Tuple{Circle, Any}","content":" ManifoldsBase.embed  ‚Äî  Method embed(M::Circle, p) Embed a point  p  on  Circle M  in the ambient space. It returns  p . source"},{"id":644,"pagetitle":"Circle","title":"ManifoldsBase.get_coordinates","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.get_coordinates-Tuple{Circle{‚ÑÇ}, Any, Any, DefaultOrthonormalBasis{<:Any, TangentSpaceType}}","content":" ManifoldsBase.get_coordinates  ‚Äî  Method get_coordinates(M::Circle{‚ÑÇ}, p, X, B::DefaultOrthonormalBasis) Return tangent vector coordinates in the Lie algebra of the  Circle . source"},{"id":645,"pagetitle":"Circle","title":"ManifoldsBase.get_vector_orthonormal","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.get_vector_orthonormal-Tuple{Circle{‚ÑÇ}, StaticArraysCore.StaticArray, Any, Union{ManifoldsBase.ComplexNumbers, ManifoldsBase.RealNumbers}}","content":" ManifoldsBase.get_vector_orthonormal  ‚Äî  Method get_vector(M::Circle{‚ÑÇ}, p, X, B::DefaultOrthonormalBasis) Return tangent vector from the coordinates in the Lie algebra of the  Circle . source"},{"id":646,"pagetitle":"Circle","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.injectivity_radius-Tuple{Circle}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::Circle[, p]) Return the injectivity radius on the  Circle M , i.e.  $œÄ$ . source"},{"id":647,"pagetitle":"Circle","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.inner-Tuple{Circle, Vararg{Any}}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::Circle, p, X, Y) Compute the inner product of the two tangent vectors  X,Y  from the tangent plane at  p  on the  Circle M  using the restriction of the metric from the embedding, i.e. \\[g_p(X,Y) = X*Y\\] for the real case and \\[g_p(X,Y) = Y^\\mathrm{T}X\\] for the complex case interpreting complex numbers in the Gaussian plane. source"},{"id":648,"pagetitle":"Circle","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.is_flat-Tuple{Circle}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::Circle) Return true.  Circle  is a flat manifold. source"},{"id":649,"pagetitle":"Circle","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.manifold_dimension-Tuple{Circle}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::Circle) Return the dimension of the  Circle M , i.e.  $\\dim(ùïä^1) = 1$ . source"},{"id":650,"pagetitle":"Circle","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.parallel_transport_to-Tuple{Circle, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method  parallel_transport_to(M::Circle, p, X, q) Compute the parallel transport of  X  from the tangent space at  p  to the tangent space at  q  on the  Circle M . For the real-valued case this results in the identity. For the complex-valued case, the formula is the same as for the  Sphere (1)  in the complex plane. \\[\\mathcal P_{q‚Üêp} X = X - \\frac{‚ü®\\log_p q,X‚ü©_p}{d^2_{‚ÑÇ}(p,q)}\n\\bigl(\\log_p q + \\log_q p \\bigr),\\] where  log  denotes the logarithmic map on  M . source"},{"id":651,"pagetitle":"Circle","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.project-Tuple{Circle, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Circle, p, X) Project a value  X  onto the tangent space of the point  p  on the  Circle M . For the real-valued case this is just the identity. For the complex valued case  X  is projected onto the line in the complex plane that is parallel to the tangent to  p  on the unit circle and contains  0 . source"},{"id":652,"pagetitle":"Circle","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/circle/#ManifoldsBase.project-Tuple{Circle, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Circle, p) Project a point  p  onto the  Circle M . For the real-valued case this is the remainder with respect to modulus  $2œÄ$ . For the complex-valued case the result is the projection of  p  onto the unit circle in the complex plane. source"},{"id":653,"pagetitle":"Circle","title":"Statistics.mean","ref":"/manifolds/stable/manifolds/circle/#Statistics.mean-Tuple{Circle{‚ÑÇ}, Any}","content":" Statistics.mean  ‚Äî  Method mean(M::Circle{‚ÑÇ}, x::AbstractVector[, w::AbstractWeights]) Compute the Riemannian  mean  of  x  of points on the  Circle $ùïä^1$ , represented by complex numbers, i.e. embedded in the complex plane. Comuting the sum \\[s = \\sum_{i=1}^n x_i\\] the mean is the angle of the complex number  $s$ , so represented in the complex plane as  $\\frac{s}{\\lvert s \\rvert}$ , whenever  $s \\neq 0$ . If the sum  $s=0$ , the mean is not unique. For example for opposite points or equally spaced angles. source"},{"id":654,"pagetitle":"Circle","title":"Statistics.mean","ref":"/manifolds/stable/manifolds/circle/#Statistics.mean-Tuple{Circle{‚Ñù}, Any}","content":" Statistics.mean  ‚Äî  Method mean(M::Circle{‚Ñù}, x::AbstractVector[, w::AbstractWeights]) Compute the Riemannian  mean  of  x  of points on the  Circle $ùïä^1$ , represented by real numbers, i.e. the angular mean \\[\\operatorname{atan}\\Bigl( \\sum_{i=1}^n w_i\\sin(x_i),  \\sum_{i=1}^n w_i\\sin(x_i) \\Bigr).\\] source"},{"id":657,"pagetitle":"Connection manifold","title":"Connection manifold","ref":"/manifolds/stable/manifolds/connection/#ConnectionSection","content":" Connection manifold A connection manifold always consists of a  topological manifold  together with a  connection $Œì$ . However, often there is an implicitly assumed (default) connection, like the  LeviCivitaConnection  connection on a Riemannian manifold. It is not necessary to use this decorator if you implement just one (or the first) connection. If you later introduce a second, the old (first) connection can be used without an explicitly stated connection. This manifold decorator serves two purposes: to implement different connections (e.g. in closed form) for one  AbstractManifold to provide a way to compute geodesics on manifolds, where this  AbstractAffineConnection  does not yield a closed formula. An example of usage can be found in Cartan-Schouten connections, see  AbstractCartanSchoutenConnection . Connection manifold Types Functions Charts and bases of vector spaces"},{"id":658,"pagetitle":"Connection manifold","title":"Types","ref":"/manifolds/stable/manifolds/connection/#Types","content":" Types"},{"id":659,"pagetitle":"Connection manifold","title":"Manifolds.AbstractAffineConnection","ref":"/manifolds/stable/manifolds/connection/#Manifolds.AbstractAffineConnection","content":" Manifolds.AbstractAffineConnection  ‚Äî  Type AbstractAffineConnection Abstract type for affine connections on a manifold. source"},{"id":660,"pagetitle":"Connection manifold","title":"Manifolds.ConnectionManifold","ref":"/manifolds/stable/manifolds/connection/#Manifolds.ConnectionManifold","content":" Manifolds.ConnectionManifold  ‚Äî  Type ConnectionManifold{ùîΩ,,M<:AbstractManifold{ùîΩ},G<:AbstractAffineConnection} <: AbstractDecoratorManifold{ùîΩ} Constructor ConnectionManifold(M, C) Decorate the  AbstractManifold M  with  AbstractAffineConnection C . source"},{"id":661,"pagetitle":"Connection manifold","title":"Manifolds.IsConnectionManifold","ref":"/manifolds/stable/manifolds/connection/#Manifolds.IsConnectionManifold","content":" Manifolds.IsConnectionManifold  ‚Äî  Type IsConnectionManifold <: AbstractTrait Specify that a certain decorated Manifold is a connection manifold in the sence that it provides explicit connection properties, extending/changing the default connection properties of a manifold. source"},{"id":662,"pagetitle":"Connection manifold","title":"Manifolds.IsDefaultConnection","ref":"/manifolds/stable/manifolds/connection/#Manifolds.IsDefaultConnection","content":" Manifolds.IsDefaultConnection  ‚Äî  Type IsDefaultConnection{G<:AbstractAffineConnection} Specify that a certain  AbstractAffineConnection  is the default connection for a manifold. This way the corresponding  ConnectionManifold  falls back to the default methods of the manifold it decorates. source"},{"id":663,"pagetitle":"Connection manifold","title":"Manifolds.LeviCivitaConnection","ref":"/manifolds/stable/manifolds/connection/#Manifolds.LeviCivitaConnection","content":" Manifolds.LeviCivitaConnection  ‚Äî  Type LeviCivitaConnection The  Levi-Civita connection  of a Riemannian manifold. source"},{"id":664,"pagetitle":"Connection manifold","title":"Functions","ref":"/manifolds/stable/manifolds/connection/#Functions","content":" Functions"},{"id":665,"pagetitle":"Connection manifold","title":"Base.exp","ref":"/manifolds/stable/manifolds/connection/#Base.exp-Tuple{ManifoldsBase.TraitList{IsConnectionManifold}, AbstractDecoratorManifold, Any, Any}","content":" Base.exp  ‚Äî  Method exp(::TraitList{IsConnectionManifold}, M::AbstractDecoratorManifold, p, X) Compute the exponential map on a manifold that  IsConnectionManifold M  equipped with corresponding affine connection. If  M  is a  MetricManifold  with a  IsDefaultMetric  trait, this method falls back to  exp(M, p, X) . Otherwise it numerically integrates the underlying ODE, see  solve_exp_ode . Currently, the numerical integration is only accurate when using a single coordinate chart that covers the entire manifold. This excludes coordinates in an embedded space. source"},{"id":666,"pagetitle":"Connection manifold","title":"Manifolds.christoffel_symbols_first","ref":"/manifolds/stable/manifolds/connection/#Manifolds.christoffel_symbols_first-Tuple{AbstractManifold, Any, AbstractBasis}","content":" Manifolds.christoffel_symbols_first  ‚Äî  Method christoffel_symbols_first(\n    M::AbstractManifold,\n    p,\n    B::AbstractBasis;\n    backend::AbstractDiffBackend = default_differential_backend(),\n) Compute the Christoffel symbols of the first kind in local coordinates of basis  B . The Christoffel symbols are (in Einstein summation convention) \\[Œì_{ijk} = \\frac{1}{2} \\Bigl[g_{kj,i} + g_{ik,j} - g_{ij,k}\\Bigr],\\] where  $g_{ij,k}=\\frac{‚àÇ}{‚àÇ p^k} g_{ij}$  is the coordinate derivative of the local representation of the metric tensor. The dimensions of the resulting multi-dimensional array are ordered  $(i,j,k)$ . source"},{"id":667,"pagetitle":"Connection manifold","title":"Manifolds.christoffel_symbols_second","ref":"/manifolds/stable/manifolds/connection/#Manifolds.christoffel_symbols_second-Tuple{AbstractManifold, Any, AbstractBasis}","content":" Manifolds.christoffel_symbols_second  ‚Äî  Method christoffel_symbols_second(\n    M::AbstractManifold,\n    p,\n    B::AbstractBasis;\n    backend::AbstractDiffBackend = default_differential_backend(),\n) Compute the Christoffel symbols of the second kind in local coordinates of basis  B . For affine connection manifold the Christoffel symbols need to be explicitly implemented while, for a  MetricManifold  they are computed as (in Einstein summation convention) \\[Œì^{l}_{ij} = g^{kl} Œì_{ijk},\\] where  $Œì_{ijk}$  are the Christoffel symbols of the first kind (see  christoffel_symbols_first ), and  $g^{kl}$  is the inverse of the local representation of the metric tensor. The dimensions of the resulting multi-dimensional array are ordered  $(l,i,j)$ . source"},{"id":668,"pagetitle":"Connection manifold","title":"Manifolds.christoffel_symbols_second_jacobian","ref":"/manifolds/stable/manifolds/connection/#Manifolds.christoffel_symbols_second_jacobian-Tuple{AbstractManifold, Any, AbstractBasis}","content":" Manifolds.christoffel_symbols_second_jacobian  ‚Äî  Method christoffel_symbols_second_jacobian(\n    M::AbstractManifold,\n    p,\n    B::AbstractBasis;\n    backend::AbstractDiffBackend = default_differential_backend(),\n) Get partial derivatives of the Christoffel symbols of the second kind for manifold  M  at  p  with respect to the coordinates of  B , i.e. \\[\\frac{‚àÇ}{‚àÇ p^l} Œì^{k}_{ij} = Œì^{k}_{ij,l}.\\] The dimensions of the resulting multi-dimensional array are ordered  $(i,j,k,l)$ . source"},{"id":669,"pagetitle":"Connection manifold","title":"Manifolds.connection","ref":"/manifolds/stable/manifolds/connection/#Manifolds.connection-Tuple{AbstractManifold}","content":" Manifolds.connection  ‚Äî  Method connection(M::AbstractManifold) Get the connection (an object of a subtype of  AbstractAffineConnection ) of  AbstractManifold M . source"},{"id":670,"pagetitle":"Connection manifold","title":"Manifolds.connection","ref":"/manifolds/stable/manifolds/connection/#Manifolds.connection-Tuple{ConnectionManifold}","content":" Manifolds.connection  ‚Äî  Method connection(M::ConnectionManifold) Return the connection associated with  ConnectionManifold M . source"},{"id":671,"pagetitle":"Connection manifold","title":"Manifolds.gaussian_curvature","ref":"/manifolds/stable/manifolds/connection/#Manifolds.gaussian_curvature-Tuple{AbstractManifold, Any, AbstractBasis}","content":" Manifolds.gaussian_curvature  ‚Äî  Method gaussian_curvature(M::AbstractManifold, p, B::AbstractBasis; backend::AbstractDiffBackend = default_differential_backend()) Compute the Gaussian curvature of the manifold  M  at the point  p  using basis  B . This is equal to half of the scalar Ricci curvature, see  ricci_curvature . source"},{"id":672,"pagetitle":"Connection manifold","title":"Manifolds.is_default_connection","ref":"/manifolds/stable/manifolds/connection/#Manifolds.is_default_connection-Tuple{AbstractManifold, AbstractAffineConnection}","content":" Manifolds.is_default_connection  ‚Äî  Method is_default_connection(M::AbstractManifold, G::AbstractAffineConnection) returns whether an  AbstractAffineConnection  is the default metric on the manifold  M  or not. This can be set by defining this function, or setting the  IsDefaultConnection  trait for an  AbstractDecoratorManifold . source"},{"id":673,"pagetitle":"Connection manifold","title":"Manifolds.ricci_tensor","ref":"/manifolds/stable/manifolds/connection/#Manifolds.ricci_tensor-Tuple{AbstractManifold, Any, AbstractBasis}","content":" Manifolds.ricci_tensor  ‚Äî  Method ricci_tensor(M::AbstractManifold, p, B::AbstractBasis; backend::AbstractDiffBackend = default_differential_backend()) Compute the Ricci tensor, also known as the Ricci curvature tensor, of the manifold  M  at the point  p  using basis  B , see  https://en.wikipedia.org/wiki/Ricci_curvature#Introduction_and_local_definition . source"},{"id":674,"pagetitle":"Connection manifold","title":"Manifolds.solve_exp_ode","ref":"/manifolds/stable/manifolds/connection/#Manifolds.solve_exp_ode-Tuple{AbstractManifold, Any, Any, Number}","content":" Manifolds.solve_exp_ode  ‚Äî  Method solve_exp_ode(\n    M::ConnectionManifold,\n    p,\n    X,\n    t::Number;\n    B::AbstractBasis = DefaultOrthonormalBasis(),\n    backend::AbstractDiffBackend = default_differential_backend(),\n    solver = AutoVern9(Rodas5()),\n    kwargs...,\n) Approximate the exponential map on the manifold by evaluating the ODE descripting the geodesic at 1, assuming the default connection of the given manifold by solving the ordinary differential equation \\[\\frac{d^2}{dt^2} p^k + Œì^k_{ij} \\frac{d}{dt} p_i \\frac{d}{dt} p_j = 0,\\] where  $Œì^k_{ij}$  are the Christoffel symbols of the second kind, and the Einstein summation convention is assumed. The argument  solver  follows the  OrdinaryDiffEq  conventions.  kwargs...  specify keyword arguments that will be passed to  OrdinaryDiffEq.solve . Currently, the numerical integration is only accurate when using a single coordinate chart that covers the entire manifold. This excludes coordinates in an embedded space. Note This function only works when  OrdinaryDiffEq.jl  is loaded with using OrdinaryDiffEq source"},{"id":675,"pagetitle":"Connection manifold","title":"ManifoldsBase.riemann_tensor","ref":"/manifolds/stable/manifolds/connection/#ManifoldsBase.riemann_tensor-Tuple{AbstractManifold, Any, AbstractBasis}","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(M::AbstractManifold, p, B::AbstractBasis; backend::AbstractDiffBackend=default_differential_backend()) Compute the Riemann tensor  $R^l_{ijk}$ , also known as the Riemann curvature tensor, at the point  p  in local coordinates defined by  B . The dimensions of the resulting multi-dimensional array are ordered  $(l,i,j,k)$ . The function uses the coordinate expression involving the second Christoffel symbol, see  https://en.wikipedia.org/wiki/Riemann_curvature_tensor#Coordinate_expression  for details. See also christoffel_symbols_second ,  christoffel_symbols_second_jacobian source"},{"id":676,"pagetitle":"Connection manifold","title":"Charts and bases of vector spaces","ref":"/manifolds/stable/manifolds/connection/#connections_charts","content":" Charts and bases of vector spaces All connection-related functions take a basis of a vector space as one of the arguments. This is needed because generally there is no way to define these functions without referencing a basis. In some cases there is no need to be explicit about this basis, and then for example a  DefaultOrthonormalBasis  object can be used. In cases where being explicit about these bases is needed, for example when using multiple charts, a basis can be specified, for example using  induced_basis ."},{"id":679,"pagetitle":"Determinant one matrices","title":"Matrices of determinant one","ref":"/manifolds/stable/manifolds/determinantone/#Matrices-of-determinant-one","content":" Matrices of determinant one"},{"id":680,"pagetitle":"Determinant one matrices","title":"Manifolds.DeterminantOneMatrices","ref":"/manifolds/stable/manifolds/determinantone/#Manifolds.DeterminantOneMatrices","content":" Manifolds.DeterminantOneMatrices  ‚Äî  Type DeterminantOneMatrices{ùîΩ,T} <: AbstractDecoratorManifold{ùîΩ} The  AbstractManifold  consisting of the real- or complex-valued (invertible) matrices od determinant one, that is the set \\[\\bigl\\{p  ‚àà ùîΩ^{n√ón}\\ \\big|\\ \\det(p) = 1 \\bigr\\},\\] where the field  $ùîΩ ‚àà \\{ ‚Ñù, ‚ÑÇ\\}$ . Note that this is a subset of  InvertibleMatrices , and a superset of any of the  GeneralUnitaryMatrices The tangent space at any point  p  is the set of matrices with trace 0. Constructor DeterminantOneMatrices(n::Int, field::AbstractNumbers=‚Ñù) Generate the manifold of  $n√ón$  matrices of determinant one. source"},{"id":681,"pagetitle":"Determinant one matrices","title":"Base.rand","ref":"/manifolds/stable/manifolds/determinantone/#Base.rand-Tuple{DeterminantOneMatrices}","content":" Base.rand  ‚Äî  Method Random.rand(M::DeterminantOneMatrices; vector_at=nothing, kwargs...) If  vector_at  is  nothing , return a random point on the  DeterminantOneMatrices  manifold  M  by using  rand  in the embedding. If  vector_at  is not  nothing , return a random tangent vector from the tangent space of the point  vector_at  on the  DeterminantOneMatrices  by using by using  rand  in the embedding. source"},{"id":682,"pagetitle":"Determinant one matrices","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/determinantone/#ManifoldsBase.check_point-Tuple{DeterminantOneMatrices, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::DeterminantOneMatrices{n,ùîΩ}, p; kwargs...) Check whether  p  is a valid manifold point on the  DeterminantOneMatrices M , i.e. whether  p  has a determinant of  $1$ . The check is perfomed with  isapprox  and all keyword arguments are passed to this source"},{"id":683,"pagetitle":"Determinant one matrices","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/determinantone/#ManifoldsBase.check_vector-Tuple{DeterminantOneMatrices, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::DeterminantOneMatrices{n,ùîΩ}, p, X; kwargs... ) Check whether  X  is a tangent vector to manifold point  p  on the  DeterminantOneMatrices M , which are all matrices of size  $n√ón$  with trace 0. source"},{"id":684,"pagetitle":"Determinant one matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/determinantone/#ManifoldsBase.manifold_dimension-Union{Tuple{DeterminantOneMatrices{<:Any, ùîΩ}}, Tuple{ùîΩ}} where ùîΩ","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::DeterminantOneMatrices{n,ùîΩ}) Return the dimension of the  DeterminantOneMatrices  matrix  M  over the number system  ùîΩ , which is one dimension less than its embedding, the  Euclidean (n, n; field=ùîΩ) . source"},{"id":685,"pagetitle":"Determinant one matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/determinantone/#ManifoldsBase.project-Tuple{DeterminantOneMatrices, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(G::DeterminantOneMatrices, p, X)\nproject!(G::DeterminantOneMatrices, Y, p, X) Orthogonally project  $X ‚àà ùîΩ^{n√ón}$  onto the tangent space of  $p$  to the  DeterminantOneMatrices . This first changes the representation from  X  to the trace-zero component, i.e. computes  Y = p \\ X  and then subtracts  c = tr(Y) / n  from all diagonal entries. source"},{"id":686,"pagetitle":"Determinant one matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/determinantone/#ManifoldsBase.project-Tuple{DeterminantOneMatrices, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(G::DeterminantOneMatrices, p)\nproject!(G::DeterminantOneMatrices, q, p) Project  $p ‚àà \\mathrm{GL}(n, ùîΩ)$  to the  DeterminantOneMatrices  using the singular value decomposition of  $p = U S V^\\mathrm{H}$ . The formula for the projection is \\[\\operatorname{proj}(p) = U S D V^\\mathrm{H},\\] where \\[D_{ij} = Œ¥_{ij} \\begin{cases}\n    1            & \\text{ if } i ‚â† n \\\\\n    \\det(p)^{-1} & \\text{ if } i = n\n\\end{cases}.\\] The operation can be done in-place of  q . source"},{"id":689,"pagetitle":"Elliptope","title":"Elliptope","ref":"/manifolds/stable/manifolds/elliptope/#Elliptope","content":" Elliptope"},{"id":690,"pagetitle":"Elliptope","title":"Manifolds.Elliptope","ref":"/manifolds/stable/manifolds/elliptope/#Manifolds.Elliptope","content":" Manifolds.Elliptope  ‚Äî  Type Elliptope{T} <: AbstractDecoratorManifold{‚Ñù} The Elliptope manifold, also known as the set of correlation matrices, consists of all symmetric positive semidefinite matrices of rank  $k$  with unit diagonal, i.e., \\[\\begin{aligned}\n\\mathcal E(n,k) =\n\\bigl\\{p ‚àà ‚Ñù^{n√ón}\\ \\big|\\ &a^\\mathrm{T}pa \\geq 0 \\text{ for all } a ‚àà ‚Ñù^{n},\\\\\n&p_{ii} = 1 \\text{ for all } i=1,\\ldots,n,\\\\\n&\\text{and } p = qq^{\\mathrm{T}} \\text{ for } q \\in  ‚Ñù^{n√ók} \\text{ with } \\operatorname{rank}(p) = \\operatorname{rank}(q) = k\n\\bigr\\}.\n\\end{aligned}\\] And this manifold is working solely on the matrices  $q$ . Note that this  $q$  is not unique, indeed for any orthogonal matrix  $A$  we have  $(qA)(qA)^{\\mathrm{T}} = qq^{\\mathrm{T}} = p$ , so the manifold implemented here is the quotient manifold. The unit diagonal translates to unit norm columns of  $q$ . The tangent space at  $p$ , denoted  $T_p\\mathcal E(n,k)$ , is also represented by matrices  $Y\\in ‚Ñù^{n√ók}$  and reads as \\[T_p\\mathcal E(n,k) = \\bigl\\{\nX ‚àà ‚Ñù^{n√ón}\\,|\\,X = qY^{\\mathrm{T}} + Yq^{\\mathrm{T}} \\text{ with } X_{ii} = 0 \\text{ for } i=1,\\ldots,n\n\\bigr\\}\\] endowed with the  Euclidean  metric from the embedding, i.e. from the  $‚Ñù^{n√ók}$ This manifold was for example investigated in[ JBAS10 ]. Constructor Elliptope(n::Int, k::Int; parameter::Symbol=:type) generates the manifold  $\\mathcal E(n,k) \\subset ‚Ñù^{n√ón}$ . parameter : whether a type parameter should be used to store  n  and  k . By default size is stored in type. Value can either be  :field  or  :type . source"},{"id":691,"pagetitle":"Elliptope","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/elliptope/#ManifoldsBase.check_point-Tuple{Elliptope, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::Elliptope, q; kwargs...) checks, whether  q  is a valid representation of a point  $p=qq^{\\mathrm{T}}$  on the  Elliptope M , i.e. is a matrix of size  (N,K) , such that  $p$  is symmetric positive semidefinite and has unit trace. Since by construction  $p$  is symmetric, this is not explicitly checked. Since  $p$  is by construction positive semidefinite, this is not checked. The tolerances for positive semidefiniteness and unit trace can be set using the  kwargs... . source"},{"id":692,"pagetitle":"Elliptope","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/elliptope/#ManifoldsBase.check_vector-Union{Tuple{T}, Tuple{Elliptope, Any, T}} where T","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::Elliptope, q, Y; kwargs... ) Check whether  $X = qY^{\\mathrm{T}} + Yq^{\\mathrm{T}}$  is a tangent vector to  $p=qq^{\\mathrm{T}}$  on the  Elliptope M , i.e.  Y  has to be of same dimension as  q  and a  $X$  has to be a symmetric matrix with zero diagonal. The tolerance for the base point check and zero diagonal can be set using the  kwargs... . Note that symmetric of  $X$  holds by construction an is not explicitly checked. source"},{"id":693,"pagetitle":"Elliptope","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/elliptope/#ManifoldsBase.is_flat-Tuple{Elliptope}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::Elliptope) Return false.  Elliptope  is not a flat manifold. source"},{"id":694,"pagetitle":"Elliptope","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/elliptope/#ManifoldsBase.manifold_dimension-Tuple{Elliptope}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::Elliptope) returns the dimension of  Elliptope M $=\\mathcal E(n,k), n,k ‚àà ‚Ñï$ , i.e. \\[\\dim \\mathcal E(n,k) = n(k-1) - \\frac{k(k-1)}{2}.\\] source"},{"id":695,"pagetitle":"Elliptope","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/elliptope/#ManifoldsBase.project-Tuple{Elliptope, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Elliptope, q) project  q  onto the manifold  Elliptope M , by normalizing the rows of  q . source"},{"id":696,"pagetitle":"Elliptope","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/elliptope/#ManifoldsBase.project-Tuple{Elliptope, Vararg{Any}}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Elliptope, q, Y) Project  Y  onto the tangent space at  q , i.e. row-wise onto the oblique manifold. source"},{"id":697,"pagetitle":"Elliptope","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/elliptope/#ManifoldsBase.representation_size-Tuple{Elliptope}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::Elliptope) Return the size of an array representing an element on the  Elliptope  manifold  M , i.e.  $n√ók$ , the size of such factor of  $p=qq^{\\mathrm{T}}$  on  $\\mathcal M = \\mathcal E(n,k)$ . source"},{"id":698,"pagetitle":"Elliptope","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/elliptope/#ManifoldsBase.retract-Tuple{Elliptope, Any, Any, ProjectionRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::Elliptope, q, Y, ::ProjectionRetraction) compute a projection based retraction by projecting  $q+Y$  back onto the manifold. source"},{"id":699,"pagetitle":"Elliptope","title":"ManifoldsBase.vector_transport_to","ref":"/manifolds/stable/manifolds/elliptope/#ManifoldsBase.vector_transport_to-Tuple{Elliptope, Any, Any, Any, ProjectionTransport}","content":" ManifoldsBase.vector_transport_to  ‚Äî  Method vector_transport_to(M::Elliptope, p, X, q) transport the tangent vector  X  at  p  to  q  by projecting it onto the tangent space at  q . source"},{"id":700,"pagetitle":"Elliptope","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/elliptope/#ManifoldsBase.zero_vector-Tuple{Elliptope, Vararg{Any}}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(M::Elliptope,p) returns the zero tangent vector in the tangent space of the symmetric positive definite matrix  p  on the  Elliptope  manifold  M . source"},{"id":701,"pagetitle":"Elliptope","title":"Literature","ref":"/manifolds/stable/manifolds/elliptope/#Literature","content":" Literature"},{"id":704,"pagetitle":"Essential manifold","title":"Essential Manifold","ref":"/manifolds/stable/manifolds/essentialmanifold/#Essential-Manifold","content":" Essential Manifold The essential manifold is modeled as an  AbstractPowerManifold   of the  $3√ó3$ Rotations  and uses  NestedPowerRepresentation ."},{"id":705,"pagetitle":"Essential manifold","title":"Manifolds.EssentialManifold","ref":"/manifolds/stable/manifolds/essentialmanifold/#Manifolds.EssentialManifold","content":" Manifolds.EssentialManifold  ‚Äî  Type EssentialManifold <: AbstractPowerManifold{‚Ñù} The essential manifold is the space of the essential matrices which is represented as a quotient space of the  Rotations  manifold product  $\\mathrm{SO}(3)^2$ . Let  $R_x(Œ∏), R_y(Œ∏), R_x(Œ∏) \\in ‚Ñù^{x√ó3}$  denote the rotation around the  $z$ ,  $y$ , and  $x$  axis in  $‚Ñù^3$ , respectively, and further the groups \\[H_z = \\bigl\\{(R_z(Œ∏),R_z(Œ∏))\\ \\big|\\ Œ∏ ‚àà [-œÄ,œÄ) \\bigr\\}\\] and \\[H_œÄ = \\bigl\\{ (I,I), (R_x(œÄ), R_x(œÄ)), (I,R_z(œÄ)), (R_x(œÄ), R_y(œÄ))  \\bigr\\}\\] acting elementwise on the left from  $\\mathrm{SO}(3)^2$  (component wise). Then the unsigned Essential manifold  $\\mathcal{M}_{\\text{E}}$  can be identified with the quotient space \\[\\mathcal{M}_{\\text{E}} := (\\text{SO}(3)√ó\\text{SO}(3))/(H_z √ó H_œÄ),\\] and for the signed Essential manifold  $\\mathcal{M}_{\\text{∆é}}$ , the quotient reads \\[\\mathcal{M}_{\\text{∆é}} := (\\text{SO}(3)√ó\\text{SO}(3))/(H_z).\\] An essential matrix is defined as \\[E = (R'_1)^T [T'_2 - T'_1]_{√ó} R'_2,\\] where the poses of two cameras  $(R_i', T_i'), i=1,2$ , are contained in the space of rigid body transformations  $SE(3)$  and the operator  $[‚ãÖ]_{√ó}\\colon ‚Ñù^3 ‚Üí \\operatorname{SkewSym}(3)$  denotes the matrix representation of the cross product operator. For more details see [ TD17 ]. Constructor EssentialManifold(is_signed=true) Generate the manifold of essential matrices, either the signed ( is_signed=true ) or unsigned ( is_signed=false ) variant. source"},{"id":706,"pagetitle":"Essential manifold","title":"Functions","ref":"/manifolds/stable/manifolds/essentialmanifold/#Functions","content":" Functions"},{"id":707,"pagetitle":"Essential manifold","title":"Base.exp","ref":"/manifolds/stable/manifolds/essentialmanifold/#Base.exp-Tuple{EssentialManifold, Vararg{Any}}","content":" Base.exp  ‚Äî  Method exp(M::EssentialManifold, p, X) Compute the exponential map on the  EssentialManifold  from  p  into direction  X , i.e. \\[\\text{exp}_p(X) =\\text{exp}_g( \\tilde X),  \\quad g \\in \\text(SO)(3)^2,\\] where  $\\tilde X$  is the horizontal lift of  $X$ [ TD17 ]. source"},{"id":708,"pagetitle":"Essential manifold","title":"Base.log","ref":"/manifolds/stable/manifolds/essentialmanifold/#Base.log-Tuple{EssentialManifold, Any, Any}","content":" Base.log  ‚Äî  Method log(M::EssentialManifold, p, q) Compute the logarithmic map on the  EssentialManifold M , i.e. the tangent vector, whose geodesic starting from  p  reaches  q  after time 1. Here,  $p=(R_{p_1},R_{p_2})$  and  $q=(R_{q_1},R_{q_2})$  are elements of  $SO(3)^2$ . We use that any essential matrix can, up to scale, be decomposed to \\[E = R_1^T [e_z]_{√ó}R_2,\\] where  $(R_1,R_2)‚ààSO(3)^2$ . Two points in  $SO(3)^2$  are equivalent iff their corresponding essential matrices are equal (up to a sign flip). To compute the logarithm, we first move  q  to another representative of its equivalence class. For this, we find  $t= t_{\\text{opt}}$  for which the function \\[f(t) = f_1 + f_2, \\quad f_i = \\frac{1}{2} Œ∏^2_i(t), \\quad Œ∏_i(t)=d(R_{p_i},R_z(t)R_{b_i}) \\text{ for } i=1,2,\\] where  $d(‚ãÖ,‚ãÖ)$  is the distance function in  $SO(3)$ , is minimized. Further, the group  $H_z$  acting on the left on  $SO(3)^2$  is defined as \\[H_z = \\{(R_z(Œ∏),R_z(Œ∏))\\colon Œ∏ \\in [-œÄ,œÄ) \\},\\] where  $R_z(Œ∏)$  is the rotation around the z axis with angle  $Œ∏$ . Points in  $H_z$  are denoted by  $S_z$ . Then, the logarithm is defined as \\[\\log_p (S_z(t_{\\text{opt}})q) = [\\text{Log}(R_{p_i}^T R_z(t_{\\text{opt}})R_{b_i})]_{i=1,2},\\] where  $\\text{Log}$  is the  logarithm  on  $SO(3)$ . For more details see [ TD17 ]. source"},{"id":709,"pagetitle":"Essential manifold","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/essentialmanifold/#ManifoldsBase.check_point-Tuple{EssentialManifold, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::EssentialManifold, p; kwargs...) Check whether the matrix is a valid point on the  EssentialManifold M , i.e. a 2-element array containing SO(3) matrices. source"},{"id":710,"pagetitle":"Essential manifold","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/essentialmanifold/#ManifoldsBase.check_vector-Tuple{EssentialManifold, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::EssentialManifold, p, X; kwargs... ) Check whether  X  is a tangent vector to manifold point  p  on the  EssentialManifold M , i.e.  X  has to be a 2-element array of  3 -by- 3  skew-symmetric matrices. source"},{"id":711,"pagetitle":"Essential manifold","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/essentialmanifold/#ManifoldsBase.distance-Tuple{EssentialManifold, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::EssentialManifold, p, q) Compute the Riemannian distance between the two points  p  and  q  on the  EssentialManifold . This is done by computing the distance of the equivalence classes  $[p]$  and  $[q]$  of the points  $p=(R_{p_1},R_{p_2}), q=(R_{q_1},R_{q_2}) ‚àà SO(3)^2$ , respectively. Two points in  $SO(3)^2$  are equivalent iff their corresponding essential matrices, given by \\[E = R_1^T [e_z]_{√ó}R_2,\\] are equal (up to a sign flip). Using the logarithmic map, the distance is given by \\[\\text{dist}([p],[q]) = \\| \\text{log}_{[p]} [q] \\| = \\| \\log_p (S_z(t_{\\text{opt}})q) \\|,\\] where  $S_z ‚àà H_z = \\{(R_z(Œ∏),R_z(Œ∏))\\colon Œ∏ \\in [-œÄ,œÄ) \\}$  in which  $R_z(Œ∏)$  is the rotation around the z axis with angle  $Œ∏$  and  $t_{\\text{opt}}$  is the minimizer of the cost function \\[f(t) = f_1 + f_2, \\quad f_i = \\frac{1}{2} Œ∏^2_i(t), \\quad Œ∏_i(t)=d(R_{p_i},R_z(t)R_{b_i}) \\text{ for } i=1,2,\\] where  $d(‚ãÖ,‚ãÖ)$  is the distance function in  $SO(3)$  [ TD17 ]. source"},{"id":712,"pagetitle":"Essential manifold","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/essentialmanifold/#ManifoldsBase.is_flat-Tuple{EssentialManifold}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::EssentialManifold) Return false.  EssentialManifold  is not a flat manifold. source"},{"id":713,"pagetitle":"Essential manifold","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/essentialmanifold/#ManifoldsBase.manifold_dimension-Tuple{EssentialManifold}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::EssentialManifold{is_signed, ‚Ñù}) Return the manifold dimension of the  EssentialManifold , which is  5 [ TD17 ]. source"},{"id":714,"pagetitle":"Essential manifold","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/essentialmanifold/#ManifoldsBase.parallel_transport_to-Tuple{EssentialManifold, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::EssentialManifold, p, X, q) Compute the vector transport of the tangent vector  X  at  p  to  q  on the  EssentialManifold M  using left translation of the ambient group. source"},{"id":715,"pagetitle":"Essential manifold","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/essentialmanifold/#ManifoldsBase.project-Tuple{EssentialManifold, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::EssentialManifold, p, X) Project the matrix  X  onto the tangent space \\[T_{p} \\text{SO}(3)^2 = T_{\\text{vp}}\\text{SO}(3)^2 ‚äï T_{\\text{hp}}\\text{SO}(3)^2,\\] by first computing its projection onto the vertical space  $T_{\\text{vp}}\\text{SO}(3)^2$  using  vert_proj . Then the orthogonal projection of  X  onto the horizontal space  $T_{\\text{hp}}\\text{SO}(3)^2$  is defined as \\[\\Pi_h(X) = X - \\frac{\\text{vert\\_proj}_p(X)}{2} \\begin{bmatrix} R_1^T e_z \\\\ R_2^T e_z \\end{bmatrix},\\] with  $R_i = R_0 R'_i, i=1,2,$  where  $R'_i$  is part of the pose of camera  $i$ $g_i = (R'_i,T'_i) ‚àà \\text{SE}(3)$  and  $R_0 ‚àà \\text{SO}(3)$  such that  $R_0(T'_2-T'_1) = e_z$ . source"},{"id":716,"pagetitle":"Essential manifold","title":"Internal Functions","ref":"/manifolds/stable/manifolds/essentialmanifold/#Internal-Functions","content":" Internal Functions"},{"id":717,"pagetitle":"Essential manifold","title":"Manifolds.dist_min_angle_pair","ref":"/manifolds/stable/manifolds/essentialmanifold/#Manifolds.dist_min_angle_pair-Tuple{Any, Any}","content":" Manifolds.dist_min_angle_pair  ‚Äî  Method dist_min_angle_pair(p, q) This function computes the global minimizer of the function \\[f(t) = f_1 + f_2, \\quad f_i = \\frac{1}{2} Œ∏^2_i(t), \\quad Œ∏_i(t)=d(R_{p_i},R_z(t)R_{b_i}) \\text{ for } i=1,2,\\] for the given values. This is done by finding the discontinuity points  $t_{d_i}, i=1,2$  of its derivative and using Newton's method to minimize the function over the intervals  $[t_{d_1},t_{d_2}]$  and  $[t_{d_2},t_{d_1}+2œÄ]$  separately. Then, the minimizer for which  $f$  is minimal is chosen and given back together with the minimal value. For more details see Algorithm 1 in [ TD17 ]. source"},{"id":718,"pagetitle":"Essential manifold","title":"Manifolds.dist_min_angle_pair_compute_df_break","ref":"/manifolds/stable/manifolds/essentialmanifold/#Manifolds.dist_min_angle_pair_compute_df_break-Tuple{Any, Any}","content":" Manifolds.dist_min_angle_pair_compute_df_break  ‚Äî  Method dist_min_angle_pair_compute_df_break(t_break, q) This function computes the derivatives of each term  $f_i, i=1,2,$  at discontinuity point  t_break . For more details see [ TD17 ]. source"},{"id":719,"pagetitle":"Essential manifold","title":"Manifolds.dist_min_angle_pair_df_newton","ref":"/manifolds/stable/manifolds/essentialmanifold/#Manifolds.dist_min_angle_pair_df_newton-NTuple{9, Any}","content":" Manifolds.dist_min_angle_pair_df_newton  ‚Äî  Method dist_min_angle_pair_df_newton(m1, Œ¶1, c1, m2, Œ¶2, c2, t_min, t_low, t_high) This function computes the minimizer of the function \\[f(t) = f_1 + f_2, \\quad f_i = \\frac{1}{2} Œ∏^2_i(t), \\quad Œ∏_i(t)=d(R_{p_i},R_z(t)R_{b_i}) \\text{ for } i=1,2,\\] in the interval  $[$ t_low ,  t_high $]$  using Newton's method. For more details see [ TD17 ]. source"},{"id":720,"pagetitle":"Essential manifold","title":"Manifolds.dist_min_angle_pair_discontinuity_distance","ref":"/manifolds/stable/manifolds/essentialmanifold/#Manifolds.dist_min_angle_pair_discontinuity_distance-Tuple{Any}","content":" Manifolds.dist_min_angle_pair_discontinuity_distance  ‚Äî  Method dist_min_angle_pair_discontinuity_distance(q) This function computes the point  $t_{\\text{di}}$  for which the first derivative of \\[f(t) = f_1 + f_2, \\quad f_i = \\frac{1}{2} Œ∏^2_i(t), \\quad Œ∏_i(t)=d(R_{p_i},R_z(t)R_{b_i}) \\text{ for } i=1,2,\\] does not exist. This is the case for  $\\sin(Œ∏_i(t_{\\text{di}})) = 0$ . For more details see Proposition 9 and its proof, as well as Lemma 1 in [ TD17 ]. source"},{"id":721,"pagetitle":"Essential manifold","title":"Manifolds.vert_proj","ref":"/manifolds/stable/manifolds/essentialmanifold/#Manifolds.vert_proj-Tuple{EssentialManifold, Any, Any}","content":" Manifolds.vert_proj  ‚Äî  Method vert_proj(M::EssentialManifold, p, X) Project  X  onto the vertical space  $T_{\\text{vp}}\\text{SO}(3)^2$  with \\[\\text{vert\\_proj}_p(X) = e_z^T(R_1 X_1 + R_2 X_2),\\] where  $e_z$  is the third unit vector,  $X_i ‚àà T_{p}\\text{SO}(3)$  for  $i=1,2,$  and it holds  $R_i = R_0 R'_i, i=1,2,$  where  $R'_i$  is part of the pose of camera  $i$ $g_i = (R_i,T'_i) ‚àà \\text{SE}(3)$  and  $R_0 ‚àà \\text{SO}(3)$  such that  $R_0(T'_2-T'_1) = e_z$  [ TD17 ]. source"},{"id":722,"pagetitle":"Essential manifold","title":"Literature","ref":"/manifolds/stable/manifolds/essentialmanifold/#Literature","content":" Literature"},{"id":725,"pagetitle":"Euclidean","title":"Euclidean space","ref":"/manifolds/stable/manifolds/euclidean/#EuclideanSection","content":" Euclidean space The Euclidean space  $‚Ñù^n$  is a simple model space, since it has curvature constantly zero everywhere; hence, nearly all operations simplify. The easiest way to generate an Euclidean space is to use a field, i.e.  AbstractNumbers , e.g. to create the  $‚Ñù^n$  or  $‚Ñù^{n√ón}$  you can simply type  M = ‚Ñù^n  or  ‚Ñù^(n,n) , respectively."},{"id":726,"pagetitle":"Euclidean","title":"Manifolds.Euclidean","ref":"/manifolds/stable/manifolds/euclidean/#Manifolds.Euclidean","content":" Manifolds.Euclidean  ‚Äî  Type Euclidean{T,ùîΩ} <: AbstractManifold{ùîΩ} Euclidean vector space. Constructor Euclidean(n) Generate the  $n$ -dimensional vector space  $‚Ñù^n$ . Euclidean(n‚ÇÅ,n‚ÇÇ,...,n·µ¢; field=‚Ñù, parameter::Symbol = :field)\nùîΩ^(n‚ÇÅ,n‚ÇÇ,...,n·µ¢) = Euclidean(n‚ÇÅ,n‚ÇÇ,...,n·µ¢; field=ùîΩ) Generate the vector space of  $k = n_1 ‚ãÖ n_2 ‚ãÖ ‚Ä¶ ‚ãÖ n_i$  values, i.e. the manifold  $ùîΩ^{n_1, n_2, ‚Ä¶, n_i}$ ,  $ùîΩ\\in\\{‚Ñù,‚ÑÇ\\}$ , whose elements are interpreted as  $n_1 √ó n_2 √ó ‚Ä¶ √ó n_i$  arrays. For  $i=2$  we obtain a matrix space. The default  field=‚Ñù  can also be set to  field=‚ÑÇ . The dimension of this space is  $k \\dim_‚Ñù ùîΩ$ , where  $\\dim_‚Ñù ùîΩ$  is the  real_dimension  of the field  $ùîΩ$ . parameter : whether a type parameter should be used to store  n . By default size is stored in type. Value can either be  :field  or  :type . Euclidean(; field=‚Ñù) Generate the 1D Euclidean manifold for an  ‚Ñù -,  ‚ÑÇ -valued  real- or complex-valued immutable values (in contrast to 1-element arrays from the constructor above). source"},{"id":727,"pagetitle":"Euclidean","title":"Base.exp","ref":"/manifolds/stable/manifolds/euclidean/#Base.exp-Tuple{Euclidean, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::Euclidean, p, X) Compute the exponential map on the  Euclidean  manifold  M  from  p  in direction  X , which in this case is just \\[\\exp_p X = p + X.\\] source"},{"id":728,"pagetitle":"Euclidean","title":"Base.log","ref":"/manifolds/stable/manifolds/euclidean/#Base.log-Tuple{Euclidean, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::Euclidean, p, q) Compute the logarithmic map on the  Euclidean M  from  p  to  q , which in this case is just \\[\\log_p q = q-p.\\] source"},{"id":729,"pagetitle":"Euclidean","title":"LinearAlgebra.norm","ref":"/manifolds/stable/manifolds/euclidean/#LinearAlgebra.norm","content":" LinearAlgebra.norm  ‚Äî  Function norm(M::Euclidean, p, X, r::Real=2) Compute the norm of a tangent vector  X  at  p  on the  Euclidean M , i.e. since every tangent space can be identified with  M  itself in this case, just the (Frobenius) norm of  X . Specifying  r , other norms are available as well source"},{"id":730,"pagetitle":"Euclidean","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/euclidean/#Manifolds.manifold_volume-Tuple{Euclidean}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(::Euclidean) Return volume of the  Euclidean  manifold, i.e. infinity. source"},{"id":731,"pagetitle":"Euclidean","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/euclidean/#Manifolds.volume_density-Tuple{Euclidean, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(M::Euclidean, p, X) Return volume density function of  Euclidean  manifold  M , i.e. 1. source"},{"id":732,"pagetitle":"Euclidean","title":"ManifoldsBase.Weingarten","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.Weingarten-Tuple{Euclidean, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Y = Weingarten(M::Euclidean, p, X, V)\nWeingarten!(M::Euclidean, Y, p, X, V) Compute the Weingarten map  $\\mathcal W_p$  at  p  on the  Euclidean M  with respect to the tangent vector  $X \\in T_p\\mathcal M$  and the normal vector  $V \\in N_p\\mathcal M$ . Since this a flat space by itself, the result is always the zero tangent vector. source"},{"id":733,"pagetitle":"Euclidean","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.distance-Tuple{Euclidean, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::Euclidean, p, q, r::Real=2) Compute the Euclidean distance between two points on the  Euclidean  manifold  M , i.e. for vectors it's just the norm of the difference, for matrices and higher order arrays, the matrix and tensor Frobenius norm, respectively. Specifying further an  r‚â†2 , other norms, like the 1-norm or the ‚àû-norm can also be computed. source"},{"id":734,"pagetitle":"Euclidean","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.embed-Tuple{Euclidean, Any, Any}","content":" ManifoldsBase.embed  ‚Äî  Method embed(M::Euclidean, p, X) Embed the tangent vector  X  at point  p  in  M . Equivalent to an identity map. source"},{"id":735,"pagetitle":"Euclidean","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.embed-Tuple{Euclidean, Any}","content":" ManifoldsBase.embed  ‚Äî  Method embed(M::Euclidean, p) Embed the point  p  in  M . Equivalent to an identity map. source"},{"id":736,"pagetitle":"Euclidean","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.injectivity_radius-Tuple{Euclidean}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::Euclidean) Return the injectivity radius on the  Euclidean M , which is  $‚àû$ . source"},{"id":737,"pagetitle":"Euclidean","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.inner-Tuple{Euclidean, Vararg{Any}}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::Euclidean, p, X, Y) Compute the inner product on the  Euclidean M , which is just the inner product on the real-valued or complex valued vector space of arrays (or tensors) of size  $n_1 √ó n_2  √ó  ‚Ä¶  √ó n_i$ , i.e. \\[g_p(X,Y) = \\sum_{k ‚àà I} \\overline{X}_{k} Y_{k},\\] where  $I$  is the set of vectors  $k ‚àà ‚Ñï^i$ , such that for all $i ‚â§ j ‚â§ i$  it holds  $1 ‚â§ k_j ‚â§ n_j$  and  $\\overline{‚ãÖ}$  denotes the complex conjugate. For the special case of  $i ‚â§ 2$ , i.e. matrices and vectors, this simplifies to \\[g_p(X,Y) = \\operatorname{tr}(X^{\\mathrm{H}}Y),\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the Hermitian, i.e. complex conjugate transposed. source"},{"id":738,"pagetitle":"Euclidean","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.is_flat-Tuple{Euclidean}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::Euclidean) Return true.  Euclidean  is a flat manifold. source"},{"id":739,"pagetitle":"Euclidean","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.manifold_dimension-Union{Tuple{Euclidean{<:Any, ùîΩ}}, Tuple{ùîΩ}} where ùîΩ","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::Euclidean) Return the manifold dimension of the  Euclidean M , i.e. the product of all array dimensions and the  real_dimension  of the underlying number system. source"},{"id":740,"pagetitle":"Euclidean","title":"ManifoldsBase.parallel_transport_direction","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.parallel_transport_direction-Tuple{Euclidean, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_direction  ‚Äî  Method parallel_transport_direction(M::Euclidean, p, X, d) the parallel transport on  Euclidean  is the identity, i.e. returns  X . source"},{"id":741,"pagetitle":"Euclidean","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.parallel_transport_to-Tuple{Euclidean, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::Euclidean, p, X, q) the parallel transport on  Euclidean  is the identity, i.e. returns  X . source"},{"id":742,"pagetitle":"Euclidean","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.project-Tuple{Euclidean, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Euclidean, p, X) Project an arbitrary vector  X  into the tangent space of a point  p  on the  Euclidean M , which is just the identity, since any tangent space of  M  can be identified with all of  M . source"},{"id":743,"pagetitle":"Euclidean","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.project-Tuple{Euclidean, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Euclidean, p) Project an arbitrary point  p  onto the  Euclidean  manifold  M , which is of course just the identity map. source"},{"id":744,"pagetitle":"Euclidean","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.representation_size-Tuple{Euclidean}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::Euclidean) Return the array dimensions required to represent an element on the  Euclidean M , i.e. the vector of all array dimensions. source"},{"id":745,"pagetitle":"Euclidean","title":"ManifoldsBase.riemann_tensor","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.riemann_tensor-Tuple{Euclidean, Vararg{Any, 4}}","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(M::Euclidean, p, X, Y, Z) Compute the Riemann tensor  $R(X,Y)Z$  at point  p  on  Euclidean  manifold  M . Its value is always the zero tangent vector. source"},{"id":746,"pagetitle":"Euclidean","title":"ManifoldsBase.sectional_curvature","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.sectional_curvature-Tuple{Euclidean, Any, Any, Any}","content":" ManifoldsBase.sectional_curvature  ‚Äî  Method sectional_curvature(::Euclidean, p, X, Y) Sectional curvature of  Euclidean  manifold  M  is 0. source"},{"id":747,"pagetitle":"Euclidean","title":"ManifoldsBase.sectional_curvature_max","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.sectional_curvature_max-Tuple{Euclidean}","content":" ManifoldsBase.sectional_curvature_max  ‚Äî  Method sectional_curvature_max(::Euclidean) Sectional curvature of  Euclidean  manifold  M  is 0. source"},{"id":748,"pagetitle":"Euclidean","title":"ManifoldsBase.sectional_curvature_min","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.sectional_curvature_min-Tuple{Euclidean}","content":" ManifoldsBase.sectional_curvature_min  ‚Äî  Method sectional_curvature_min(M::Euclidean) Sectional curvature of  Euclidean  manifold  M  is 0. source"},{"id":749,"pagetitle":"Euclidean","title":"ManifoldsBase.vector_transport_to","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.vector_transport_to-Tuple{Euclidean, Any, Any, Any, AbstractVectorTransportMethod}","content":" ManifoldsBase.vector_transport_to  ‚Äî  Method vector_transport_to(M::Euclidean, p, X, q, ::AbstractVectorTransportMethod) Transport the vector  X  from the tangent space at  p  to the tangent space at  q  on the  Euclidean M , which simplifies to the identity. source"},{"id":750,"pagetitle":"Euclidean","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/euclidean/#ManifoldsBase.zero_vector-Tuple{Euclidean, Vararg{Any}}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(M::Euclidean, p) Return the zero vector in the tangent space of  p  on the  Euclidean M , which here is just a zero filled array the same size as  p . source"},{"id":753,"pagetitle":"Fiber bundle","title":"Fiber bundles","ref":"/manifolds/stable/manifolds/fiber_bundle/#FiberBundleSection","content":" Fiber bundles Fiber bundle  $E$  is a manifold that is built on top of another manifold  $\\mathcal M$  (base space). It is characterized by a continuous function  $Œ† : E ‚Üí \\mathcal M$ . For each point  $p ‚àà \\mathcal M$  the preimage of  $p$  by  $Œ†$ ,  $Œ†^{-1}(\\{p\\})$  is called a fiber  $F$ . Bundle projection can be performed using function  bundle_projection . Manifolds.jl  primarily deals with the case of trivial bundles, where  $E$  can be topologically identified with a product  $M√óF$ . Vector bundles  is a special case of a fiber bundle. Other examples include unit tangent bundle. Note that in general fiber bundles don't have a canonical Riemannian structure but can at least be equipped with an  Ehresmann connection , providing notions of parallel transport and curvature."},{"id":754,"pagetitle":"Fiber bundle","title":"Documentation","ref":"/manifolds/stable/manifolds/fiber_bundle/#Documentation","content":" Documentation"},{"id":755,"pagetitle":"Fiber bundle","title":"Manifolds.FiberBundle","ref":"/manifolds/stable/manifolds/fiber_bundle/#Manifolds.FiberBundle","content":" Manifolds.FiberBundle  ‚Äî  Type FiberBundle{ùîΩ,TVS<:FiberType,TM<:AbstractManifold{ùîΩ},TVT<:FiberBundleProductVectorTransport} <: AbstractManifold{ùîΩ} Fiber bundle on a  AbstractManifold M  of type  FiberType . Examples include vector bundles, principal bundles or unit tangent bundles, see also  Fiber Bundle . Fields manifold  ‚Äì the  AbstractManifold               manifold the Fiber bundle is defined on, type      ‚Äì representing the type of fiber we use. Constructor FiberBundle(M::AbstractManifold, type::FiberType) source"},{"id":756,"pagetitle":"Fiber bundle","title":"Manifolds.FiberBundleInverseProductRetraction","ref":"/manifolds/stable/manifolds/fiber_bundle/#Manifolds.FiberBundleInverseProductRetraction","content":" Manifolds.FiberBundleInverseProductRetraction  ‚Äî  Type struct FiberBundleInverseProductRetraction <: AbstractInverseRetractionMethod end Inverse retraction of the point  y  at point  p  from vector bundle  B  over manifold  B.fiber  (denoted  $\\mathcal M$ ). The inverse retraction is derived as a product manifold-style approximation to the logarithmic map in the Sasaki metric. The considered product manifold is the product between the manifold  $\\mathcal M$  and the topological vector space isometric to the fiber. Notation The point  $p = (x_p, V_p)$  where  $x_p ‚àà \\mathcal M$  and  $V_p$  belongs to the fiber  $F=œÄ^{-1}(\\{x_p\\})$  of the vector bundle  $B$  where  $œÄ$  is the canonical projection of that vector bundle  $B$ . Similarly,  $q = (x_q, V_q)$ . The inverse retraction is calculated as \\[\\operatorname{retr}^{-1}_p q = (\\operatorname{retr}^{-1}_{x_p}(x_q), V_{\\operatorname{retr}^{-1}} - V_p)\\] where  $V_{\\operatorname{retr}^{-1}}$  is the result of vector transport of  $V_q$  to the point  $x_p$ . The difference  $V_{\\operatorname{retr}^{-1}} - V_p$  corresponds to the logarithmic map in the vector space  $F$ . See also  FiberBundleProductRetraction . source"},{"id":757,"pagetitle":"Fiber bundle","title":"Manifolds.FiberBundleProductRetraction","ref":"/manifolds/stable/manifolds/fiber_bundle/#Manifolds.FiberBundleProductRetraction","content":" Manifolds.FiberBundleProductRetraction  ‚Äî  Type struct FiberBundleProductRetraction <: AbstractRetractionMethod end Product retraction map of tangent vector  $X$  at point  $p$  from vector bundle  B  over manifold  B.fiber  (denoted  $\\mathcal M$ ). The retraction is derived as a product manifold-style approximation to the exponential map in the Sasaki metric. The considered product manifold is the product between the manifold  $\\mathcal M$  and the topological vector space isometric to the fiber. Notation: The point  $p = (x_p, V_p)$  where  $x_p ‚àà \\mathcal M$  and  $V_p$  belongs to the fiber  $F=œÄ^{-1}(\\{x_p\\})$  of the vector bundle  $B$  where  $œÄ$  is the canonical projection of that vector bundle  $B$ . The tangent vector  $X = (V_{X,M}, V_{X,F}) ‚àà T_pB$  where  $V_{X,M}$  is a tangent vector from the tangent space  $T_{x_p}\\mathcal M$  and  $V_{X,F}$  is a tangent vector from the tangent space  $T_{V_p}F$  (isomorphic to  $F$ ). The retraction is calculated as math \\operatorname{retr}_p(X) = (\\exp_{x_p}(V_{X,M}), V_{\\exp}) ` where  $V_{\\exp}$  is the result of vector transport of  $V_p + V_{X,F}$  to the point  $\\exp_{x_p}(V_{X,M})$ . The sum  $V_p + V_{X,F}$  corresponds to the exponential map in the vector space  $F$ . See also  FiberBundleInverseProductRetraction . source"},{"id":758,"pagetitle":"Fiber bundle","title":"Manifolds.FiberBundleProductVectorTransport","ref":"/manifolds/stable/manifolds/fiber_bundle/#Manifolds.FiberBundleProductVectorTransport","content":" Manifolds.FiberBundleProductVectorTransport  ‚Äî  Type FiberBundleProductVectorTransport{\n    TMP<:AbstractVectorTransportMethod,\n    TMV<:AbstractVectorTransportMethod,\n} <: AbstractVectorTransportMethod Vector transport type on  FiberBundle . Fields method_horizontal  ‚Äì vector transport method of the horizontal part (related to manifold M) method_vertical  ‚Äì vector transport method of the vertical part (related to fibers). The vector transport is derived as a product manifold-style vector transport. The considered product manifold is the product between the manifold  $\\mathcal M$  and the space corresponding to the fiber. Constructor FiberBundleProductVectorTransport(\n    M::AbstractManifold=DefaultManifold();\n    vector_transport_method_horizontal::AbstractVectorTransportMethod = default_vector_transport_method(M),\n    vector_transport_method_vertical::AbstractVectorTransportMethod = default_vector_transport_method(M),\n) Construct the  FiberBundleProductVectorTransport  using the  default_vector_transport_method , which uses  ParallelTransport  if no manifold is provided. source"},{"id":759,"pagetitle":"Fiber bundle","title":"Manifolds.bundle_projection","ref":"/manifolds/stable/manifolds/fiber_bundle/#Manifolds.bundle_projection-Tuple{FiberBundle, Any}","content":" Manifolds.bundle_projection  ‚Äî  Method bundle_projection(B::FiberBundle, p) Projection of point  p  from the bundle  M  to the base manifold. Returns the point on the base manifold  B.manifold  at which the vector part of  p  is attached. source"},{"id":760,"pagetitle":"Fiber bundle","title":"Manifolds.bundle_transport_tangent_direction","ref":"/manifolds/stable/manifolds/fiber_bundle/#Manifolds.bundle_transport_tangent_direction","content":" Manifolds.bundle_transport_tangent_direction  ‚Äî  Function bundle_transport_tangent_direction(B::FiberBundle, p, pf, X, d) Compute parallel transport of vertical vector  X  according to Ehresmann connection on  FiberBundle B , in direction  $d\\in T_p \\mathcal M$ .  $X$  is an element of the vertical bundle  $VF\\mathcal M$  at  pf  from tangent to fiber  $\\pi^{-1}({p})$ ,  $p\\in \\mathcal M$ . source"},{"id":761,"pagetitle":"Fiber bundle","title":"Manifolds.bundle_transport_tangent_to","ref":"/manifolds/stable/manifolds/fiber_bundle/#Manifolds.bundle_transport_tangent_to","content":" Manifolds.bundle_transport_tangent_to  ‚Äî  Function bundle_transport_tangent_to(B::FiberBundle, p, pf, X, q) Compute parallel transport of vertical vector  X  according to Ehresmann connection on  FiberBundle B , to point  $q\\in \\mathcal M$ .  $X$  is an element of the vertical bundle  $VF\\mathcal M$  at  pf  from tangent to fiber  $\\pi^{-1}({p})$ ,  $p\\in \\mathcal M$ . source"},{"id":762,"pagetitle":"Fiber bundle","title":"Manifolds.bundle_transport_to","ref":"/manifolds/stable/manifolds/fiber_bundle/#Manifolds.bundle_transport_to-Tuple{FiberBundle, Any, Any, Any}","content":" Manifolds.bundle_transport_to  ‚Äî  Method bundle_transport_to(B::FiberBundle, p, X, q) Given a fiber bundle  $B=F \\mathcal M$ , points  $p, q\\in\\mathcal M$ , an element  $X$  of the fiber over  $p$ , transport  $X$  to fiber over  $q$ . Exact meaning of the operation depends on the fiber bundle, or may even be undefined. Some fiber bundles may declare a default local section around each point crossing  X , represented by this function. source"},{"id":763,"pagetitle":"Fiber bundle","title":"ManifoldsBase.base_manifold","ref":"/manifolds/stable/manifolds/fiber_bundle/#ManifoldsBase.base_manifold-Tuple{FiberBundle}","content":" ManifoldsBase.base_manifold  ‚Äî  Method base_manifold(B::FiberBundle) Return the manifold the  FiberBundle s is build on. source"},{"id":764,"pagetitle":"Fiber bundle","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/fiber_bundle/#ManifoldsBase.zero_vector-Tuple{FiberBundle, Vararg{Any}}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(B::FiberBundle, p) Zero tangent vector at point  p  from the fiber bundle  B  over manifold  B.fiber  (denoted  $\\mathcal M$ ). The zero vector belongs to the space  $T_{p}B$ Notation: The point  $p = (x_p, V_p)$  where  $x_p ‚àà \\mathcal M$  and  $V_p$  belongs to the fiber  $F=œÄ^{-1}(\\{x_p\\})$  of the vector bundle  $B$  where  $œÄ$  is the canonical projection of that vector bundle  $B$ . The zero vector is calculated as $\\mathbf{0}_{p} = (\\mathbf{0}_{x_p}, \\mathbf{0}_F)$ where  $\\mathbf{0}_{x_p}$  is the zero tangent vector from  $T_{x_p}\\mathcal M$  and  $\\mathbf{0}_F$  is the zero element of the vector space  $F$ . source"},{"id":767,"pagetitle":"Fixed-rank matrices","title":"Fixed-rank matrices","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#FixedRankMatrices","content":" Fixed-rank matrices"},{"id":768,"pagetitle":"Fixed-rank matrices","title":"Manifolds.FixedRankMatrices","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#Manifolds.FixedRankMatrices","content":" Manifolds.FixedRankMatrices  ‚Äî  Type FixedRankMatrices{T,ùîΩ} <: AbstractDecoratorManifold{ùîΩ} The manifold of  $m√ón$  real-valued or complex-valued matrices of fixed rank  $k$ , i.e. \\[\\bigl\\{ p ‚àà ùîΩ^{m√ón}\\ \\big|\\ \\operatorname{rank}(p) = k\\bigr\\},\\] where  $ùîΩ ‚àà \\{‚Ñù,‚ÑÇ\\}$  and the rank is the number of linearly independent columns of a matrix. Representation with 3 matrix factors A point  $p ‚àà \\mathcal M$  can be stored using unitary matrices  $U ‚àà ùîΩ^{m√ók}$ ,  $V ‚àà ùîΩ^{n√ók}$  as well as the  $k$  singular values of  $p = U_p S V_p^\\mathrm{H}$ , where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transpose or Hermitian. In other words,  $U$  and  $V$  are from the manifolds  Stiefel (m,k,ùîΩ)  and  Stiefel (n,k,ùîΩ) , respectively; see  SVDMPoint  for details. The tangent space  $T_p \\mathcal M$  at a point  $p ‚àà \\mathcal M$  with  $p=U_p S V_p^\\mathrm{H}$  is given by \\[T_p\\mathcal M = \\bigl\\{ U_p M V_p^\\mathrm{H} + U_X V_p^\\mathrm{H} + U_p V_X^\\mathrm{H} :\n    M  ‚àà ùîΩ^{k√ók},\n    U_X  ‚àà ùîΩ^{m√ók},\n    V_X  ‚àà ùîΩ^{n√ók}\n    \\text{ s.t. }\n    U_p^\\mathrm{H}U_X = 0_k,\n    V_p^\\mathrm{H}V_X = 0_k\n\\bigr\\},\\] where  $0_k$  is the  $k√ók$  zero matrix. See  UMVTangentVector  for details. The (default) metric of this manifold is obtained by restricting the metric on  $‚Ñù^{m√ón}$  to the tangent bundle [ Van13 ]. Constructor FixedRankMatrices(m, n, k[, field=‚Ñù]) Generate the manifold of  m -by- n  ( field -valued) matrices of rank  k . source"},{"id":769,"pagetitle":"Fixed-rank matrices","title":"Manifolds.OrthographicInverseRetraction","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#Manifolds.OrthographicInverseRetraction","content":" Manifolds.OrthographicInverseRetraction  ‚Äî  Type OrthographicInverseRetraction <: AbstractInverseRetractionMethod Retractions that are related to orthographic projections, which was first used in [ AM12 ]. source"},{"id":770,"pagetitle":"Fixed-rank matrices","title":"Manifolds.OrthographicRetraction","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#Manifolds.OrthographicRetraction","content":" Manifolds.OrthographicRetraction  ‚Äî  Type OrthographicRetraction <: AbstractRetractionMethod Retractions that are related to orthographic projections, which was first used in [ AM12 ]. source"},{"id":771,"pagetitle":"Fixed-rank matrices","title":"Manifolds.SVDMPoint","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#Manifolds.SVDMPoint","content":" Manifolds.SVDMPoint  ‚Äî  Type SVDMPoint <: AbstractManifoldPoint A point on a certain manifold, where the data is stored in a svd like fashion, i.e. in the form  $USV^\\mathrm{H}$ , where this structure stores  $U$ ,  $S$  and  $V^\\mathrm{H}$ . The storage might also be shortened to just  $k$  singular values and accordingly shortened  $U$  (columns) and  $V^\\mathrm{H}$  (rows). Constructors SVDMPoint(A)  for a matrix  A , stores its svd factors (i.e. implicitly  $k=\\min\\{m,n\\}$ ) SVDMPoint(S)  for an  SVD  object, stores its svd factors (i.e. implicitly  $k=\\min\\{m,n\\}$ ) SVDMPoint(U,S,Vt)  for the svd factors to initialize the  SVDMPoint (i.e. implicitly k=\\min\\{m,n\\} `) SVDMPoint(A,k)  for a matrix  A , stores its svd factors shortened to the best rank  $k$  approximation SVDMPoint(S,k)  for an  SVD  object, stores its svd factors shortened to the best rank  $k$  approximation SVDMPoint(U,S,Vt,k)  for the svd factors to initialize the  SVDMPoint , stores its svd factors shortened to the best rank  $k$  approximation source"},{"id":772,"pagetitle":"Fixed-rank matrices","title":"Manifolds.UMVTangentVector","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#Manifolds.UMVTangentVector","content":" Manifolds.UMVTangentVector  ‚Äî  Type UMVTangentVector <: AbstractTangentVector A tangent vector that can be described as a product  $U_p M V_p^\\mathrm{H} + U_X V_p^\\mathrm{H} + U_p V_X^\\mathrm{H}$ , where  $X = U_X S V_X^\\mathrm{H}$  is its base point, see for example  FixedRankMatrices . The base point  $p$  is required for example embedding this point, but it is not stored. The fields of thie tangent vector are  U  for  $U_X$ ,  M  and  Vt  to store  $V_X^\\mathrm{H}$ Constructors UMVTangentVector(U,M,Vt)  store umv factors to initialize the  UMVTangentVector UMVTangentVector(U,M,Vt,k)  store the umv factors after shortening them down to inner dimensions  k . source"},{"id":773,"pagetitle":"Fixed-rank matrices","title":"Base.rand","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#Base.rand-Tuple{FixedRankMatrices}","content":" Base.rand  ‚Äî  Method Random.rand(M::FixedRankMatrices; vector_at=nothing, kwargs...) If  vector_at  is  nothing , return a random point on the  FixedRankMatrices  manifold. The orthogonal matrices are sampled from the  Stiefel  manifold and the singular values are sampled uniformly at random. If  vector_at  is not  nothing , generate a random tangent vector in the tangent space of the point  vector_at  on the  FixedRankMatrices  manifold  M . source"},{"id":774,"pagetitle":"Fixed-rank matrices","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldDiff.riemannian_Hessian-Tuple{FixedRankMatrices, Vararg{Any, 4}}","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method Y = riemannian_Hessian(M::FixedRankMatrices, p, G, H, X)\nriemannian_Hessian!(M::FixedRankMatrices, Y, p, G, H, X) Compute the Riemannian Hessian  $\\operatorname{Hess} f(p)[X]$  given the Euclidean gradient  $‚àá f(\\tilde p)$  in  G  and the Euclidean Hessian  $‚àá^2 f(\\tilde p)[\\tilde X]$  in  H , where  $\\tilde p, \\tilde X$  are the representations of  $p,X$  in the embedding,. The Riemannian Hessian can be computed as stated in Remark 4.1 [ Ngu23 ] or Section 2.3 [ Van13 ], that B. Vandereycken adopted for  Manopt (Matlab) . source"},{"id":775,"pagetitle":"Fixed-rank matrices","title":"Manifolds.inverse_retract_orthographic!","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#Manifolds.inverse_retract_orthographic!-Tuple{AbstractManifold, Any, Any, Any}","content":" Manifolds.inverse_retract_orthographic!  ‚Äî  Method inverse_retract_orthographic!(M::AbstractManifold, X, p, q) Compute the in-place variant of the  OrthographicInverseRetraction . source"},{"id":776,"pagetitle":"Fixed-rank matrices","title":"Manifolds.retract_orthographic!","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#Manifolds.retract_orthographic!-Tuple{AbstractManifold, Any, Any, Any}","content":" Manifolds.retract_orthographic!  ‚Äî  Method retract_orthographic!(M::AbstractManifold, q, p, X) Compute the in-place variant of the  OrthographicRetraction . source"},{"id":777,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.check_point-Tuple{FixedRankMatrices, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::FixedRankMatrices, p; kwargs...) Check whether the matrix or  SVDMPoint x  ids a valid point on the  FixedRankMatrices M , i.e. is an  m -by n  matrix of rank  k . For the  SVDMPoint  the internal representation also has to have the right shape, i.e.  p.U  and  p.Vt  have to be unitary. The keyword arguments are passed to the  rank  function that verifies the rank of  p . source"},{"id":778,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.check_vector-Tuple{FixedRankMatrices, SVDMPoint, UMVTangentVector}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M:FixedRankMatrices, p, X; kwargs...) Check whether the tangent  UMVTangentVector X  is from the tangent space of the  SVDMPoint p  on the  FixedRankMatrices M , i.e. that  v.U  and  v.Vt  are (columnwise) orthogonal to  x.U  and  x.Vt , respectively, and its dimensions are consistent with  p  and  X.M , i.e. correspond to  m -by- n  matrices of rank  k . source"},{"id":779,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.default_inverse_retraction_method","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.default_inverse_retraction_method-Tuple{FixedRankMatrices}","content":" ManifoldsBase.default_inverse_retraction_method  ‚Äî  Method default_inverse_retraction_method(M::FixedRankMatrices) Return  PolarInverseRetraction  as the default inverse retraction for the  FixedRankMatrices  manifold. source"},{"id":780,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.default_retraction_method","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.default_retraction_method-Tuple{FixedRankMatrices}","content":" ManifoldsBase.default_retraction_method  ‚Äî  Method default_retraction_method(M::FixedRankMatrices) Return  PolarRetraction  as the default retraction for the  FixedRankMatrices  manifold. source"},{"id":781,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.default_vector_transport_method","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.default_vector_transport_method-Tuple{FixedRankMatrices}","content":" ManifoldsBase.default_vector_transport_method  ‚Äî  Method default_vector_transport_method(M::FixedRankMatrices) Return the  ProjectionTransport  as the default vector transport method for the  FixedRankMatrices  manifold. source"},{"id":782,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.embed-Tuple{FixedRankMatrices, SVDMPoint, UMVTangentVector}","content":" ManifoldsBase.embed  ‚Äî  Method embed(M::FixedRankMatrices, p, X) Embed the tangent vector  X  at point  p  in  M  from its  UMVTangentVector  representation  into the set of  $m√ón$  matrices. The formula reads \\[U_pMV_p^{\\mathrm{H}} + U_XV_p^{\\mathrm{H}} + U_pV_X^{\\mathrm{H}}\\] source"},{"id":783,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.embed-Tuple{FixedRankMatrices, SVDMPoint}","content":" ManifoldsBase.embed  ‚Äî  Method embed(::FixedRankMatrices, p::SVDMPoint) Embed the point  p  from its  SVDMPoint  representation into the set of  $m√ón$  matrices by computing  $USV^{\\mathrm{H}}$ . source"},{"id":784,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.injectivity_radius-Tuple{FixedRankMatrices}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(::FixedRankMatrices) Return the incjectivity radius of the manifold of  FixedRankMatrices , i.e. 0. See [ HU17 ]. source"},{"id":785,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.inner-Tuple{FixedRankMatrices, SVDMPoint, UMVTangentVector, UMVTangentVector}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::FixedRankMatrices, p::SVDMPoint, X::UMVTangentVector, Y::UMVTangentVector) Compute the inner product of  X  and  Y  in the tangent space of  p  on the  FixedRankMatrices M , which is inherited from the embedding, i.e. can be computed using  dot  on the elements ( U ,  Vt ,  M ) of  X  and  Y . source"},{"id":786,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.inverse_retract-Tuple{FixedRankMatrices, Any, Any, OrthographicInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M, p, q, ::OrthographicInverseRetraction) Compute the orthographic inverse retraction  FixedRankMatrices M  by computing \\[    X = P_{T_{p}M}(q - p) = qVV^\\mathrm{T} + UU^{\\mathrm{T}}q - UU^{\\mathrm{T}}qVV^{\\mathrm{T}} - p,\\] where  $p$  is a  SVDMPoint (U,S,Vt)  and  $P_{T_{p}M}$  is the  project ion onto the tangent space at  $p$ . For more details, see [ AO14 ]. source"},{"id":787,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.is_flat-Tuple{FixedRankMatrices}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::FixedRankMatrices) Return false.  FixedRankMatrices  is not a flat manifold. source"},{"id":788,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.manifold_dimension-Union{Tuple{FixedRankMatrices{<:Any, ùîΩ}}, Tuple{ùîΩ}} where ùîΩ","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::FixedRankMatrices) Return the manifold dimension for the  ùîΩ -valued  FixedRankMatrices M  of dimension  m x n  of rank  k , namely \\[\\dim(\\mathcal M) = k(m + n - k) \\dim_‚Ñù ùîΩ,\\] where  $\\dim_‚Ñù ùîΩ$  is the  real_dimension  of  ùîΩ . source"},{"id":789,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.project-Tuple{FixedRankMatrices, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M, p, A) Project the matrix  $A ‚àà ‚Ñù^{m,n}$  or from the embedding the tangent space at  $p$  on the  FixedRankMatrices M , further decomposing the result into  $X=UMV^\\mathrm{H}$ , i.e. a  UMVTangentVector . source"},{"id":790,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.representation_size-Tuple{FixedRankMatrices}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::FixedRankMatrices) Return the element size of a point on the  FixedRankMatrices M , i.e. the size of matrices on this manifold  $(m,n)$ . source"},{"id":791,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.retract-Tuple{FixedRankMatrices, Any, Any, OrthographicRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::FixedRankMatrices, p, X, ::OrthographicRetraction) Compute the OrthographicRetraction on the  FixedRankMatrices M  by finding the nearest point to  $p + X$  in \\[    p + X + N_{p}\\mathcal M \\cap \\mathcal M\\] where  $N_{p}\\mathcal M$  is the Normal Space of  $T_{p}\\mathcal M$ . If  $X$  is sufficiently small, then the nearest such point is unique and can be expressed by \\[    q = (U(S + M) + U_{p})(S + M)^{-1}((S + M)V^{\\mathrm{T}} + V^{\\mathrm{T}}_{p}),\\] where  $p$  is a  SVDMPoint (U,S,Vt)  and  $X$  is an  UMVTangentVector (Up,M,Vtp) . For more details, see [ AO14 ]. source"},{"id":792,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.retract-Tuple{FixedRankMatrices, Any, Any, PolarRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M, p, X, ::PolarRetraction) Compute an SVD-based retraction on the  FixedRankMatrices M  by computing \\[    q = U_kS_kV_k^\\mathrm{H},\\] where  $U_k S_k V_k^\\mathrm{H}$  is the shortened singular value decomposition  $USV^\\mathrm{H}=p+X$ , in the sense that  $S_k$  is the diagonal matrix of size  $k√ók$  with the  $k$  largest singular values and  $U$  and  $V$  are shortened accordingly. source"},{"id":793,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.vector_transport_to!","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.vector_transport_to!-Tuple{FixedRankMatrices, Any, Any, Any, ProjectionTransport}","content":" ManifoldsBase.vector_transport_to!  ‚Äî  Method vector_transport_to(M::FixedRankMatrices, p, X, q, ::ProjectionTransport) Compute the vector transport of the tangent vector  X  at  p  to  q , using the  project  of  X  to  q . source"},{"id":794,"pagetitle":"Fixed-rank matrices","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#ManifoldsBase.zero_vector-Tuple{FixedRankMatrices, SVDMPoint}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(M::FixedRankMatrices, p::SVDMPoint) Return a  UMVTangentVector  representing the zero tangent vector in the tangent space of  p  on the  FixedRankMatrices M , for example all three elements of the resulting structure are zero matrices. source"},{"id":795,"pagetitle":"Fixed-rank matrices","title":"Literature","ref":"/manifolds/stable/manifolds/fixedrankmatrices/#Literature","content":" Literature [AM12] P.-A.¬†Absil and J.¬†Malick.  Projection-like Retractions on Matrix Manifolds .  SIAM¬†Journal¬†on¬†Optimization  22 , 135‚Äì158  (2012). [AO14] P.-A.¬†Absil and I.¬†V.¬†Oseledets.  Low-rank retractions: a survey and new results .  Computational¬†Optimization¬†and¬†Applications  62 , 5‚Äì29  (2014). [HU17] S.¬†Hosseini and A.¬†Uschmajew.  A Riemannian Gradient Sampling Algorithm for Nonsmooth Optimization on Manifolds .  SIAM¬†J.¬†Optim.  27 , 173‚Äì189  (2017). [Ngu23] D.¬†Nguyen.  Operator-Valued Formulas for Riemannian Gradient and Hessian and Families of Tractable Metrics in Riemannian Optimization .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  198 , 135‚Äì164  (2023),  arXiv:2009.10159 . [Van13] B.¬†Vandereycken.  Low-rank matrix completion by Riemannian optimization .  SIAM¬†Journal¬†on¬†Optimization  23 , 1214‚Äì1236  (2013)."},{"id":798,"pagetitle":"Flag","title":"Flag manifold","ref":"/manifolds/stable/manifolds/flag/#Flag-manifold","content":" Flag manifold"},{"id":799,"pagetitle":"Flag","title":"Manifolds.Flag","ref":"/manifolds/stable/manifolds/flag/#Manifolds.Flag","content":" Manifolds.Flag  ‚Äî  Type Flag{T,d} <: AbstractDecoratorManifold{‚Ñù} Flag manifold of  $d$  subspaces of  $‚Ñù^N$  [ YWL21 ]. By default the manifold uses the Stiefel coordinates representation, embedding it in the  Stiefel  manifold. The other available representation is an embedding in  OrthogonalMatrices . It can be utilized using  OrthogonalPoint  and  OrthogonalTangentVector  wrappers. Tangent space is represented in the block-skew-symmetric form. Constructor Flag(N, n1, n2, ..., nd; parameter::Symbol=:type) Generate the manifold  $\\operatorname{Flag}(n_1, n_2, ..., n_d; N)$  of subspaces \\[ùïç_1 ‚äÜ ùïç_2 ‚äÜ ‚ãØ ‚äÜ V_d, \\quad \\operatorname{dim}(ùïç_i) = n_i\\] where  $ùïç_i$  for  $i ‚àà 1, 2, ‚Ä¶, d$  are subspaces of  $‚Ñù^N$  of dimension  $\\operatorname{dim} ùïç_i = n_i$ . parameter : whether a type parameter should be used to store  n . By default size is stored in type. Value can either be  :field  or  :type . source"},{"id":800,"pagetitle":"Flag","title":"Manifolds.OrthogonalPoint","ref":"/manifolds/stable/manifolds/flag/#Manifolds.OrthogonalPoint","content":" Manifolds.OrthogonalPoint  ‚Äî  Type OrthogonalPoint <: AbstractManifoldPoint A type to represent points on a manifold  Flag  in the orthogonal coordinates representation, i.e. a rotation matrix. source"},{"id":801,"pagetitle":"Flag","title":"Manifolds.OrthogonalTangentVector","ref":"/manifolds/stable/manifolds/flag/#Manifolds.OrthogonalTangentVector","content":" Manifolds.OrthogonalTangentVector  ‚Äî  Type OrthogonalTangentVector <: AbstractTangentVector A type to represent tangent vectors to points on a  Flag  manifold  in the orthogonal coordinates representation. source"},{"id":802,"pagetitle":"Flag","title":"Manifolds.ZeroTuple","ref":"/manifolds/stable/manifolds/flag/#Manifolds.ZeroTuple","content":" Manifolds.ZeroTuple  ‚Äî  Type ZeroTuple Internal structure for representing shape of a  Flag  manifold. Behaves like a normal tuple, except at index zero returns value 0. source"},{"id":803,"pagetitle":"Flag","title":"Base.convert","ref":"/manifolds/stable/manifolds/flag/#Base.convert-Tuple{Type{AbstractMatrix}, Flag, OrthogonalPoint, OrthogonalTangentVector}","content":" Base.convert  ‚Äî  Method convert(::Type{AbstractMatrix}, M::Flag, p::OrthogonalPoint, X::OrthogonalTangentVector) Convert tangent vector from  Flag  manifold  M  from orthogonal representation to Stiefel representation. source"},{"id":804,"pagetitle":"Flag","title":"Base.convert","ref":"/manifolds/stable/manifolds/flag/#Base.convert-Tuple{Type{AbstractMatrix}, Flag, OrthogonalPoint}","content":" Base.convert  ‚Äî  Method convert(::Type{AbstractMatrix}, M::Flag, p::OrthogonalPoint) Convert point  p  from  Flag  manifold  M  from orthogonal representation to Stiefel representation. source"},{"id":805,"pagetitle":"Flag","title":"Base.convert","ref":"/manifolds/stable/manifolds/flag/#Base.convert-Tuple{Type{OrthogonalPoint}, Flag, AbstractMatrix}","content":" Base.convert  ‚Äî  Method convert(::Type{OrthogonalPoint}, M::Flag, p::AbstractMatrix) Convert point  p  from  Flag  manifold  M  from Stiefel representation to orthogonal representation. source"},{"id":806,"pagetitle":"Flag","title":"Base.convert","ref":"/manifolds/stable/manifolds/flag/#Base.convert-Tuple{Type{OrthogonalTangentVector}, Flag, AbstractMatrix, AbstractMatrix}","content":" Base.convert  ‚Äî  Method convert(::Type{OrthogonalTangentVector}, M::Flag, p::AbstractMatrix, X::AbstractMatrix) Convert tangent vector from  Flag  manifold  M  from Stiefel representation to orthogonal representation. source"},{"id":807,"pagetitle":"Flag","title":"ManifoldsBase.get_embedding","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.get_embedding-Union{Tuple{Flag{Tuple{Int64}, dp1}}, Tuple{dp1}} where dp1","content":" ManifoldsBase.get_embedding  ‚Äî  Method get_embedding(M::Flag) Get the embedding of the  Flag  manifold  M , i.e. the  Stiefel  manifold. source"},{"id":808,"pagetitle":"Flag","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.injectivity_radius-Tuple{Flag}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::Flag)\ninjectivity_radius(M::Flag, p) Return the injectivity radius on the  Flag M , which is  $\\frac{œÄ}{2}$ . source"},{"id":809,"pagetitle":"Flag","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.manifold_dimension-Union{Tuple{Flag{<:Any, dp1}}, Tuple{dp1}} where dp1","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::Flag) Return dimension of flag manifold  $\\operatorname{Flag}(n_1, n_2, ..., n_d; N)$ . The formula reads  $\\sum_{i=1}^d (n_i-n_{i-1})(N-n_i)$ . source"},{"id":810,"pagetitle":"Flag","title":"The flag manifold represented as points on the Stiefel manifold","ref":"/manifolds/stable/manifolds/flag/#The-flag-manifold-represented-as-points-on-the-[Stiefel](@ref)-manifold","content":" The flag manifold represented as points on the  Stiefel  manifold"},{"id":811,"pagetitle":"Flag","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.check_vector-Union{Tuple{dp1}, Tuple{Flag{<:Any, dp1}, AbstractMatrix, AbstractMatrix}} where dp1","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::Flag, p::AbstractMatrix, X::AbstractMatrix; kwargs... ) Check whether  X  is a tangent vector to point  p  on the  Flag  manifold  M $\\operatorname{Flag}(n_1, n_2, ..., n_d; N)$  in the Stiefel representation, i.e. that  X  is a matrix of the form \\[X = \\begin{bmatrix}\n0                     & B_{1,2}               & ‚ãØ & B_{1,d} \\\\\n-B_{1,2}^\\mathrm{T}   & 0                     & ‚ãØ & B_{2,d} \\\\\n\\vdots                & \\vdots                & ‚ã± & \\vdots  \\\\\n-B_{1,d}^\\mathrm{T}   & -B_{2,d}^\\mathrm{T}   & ‚ãØ & 0       \\\\\n-B_{1,d+1}^\\mathrm{T} & -B_{2,d+1}^\\mathrm{T} & ‚ãØ & -B_{d,d+1}^\\mathrm{T}\n\\end{bmatrix}\\] where  $B_{i,j} ‚àà ‚Ñù^{(n_i - n_{i-1}) √ó (n_j - n_{j-1})}$ , for   $1 ‚â§ i < j ‚â§ d+1$ . source"},{"id":812,"pagetitle":"Flag","title":"ManifoldsBase.default_inverse_retraction_method","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.default_inverse_retraction_method-Tuple{Flag}","content":" ManifoldsBase.default_inverse_retraction_method  ‚Äî  Method default_inverse_retraction_method(M::Flag) Return  PolarInverseRetraction  as the default inverse retraction for the  Flag  manifold. source"},{"id":813,"pagetitle":"Flag","title":"ManifoldsBase.default_retraction_method","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.default_retraction_method-Tuple{Flag}","content":" ManifoldsBase.default_retraction_method  ‚Äî  Method default_retraction_method(M::Flag) Return  PolarRetraction  as the default retraction for the  Flag  manifold. source"},{"id":814,"pagetitle":"Flag","title":"ManifoldsBase.default_vector_transport_method","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.default_vector_transport_method-Tuple{Flag}","content":" ManifoldsBase.default_vector_transport_method  ‚Äî  Method default_vector_transport_method(M::Flag) Return the  ProjectionTransport  as the default vector transport method for the  Flag  manifold. source"},{"id":815,"pagetitle":"Flag","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.inverse_retract-Tuple{Flag, Any, Any, PolarInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::Flag, p, q, ::PolarInverseRetraction) Compute the inverse retraction for the  PolarRetraction , on the  Flag  manifold  M . source"},{"id":816,"pagetitle":"Flag","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.project-Tuple{Flag, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(::Flag, p, X) Project vector  X  in the Euclidean embedding to the tangent space at point  p  on  Flag  manifold. The formula reads [ YWL21 ]: \\[Y_i = X_i - (p_i p_i^{\\mathrm{T}}) X_i + \\sum_{j \\neq i} p_j X_j^{\\mathrm{T}} p_i\\] for  $i$  from 1 to  $d$  where the resulting vector is  $Y = [Y_1, Y_2, ‚Ä¶, Y_d]$  and  $X = [X_1, X_2, ‚Ä¶, X_d]$ ,  $p = [p_1, p_2, ‚Ä¶, p_d]$  are decompositions into basis vector matrices for consecutive subspaces of the flag. source"},{"id":817,"pagetitle":"Flag","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.retract-Tuple{Flag, Any, Any, PolarRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::Flag, p, X, ::PolarRetraction) Compute the SVD-based retraction  PolarRetraction  on the  Flag M . With  $USV = p + X$  the retraction reads \\[\\operatorname{retr}_p X = UV^\\mathrm{H},\\] where  $\\cdot^{\\mathrm{H}}$  denotes the complex conjugate transposed or Hermitian. source"},{"id":818,"pagetitle":"Flag","title":"The flag manifold represented as orthogonal matrices","ref":"/manifolds/stable/manifolds/flag/#The-flag-manifold-represented-as-orthogonal-matrices","content":" The flag manifold represented as orthogonal matrices"},{"id":819,"pagetitle":"Flag","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.check_vector-Union{Tuple{dp1}, Tuple{Flag{<:Any, dp1}, OrthogonalPoint, OrthogonalTangentVector}} where dp1","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::Flag, p::OrthogonalPoint, X::OrthogonalTangentVector; kwargs... ) Check whether  X  is a tangent vector to point  p  on the  Flag  manifold  M $\\operatorname{Flag}(n_1, n_2, ..., n_d; N)$  in the orthogonal matrix representation, i.e. that  X  is block-skew-symmetric with zero diagonal: \\[X = \\begin{bmatrix}\n0                     & B_{1,2}               & ‚ãØ & B_{1,d+1} \\\\\n-B_{1,2}^\\mathrm{T}   & 0                     & ‚ãØ & B_{2,d+1} \\\\\n\\vdots                & \\vdots                & ‚ã± & \\vdots    \\\\\n-B_{1,d+1}^\\mathrm{T} & -B_{2,d+1}^\\mathrm{T} & ‚ãØ & 0\n\\end{bmatrix}\\] where  $B_{i,j} ‚àà ‚Ñù^{(n_i - n_{i-1}) √ó (n_j - n_{j-1})}$ , for   $1 ‚â§ i < j ‚â§ d+1$ . source"},{"id":820,"pagetitle":"Flag","title":"ManifoldsBase.get_embedding","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.get_embedding-Union{Tuple{N}, Tuple{Flag{ManifoldsBase.TypeParameter{Tuple{N}}}, OrthogonalPoint}} where N","content":" ManifoldsBase.get_embedding  ‚Äî  Method get_embedding(M::Flag, p::OrthogonalPoint) Get embedding of  Flag  manifold  M , i.e. the manifold  OrthogonalMatrices . source"},{"id":821,"pagetitle":"Flag","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.project-Union{Tuple{dp1}, Tuple{Flag{<:Any, dp1}, OrthogonalPoint, OrthogonalTangentVector}} where dp1","content":" ManifoldsBase.project  ‚Äî  Method project(M::Flag, p::OrthogonalPoint, X::OrthogonalTangentVector) Project vector  X  to tangent space at point  p  from  Flag  manifold  M $\\operatorname{Flag}(n_1, n_2, ..., n_d; N)$ , in the orthogonal matrix representation. It works by first projecting  X  to the space of  SkewHermitianMatrices  and then setting diagonal blocks to 0: \\[X = \\begin{bmatrix}\n0                     & B_{1,2}               & ‚ãØ & B_{1,d+1} \\\\\n-B_{1,2}^\\mathrm{T}   & 0                     & ‚ãØ & B_{2,d+1} \\\\\n\\vdots                & \\vdots                & ‚ã± & \\vdots    \\\\\n-B_{1,d+1}^\\mathrm{T} & -B_{2,d+1}^\\mathrm{T} & ‚ãØ & 0\n\\end{bmatrix}\\] where  $B_{i,j} ‚àà ‚Ñù^{(n_i - n_{i-1}) √ó (n_j - n_{j-1})}$ , for   $1 ‚â§ i < j ‚â§ d+1$ . source"},{"id":822,"pagetitle":"Flag","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/flag/#ManifoldsBase.retract-Tuple{Flag, OrthogonalPoint, OrthogonalTangentVector, QRRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::Flag, p::OrthogonalPoint, X::OrthogonalTangentVector, ::QRRetraction) Compute the QR retraction on the  Flag  in the orthogonal matrix representation as the first order approximation to the exponential map. Similar to QR retraction for [ GeneralUnitaryMatrices ]. source"},{"id":825,"pagetitle":"Generalized Grassmann","title":"Generalized Grassmann","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#Generalized-Grassmann","content":" Generalized Grassmann"},{"id":826,"pagetitle":"Generalized Grassmann","title":"Manifolds.GeneralizedGrassmann","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#Manifolds.GeneralizedGrassmann","content":" Manifolds.GeneralizedGrassmann  ‚Äî  Type GeneralizedGrassmann{T,ùîΩ,TB<:AbstractMatrix} <: AbstractDecoratorManifold{ùîΩ} The generalized Grassmann manifold  $\\operatorname{Gr}(n,k,B)$  consists of all subspaces spanned by  $k$  linear independent vectors  $ùîΩ^n$ , where  $ùîΩ  ‚àà \\{‚Ñù, ‚ÑÇ\\}$  is either the real- (or complex-) valued vectors. This yields all  $k$ -dimensional subspaces of  $‚Ñù^n$  for the real-valued case and all  $2k$ -dimensional subspaces of  $‚ÑÇ^n$  for the second. The manifold can be represented as \\[\\operatorname{Gr}(n, k, B) := \\bigl\\{ \\operatorname{span}(p)\\ \\big|\\ p ‚àà ùîΩ^{n√ók}, p^\\mathrm{H}Bp = I_k\\},\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate (or Hermitian) transpose and  $I_k$  is the  $k√ók$  identity matrix. This means, that the columns of  $p$  form an unitary basis of the subspace with respect to the scaled inner product, that is a point on  $\\operatorname{Gr}(n,k,B)$ , and hence the subspace can actually be represented by a whole equivalence class of representers. For  $B=I_n$  this simplifies to the  Grassmann  manifold. The tangent space at a point (subspace)  $p$  is given by \\[T_x\\mathrm{Gr}(n,k,B) = \\bigl\\{\nX ‚àà ùîΩ^{n√ók} :\nX^{\\mathrm{H}}Bp + p^{\\mathrm{H}}BX = 0_{k} \\bigr\\},\\] where  $0_{k}$  denotes the  $k√ók$  zero matrix. Note that a point  $p ‚àà \\operatorname{Gr}(n,k,B)$  might be represented by different matrices (i.e. matrices with  $B$ -unitary column vectors that span the same subspace). Different representations of  $p$  also lead to different representation matrices for the tangent space  $T_p\\mathrm{Gr}(n,k,B)$ The manifold is named after  Hermann G. Gra√ümann  (1809-1877). Constructor GeneralizedGrassmann(n, k, B=I_n, field=‚Ñù) Generate the (real-valued) Generalized Grassmann manifold of  $n√ók$  dimensional orthonormal matrices with scalar product  B . source"},{"id":827,"pagetitle":"Generalized Grassmann","title":"Base.exp","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#Base.exp-Tuple{GeneralizedGrassmann, Vararg{Any}}","content":" Base.exp  ‚Äî  Method exp(M::GeneralizedGrassmann, p, X) Compute the exponential map on the  GeneralizedGrassmann M $= \\mathrm{Gr}(n,k,B)$  starting in  p  with tangent vector (direction)  X . Let  $X^{\\mathrm{H}}BX = USV$  denote the SVD decomposition of  $X^{\\mathrm{H}}BX$ . Then the exponential map is written using \\[\\exp_p X = p V\\cos(S)V^\\mathrm{H} + U\\sin(S)V^\\mathrm{H},\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transposed or Hermitian and the cosine and sine are applied element wise to the diagonal entries of  $S$ . source"},{"id":828,"pagetitle":"Generalized Grassmann","title":"Base.log","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#Base.log-Tuple{GeneralizedGrassmann, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::GeneralizedGrassmann, p, q) Compute the logarithmic map on the  GeneralizedGrassmann M $= \\mathcal M=\\mathrm{Gr}(n,k,B)$ , i.e. the tangent vector  X  whose corresponding  geodesic  starting from  p  reaches  q  after time 1 on  M . The formula reads \\[\\log_p q = V‚ãÖ \\operatorname{atan}(S) ‚ãÖ U^\\mathrm{H},\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transposed or Hermitian. The matrices  $U$  and  $V$  are the unitary matrices, and  $S$  is the diagonal matrix containing the singular values of the SVD-decomposition \\[USV = (q^\\mathrm{H}Bp)^{-1} ( q^\\mathrm{H} - q^\\mathrm{H}Bpp^\\mathrm{H}).\\] In this formula the  $\\operatorname{atan}$  is meant elementwise. source"},{"id":829,"pagetitle":"Generalized Grassmann","title":"Base.rand","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#Base.rand-Tuple{GeneralizedGrassmann}","content":" Base.rand  ‚Äî  Method rand(::GeneralizedGrassmann; vector_at=nothing, œÉ::Real=1.0) When  vector_at  is  nothing , return a random (Gaussian) point  p  on the  GeneralizedGrassmann  manifold  M  by generating a (Gaussian) matrix with standard deviation  œÉ  and return the (generalized) orthogonalized version, i.e. return the projection onto the manifold of the Q component of the QR decomposition of the random matrix of size  $n√ók$ . When  vector_at  is not  nothing , return a (Gaussian) random vector from the tangent space  $T_{vector\\_at}\\mathrm{St}(n,k)$  with mean zero and standard deviation  œÉ  by projecting a random Matrix onto the tangent vector at  vector_at . source"},{"id":830,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.change_metric","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.change_metric-Tuple{GeneralizedGrassmann, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_metric  ‚Äî  Method change_metric(M::GeneralizedGrassmann, ::EuclideanMetric, p X) Change  X  to the corresponding vector with respect to the metric of the  GeneralizedGrassmann M , i.e. let  $B=LL'$  be the Cholesky decomposition of the matrix  M.B , then the corresponding vector is  $L\\X$ . source"},{"id":831,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.change_representer","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.change_representer-Tuple{GeneralizedGrassmann, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(M::GeneralizedGrassmann, ::EuclideanMetric, p, X) Change  X  to the corresponding representer of a cotangent vector at  p  with respect to the scaled metric of the  GeneralizedGrassmann M , i.e, since \\[g_p(X,Y) = \\operatorname{tr}(Y^{\\mathrm{H}}BZ) = \\operatorname{tr}(X^{\\mathrm{H}}Z) = ‚ü®X,Z‚ü©\\] has to hold for all  $Z$ , where the repreenter  X  is given, the resulting representer with respect to the metric on the  GeneralizedGrassmann  is given by  $Y = B^{-1}X$ . source"},{"id":832,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.check_point-Tuple{GeneralizedGrassmann, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::GeneralizedGrassmann, p) Check whether  p  is representing a point on the  GeneralizedGrassmann M , i.e. its a  n -by- k  matrix of unitary column vectors with respect to the B inner product and of correct  eltype  with respect to  ùîΩ . source"},{"id":833,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.check_vector-Tuple{GeneralizedGrassmann, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::GeneralizedGrassmann, p, X; kwargs...) Check whether  X  is a tangent vector in the tangent space of  p  on the  GeneralizedGrassmann M , i.e. that  X  is of size and type as well as that \\[    p^{\\mathrm{H}}BX + \\overline{X^{\\mathrm{H}}Bp} = 0_k,\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transpose or Hermitian,  $\\overline{‚ãÖ}$  the (elementwise) complex conjugate, and  $0_k$  denotes the  $k√ók$  zero natrix. source"},{"id":834,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.distance-Tuple{GeneralizedGrassmann, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::GeneralizedGrassmann, p, q) Compute the Riemannian distance on  GeneralizedGrassmann  manifold  M $= \\mathrm{Gr}(n,k,B)$ . The distance is given by \\[d_{\\mathrm{Gr}(n,k,B)}(p,q) = \\operatorname{norm}(\\log_p(q)).\\] source"},{"id":835,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.injectivity_radius-Tuple{GeneralizedGrassmann}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::GeneralizedGrassmann)\ninjectivity_radius(M::GeneralizedGrassmann, p) Return the injectivity radius on the  GeneralizedGrassmann M , which is  $\\frac{œÄ}{2}$ . source"},{"id":836,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.inner-Tuple{GeneralizedGrassmann, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::GeneralizedGrassmann, p, X, Y) Compute the inner product for two tangent vectors  X ,  Y  from the tangent space of  p  on the  GeneralizedGrassmann  manifold  M . The formula reads \\[g_p(X,Y) = \\operatorname{tr}(X^{\\mathrm{H}}BY),\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transposed or Hermitian. source"},{"id":837,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.is_flat-Tuple{GeneralizedGrassmann}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(M::GeneralizedGrassmann) Return true if  GeneralizedGrassmann M  is one-dimensional. source"},{"id":838,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.manifold_dimension-Union{Tuple{GeneralizedGrassmann{<:Any, ùîΩ}}, Tuple{ùîΩ}} where ùîΩ","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::GeneralizedGrassmann) Return the dimension of the  GeneralizedGrassmann(n,k,ùîΩ)  manifold  M , i.e. \\[\\dim \\operatorname{Gr}(n,k,B) = k(n-k) \\dim_‚Ñù ùîΩ,\\] where  $\\dim_‚Ñù ùîΩ$  is the  real_dimension  of  ùîΩ . source"},{"id":839,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.project-Tuple{GeneralizedGrassmann, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::GeneralizedGrassmann, p, X) Project the  n -by- k X  onto the tangent space of  p  on the  GeneralizedGrassmann M , which is computed by \\[\\operatorname{proj_p}(X) = X - pp^{\\mathrm{H}}B^\\mathrm{T}X,\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transposed or Hermitian and  $‚ãÖ^{\\mathrm{T}}$  the transpose. source"},{"id":840,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.project-Tuple{GeneralizedGrassmann, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::GeneralizedGrassmann, p) Project  p  from the embedding onto the  GeneralizedGrassmann M , i.e. compute  q  as the polar decomposition of  $p$  such that  $q^{\\mathrm{H}}Bq$  is the identity, where  $‚ãÖ^{\\mathrm{H}}$  denotes the Hermitian, i.e. complex conjugate transpose. source"},{"id":841,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.representation_size-Tuple{GeneralizedGrassmann}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::GeneralizedGrassmann) Return the representation size or matrix dimension of a point on the  GeneralizedGrassmann M , i.e.  $(n,k)$  for both the real-valued and the complex value case. source"},{"id":842,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.retract-Tuple{GeneralizedGrassmann, Any, Any, PolarRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::GeneralizedGrassmann, p, X, ::PolarRetraction) Compute the SVD-based retraction  PolarRetraction  on the  GeneralizedGrassmann M , by  project ing  $p + X$  onto  M . source"},{"id":843,"pagetitle":"Generalized Grassmann","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#ManifoldsBase.zero_vector-Tuple{GeneralizedGrassmann, Vararg{Any}}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(M::GeneralizedGrassmann, p) Return the zero tangent vector from the tangent space at  p  on the  GeneralizedGrassmann M , which is given by a zero matrix the same size as  p . source"},{"id":844,"pagetitle":"Generalized Grassmann","title":"Statistics.mean","ref":"/manifolds/stable/manifolds/generalizedgrassmann/#Statistics.mean-Tuple{GeneralizedGrassmann, Vararg{Any}}","content":" Statistics.mean  ‚Äî  Method mean(\n    M::GeneralizedGrassmann,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method = GeodesicInterpolationWithinRadius(œÄ/4);\n    kwargs...,\n) Compute the Riemannian  mean  of  x  using  GeodesicInterpolationWithinRadius . source"},{"id":847,"pagetitle":"Generalized Stiefel","title":"Generalized Stiefel","ref":"/manifolds/stable/manifolds/generalizedstiefel/#Generalized-Stiefel","content":" Generalized Stiefel"},{"id":848,"pagetitle":"Generalized Stiefel","title":"Manifolds.GeneralizedStiefel","ref":"/manifolds/stable/manifolds/generalizedstiefel/#Manifolds.GeneralizedStiefel","content":" Manifolds.GeneralizedStiefel  ‚Äî  Type GeneralizedStiefel{T,ùîΩ,B} <: AbstractDecoratorManifold{ùîΩ} The Generalized Stiefel manifold consists of all  $n√ók$ ,  $n\\geq k$  orthonormal matrices w.r.t. an arbitrary scalar product with symmetric positive definite matrix  $B\\in R^{n√ón}$ , i.e. \\[\\operatorname{St}(n,k,B) = \\bigl\\{ p \\in \\mathbb F^{n√ók}\\ \\big|\\ p^{\\mathrm{H}} B p = I_k \\bigr\\},\\] where  $ùîΩ ‚àà \\{‚Ñù, ‚ÑÇ\\}$ ,  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transpose or Hermitian, and  $I_k \\in \\mathbb R^{k√ók}$  denotes the  $k√ók$  identity matrix. In the case  $B=I_k$  one gets the usual  Stiefel  manifold. The tangent space at a point  $p\\in\\mathcal M=\\operatorname{St}(n,k,B)$  is given by \\[T_p\\mathcal M = \\{ X \\in ùîΩ^{n√ók} : p^{\\mathrm{H}}BX + X^{\\mathrm{H}}Bp=0_n\\},\\] where  $0_k$  is the  $k√ók$  zero matrix. This manifold is modeled as an embedded manifold to the  Euclidean , i.e. several functions like the  zero_vector  are inherited from the embedding. The manifold is named after  Eduard L. Stiefel  (1909‚Äì1978). Constructor GeneralizedStiefel(n, k, B=I_n, F=‚Ñù) Generate the (real-valued) Generalized Stiefel manifold of  $n√ók$  dimensional orthonormal matrices with scalar product  B . source"},{"id":849,"pagetitle":"Generalized Stiefel","title":"Base.rand","ref":"/manifolds/stable/manifolds/generalizedstiefel/#Base.rand-Tuple{GeneralizedStiefel}","content":" Base.rand  ‚Äî  Method rand(::GeneralizedStiefel; vector_at=nothing, œÉ::Real=1.0) When  vector_at  is  nothing , return a random (Gaussian) point  p  on the  GeneralizedStiefel  manifold  M  by generating a (Gaussian) matrix with standard deviation  œÉ  and return the (generalized) orthogonalized version, i.e. return the projection onto the manifold of the Q component of the QR decomposition of the random matrix of size  $n√ók$ . When  vector_at  is not  nothing , return a (Gaussian) random vector from the tangent space  $T_{vector\\_at}\\mathrm{St}(n,k)$  with mean zero and standard deviation  œÉ  by projecting a random Matrix onto the tangent vector at  vector_at . source"},{"id":850,"pagetitle":"Generalized Stiefel","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/generalizedstiefel/#ManifoldsBase.check_point-Tuple{GeneralizedStiefel, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::GeneralizedStiefel, p; kwargs...) Check whether  p  is a valid point on the  GeneralizedStiefel M = $\\operatorname{St}(n,k,B)$ , i.e. that it has the right  AbstractNumbers  type and  $x^{\\mathrm{H}}Bx$  is (approximately) the identity, where  $‚ãÖ^{\\mathrm{H}}$  is the complex conjugate transpose. The settings for approximately can be set with  kwargs... . source"},{"id":851,"pagetitle":"Generalized Stiefel","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/generalizedstiefel/#ManifoldsBase.check_vector-Tuple{GeneralizedStiefel, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::GeneralizedStiefel, p, X; kwargs...) Check whether  X  is a valid tangent vector at  p  on the  GeneralizedStiefel M = $\\operatorname{St}(n,k,B)$ , i.e. the  AbstractNumbers  fits,  p  is a valid point on  M  and it (approximately) holds that  $p^{\\mathrm{H}}BX + \\overline{X^{\\mathrm{H}}Bp} = 0$ , where  kwargs...  is passed to the  isapprox . source"},{"id":852,"pagetitle":"Generalized Stiefel","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/generalizedstiefel/#ManifoldsBase.inner-Tuple{GeneralizedStiefel, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::GeneralizedStiefel, p, X, Y) Compute the inner product for two tangent vectors  X ,  Y  from the tangent space of  p  on the  GeneralizedStiefel  manifold  M . The formula reads \\[(X, Y)_p = \\operatorname{trace}(v^{\\mathrm{H}}Bw),\\] i.e. the metric induced by the scalar product  B  from the embedding, restricted to the tangent space. source"},{"id":853,"pagetitle":"Generalized Stiefel","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/generalizedstiefel/#ManifoldsBase.is_flat-Tuple{GeneralizedStiefel}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(M::GeneralizedStiefel) Return true if  GeneralizedStiefel M  is one-dimensional. source"},{"id":854,"pagetitle":"Generalized Stiefel","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/generalizedstiefel/#ManifoldsBase.manifold_dimension-Tuple{GeneralizedStiefel{<:Any, ‚Ñù}}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::GeneralizedStiefel) Return the dimension of the  GeneralizedStiefel  manifold  M = $\\operatorname{St}(n,k,B,ùîΩ)$ . The dimension is given by \\[\\begin{aligned}\n\\dim \\mathrm{St}(n, k, B, ‚Ñù) &= nk - \\frac{1}{2}k(k+1) \\\\\n\\dim \\mathrm{St}(n, k, B, ‚ÑÇ) &= 2nk - k^2\\\\\n\\dim \\mathrm{St}(n, k, B, ‚Ñç) &= 4nk - k(2k-1)\n\\end{aligned}\\] source"},{"id":855,"pagetitle":"Generalized Stiefel","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/generalizedstiefel/#ManifoldsBase.project-Tuple{GeneralizedStiefel, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M:GeneralizedStiefel, p, X) Project  X  onto the tangent space of  p  to the  GeneralizedStiefel  manifold  M . The formula reads \\[\\operatorname{proj}_{\\operatorname{St}(n,k)}(p,X) = X - p\\operatorname{Sym}(p^{\\mathrm{H}}BX),\\] where  $\\operatorname{Sym}(y)$  is the symmetrization of  $y$ , e.g. by  $\\operatorname{Sym}(y) = \\frac{y^{\\mathrm{H}}+y}{2}$ . source"},{"id":856,"pagetitle":"Generalized Stiefel","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/generalizedstiefel/#ManifoldsBase.project-Tuple{GeneralizedStiefel, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::GeneralizedStiefel, p) Project  p  from the embedding onto the  GeneralizedStiefel M , i.e. compute  q  as the polar decomposition of  $p$  such that  $q^{\\mathrm{H}}Bq$  is the identity, where  $‚ãÖ^{\\mathrm{H}}$  denotes the hermitian, i.e. complex conjugate transposed. source"},{"id":857,"pagetitle":"Generalized Stiefel","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/generalizedstiefel/#ManifoldsBase.retract-Tuple{GeneralizedStiefel, Vararg{Any}}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::GeneralizedStiefel, p, X)\nretract(M::GeneralizedStiefel, p, X, ::PolarRetraction)\nretract(M::GeneralizedStiefel, p, X, ::ProjectionRetraction) Compute the SVD-based retraction  PolarRetraction  on the  GeneralizedStiefel  manifold  M , which in this case is the same as the projection based retraction employing the exponential map in the embedding and projecting the result back to the manifold. The default retraction for this manifold is the  ProjectionRetraction . source"},{"id":860,"pagetitle":"Orthogonal and Unitary Matrices","title":"Orthogonal and Unitary matrices","ref":"/manifolds/stable/manifolds/generalunitary/#Orthogonal-and-Unitary-matrices","content":" Orthogonal and Unitary matrices Both  OrthogonalMatrices  and  UnitaryMatrices  have similar formulae and implementations, as are  Rotations , as well as unitary matrices with determinant equal to one. These share a  common implementation ."},{"id":861,"pagetitle":"Orthogonal and Unitary Matrices","title":"Orthogonal matrices","ref":"/manifolds/stable/manifolds/generalunitary/#Orthogonal-matrices","content":" Orthogonal matrices"},{"id":862,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.OrthogonalMatrices","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.OrthogonalMatrices","content":" Manifolds.OrthogonalMatrices  ‚Äî  Type  OrthogonalMatrices{n} = GeneralUnitaryMatrices{n,‚Ñù,AbsoluteDeterminantOneMatrixType} The manifold of (real) orthogonal  $n√ón$  matrices  $\\mathrm{O}(n)$ . They are precisely the  $n√ón$  real matrices  $M$  that satisfy \\[    M^{T}M=MM^{T}= \\mathrm{I}_n,\\] where  $M^{T}$  is the transpose of  $M$  and  $\\mathrm{I}_n$  is the  $n√ón$  identity matrix. Thus, their columns and rows represent  $n$  pairwise orthonormal vectors. Such matrices  $M$  have the property that  $\\lVert \\det(M) \\rVert = 1$ . This is the same manifold as the  Stiefel (n, n, ‚ÑÇ)  manifold. Constructor OrthogonalMatrices(n) source"},{"id":863,"pagetitle":"Orthogonal and Unitary Matrices","title":"Unitary matrices","ref":"/manifolds/stable/manifolds/generalunitary/#Unitary-matrices","content":" Unitary matrices"},{"id":864,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.UnitaryMatrices","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.UnitaryMatrices","content":" Manifolds.UnitaryMatrices  ‚Äî  Type const UnitaryMatrices{n,ùîΩ} = GeneralUnitaryMatrices{T,ùîΩ,AbsoluteDeterminantOneMatrixType} The manifold  $U(n,ùîΩ)$  of  $n√ón$  complex matrices (when ùîΩ=‚ÑÇ) or quaternionic matrices (when ùîΩ=‚Ñç) such that \\[    p^{\\mathrm{H}}p = \\mathrm{I}_n,\\] where  $p^{\\mathrm{H}}$  is the conjugate transpose of  $p$  and  $\\mathrm{I}_n$  is the  $n√ón$  identity matrix. Such matrices  p  have a property that  $\\lVert \\det(p) \\rVert = 1$ . The tangent spaces are given by \\[    T_pU(n) \\coloneqq \\bigl\\{\n    X \\big| pY \\text{ where } Y \\text{ is skew symmetric, i. e. } Y = -Y^{\\mathrm{H}}\n    \\bigr\\}\\] But note that tangent vectors are represented in the Lie algebra, i.e. just using  $Y$  in the representation above. If you prefer the representation as  X  you can use the  Stiefel (n, n, ‚ÑÇ)  manifold. Constructor UnitaryMatrices(n, ùîΩ::AbstractNumbers=‚ÑÇ) see also  OrthogonalMatrices  for the real valued case. source"},{"id":865,"pagetitle":"Orthogonal and Unitary Matrices","title":"Base.rand","ref":"/manifolds/stable/manifolds/generalunitary/#Base.rand-Tuple{UnitaryMatrices}","content":" Base.rand  ‚Äî  Method rand(::Unitary; vector_at=nothing, œÉ::Real=1.0) Generate a random point on the  UnitaryMatrices  manifold, if  vector_at  is nothing, by computing the QR decomposition of an  $n√ón$  matrix. Generate a tangent vector at  vector_at  by projecting a normally distributed matrix onto the tangent space. source"},{"id":866,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldDiff.riemannian_Hessian-Tuple{UnitaryMatrices, Vararg{Any, 4}}","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method riemannian_Hessian(M::UnitaryMatrices, p, G, H, X) The Riemannian Hessian can be computed by adopting Eq. (5.6) [ Ngu23 ], so very similar to the complex Stiefel manifold. The only difference is, that here the tangent vectors are stored in the Lie algebra, i.e. the update direction is actually  $pX$  instead of just  $X$  (in Stiefel). and that means the inverse has to be applied to the (Euclidean) Hessian to map it into the Lie algebra. source"},{"id":867,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.Weingarten","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.Weingarten-Tuple{UnitaryMatrices, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Weingarten(M::UnitaryMatrices, p, X, V) Compute the Weingarten map  $\\mathcal W_p$  at  p  on the  Stiefel M  with respect to the tangent vector  $X \\in T_p\\mathcal M$  and the normal vector  $V \\in N_p\\mathcal M$ . The formula is due to [ AMT13 ] given by \\[\\mathcal W_p(X,V) = -\\frac{1}{2}p\\bigl(V^{\\mathrm{H}}X - X^\\mathrm{H}V\\bigr).\\] source"},{"id":868,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.manifold_dimension-Tuple{UnitaryMatrices{<:Any, ‚ÑÇ}}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::UnitaryMatrices{n,‚ÑÇ}) where {n} Return the dimension of the manifold unitary matrices. \\[\\dim_{\\mathrm{U}(n)} = n^2.\\] source"},{"id":869,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.manifold_dimension-Tuple{UnitaryMatrices{<:Any, ‚Ñç}}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::UnitaryMatrices{<:Any,‚Ñç}) Return the dimension of the manifold unitary matrices. \\[\\dim_{\\mathrm{U}(n, ‚Ñç)} = n(2n+1).\\] source"},{"id":870,"pagetitle":"Orthogonal and Unitary Matrices","title":"Common functions","ref":"/manifolds/stable/manifolds/generalunitary/#generalunitarymatrices","content":" Common functions"},{"id":871,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.AbsoluteDeterminantOneMatrixType","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.AbsoluteDeterminantOneMatrixType","content":" Manifolds.AbsoluteDeterminantOneMatrixType  ‚Äî  Type AbsoluteDeterminantOneMatrixType <: AbstractMatrixType A type to indicate that we require (orthogonal / unitary) matrices with normed determinant, i.e. that the absolute value of the determinant is 1. source"},{"id":872,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.AbstractMatrixType","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.AbstractMatrixType","content":" Manifolds.AbstractMatrixType  ‚Äî  Type AbstractMatrixType A plain type to distinguish different types of matrices, for example  DeterminantOneMatrixType  and  AbsoluteDeterminantOneMatrixType . source"},{"id":873,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.DeterminantOneMatrixType","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.DeterminantOneMatrixType","content":" Manifolds.DeterminantOneMatrixType  ‚Äî  Type DeterminantOneMatrixType <: AbstractMatrixType A type to indicate that we require special (orthogonal / unitary) matrices, i.e. of determinant 1. source"},{"id":874,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.GeneralUnitaryMatrices","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.GeneralUnitaryMatrices","content":" Manifolds.GeneralUnitaryMatrices  ‚Äî  Type GeneralUnitaryMatrices{T,ùîΩ,S<:AbstractMatrixType} <: AbstractDecoratorManifold A common parametric type for matrices with a unitary property of size  $n√ón$  over the field  $ùîΩ$  which additionally have the  AbstractMatrixType , e.g. are  DeterminantOneMatrixType . source"},{"id":875,"pagetitle":"Orthogonal and Unitary Matrices","title":"Base.exp","ref":"/manifolds/stable/manifolds/generalunitary/#Base.exp-Tuple{Manifolds.GeneralUnitaryMatrices, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::Rotations, p, X)\nexp(M::OrthogonalMatrices, p, X)\nexp(M::UnitaryMatrices, p, X) Compute the exponential map, that is, since  $X$  is represented in the Lie algebra, \\[exp_p(X) = p\\mathrm{e}^X\\] For different sizes, like  $n=2,3,4$ , there are specialized implementations. The algorithm used is a more numerically stable form of those proposed in [ GX02 ] and [ AR13 ]. source"},{"id":876,"pagetitle":"Orthogonal and Unitary Matrices","title":"Base.log","ref":"/manifolds/stable/manifolds/generalunitary/#Base.log-Tuple{Manifolds.GeneralUnitaryMatrices, Any, Any}","content":" Base.log  ‚Äî  Method log(M::Rotations, p, X)\nlog(M::OrthogonalMatrices, p, X)\nlog(M::UnitaryMatrices, p, X) Compute the logarithmic map, that is, since the resulting  $X$  is represented in the Lie algebra, \\[\\log_p q = \\log(p^{\\mathrm{H}}q)\\] which is projected onto the skew symmetric matrices for numerical stability. source"},{"id":877,"pagetitle":"Orthogonal and Unitary Matrices","title":"Base.log","ref":"/manifolds/stable/manifolds/generalunitary/#Base.log-Tuple{Manifolds.GeneralUnitaryMatrices{<:Any, ‚Ñù}, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::Rotations, p, q) Compute the logarithmic map on the  Rotations  manifold  M  which is given by \\[\\log_p q = \\log(p^{\\mathrm{T}}q)\\] where  $\\log$  denotes the matrix logarithm. For numerical stability, the result is projected onto the set of skew symmetric matrices. For antipodal rotations the function returns deterministically one of the tangent vectors that point at  q . source"},{"id":878,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.cos_angles_4d_rotation_matrix","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.cos_angles_4d_rotation_matrix-Tuple{Any}","content":" Manifolds.cos_angles_4d_rotation_matrix  ‚Äî  Method cos_angles_4d_rotation_matrix(R) 4D rotations can be described by two orthogonal planes that are unchanged by the action of the rotation (vectors within a plane rotate only within the plane). The cosines of the two angles  $Œ±,Œ≤$  of rotation about these planes may be obtained from the distinct real parts of the eigenvalues of the rotation matrix. This function computes these more efficiently by solving the system \\[\\begin{aligned}\n\\cos Œ± + \\cos Œ≤ &= \\frac{1}{2} \\operatorname{tr}(R)\\\\\n\\cos Œ± \\cos Œ≤ &= \\frac{1}{8} \\operatorname{tr}(R)^2\n                 - \\frac{1}{16} \\operatorname{tr}((R - R^T)^2) - 1.\n\\end{aligned}\\] By convention, the returned values are sorted in decreasing order. See also  angles_4d_skew_sym_matrix . source"},{"id":879,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.manifold_volume-Tuple{Manifolds.GeneralUnitaryMatrices{<:Any, ‚ÑÇ, Manifolds.DeterminantOneMatrixType}}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(::GeneralUnitaryMatrices{<:Any,‚ÑÇ,DeterminantOneMatrixType}) Volume of the manifold of complex general unitary matrices of determinant one. The formula reads [ BST03 ] \\[\\sqrt{n 2^{n-1}} œÄ^{(n-1)(n+2)/2} \\prod_{k=1}^{n-1}\\frac{1}{k!}.\\] source"},{"id":880,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.manifold_volume-Tuple{Manifolds.GeneralUnitaryMatrices{<:Any, ‚Ñù, Manifolds.AbsoluteDeterminantOneMatrixType}}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(::GeneralUnitaryMatrices{<:Any,‚Ñù,AbsoluteDeterminantOneMatrixType}) Volume of the manifold of real orthogonal matrices of absolute determinant one. The formula reads [ BST03 ]: \\[\\begin{cases}\n\\frac{2^{k}(2\\pi)^{k^2}}{\\prod_{s=1}^{k-1} (2s)!} & \\text{ if } n = 2k \\\\\n\n\\frac{2^{k+1}(2\\pi)^{k(k+1)}}{\\prod_{s=1}^{k-1} (2s+1)!} & \\text{ if } n = 2k+1\n\\end{cases}\\] source"},{"id":881,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.manifold_volume-Tuple{Rotations}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(::GeneralUnitaryMatrices{<:Any,‚Ñù,DeterminantOneMatrixType}) Volume of the manifold of real orthogonal matrices of determinant one. The formula reads [ BST03 ]: \\[\\begin{cases}\n2 & \\text{ if } n = 0 \\\\\n\\frac{2^{k-1/2}(2\\pi)^{k^2}}{\\prod_{s=1}^{k-1} (2s)!} & \\text{ if } n = 2k+2 \\\\\n\\frac{2^{k+1/2}(2\\pi)^{k(k+1)}}{\\prod_{s=1}^{k-1} (2s+1)!} & \\text{ if } n = 2k+1\n\\end{cases}\\] It differs from the paper by a factor of  sqrt(2)  due to a different choice of normalization. source"},{"id":882,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.manifold_volume-Tuple{UnitaryMatrices{<:Any, ‚ÑÇ}}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(::GeneralUnitaryMatrices{<:Any,‚ÑÇ,AbsoluteDeterminantOneMatrixType}) Volume of the manifold of complex general unitary matrices of absolute determinant one. The formula reads [ BST03 ] \\[\\sqrt{n 2^{n+1}} œÄ^{n(n+1)/2} \\prod_{k=1}^{n-1}\\frac{1}{k!}.\\] source"},{"id":883,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.volume_density-Tuple{Manifolds.GeneralUnitaryMatrices{<:Any, ‚Ñù}, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(M::GeneralUnitaryMatrices{<:Any,‚Ñù}, p, X) Compute volume density function of a sphere, i.e. determinant of the differential of exponential map  exp(M, p, X) . It is derived from Eq. (4.1) and Corollary 4.4 in [ CLLD22 ]. See also Theorem 4.1 in [ FdHDF19 ], (note that it uses a different convention). source"},{"id":884,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.volume_density-Tuple{Manifolds.GeneralUnitaryMatrices{ManifoldsBase.TypeParameter{Tuple{2}}, ‚Ñù}, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(M::GeneralUnitaryMatrices{TypeParameter{Tuple{2}},‚Ñù}, p, X) Volume density on O(2)/SO(2) is equal to 1. source"},{"id":885,"pagetitle":"Orthogonal and Unitary Matrices","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/generalunitary/#Manifolds.volume_density-Tuple{Manifolds.GeneralUnitaryMatrices{ManifoldsBase.TypeParameter{Tuple{3}}, ‚Ñù}, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(M::GeneralUnitaryMatrices{TypeParameter{Tuple{3}},‚Ñù}, p, X) Compute the volume density on O(3)/SO(3). The formula reads [ FdHDF19 ] \\[\\frac{1-1\\cos(\\sqrt{2}\\lVert X \\rVert)}{\\lVert X \\rVert^2}.\\] source"},{"id":886,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.check_point-Union{Tuple{ùîΩ}, Tuple{Manifolds.GeneralUnitaryMatrices{<:Any, ùîΩ, Manifolds.DeterminantOneMatrixType}, Any}} where ùîΩ","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::Rotations, p; kwargs...) Check whether  p  is a valid point on the  UnitaryMatrices M , i.e. that  $p$  has a determinant of absolute value one, i.e. that  $p^{\\mathrm{H}}p = \\mathrm{I}_n$ The tolerance for the last test can be set using the  kwargs... . source"},{"id":887,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.check_point-Union{Tuple{ùîΩ}, Tuple{UnitaryMatrices{<:Any, ùîΩ}, Any}} where ùîΩ","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::UnitaryMatrices, p; kwargs...)\ncheck_point(M::OrthogonalMatrices, p; kwargs...)\ncheck_point(M::GeneralUnitaryMatrices, p; kwargs...) Check whether  p  is a valid point on the  UnitaryMatrices  or [ OrthogonalMatrices ]  M , i.e. that  $p$  has a determinant of absolute value one. The tolerance for the last test can be set using the  kwargs... . source"},{"id":888,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.check_vector-Union{Tuple{ùîΩ}, Tuple{Manifolds.GeneralUnitaryMatrices{<:Any, ùîΩ}, Any, Any}} where ùîΩ","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::UnitaryMatrices, p, X; kwargs... )\ncheck_vector(M::OrthogonalMatrices, p, X; kwargs... )\ncheck_vector(M::Rotations, p, X; kwargs... )\ncheck_vector(M::GeneralUnitaryMatrices, p, X; kwargs... ) Check whether  X  is a tangent vector to  p  on the  UnitaryMatrices  space  M , i.e. after  check_point (M,p) ,  X  has to be skew symmetric (Hermitian) and orthogonal to  p . The tolerance for the last test can be set using the  kwargs... . source"},{"id":889,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.embed-Tuple{Manifolds.GeneralUnitaryMatrices, Any, Any}","content":" ManifoldsBase.embed  ‚Äî  Method embed(M::GeneralUnitaryMatrices, p, X) Embed the tangent vector  X  at point  p  in  M  from its Lie algebra representation (set of skew matrices) into the Riemannian submanifold representation The formula reads \\[X_{\\text{embedded}} = p * X\\] source"},{"id":890,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.get_coordinates","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.get_coordinates-Tuple{Manifolds.GeneralUnitaryMatrices{<:Any, ‚Ñù}, Vararg{Any}}","content":" ManifoldsBase.get_coordinates  ‚Äî  Method get_coordinates(M::Rotations, p, X)\nget_coordinates(M::OrthogonalMatrices, p, X)\nget_coordinates(M::UnitaryMatrices, p, X) Extract the unique tangent vector components  $X^i$  at point  p  on  Rotations $\\mathrm{SO}(n)$  from the matrix representation  X  of the tangent vector. The basis on the Lie algebra  $ùî∞ùî¨(n)$  is chosen such that for  $\\mathrm{SO}(2)$ ,  $X^1 = Œ∏ = X_{21}$  is the angle of rotation, and for  $\\mathrm{SO}(3)$ ,  $(X^1, X^2, X^3) = (X_{32}, X_{13}, X_{21}) = Œ∏ u$  is the angular velocity and axis-angle representation, where  $u$  is the unit vector along the axis of rotation. For  $\\mathrm{SO}(n)$  where  $n ‚â• 4$ , the additional elements of  $X^i$  are  $X^{j (j - 3)/2 + k + 1} = X_{jk}$ , for  $j ‚àà [4,n], k ‚àà [1,j)$ . source"},{"id":891,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.get_embedding","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.get_embedding-Union{Tuple{Manifolds.GeneralUnitaryMatrices{ManifoldsBase.TypeParameter{Tuple{n}}, ùîΩ}}, Tuple{ùîΩ}, Tuple{n}} where {n, ùîΩ}","content":" ManifoldsBase.get_embedding  ‚Äî  Method get_embedding(M::OrthogonalMatrices)\nget_embedding(M::Rotations)\nget_embedding(M::UnitaryMatrices) Return the embedding, i.e. The  $\\mathbb F^{n√ón}$ , where  $\\mathbb F = \\mathbb R$  for the first two and  $\\mathbb F = \\mathbb C$  for the unitary matrices. source"},{"id":892,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.get_vector","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.get_vector-Tuple{Manifolds.GeneralUnitaryMatrices{<:Any, ‚Ñù}, Vararg{Any}}","content":" ManifoldsBase.get_vector  ‚Äî  Method get_vector(M::OrthogonalMatrices, p, X‚Å±, B::DefaultOrthogonalBasis)\nget_vector(M::Rotations, p, X‚Å±, B::DefaultOrthogonalBasis) Convert the unique tangent vector components  X‚Å±  at point  p  on  Rotations  or  OrthogonalMatrices  to the matrix representation  $X$  of the tangent vector. See  get_coordinates  for the conventions used. source"},{"id":893,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.injectivity_radius-Tuple{Manifolds.GeneralUnitaryMatrices{<:Any, ‚ÑÇ, Manifolds.DeterminantOneMatrixType}}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(G::GeneralUnitaryMatrices{<:Any,‚ÑÇ,DeterminantOneMatrixType}) Return the injectivity radius for general complex unitary matrix manifolds, where the determinant is  $+1$ , which is [1] \\[    \\operatorname{inj}_{\\mathrm{SU}(n)} = œÄ \\sqrt{2}.\\] source"},{"id":894,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.injectivity_radius-Tuple{Manifolds.GeneralUnitaryMatrices}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(G::GeneraliUnitaryMatrices) Return the injectivity radius for general unitary matrix manifolds, which is [1] \\[    \\operatorname{inj}_{\\mathrm{U}(n)} = œÄ.\\] source"},{"id":895,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.injectivity_radius-Union{Tuple{Manifolds.GeneralUnitaryMatrices{ManifoldsBase.TypeParameter{Tuple{n}}, ‚Ñù}}, Tuple{n}} where n","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(G::SpecialOrthogonal)\ninjectivity_radius(G::Orthogonal)\ninjectivity_radius(M::Rotations)\ninjectivity_radius(M::Rotations, ::ExponentialRetraction) Return the radius of injectivity on the  Rotations  manifold  M , which is  $œÄ\\sqrt{2}$ .  [1] source"},{"id":896,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.is_flat-Tuple{Manifolds.GeneralUnitaryMatrices}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(M::GeneralUnitaryMatrices) Return true if  GeneralUnitaryMatrices M  is SO(2) or U(1) and false otherwise. source"},{"id":897,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.manifold_dimension-Tuple{Manifolds.GeneralUnitaryMatrices{<:Any, ‚ÑÇ, Manifolds.DeterminantOneMatrixType}}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::GeneralUnitaryMatrices{<:Any,‚ÑÇ,DeterminantOneMatrixType}) Return the dimension of the manifold of special unitary matrices. \\[\\dim_{\\mathrm{SU}(n)} = n^2-1.\\] source"},{"id":898,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.manifold_dimension-Tuple{Manifolds.GeneralUnitaryMatrices{<:Any, ‚Ñù}}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::Rotations)\nmanifold_dimension(M::OrthogonalMatrices) Return the dimension of the manifold orthogonal matrices and of the manifold of rotations \\[\\dim_{\\mathrm{O}(n)} = \\dim_{\\mathrm{SO}(n)} = \\frac{n(n-1)}{2}.\\] source"},{"id":899,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.project-Tuple{Manifolds.GeneralUnitaryMatrices, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::OrthogonalMatrices, p, X)\nproject(M::Rotations, p, X)\nproject(M::UnitaryMatrices, p, X) Orthogonally project the tangent vector  $X ‚àà ùîΩ^{n√ón}$ ,  $\\mathbb F ‚àà \\{\\mathbb R, \\mathbb C\\}$  to the tangent space of  M  at  p , and change the representer to use the corresponding Lie algebra, i.e. we compute \\[    \\operatorname{proj}_p(X) = \\frac{p^{\\mathrm{H}} X - (p^{\\mathrm{H}} X)^{\\mathrm{H}}}{2}.\\] source"},{"id":900,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.project-Union{Tuple{ùîΩ}, Tuple{UnitaryMatrices{<:Any, ùîΩ}, Any}} where ùîΩ","content":" ManifoldsBase.project  ‚Äî  Method  project(G::UnitaryMatrices, p)\n project(G::OrthogonalMatrices, p) Project the point  $p ‚àà ùîΩ^{n√ón}$  to the nearest point in  $\\mathrm{U}(n,ùîΩ)=$ Unitary(n,ùîΩ)  under the Frobenius norm. If  $p = U S V^\\mathrm{H}$  is the singular value decomposition of  $p$ , then the projection is \\[  \\operatorname{proj}_{\\mathrm{U}(n,ùîΩ)} \\colon p ‚Ü¶ U V^\\mathrm{H}.\\] source"},{"id":901,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.retract-Tuple{Manifolds.GeneralUnitaryMatrices, Any, Any, PolarRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::Rotations, p, X, ::PolarRetraction)\nretract(M::OrthogonalMatrices, p, X, ::PolarRetraction) Compute the SVD-based retraction on the  Rotations  and  OrthogonalMatrices M  from  p  in direction  X  (as an element of the Lie group) and is a second-order approximation of the exponential map. Let \\[USV = p + pX\\] be the singular value decomposition, then the formula reads \\[\\operatorname{retr}_p X = UV^\\mathrm{T}.\\] source"},{"id":902,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.retract-Tuple{Manifolds.GeneralUnitaryMatrices, Any, Any, QRRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::Rotations, p, X, ::QRRetraction)\nretract(M::OrthogonalMatrices, p. X, ::QRRetraction) Compute the QR-based retraction on the  Rotations  and  OrthogonalMatrices M  from  p  in direction  X  (as an element of the Lie group), which is a first-order approximation of the exponential map. This is also the default retraction on these manifolds. source"},{"id":903,"pagetitle":"Orthogonal and Unitary Matrices","title":"ManifoldsBase.riemann_tensor","ref":"/manifolds/stable/manifolds/generalunitary/#ManifoldsBase.riemann_tensor-Tuple{Manifolds.GeneralUnitaryMatrices, Vararg{Any, 4}}","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(::GeneralUnitaryMatrices, p, X, Y, Z) Compute the value of Riemann tensor on the  GeneralUnitaryMatrices  manifold. The formula reads [ Ren11 ] \\[R(X,Y)Z=\\frac{1}{4}[Z, [X, Y]].\\] source"},{"id":904,"pagetitle":"Orthogonal and Unitary Matrices","title":"Statistics.mean","ref":"/manifolds/stable/manifolds/generalunitary/#Statistics.mean-Tuple{Manifolds.GeneralUnitaryMatrices{<:Any, ‚Ñù}, Any}","content":" Statistics.mean  ‚Äî  Method mean(\n    M::Rotations,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method = GeodesicInterpolationWithinRadius(œÄ/2/‚àö2);\n    kwargs...,\n) Compute the Riemannian  mean  of  x  using  GeodesicInterpolationWithinRadius . source"},{"id":905,"pagetitle":"Orthogonal and Unitary Matrices","title":"Footnotes and References","ref":"/manifolds/stable/manifolds/generalunitary/#Footnotes-and-References","content":" Footnotes and References 1 For a derivation of the injectivity radius, see  sethaxen.com/blog/2023/02/the-injectivity-radii-of-the-unitary-groups/ ."},{"id":908,"pagetitle":"Graph manifold","title":"Graph manifold","ref":"/manifolds/stable/manifolds/graph/#Graph-manifold","content":" Graph manifold For a given graph  $G(V,E)$  implemented using  Graphs.jl , the  GraphManifold  models a  PowerManifold  either on the nodes or edges of the graph, depending on the  GraphManifoldType . i.e., it's either a  $\\mathcal M^{\\lvert V \\rvert}$  for the case of a vertex manifold or a  $\\mathcal M^{\\lvert E \\rvert}$  for the case of a edge manifold."},{"id":909,"pagetitle":"Graph manifold","title":"Example","ref":"/manifolds/stable/manifolds/graph/#Example","content":" Example To make a graph manifold over  $‚Ñù^2$  with three vertices and two edges, one can use using Manifolds\nusing Graphs\nM = Euclidean(2)\np = [[1., 4.], [2., 5.], [3., 6.]]\nq = [[4., 5.], [6., 7.], [8., 9.]]\nx = [[6., 5.], [4., 3.], [2., 8.]]\nG = SimpleGraph(3)\nadd_edge!(G, 1, 2)\nadd_edge!(G, 2, 3)\nN = GraphManifold(G, M, VertexManifold()) GraphManifold\nGraph:\n {3, 2} undirected simple Int64 graph\nAbstractManifold on vertices:\n Euclidean(2; field=‚Ñù) It supports all  AbstractPowerManifold   operations (it is based on  NestedPowerRepresentation ) and furthermore it is possible to compute a graph logarithm: incident_log(N, p) 3-element Vector{Vector{Float64}}:\n [1.0, 1.0]\n [0.0, 0.0]\n [-1.0, -1.0]"},{"id":910,"pagetitle":"Graph manifold","title":"Types and functions","ref":"/manifolds/stable/manifolds/graph/#Types-and-functions","content":" Types and functions"},{"id":911,"pagetitle":"Graph manifold","title":"Manifolds.EdgeManifold","ref":"/manifolds/stable/manifolds/graph/#Manifolds.EdgeManifold","content":" Manifolds.EdgeManifold  ‚Äî  Type EdgeManifoldManifold <: GraphManifoldType A type for a  GraphManifold  where the data is given on the edges. source"},{"id":912,"pagetitle":"Graph manifold","title":"Manifolds.GraphManifold","ref":"/manifolds/stable/manifolds/graph/#Manifolds.GraphManifold","content":" Manifolds.GraphManifold  ‚Äî  Type GraphManifold{G,ùîΩ,M,T} <: AbstractPowerManifold{ùîΩ,M,NestedPowerRepresentation} Build a manifold, that is a  PowerManifold  of the  AbstractManifold M  either on the edges or vertices of a graph  G  depending on the  GraphManifoldType T . Fields G  is an  AbstractSimpleGraph M  is a  AbstractManifold source"},{"id":913,"pagetitle":"Graph manifold","title":"Manifolds.GraphManifoldType","ref":"/manifolds/stable/manifolds/graph/#Manifolds.GraphManifoldType","content":" Manifolds.GraphManifoldType  ‚Äî  Type GraphManifoldType This type represents the type of data on the graph that the  GraphManifold  represents. source"},{"id":914,"pagetitle":"Graph manifold","title":"Manifolds.VertexManifold","ref":"/manifolds/stable/manifolds/graph/#Manifolds.VertexManifold","content":" Manifolds.VertexManifold  ‚Äî  Type VectexGraphManifold <: GraphManifoldType A type for a  GraphManifold  where the data is given on the vertices. source"},{"id":915,"pagetitle":"Graph manifold","title":"Manifolds.incident_log","ref":"/manifolds/stable/manifolds/graph/#Manifolds.incident_log-Tuple{GraphManifold{<:Graphs.AbstractGraph, ùîΩ, <:AbstractManifold{ùîΩ}, VertexManifold} where ùîΩ, Any}","content":" Manifolds.incident_log  ‚Äî  Method incident_log(M::GraphManifold, x) Return the tangent vector on the (vertex)  GraphManifold , where at each node the sum of the  log s to incident nodes is computed. For a  SimpleGraph , an egde is interpreted as double edge in the corresponding SimpleDiGraph If the internal graph is a  SimpleWeightedGraph  the weighted sum of the tangent vectors is computed. source"},{"id":916,"pagetitle":"Graph manifold","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/graph/#ManifoldsBase.check_point-Tuple{GraphManifold, Vararg{Any}}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::GraphManifold, p) Check whether  p  is a valid point on the  GraphManifold , i.e. its length equals the number of vertices (for  VertexManifold s) or the number of edges (for  EdgeManifold s) and that each element of  p  passes the  check_point  test for the base manifold  M.manifold . source"},{"id":917,"pagetitle":"Graph manifold","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/graph/#ManifoldsBase.check_vector-Tuple{GraphManifold, Vararg{Any}}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::GraphManifold, p, X; kwargs...) Check whether  p  is a valid point on the  GraphManifold , and  X  it from its tangent space, i.e. its length equals the number of vertices (for  VertexManifold s) or the number of edges (for  EdgeManifold s) and that each element of  X  together with its corresponding entry of  p  passes the  check_vector  test for the base manifold  M.manifold . source"},{"id":918,"pagetitle":"Graph manifold","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/graph/#ManifoldsBase.manifold_dimension-Tuple{GraphManifold{<:Graphs.AbstractGraph, ùîΩ, <:AbstractManifold{ùîΩ}, EdgeManifold} where ùîΩ}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(N::GraphManifold{G,ùîΩ,M,EdgeManifold}) returns the manifold dimension of the  GraphManifold N  on the edges of a graph  $G=(V,E)$ , i.e. \\[\\dim(\\mathcal N) = \\lvert E \\rvert \\dim(\\mathcal M),\\] where  $\\mathcal M$  is the manifold of the data on the edges. source"},{"id":919,"pagetitle":"Graph manifold","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/graph/#ManifoldsBase.manifold_dimension-Tuple{GraphManifold{<:Graphs.AbstractGraph, ùîΩ, <:AbstractManifold{ùîΩ}, VertexManifold} where ùîΩ}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(N::GraphManifold{G,ùîΩ,M,VertexManifold}) returns the manifold dimension of the  GraphManifold N  on the vertices of a graph  $G=(V,E)$ , i.e. \\[\\dim(\\mathcal N) = \\lvert V \\rvert \\dim(\\mathcal M),\\] where  $\\mathcal M$  is the manifold of the data on the nodes. source"},{"id":922,"pagetitle":"Grassmann","title":"Grassmannian manifold","ref":"/manifolds/stable/manifolds/grassmann/#Grassmannian-manifold","content":" Grassmannian manifold"},{"id":923,"pagetitle":"Grassmann","title":"Manifolds.Grassmann","ref":"/manifolds/stable/manifolds/grassmann/#Manifolds.Grassmann","content":" Manifolds.Grassmann  ‚Äî  Type Grassmann{T,ùîΩ} <: AbstractDecoratorManifold{ùîΩ} The Grassmann manifold  $\\mathrm{Gr}(n,k)$  consists of all subspaces spanned by  $k$  linear independent vectors  $ùîΩ^n$ , where  $ùîΩ  ‚àà \\{‚Ñù, ‚ÑÇ\\}$  is either the real- (or complex-) valued vectors. This yields all  $k$ -dimensional subspaces of  $‚Ñù^n$  for the real-valued case and all  $2k$ -dimensional subspaces of  $‚ÑÇ^n$  for the second. The manifold can be represented as \\[\\mathrm{Gr}(n,k) := \\bigl\\{ \\operatorname{span}(p) : p ‚àà ùîΩ^{n√ók}, p^\\mathrm{H}p = I_k\\},\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transpose or Hermitian and  $I_k$  is the  $k√ók$  identity matrix. This means, that the columns of  $p$  form an unitary basis of the subspace, that is a point on  $\\operatorname{Gr}(n,k)$ , and hence the subspace can actually be represented by a whole equivalence class of representers. Another interpretation is, that \\[\\mathrm{Gr}(n,k) = \\mathrm{St}(n,k) / \\operatorname{O}(k),\\] i.e the Grassmann manifold is the quotient of the  Stiefel  manifold and the orthogonal group  $\\operatorname{O}(k)$  of orthogonal  $k√ók$  matrices. Note that it doesn't matter whether we start from the Euclidean or canonical metric on the Stiefel manifold, the resulting quotient metric on Grassmann is the same. The tangent space at a point (subspace)  $p$  is given by \\[T_p\\mathrm{Gr}(n,k) = \\bigl\\{\nX ‚àà ùîΩ^{n√ók} :\nX^{\\mathrm{H}}p + p^{\\mathrm{H}}X = 0_{k} \\bigr\\},\\] where  $0_k$  is the  $k√ók$  zero matrix. Note that a point  $p ‚àà \\operatorname{Gr}(n,k)$  might be represented by different matrices (i.e. matrices with unitary column vectors that span the same subspace). Different representations of  $p$  also lead to different representation matrices for the tangent space  $T_p\\mathrm{Gr}(n,k)$ For a representation of points as orthogonal projectors. Here \\[\\operatorname{Gr}(n,k) := \\bigl\\{ p \\in \\mathbb R^{n√ón} : p = p^Àú\\mathrm{T}, p^2 = p, \\operatorname{rank}(p) = k\\},\\] with tangent space \\[T_p\\mathrm{Gr}(n,k) = \\bigl\\{\nX ‚àà \\mathbb R^{n√ón} : X=X^{\\mathrm{T}} \\text{ and } X = pX+Xp \\bigr\\},\\] see also  ProjectorPoint  and  ProjectorTangentVector . The manifold is named after  Hermann G. Gra√ümann  (1809-1877). A good overview can be found in[ BZA20 ]. Constructor Grassmann(n, k, field=‚Ñù, parameter::Symbol=:type) Generate the Grassmann manifold  $\\operatorname{Gr}(n,k)$ , where the real-valued case  field=‚Ñù  is the default. source"},{"id":924,"pagetitle":"Grassmann","title":"Base.convert","ref":"/manifolds/stable/manifolds/grassmann/#Base.convert-Tuple{Type{ProjectorPoint}, AbstractMatrix}","content":" Base.convert  ‚Äî  Method convert(::Type{ProjectorPoint}, p::AbstractMatrix) Convert a point  p  on  Stiefel  that also represents a point (i.e. subspace) on  Grassmann  to a projector representation of said subspace, i.e. compute the  canonical_project!  for \\[  œÄ^{\\mathrm{SG}}(p) = pp^{\\mathrm{T}}.\\] source"},{"id":925,"pagetitle":"Grassmann","title":"Base.convert","ref":"/manifolds/stable/manifolds/grassmann/#Base.convert-Tuple{Type{ProjectorPoint}, StiefelPoint}","content":" Base.convert  ‚Äî  Method convert(::Type{ProjectorPoint}, ::Stiefelpoint) Convert a point  p  on  Stiefel  that also represents a point (i.e. subspace) on  Grassmann  to a projector representation of said subspace, i.e. compute the  canonical_project!  for \\[  œÄ^{\\mathrm{SG}}(p) = pp^{\\mathrm{T}}.\\] source"},{"id":926,"pagetitle":"Grassmann","title":"Manifolds.get_total_space","ref":"/manifolds/stable/manifolds/grassmann/#Manifolds.get_total_space-Union{Tuple{Grassmann{ManifoldsBase.TypeParameter{Tuple{n, k}}, ùîΩ}}, Tuple{ùîΩ}, Tuple{k}, Tuple{n}} where {n, k, ùîΩ}","content":" Manifolds.get_total_space  ‚Äî  Method get_total_space(::Grassmann) Return the total space of the  Grassmann  manifold, which is the corresponding Stiefel manifold, independent of whether the points are represented already in the total space or as  ProjectorPoint s. source"},{"id":927,"pagetitle":"Grassmann","title":"ManifoldsBase.change_metric","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.change_metric-Tuple{Grassmann, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_metric  ‚Äî  Method change_metric(M::Grassmann, ::EuclideanMetric, p X) Change  X  to the corresponding vector with respect to the metric of the  Grassmann M , which is just the identity, since the manifold is isometrically embedded. source"},{"id":928,"pagetitle":"Grassmann","title":"ManifoldsBase.change_representer","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.change_representer-Tuple{Grassmann, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(M::Grassmann, ::EuclideanMetric, p, X) Change  X  to the corresponding representer of a cotangent vector at  p . Since the  Grassmann  manifold  M , is isometrically embedded, this is the identity source"},{"id":929,"pagetitle":"Grassmann","title":"ManifoldsBase.default_retraction_method","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.default_retraction_method-Tuple{Grassmann}","content":" ManifoldsBase.default_retraction_method  ‚Äî  Method default_retraction_method(M::Grassmann)\ndefault_retraction_method(M::Grassmann, ::Type{StiefelPoint})\ndefault_retraction_method(M::Grassmann, ::Type{ProjectorPoint}) Return  ExponentialRetraction  as the default on the  Grassmann  manifold for both representations. source"},{"id":930,"pagetitle":"Grassmann","title":"ManifoldsBase.default_vector_transport_method","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.default_vector_transport_method-Tuple{Grassmann}","content":" ManifoldsBase.default_vector_transport_method  ‚Äî  Method default_vector_transport_method(M::Grassmann) Return the default vector transport method for the  Grassmann  manifold, which is  ParallelTransport () . source"},{"id":931,"pagetitle":"Grassmann","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.injectivity_radius-Tuple{Grassmann}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::Grassmann)\ninjectivity_radius(M::Grassmann, p) Return the injectivity radius on the  Grassmann M , which is  $\\frac{œÄ}{2}$ . source"},{"id":932,"pagetitle":"Grassmann","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.is_flat-Tuple{Grassmann}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(M::Grassmann) Return true if  Grassmann M  is one-dimensional. source"},{"id":933,"pagetitle":"Grassmann","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.manifold_dimension-Union{Tuple{Grassmann{<:Any, ùîΩ}}, Tuple{ùîΩ}} where ùîΩ","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::Grassmann) Return the dimension of the  Grassmann (n,k,ùîΩ)  manifold  M , i.e. \\[\\dim \\operatorname{Gr}(n,k) = k(n-k) \\dim_‚Ñù ùîΩ,\\] where  $\\dim_‚Ñù ùîΩ$  is the  real_dimension  of  ùîΩ . source"},{"id":934,"pagetitle":"Grassmann","title":"Statistics.mean","ref":"/manifolds/stable/manifolds/grassmann/#Statistics.mean-Tuple{Grassmann, Vararg{Any}}","content":" Statistics.mean  ‚Äî  Method mean(\n    M::Grassmann,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method = GeodesicInterpolationWithinRadius(œÄ/4);\n    kwargs...,\n) Compute the Riemannian  mean  of  x  using  GeodesicInterpolationWithinRadius . source"},{"id":935,"pagetitle":"Grassmann","title":"The Grassmanian represented as points on the Stiefel manifold","ref":"/manifolds/stable/manifolds/grassmann/#The-Grassmanian-represented-as-points-on-the-[Stiefel](@ref)-manifold","content":" The Grassmanian represented as points on the  Stiefel  manifold"},{"id":936,"pagetitle":"Grassmann","title":"Manifolds.StiefelPoint","ref":"/manifolds/stable/manifolds/grassmann/#Manifolds.StiefelPoint","content":" Manifolds.StiefelPoint  ‚Äî  Type StiefelPoint <: AbstractManifoldPoint A point on a  Stiefel  manifold. This point is mainly used for representing points on the  Grassmann  where this is also the default representation and hence equivalent to using  AbstractMatrices  thereon. they can also used be used as points on Stiefel. source"},{"id":937,"pagetitle":"Grassmann","title":"Manifolds.StiefelTangentVector","ref":"/manifolds/stable/manifolds/grassmann/#Manifolds.StiefelTangentVector","content":" Manifolds.StiefelTangentVector  ‚Äî  Type StiefelTangentVector <: AbstractTangentVector A tangent vector on the  Grassmann  manifold represented by a tangent vector from the tangent space of a corresponding point from the  Stiefel  manifold, see  StiefelPoint . This is the default representation so is can be used interchangeably with just abstract matrices. source"},{"id":938,"pagetitle":"Grassmann","title":"Base.exp","ref":"/manifolds/stable/manifolds/grassmann/#Base.exp-Tuple{Grassmann, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::Grassmann, p, X) Compute the exponential map on the  Grassmann M $= \\mathrm{Gr}(n,k)$  starting in  p  with tangent vector (direction)  X . Let  $X = USV$  denote the SVD decomposition of  $X$ . Then the exponential map is written using \\[z = p V\\cos(S)V^\\mathrm{H} + U\\sin(S)V^\\mathrm{H},\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transposed or Hermitian and the cosine and sine are applied element wise to the diagonal entries of  $S$ . A final QR decomposition  $z=QR$  is performed for numerical stability reasons, yielding the result as \\[\\exp_p X = Q.\\] source"},{"id":939,"pagetitle":"Grassmann","title":"Base.log","ref":"/manifolds/stable/manifolds/grassmann/#Base.log-Tuple{Grassmann, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::Grassmann, p, q) Compute the logarithmic map on the  Grassmann M $= \\mathcal M=\\mathrm{Gr}(n,k)$ , i.e. the tangent vector  X  whose corresponding  geodesic  starting from  p  reaches  q  after time 1 on  M . The formula reads \\[\\log_p q = V‚ãÖ \\operatorname{atan}(S) ‚ãÖ U^\\mathrm{H},\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transposed or Hermitian. The matrices  $U$  and  $V$  are the unitary matrices, and  $S$  is the diagonal matrix containing the singular values of the SVD-decomposition \\[USV = (q^\\mathrm{H}p)^{-1} ( q^\\mathrm{H} - q^\\mathrm{H}pp^\\mathrm{H}).\\] In this formula the  $\\operatorname{atan}$  is meant elementwise. source"},{"id":940,"pagetitle":"Grassmann","title":"Base.rand","ref":"/manifolds/stable/manifolds/grassmann/#Base.rand-Tuple{Grassmann}","content":" Base.rand  ‚Äî  Method rand(M::Grassmann; œÉ::Real=1.0, vector_at=nothing) When  vector_at  is  nothing , return a random point  p  on  Grassmann  manifold  M  by generating a random (Gaussian) matrix with standard deviation  œÉ  in matching size, which is orthonormal. When  vector_at  is not  nothing , return a (Gaussian) random vector from the tangent space  $T_p\\mathrm{Gr}(n,k)$  with mean zero and standard deviation  œÉ  by projecting a random Matrix onto the tangent space at  vector_at . source"},{"id":941,"pagetitle":"Grassmann","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldDiff.riemannian_Hessian-Tuple{Grassmann, Vararg{Any, 4}}","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method riemannian_Hessian(M::Grassmann, p, G, H, X) The Riemannian Hessian can be computed by adopting Eq. (6.6) [ Ngu23 ], where we use for the  EuclideanMetric $Œ±_0=Œ±_1=1$  in their formula. Let  $\\nabla f(p)$  denote the Euclidean gradient  G ,  $\\nabla^2 f(p)[X]$  the Euclidean Hessian  H . Then the formula reads \\[    \\operatorname{Hess}f(p)[X]\n    =\n    \\operatorname{proj}_{T_p\\mathcal M}\\Bigl(\n        ‚àá^2f(p)[X] - X p^{\\mathrm{H}}‚àáf(p)\n    \\Bigr).\\] Compared to Eq. (5.6) also the metric conversion simplifies to the identity. source"},{"id":942,"pagetitle":"Grassmann","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.distance-Tuple{Grassmann, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::Grassmann, p, q) Compute the Riemannian distance on  Grassmann  manifold  M $= \\mathrm{Gr}(n,k)$ . The distance is given by \\[d_{\\mathrm{Gr}(n,k)}(p,q) = \\operatorname{norm}(\\log_p(q)).\\] source"},{"id":943,"pagetitle":"Grassmann","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.inner-Tuple{Grassmann, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::Grassmann, p, X, Y) Compute the inner product for two tangent vectors  X ,  Y  from the tangent space of  p  on the  Grassmann  manifold  M . The formula reads \\[g_p(X,Y) = \\operatorname{tr}(X^{\\mathrm{H}}Y),\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transposed or Hermitian. source"},{"id":944,"pagetitle":"Grassmann","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.inverse_retract-Tuple{Grassmann, Any, Any, PolarInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::Grassmann, p, q, ::PolarInverseRetraction) Compute the inverse retraction for the  PolarRetraction , on the  Grassmann  manifold  M , i.e., \\[\\operatorname{retr}_p^{-1}q = q*(p^\\mathrm{H}q)^{-1} - p,\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transposed or Hermitian. source"},{"id":945,"pagetitle":"Grassmann","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.inverse_retract-Tuple{Grassmann, Any, Any, QRInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M, p, q, ::QRInverseRetraction) Compute the inverse retraction for the  QRRetraction , on the  Grassmann  manifold  M , i.e., \\[\\operatorname{retr}_p^{-1}q = q(p^\\mathrm{H}q)^{-1} - p,\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transposed or Hermitian. source"},{"id":946,"pagetitle":"Grassmann","title":"ManifoldsBase.parallel_transport_direction","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.parallel_transport_direction-Tuple{Grassmann, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_direction  ‚Äî  Method parallel_transport_direction(M::Grassmann, p, X, Y) Compute the parallel transport of  $X \\in   T_p\\mathcal M$  along the geodesic starting in direction  $\\dot Œ≥ (0) = Y$ . Let  $Y = USV$  denote the SVD decomposition of  $Y$ . Then the parallel transport is given by the formula according to Equation (8.5) (p. 171) [ AMS08 ] as \\[\\mathcal P_{p,Y} X = -pV \\sin(S)U^{\\mathrm{T}}X + U\\cos(S)U^{\\mathrm{T}}X + (I-UU^{\\mathrm{T}})X\\] where the sine and cosine applied to the diagonal matrix  $S$  are meant to be elementwise source"},{"id":947,"pagetitle":"Grassmann","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.parallel_transport_to-Tuple{Grassmann, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::Grassmann, p, X, q) Compute the parallel transport of  $X ‚àà  T_p\\mathcal M$  along the geodesic connecting  $p$  to  $q$ . This method uses the  logarithmic map  and the  parallel transport in that direction . source"},{"id":948,"pagetitle":"Grassmann","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.project-Tuple{Grassmann, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Grassmann, p) Project  p  from the embedding onto the  Grassmann M , i.e. compute  q  as the polar decomposition of  $p$  such that  $q^{\\mathrm{H}}q$  is the identity, where  $‚ãÖ^{\\mathrm{H}}$  denotes the Hermitian, i.e. complex conjugate transposed. source"},{"id":949,"pagetitle":"Grassmann","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.project-Tuple{Grassmann, Vararg{Any}}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Grassmann, p, X) Project the  n -by- k X  onto the tangent space of  p  on the  Grassmann M , which is computed by \\[\\operatorname{proj_p}(X) = X - pp^{\\mathrm{H}}X,\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transposed or Hermitian. source"},{"id":950,"pagetitle":"Grassmann","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.representation_size-Tuple{Grassmann}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::Grassmann) Return the representation size or matrix dimension of a point on the  Grassmann M , i.e.  $(n,k)$  for both the real-valued and the complex value case. source"},{"id":951,"pagetitle":"Grassmann","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.retract-Tuple{Grassmann, Any, Any, PolarRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::Grassmann, p, X, ::PolarRetraction) Compute the SVD-based retraction  PolarRetraction  on the  Grassmann M . With  $USV = p + X$  the retraction reads \\[\\operatorname{retr}_p X = UV^\\mathrm{H},\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transposed or Hermitian. source"},{"id":952,"pagetitle":"Grassmann","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.retract-Tuple{Grassmann, Any, Any, QRRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::Grassmann, p, X, ::QRRetraction ) Compute the QR-based retraction  QRRetraction  on the  Grassmann M . With  $QR = p + X$  the retraction reads \\[\\operatorname{retr}_p X = QD,\\] where D is a  $m√ón$  matrix with \\[D = \\operatorname{diag}\\left( \\operatorname{sgn}\\left(R_{ii}+\\frac{1}{2}\\right)_{i=1}^n \\right).\\] source"},{"id":953,"pagetitle":"Grassmann","title":"ManifoldsBase.riemann_tensor","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.riemann_tensor-Tuple{Grassmann{<:Any, ‚Ñù}, Vararg{Any, 4}}","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(::Grassmann{<:Any,‚Ñù}, p, X, Y, Z) Compute the value of Riemann tensor on the real  Grassmann  manifold. The formula reads [ Ren11 ] \\[R(X,Y)Z = (XY^\\mathrm{T} - YX^\\mathrm{T})Z + Z(Y^\\mathrm{T}X - X^\\mathrm{T}Y).\\] source"},{"id":954,"pagetitle":"Grassmann","title":"ManifoldsBase.vector_transport_to","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.vector_transport_to-Tuple{Grassmann, Any, Any, Any, ProjectionTransport}","content":" ManifoldsBase.vector_transport_to  ‚Äî  Method vector_transport_to(M::Grassmann, p, X, q, ::ProjectionTransport) compute the projection based transport on the  Grassmann M  by interpreting  X  from the tangent space at  p  as a point in the embedding and projecting it onto the tangent space at q. source"},{"id":955,"pagetitle":"Grassmann","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.zero_vector-Tuple{Grassmann, Vararg{Any}}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(M::Grassmann, p) Return the zero tangent vector from the tangent space at  p  on the  Grassmann M , which is given by a zero matrix the same size as  p . source"},{"id":956,"pagetitle":"Grassmann","title":"The Grassmannian represented as projectors","ref":"/manifolds/stable/manifolds/grassmann/#The-Grassmannian-represented-as-projectors","content":" The Grassmannian represented as projectors"},{"id":957,"pagetitle":"Grassmann","title":"Manifolds.ProjectorPoint","ref":"/manifolds/stable/manifolds/grassmann/#Manifolds.ProjectorPoint","content":" Manifolds.ProjectorPoint  ‚Äî  Type ProjectorPoint <: AbstractManifoldPoint A type to represent points on a manifold  Grassmann  that are orthogonal projectors, i.e. a matrix  $p ‚àà \\mathbb F^{n,n}$  projecting onto a  $k$ -dimensional subspace. source"},{"id":958,"pagetitle":"Grassmann","title":"Manifolds.ProjectorTangentVector","ref":"/manifolds/stable/manifolds/grassmann/#Manifolds.ProjectorTangentVector","content":" Manifolds.ProjectorTangentVector  ‚Äî  Type ProjectorTangentVector <: AbstractTangentVector A type to represent tangent vectors to points on a  Grassmann  manifold that are orthogonal projectors. source"},{"id":959,"pagetitle":"Grassmann","title":"Base.exp","ref":"/manifolds/stable/manifolds/grassmann/#Base.exp-Tuple{Grassmann, ProjectorPoint, ProjectorTangentVector}","content":" Base.exp  ‚Äî  Method exp(M::Grassmann, p::ProjectorPoint, X::ProjectorTangentVector) Compute the exponential map on the  Grassmann  as \\[    \\exp_pX = \\operatorname{Exp}([X,p])p\\operatorname{Exp}(-[X,p]),\\] where  $\\operatorname{Exp}$  denotes the matrix exponential and  $[A,B] = AB-BA$  denotes the matrix commutator. For details, see Proposition 3.2 in [ BZA20 ]. source"},{"id":960,"pagetitle":"Grassmann","title":"Manifolds.canonical_project!","ref":"/manifolds/stable/manifolds/grassmann/#Manifolds.canonical_project!-Tuple{Grassmann, ProjectorPoint, Any}","content":" Manifolds.canonical_project!  ‚Äî  Method canonical_project!(M::Grassmann, q::ProjectorPoint, p) Compute the canonical projection  $œÄ(p)$  from the  Stiefel  manifold onto the  Grassmann  manifold when represented as  ProjectorPoint , i.e. \\[    œÄ^{\\mathrm{SG}}(p) = pp^{\\mathrm{T}}\\] source"},{"id":961,"pagetitle":"Grassmann","title":"Manifolds.differential_canonical_project!","ref":"/manifolds/stable/manifolds/grassmann/#Manifolds.differential_canonical_project!-Tuple{Grassmann, ProjectorTangentVector, Any, Any}","content":" Manifolds.differential_canonical_project!  ‚Äî  Method canonical_project!(M::Grassmann, q::ProjectorPoint, p) Compute the canonical projection  $œÄ(p)$  from the  Stiefel  manifold onto the  Grassmann  manifold when represented as  ProjectorPoint , i.e. \\[    DœÄ^{\\mathrm{SG}}(p)[X] = Xp^{\\mathrm{T}} + pX^{\\mathrm{T}}\\] source"},{"id":962,"pagetitle":"Grassmann","title":"Manifolds.horizontal_lift","ref":"/manifolds/stable/manifolds/grassmann/#Manifolds.horizontal_lift-Tuple{Stiefel, Any, ProjectorTangentVector}","content":" Manifolds.horizontal_lift  ‚Äî  Method horizontal_lift(N::Stiefel{n,k}, q, X::ProjectorTangentVector) Compute the horizontal lift of  X  from the tangent space at  $p=œÄ(q)$  on the  Grassmann  manifold, i.e. \\[Y = Xq ‚àà T_q\\mathrm{St}(n,k)\\] source"},{"id":963,"pagetitle":"Grassmann","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.check_point-Tuple{Grassmann, ProjectorPoint}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(::Grassmann, p::ProjectorPoint; kwargs...) Check whether an orthogonal projector is a point from the  Grassmann (n,k)  manifold, i.e. the  ProjectorPoint $p ‚àà \\mathbb F^{n√ón}$ ,  $\\mathbb F ‚àà \\{\\mathbb R, \\mathbb C\\}$  has to fulfill  $p^{\\mathrm{T}} = p$ ,  $p^2=p$ , and ` \\operatorname{rank} p = k . source"},{"id":964,"pagetitle":"Grassmann","title":"ManifoldsBase.check_size","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.check_size-Tuple{Grassmann, ProjectorPoint}","content":" ManifoldsBase.check_size  ‚Äî  Method check_size(M::Grassmann, p::ProjectorPoint; kwargs...) Check that the  ProjectorPoint  is of correct size, i.e. from  $\\mathbb F^{n√ón}$ source"},{"id":965,"pagetitle":"Grassmann","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.check_vector-Tuple{Grassmann, ProjectorPoint, ProjectorTangentVector}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(::Grassmann, p::ProjectorPoint, X::ProjectorTangentVector; kwargs...) Check whether the  ProjectorTangentVector X  is from the tangent space  $T_p\\operatorname{Gr}(n,k)$  at the  ProjectorPoint p  on the  Grassmann  manifold  $\\operatorname{Gr}(n,k)$ . This means that  X  has to be symmetric and that \\[Xp + pX = X\\] must hold, where the  kwargs  can be used to check both for symmetrix of  $X$ ` and this equality up to a certain tolerance. source"},{"id":966,"pagetitle":"Grassmann","title":"ManifoldsBase.get_embedding","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.get_embedding-Union{Tuple{ùîΩ}, Tuple{k}, Tuple{n}, Tuple{Grassmann{ManifoldsBase.TypeParameter{Tuple{n, k}}, ùîΩ}, ProjectorPoint}} where {n, k, ùîΩ}","content":" ManifoldsBase.get_embedding  ‚Äî  Method get_embedding(M::Grassmann, p::ProjectorPoint) Return the embedding of the  ProjectorPoint  representation of the  Grassmann  manifold, i.e. the Euclidean space  $\\mathbb F^{n√ón}$ . source"},{"id":967,"pagetitle":"Grassmann","title":"ManifoldsBase.parallel_transport_direction","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.parallel_transport_direction-Tuple{Grassmann, ProjectorPoint, ProjectorTangentVector, ProjectorTangentVector}","content":" ManifoldsBase.parallel_transport_direction  ‚Äî  Method parallel_transport_direction(\n    M::Grassmann,\n    p::ProjectorPoint,\n    X::ProjectorTangentVector,\n    d::ProjectorTangentVector\n) Compute the parallel transport of  X  from the tangent space at  p  into direction  d , i.e. to  $q=\\exp_pd$ . The formula is given in Proposition 3.5 of [ BZA20 ] as \\[\\mathcal{P}_{q ‚Üê p}(X) = \\operatorname{Exp}([d,p])X\\operatorname{Exp}(-[d,p]),\\] where  $\\operatorname{Exp}$  denotes the matrix exponential and  $[A,B] = AB-BA$  denotes the matrix commutator. source"},{"id":968,"pagetitle":"Grassmann","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/grassmann/#ManifoldsBase.representation_size-Tuple{Grassmann, ProjectorPoint}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::Grassmann, p::ProjectorPoint) Return the represenation size or matrix dimension of a point on the  Grassmann M  when using  ProjectorPoint s, i.e.  $(n,n)$ . source"},{"id":969,"pagetitle":"Grassmann","title":"Literature","ref":"/manifolds/stable/manifolds/grassmann/#Literature","content":" Literature [AMS08] P.-A.¬†Absil, R.¬†Mahony and R.¬†Sepulchre.  Optimization Algorithms on Matrix Manifolds  (Princeton University Press, 2008), available online at  press.princeton.edu/chapters/absil/ . [BZA20] T.¬†Bendokat, R.¬†Zimmermann and P.-A.¬†Absil.  A Grassmann Manifold Handbook: Basic Geometry and Computational Aspects , arXiv¬†Preprint (2020),  arXiv:2011.13699 . [Ngu23] D.¬†Nguyen.  Operator-Valued Formulas for Riemannian Gradient and Hessian and Families of Tractable Metrics in Riemannian Optimization .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  198 , 135‚Äì164  (2023),  arXiv:2009.10159 . [Ren11] Q.¬†Rentmeesters.  A gradient method for geodesic data fitting on some symmetric Riemannian manifolds . In:  IEEE Conference on Decision and Control and European Control Conference  (2011); pp.¬†7141‚Äì7146."},{"id":972,"pagetitle":"Group manifold","title":"Group manifolds","ref":"/manifolds/stable/manifolds/group/#GroupManifoldSection","content":" Group manifolds Lie groups, groups that are Riemannian manifolds with a smooth binary group operation  AbstractGroupOperation , are implemented as  AbstractDecoratorManifold  and specifying the group operation using the  IsGroupManifold  or by decorating an existing manifold with a group operation using  GroupManifold . The common addition and multiplication group operations of  AdditionOperation  and  MultiplicationOperation  are provided, though their behavior may be customized for a specific group. There are short introductions at the beginning of each subsection. They briefly mention what is available with links to more detailed descriptions."},{"id":973,"pagetitle":"Group manifold","title":"Contents","ref":"/manifolds/stable/manifolds/group/#Contents","content":" Contents Group manifolds Contents Groups Group manifold GroupManifold Default Representation of Tangent Vectors Generic Operations Circle group General linear group Heisenberg group (Special) Orthogonal and (Special) Unitary group Power group Product group Semidirect product group Special Euclidean group Special linear group Translation group Metrics on groups Invariant metrics Cartan-Schouten connections"},{"id":974,"pagetitle":"Group manifold","title":"Groups","ref":"/manifolds/stable/manifolds/group/#Groups","content":" Groups The following operations are available for group manifolds: Identity : an allocation-free representation of the identity element of the group. inv : get the inverse of a given element. compose : compose two given elements of a group. identity_element  get the identity element of the group, in the representation used by other points from the group."},{"id":975,"pagetitle":"Group manifold","title":"Group manifold","ref":"/manifolds/stable/manifolds/group/#Group-manifold","content":" Group manifold GroupManifold  adds a group structure to the wrapped manifold. It does not affect metric (or connection) structure of the wrapped manifold, however it can to be further wrapped in  MetricManifold  to get invariant metrics, or in a  ConnectionManifold  to equip it with a Cartan-Schouten connection."},{"id":976,"pagetitle":"Group manifold","title":"Manifolds.AbstractGroupOperation","ref":"/manifolds/stable/manifolds/group/#Manifolds.AbstractGroupOperation","content":" Manifolds.AbstractGroupOperation  ‚Äî  Type AbstractGroupOperation Abstract type for smooth binary operations  $‚àò$  on elements of a Lie group  $\\mathcal{G}$ : \\[‚àò : \\mathcal{G} √ó \\mathcal{G} ‚Üí \\mathcal{G}\\] An operation can be either defined for a specific group manifold over number system  ùîΩ  or in general, by defining for an operation  Op  the following methods: identity_element!(::AbstractDecoratorManifold, q, q)\ninv!(::AbstractDecoratorManifold, q, p)\n_compose!(::AbstractDecoratorManifold, x, p, q) Note that a manifold is connected with an operation by wrapping it with a decorator,  AbstractDecoratorManifold  using the  IsGroupManifold  to specify the operation. For a concrete case the concrete wrapper  GroupManifold  can be used. source"},{"id":977,"pagetitle":"Group manifold","title":"Manifolds.AbstractGroupVectorRepresentation","ref":"/manifolds/stable/manifolds/group/#Manifolds.AbstractGroupVectorRepresentation","content":" Manifolds.AbstractGroupVectorRepresentation  ‚Äî  Type abstract type AbstractGroupVectorRepresentation end An abstract supertype for indicating representation of tangent vectors on a group manifold. The most common representations are  LeftInvariantRepresentation ,  TangentVectorRepresentation  and  HybridTangentRepresentation . source"},{"id":978,"pagetitle":"Group manifold","title":"Manifolds.AbstractInvarianceTrait","ref":"/manifolds/stable/manifolds/group/#Manifolds.AbstractInvarianceTrait","content":" Manifolds.AbstractInvarianceTrait  ‚Äî  Type AbstractInvarianceTrait <: AbstractTrait A common supertype for anz  AbstractTrait  related to metric invariance source"},{"id":979,"pagetitle":"Group manifold","title":"Manifolds.ActionDirection","ref":"/manifolds/stable/manifolds/group/#Manifolds.ActionDirection","content":" Manifolds.ActionDirection  ‚Äî  Type ActionDirection Direction of action on a manifold, either  LeftAction  or  RightAction . source"},{"id":980,"pagetitle":"Group manifold","title":"Manifolds.GroupActionSide","ref":"/manifolds/stable/manifolds/group/#Manifolds.GroupActionSide","content":" Manifolds.GroupActionSide  ‚Äî  Type GroupActionSide Side of action on a manifold, either  LeftSide  or  RightSide . source"},{"id":981,"pagetitle":"Group manifold","title":"Manifolds.GroupExponentialRetraction","ref":"/manifolds/stable/manifolds/group/#Manifolds.GroupExponentialRetraction","content":" Manifolds.GroupExponentialRetraction  ‚Äî  Type GroupExponentialRetraction{D<:ActionDirectionAndSide} <: AbstractRetractionMethod Retraction using the group exponential  exp_lie  \"translated\" to any point on the manifold. For more details, see  retract . Constructor GroupExponentialRetraction(conv::ActionDirectionAndSide = LeftAction()) source"},{"id":982,"pagetitle":"Group manifold","title":"Manifolds.GroupLogarithmicInverseRetraction","ref":"/manifolds/stable/manifolds/group/#Manifolds.GroupLogarithmicInverseRetraction","content":" Manifolds.GroupLogarithmicInverseRetraction  ‚Äî  Type GroupLogarithmicInverseRetraction{D<:ActionDirectionAndSide} <: AbstractInverseRetractionMethod Retraction using the group logarithm  log_lie  \"translated\" to any point on the manifold. For more details, see  inverse_retract . Constructor GroupLogarithmicInverseRetraction(conv::ActionDirectionAndSide = LeftForwardAction()) source"},{"id":983,"pagetitle":"Group manifold","title":"Manifolds.HasBiinvariantMetric","ref":"/manifolds/stable/manifolds/group/#Manifolds.HasBiinvariantMetric","content":" Manifolds.HasBiinvariantMetric  ‚Äî  Type HasBiinvariantMetric <: AbstractInvarianceTrait Specify that the default metric functions for the bi-invariant metric on a  GroupManifold  are to be used. source"},{"id":984,"pagetitle":"Group manifold","title":"Manifolds.HasLeftInvariantMetric","ref":"/manifolds/stable/manifolds/group/#Manifolds.HasLeftInvariantMetric","content":" Manifolds.HasLeftInvariantMetric  ‚Äî  Type HasLeftInvariantMetric <: AbstractInvarianceTrait Specify that the default metric functions for the left-invariant metric on a  GroupManifold  are to be used. source"},{"id":985,"pagetitle":"Group manifold","title":"Manifolds.HasRightInvariantMetric","ref":"/manifolds/stable/manifolds/group/#Manifolds.HasRightInvariantMetric","content":" Manifolds.HasRightInvariantMetric  ‚Äî  Type HasRightInvariantMetric <: AbstractInvarianceTrait Specify that the default metric functions for the right-invariant metric on a  GroupManifold  are to be used. source"},{"id":986,"pagetitle":"Group manifold","title":"Manifolds.Identity","ref":"/manifolds/stable/manifolds/group/#Manifolds.Identity","content":" Manifolds.Identity  ‚Äî  Type Identity{O<:AbstractGroupOperation} Represent the group identity element  $e ‚àà \\mathcal{G}$  on a Lie group  $\\mathcal G$  with  AbstractGroupOperation  of type  O . Similar to the philosophy that points are agnostic of their group at hand, the identity does not store the group  g  it belongs to. However it depends on the type of the  AbstractGroupOperation  used. See also  identity_element  on how to obtain the corresponding  AbstractManifoldPoint  or array representation. Constructors Identity(G::AbstractDecoratorManifold{ùîΩ})\nIdentity(o::O)\nIdentity(::Type{O}) create the identity of the corresponding subtype  O<: AbstractGroupOperation source"},{"id":987,"pagetitle":"Group manifold","title":"Manifolds.IsGroupManifold","ref":"/manifolds/stable/manifolds/group/#Manifolds.IsGroupManifold","content":" Manifolds.IsGroupManifold  ‚Äî  Type IsGroupManifold{O<:AbstractGroupOperation} <: AbstractTrait A trait to declare an  AbstractManifold   as a manifold with group structure with operation of type  O . Using this trait you can turn a manifold that you implement  implicitly  into a Lie group. If you wish to decorate an existing manifold with one (or different)  AbstractGroupAction s, see  GroupManifold . Constructor IsGroupManifold(op::AbstractGroupOperation, vectors::AbstractGroupVectorRepresentation) source"},{"id":988,"pagetitle":"Group manifold","title":"Manifolds.LeftAction","ref":"/manifolds/stable/manifolds/group/#Manifolds.LeftAction","content":" Manifolds.LeftAction  ‚Äî  Type LeftAction() Left action of a group on a manifold. For a forward action  $Œ±: G√óX ‚Üí X$  it is characterized by \\[Œ±(g, Œ±(h, x)) = Œ±(gh, x)\\] for all  $g, h ‚àà G$  and  $x ‚àà X$ . source"},{"id":989,"pagetitle":"Group manifold","title":"Manifolds.LeftInvariantRepresentation","ref":"/manifolds/stable/manifolds/group/#Manifolds.LeftInvariantRepresentation","content":" Manifolds.LeftInvariantRepresentation  ‚Äî  Type LeftInvariantRepresentation Specify that tangent vectors in a group are stored in Lie algebra using left-invariant representation. source"},{"id":990,"pagetitle":"Group manifold","title":"Manifolds.LeftSide","ref":"/manifolds/stable/manifolds/group/#Manifolds.LeftSide","content":" Manifolds.LeftSide  ‚Äî  Type LeftSide() An action of a group on a manifold that acts from the left side, i.e.  $Œ±: G√óX ‚Üí X$ . source"},{"id":991,"pagetitle":"Group manifold","title":"Manifolds.RightAction","ref":"/manifolds/stable/manifolds/group/#Manifolds.RightAction","content":" Manifolds.RightAction  ‚Äî  Type RightAction() Right action of a group on a manifold. For a forward action  $Œ±: G√óX ‚Üí X$  it is characterized by \\[Œ±(g, Œ±(h, x)) = Œ±(hg, x)\\] for all  $g, h ‚àà G$  and  $x ‚àà X$ . Note that a right action may act from either left or right side in an expression. source"},{"id":992,"pagetitle":"Group manifold","title":"Manifolds.RightSide","ref":"/manifolds/stable/manifolds/group/#Manifolds.RightSide","content":" Manifolds.RightSide  ‚Äî  Type RightSide() An action of a group on a manifold that acts from the right side, i.e.  $Œ±: X√óG ‚Üí X$ . source"},{"id":993,"pagetitle":"Group manifold","title":"Manifolds.TangentVectorRepresentation","ref":"/manifolds/stable/manifolds/group/#Manifolds.TangentVectorRepresentation","content":" Manifolds.TangentVectorRepresentation  ‚Äî  Type TangentVectorRepresentation Specify that tangent vectors in a group are stored in a non-invariant way, corresponding to the storage implied by the underlying manifold. source"},{"id":994,"pagetitle":"Group manifold","title":"Base.inv","ref":"/manifolds/stable/manifolds/group/#Base.inv-Tuple{AbstractDecoratorManifold, Vararg{Any}}","content":" Base.inv  ‚Äî  Method inv(G::AbstractDecoratorManifold, p) Inverse  $p^{-1} ‚àà \\mathcal{G}$  of an element  $p ‚àà \\mathcal{G}$ , such that  $p \\circ p^{-1} = p^{-1} \\circ p = e ‚àà \\mathcal{G}$ , where  $e$  is the  Identity  element of  $\\mathcal{G}$ . source"},{"id":995,"pagetitle":"Group manifold","title":"Manifolds.adjoint_action","ref":"/manifolds/stable/manifolds/group/#Manifolds.adjoint_action-Tuple{AbstractDecoratorManifold, Any, Any, Any}","content":" Manifolds.adjoint_action  ‚Äî  Method adjoint_action(G::AbstractDecoratorManifold, p, X, dir=LeftAction()) Adjoint action of the element  p  of the Lie group  G  on the element  X  of the corresponding Lie algebra. If  dir  is  LeftAction() , it is defined as the differential of the group automorphism  $Œ®_p(q) = pqp‚Åª¬π$  at the identity of  G . The formula reads \\[\\operatorname{Ad}_p(X) = dŒ®_p(e)[X]\\] where  $e$  is the identity element of  G . If  dir  is  RightAction() , then the formula is \\[\\operatorname{Ad}_p(X) = dŒ®_{p^{-1}}(e)[X]\\] Note that the adjoint representation of a Lie group isn't generally faithful. Notably the adjoint representation of SO(2) is trivial. source"},{"id":996,"pagetitle":"Group manifold","title":"Manifolds.adjoint_inv_diff","ref":"/manifolds/stable/manifolds/group/#Manifolds.adjoint_inv_diff-Tuple{AbstractDecoratorManifold, Any}","content":" Manifolds.adjoint_inv_diff  ‚Äî  Method adjoint_inv_diff(G::AbstractDecoratorManifold, p, X) Compute the value of pullback of inverse  $p^{-1} ‚àà \\mathcal{G}$  of an element  $p ‚àà \\mathcal{G}$  at tangent vector  X  at  $p^{-1}$ . The result is a tangent vector at  $p$ . source"},{"id":997,"pagetitle":"Group manifold","title":"Manifolds.adjoint_matrix","ref":"/manifolds/stable/manifolds/group/#Manifolds.adjoint_matrix","content":" Manifolds.adjoint_matrix  ‚Äî  Function adjoint_matrix(G::AbstractManifold, p, B::AbstractBasis=DefaultOrthonormalBasis()) Compute the adjoint matrix related to conjugation of vectors by element  p  of Lie group  G  for basis  B . It is the matrix  $A$  such that for each element  X  of the Lie algebra with coefficients  $c$  in basis  B ,  $Ac$  is the vector of coefficients of  X  conjugated by  p  in basis  B . source"},{"id":998,"pagetitle":"Group manifold","title":"Manifolds.compose","ref":"/manifolds/stable/manifolds/group/#Manifolds.compose-Tuple{AbstractDecoratorManifold, Vararg{Any}}","content":" Manifolds.compose  ‚Äî  Method compose(G::AbstractDecoratorManifold, p, q) Compose elements  $p,q ‚àà \\mathcal{G}$  using the group operation  $p \\circ q$ . For implementing composition on a new group manifold, please overload  _compose  instead so that methods with  Identity  arguments are not ambiguous. source"},{"id":999,"pagetitle":"Group manifold","title":"Manifolds.exp_inv","ref":"/manifolds/stable/manifolds/group/#Manifolds.exp_inv","content":" Manifolds.exp_inv  ‚Äî  Function exp_inv(G::AbstractManifold, p, X, t::Number=1) Compute exponential map on a Lie group  G  invariant to group operation. For groups with a bi-invariant metric or a Cartan-Schouten connection, this is the same as  exp  but for other groups it may differ. source"},{"id":1000,"pagetitle":"Group manifold","title":"Manifolds.exp_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.exp_lie-Tuple{AbstractManifold, Any}","content":" Manifolds.exp_lie  ‚Äî  Method exp_lie(G, X)\nexp_lie!(G, q, X) Compute the group exponential of the Lie algebra element  X . It is equivalent to the exponential map defined by the  CartanSchoutenMinus  connection. Given an element  $X ‚àà ùî§ = T_e \\mathcal{G}$ , where  $e$  is the  Identity  element of the group  $\\mathcal{G}$ , and  $ùî§$  is its Lie algebra, the group exponential is the map \\[\\exp : ùî§ ‚Üí \\mathcal{G},\\] such that for  $t,s ‚àà ‚Ñù$ ,  $Œ≥(t) = \\exp (t X)$  defines a one-parameter subgroup with the following properties. Note that one-parameter subgroups are commutative (see [ Suh13 ], section 3.5), even if the Lie group itself is not commutative. \\[\\begin{aligned}\nŒ≥(t) &= Œ≥(-t)^{-1}\\\\\nŒ≥(t + s) &= Œ≥(t) \\circ Œ≥(s) = Œ≥(s) \\circ Œ≥(t)\\\\\nŒ≥(0) &= e\\\\\n\\lim_{t ‚Üí 0} \\frac{d}{dt} Œ≥(t) &= X.\n\\end{aligned}\\] Note In general, the group exponential map is distinct from the Riemannian exponential map  exp . For example for the  MultiplicationOperation  and either  Number  or  AbstractMatrix  the Lie exponential is the numeric/matrix exponential. \\[\\exp X = \\operatorname{Exp} X = \\sum_{n=0}^‚àû \\frac{1}{n!} X^n.\\] Since this function also depends on the group operation, make sure to implement the corresponding trait version  exp_lie(::TraitList{<:IsGroupManifold}, G, X) . source"},{"id":1001,"pagetitle":"Group manifold","title":"Manifolds.get_coordinates_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.get_coordinates_lie-Tuple{ManifoldsBase.TraitList{<:IsGroupManifold}, AbstractManifold, Any, AbstractBasis}","content":" Manifolds.get_coordinates_lie  ‚Äî  Method get_coordinates_lie(G::AbstractManifold, X, B::AbstractBasis) Get the coordinates of an element  X  from the Lie algebra og  G  with respect to a basis  B . This is similar to calling  get_coordinates  at the  p= Identity (G) . source"},{"id":1002,"pagetitle":"Group manifold","title":"Manifolds.get_vector_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.get_vector_lie-Tuple{ManifoldsBase.TraitList{<:IsGroupManifold}, AbstractManifold, Any, AbstractBasis}","content":" Manifolds.get_vector_lie  ‚Äî  Method get_vector_lie(G::AbstractDecoratorManifold, a, B::AbstractBasis) Reconstruct a tangent vector from the Lie algebra of  G  from coordinates  a  of a basis  B . This is similar to calling  get_vector  at the  p= Identity (G) . source"},{"id":1003,"pagetitle":"Group manifold","title":"Manifolds.identity_element","ref":"/manifolds/stable/manifolds/group/#Manifolds.identity_element-Tuple{AbstractDecoratorManifold, Any}","content":" Manifolds.identity_element  ‚Äî  Method identity_element(G::AbstractDecoratorManifold, p) Return a point representation of the  Identity  on the  IsGroupManifold G , where  p  indicates the type to represent the identity. source"},{"id":1004,"pagetitle":"Group manifold","title":"Manifolds.identity_element","ref":"/manifolds/stable/manifolds/group/#Manifolds.identity_element-Tuple{AbstractDecoratorManifold}","content":" Manifolds.identity_element  ‚Äî  Method identity_element(G::AbstractDecoratorManifold) Return a point representation of the  Identity  on the  IsGroupManifold G . By default this representation is the default array or number representation. It should return the corresponding default representation of  $e$  as a point on  G  if points are not represented by arrays. source"},{"id":1005,"pagetitle":"Group manifold","title":"Manifolds.inv_diff","ref":"/manifolds/stable/manifolds/group/#Manifolds.inv_diff-Tuple{AbstractDecoratorManifold, Any}","content":" Manifolds.inv_diff  ‚Äî  Method inv_diff(G::AbstractDecoratorManifold, p, X) Compute the value of differential of inverse  $p^{-1} ‚àà \\mathcal{G}$  of an element  $p ‚àà \\mathcal{G}$  at tangent vector  X  at  p . The result is a tangent vector at  $p^{-1}$ . Note : the default implementation of  inv_diff  and  inv_diff!  assumes that the tangent vector  $X$  is stored at the point  $p ‚àà \\mathcal{G}$  as the vector  $Y ‚àà \\mathfrak{g}$   where  $X = pY$ . source"},{"id":1006,"pagetitle":"Group manifold","title":"Manifolds.inverse_translate","ref":"/manifolds/stable/manifolds/group/#Manifolds.inverse_translate-Tuple{AbstractDecoratorManifold, Vararg{Any}}","content":" Manifolds.inverse_translate  ‚Äî  Method inverse_translate(G::AbstractDecoratorManifold, p, q, conv::ActionDirectionAndSide=LeftForwardAction()) Inverse translate group element  $q$  by  $p$  with the translation  $œÑ_p^{-1}$  with the specified  conv ention, either left forward ( $L_p^{-1}$ ), left backward ( $R'_p^{-1}$ ), right backward ( $R_p^{-1}$ ) or right forward ( $L'_p^{-1}$ ), defined as ```math \\begin{aligned} L p^{-1} &: q ‚Ü¶ p^{-1} \\circ q\\\nL' p^{-1} &: q ‚Ü¶ p \\circ q\\\nR p^{-1} &: q ‚Ü¶ q \\circ p^{-1}\\\nR' p^{-1} &: q ‚Ü¶ q \\circ p. \\end{aligned} source"},{"id":1007,"pagetitle":"Group manifold","title":"Manifolds.inverse_translate_diff","ref":"/manifolds/stable/manifolds/group/#Manifolds.inverse_translate_diff-Tuple{AbstractDecoratorManifold, Vararg{Any}}","content":" Manifolds.inverse_translate_diff  ‚Äî  Method inverse_translate_diff(G::AbstractDecoratorManifold, p, q, X, conv::ActionDirectionAndSide=LeftForwardAction()) For group elements  $p, q ‚àà \\mathcal{G}$  and tangent vector  $X ‚àà T_q \\mathcal{G}$ , compute the action on  $X$  of the differential of the inverse translation  $œÑ_p$  by  $p$ , with the specified left or right  conv ention. The differential transports vectors: \\[(\\mathrm{d}œÑ_p^{-1})_q : T_q \\mathcal{G} ‚Üí T_{œÑ_p^{-1} q} \\mathcal{G}\\\\\\] source"},{"id":1008,"pagetitle":"Group manifold","title":"Manifolds.is_group_manifold","ref":"/manifolds/stable/manifolds/group/#Manifolds.is_group_manifold-Tuple{AbstractManifold, AbstractGroupOperation}","content":" Manifolds.is_group_manifold  ‚Äî  Method is_group_manifold(G::GroupManifold)\nis_group_manifold(G::AbstractManifold, o::AbstractGroupOperation) returns whether an  AbstractDecoratorManifold  is a group manifold with  AbstractGroupOperation o . For a  GroupManifold G  this checks whether the right operations is stored within  G . source"},{"id":1009,"pagetitle":"Group manifold","title":"Manifolds.is_identity","ref":"/manifolds/stable/manifolds/group/#Manifolds.is_identity-Tuple{AbstractDecoratorManifold, Any}","content":" Manifolds.is_identity  ‚Äî  Method is_identity(G::AbstractDecoratorManifold, q; kwargs) Check whether  q  is the identity on the  IsGroupManifold G , i.e. it is either the  Identity {O}  with the corresponding  AbstractGroupOperation O , or (approximately) the correct point representation. source"},{"id":1010,"pagetitle":"Group manifold","title":"Manifolds.lie_bracket","ref":"/manifolds/stable/manifolds/group/#Manifolds.lie_bracket-Tuple{AbstractDecoratorManifold, Any, Any}","content":" Manifolds.lie_bracket  ‚Äî  Method lie_bracket(G::AbstractDecoratorManifold, X, Y) Lie bracket between elements  X  and  Y  of the Lie algebra corresponding to the Lie group  G , cf.  IsGroupManifold . This can be used to compute the adjoint representation of a Lie algebra. Note that this representation isn't generally faithful. Notably the adjoint representation of ùî∞ùî¨(2) is trivial. source"},{"id":1011,"pagetitle":"Group manifold","title":"Manifolds.log_inv","ref":"/manifolds/stable/manifolds/group/#Manifolds.log_inv-Tuple{AbstractManifold, Any, Any}","content":" Manifolds.log_inv  ‚Äî  Method log_inv(G::AbstractManifold, p, q) Compute logarithmic map on a Lie group  G  invariant to group operation. For groups with a bi-invariant metric or a Cartan-Schouten connection, this is the same as  log  but for other groups it may differ. source"},{"id":1012,"pagetitle":"Group manifold","title":"Manifolds.log_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.log_lie-Tuple{AbstractDecoratorManifold, Any}","content":" Manifolds.log_lie  ‚Äî  Method log_lie(G, q)\nlog_lie!(G, X, q) Compute the Lie group logarithm of the Lie group element  q . It is equivalent to the logarithmic map defined by the  CartanSchoutenMinus  connection. Given an element  $q ‚àà \\mathcal{G}$ , compute the right inverse of the group exponential map  exp_lie , that is, the element  $\\log q = X ‚àà ùî§ = T_e \\mathcal{G}$ , such that  $q = \\exp X$ Note In general, the group logarithm map is distinct from the Riemannian logarithm map  log . For matrix Lie groups this is equal to the (matrix) logarithm: \\[\\log q = \\operatorname{Log} q = \\sum_{n=1}^‚àû \\frac{(-1)^{n+1}}{n} (q - e)^n,\\] where  $e$  here is the  Identity  element, that is,  $1$  for numeric  $q$  or the identity matrix  $I_m$  for matrix  $q ‚àà ‚Ñù^{m√óm}$ . Since this function also depends on the group operation, make sure to implement either _log_lie(G, q)  and  _log_lie!(G, X, q)  for the points not being the  Identity the trait version  log_lie(::TraitList{<:IsGroupManifold}, G, e) ,  log_lie(::TraitList{<:IsGroupManifold}, G, X, e)  for own implementations of the identity case. source"},{"id":1013,"pagetitle":"Group manifold","title":"Manifolds.switch_direction","ref":"/manifolds/stable/manifolds/group/#Manifolds.switch_direction-Tuple{ActionDirection}","content":" Manifolds.switch_direction  ‚Äî  Method switch_direction(::ActionDirection) Returns type of action between left and right. This function does not affect side of action, see  switch_side . source"},{"id":1014,"pagetitle":"Group manifold","title":"Manifolds.switch_side","ref":"/manifolds/stable/manifolds/group/#Manifolds.switch_side-Tuple{Manifolds.GroupActionSide}","content":" Manifolds.switch_side  ‚Äî  Method switch_side(::GroupActionSide) Returns side of action between left and right. This function does not affect the action being left or right, see  switch_direction . source"},{"id":1015,"pagetitle":"Group manifold","title":"Manifolds.translate","ref":"/manifolds/stable/manifolds/group/#Manifolds.translate-Tuple{AbstractDecoratorManifold, Vararg{Any}}","content":" Manifolds.translate  ‚Äî  Method translate(G::AbstractDecoratorManifold, p, q, conv::ActionDirectionAndSide=LeftForwardAction()]) Translate group element  $q$  by  $p$  with the translation  $œÑ_p$  with the specified  conv ention, either left forward  $œÑ_p(q) = p \\circ q$ left backward  $œÑ_p(q) = q \\circ p^{-1}$ right backward  $œÑ_p(q) = q \\circ p$ right forward  $œÑ_p(q) = p^{-1} \\circ q$ source"},{"id":1016,"pagetitle":"Group manifold","title":"Manifolds.translate_diff","ref":"/manifolds/stable/manifolds/group/#Manifolds.translate_diff-Tuple{AbstractDecoratorManifold, Vararg{Any}}","content":" Manifolds.translate_diff  ‚Äî  Method translate_diff(G::AbstractDecoratorManifold, p, q, X, conv::ActionDirectionAndSide=LeftForwardAction()) For group elements  $p, q ‚àà \\mathcal{G}$  and tangent vector  $X ‚àà T_q \\mathcal{G}$ , compute the action of the differential of the translation  $œÑ_p$  by  $p$  on  $X$ , with the specified left or right  conv ention. The differential transports vectors: \\[(\\mathrm{d}œÑ_p)_q : T_q \\mathcal{G} ‚Üí T_{œÑ_p q} \\mathcal{G}\\\\\\] Note : the default implementation of  translate_diff  and  translate_diff!  assumes that a tangent vector  $X$  at a point  $q ‚àà \\mathcal{G}$  is stored  as the vector  $Y ‚àà \\mathfrak{g}$   where  $X = qY$ . The implementation at  q = Identity  is independent of the storage choice. source"},{"id":1017,"pagetitle":"Group manifold","title":"ManifoldsBase.hat","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.hat-Union{Tuple{O}, Tuple{ManifoldsBase.TraitList{<:IsGroupManifold{O}}, AbstractDecoratorManifold, Identity{O}, Any}} where O<:AbstractGroupOperation","content":" ManifoldsBase.hat  ‚Äî  Method hat(M::AbstractDecoratorManifold{ùîΩ,O}, ::Identity{O}, X‚Å±) where {ùîΩ,O<:AbstractGroupOperation} Given a basis  $e_i$  on the tangent space at a the  Identity  and tangent component vector  $X^i$ , compute the equivalent vector representation ``X=X^i e_i**, where Einstein summation notation is used: \\[‚àß : X^i ‚Ü¶ X^i e_i\\] For array manifolds, this converts a vector representation of the tangent vector to an array representation. The  vee  map is the  hat  map's inverse. source"},{"id":1018,"pagetitle":"Group manifold","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.inverse_retract-Tuple{ManifoldsBase.TraitList{<:IsGroupManifold}, AbstractDecoratorManifold, Any, Any, Manifolds.GroupLogarithmicInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(\n    G::AbstractDecoratorManifold,\n    p,\n    X,\n    method::GroupLogarithmicInverseRetraction,\n) Compute the inverse retraction using the group logarithm  log_lie  \"translated\" to any point on the manifold. With a group translation ( translate )  $œÑ_p$  in a specified direction, the retraction is \\[\\operatorname{retr}_p^{-1} = (\\mathrm{d}œÑ_p)_e \\circ \\log \\circ œÑ_p^{-1},\\] where  $\\log$  is the group logarithm ( log_lie ), and  $(\\mathrm{d}œÑ_p)_e$  is the action of the differential of translation  $œÑ_p$  evaluated at the identity element  $e$  (see  translate_diff ). source"},{"id":1019,"pagetitle":"Group manifold","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.retract-Tuple{ManifoldsBase.TraitList{<:IsGroupManifold}, AbstractDecoratorManifold, Any, Any, Manifolds.GroupExponentialRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(\n    G::AbstractDecoratorManifold,\n    p,\n    X,\n    method::GroupExponentialRetraction,\n) Compute the retraction using the group exponential  exp_lie  \"translated\" to any point on the manifold. With a group translation ( translate )  $œÑ_p$  in a specified direction, the retraction is \\[\\operatorname{retr}_p = œÑ_p \\circ \\exp \\circ (\\mathrm{d}œÑ_p^{-1})_p,\\] where  $\\exp$  is the group exponential ( exp_lie ), and  $(\\mathrm{d}œÑ_p^{-1})_p$  is the action of the differential of inverse translation  $œÑ_p^{-1}$  evaluated at  $p$  (see  inverse_translate_diff ). source"},{"id":1020,"pagetitle":"Group manifold","title":"ManifoldsBase.vee","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.vee-Union{Tuple{O}, Tuple{ManifoldsBase.TraitList{<:IsGroupManifold{O}}, AbstractDecoratorManifold, Identity{O}, Any}} where O<:AbstractGroupOperation","content":" ManifoldsBase.vee  ‚Äî  Method vee(M::AbstractManifold, p, X) Given a basis  $e_i$  on the tangent space at a point  p  and tangent vector  X , compute the vector components  $X^i$ , such that  $X = X^i e_i$ , where Einstein summation notation is used: \\[\\vee : X^i e_i ‚Ü¶ X^i\\] For array manifolds, this converts an array representation of the tangent vector to a vector representation. The  hat  map is the  vee  map's inverse. source"},{"id":1021,"pagetitle":"Group manifold","title":"GroupManifold","ref":"/manifolds/stable/manifolds/group/#GroupManifold","content":" GroupManifold As a concrete wrapper for manifolds (e.g. when the manifold per se is a group manifold but another group structure should be implemented), there is the  GroupManifold"},{"id":1022,"pagetitle":"Group manifold","title":"Manifolds.GroupManifold","ref":"/manifolds/stable/manifolds/group/#Manifolds.GroupManifold","content":" Manifolds.GroupManifold  ‚Äî  Type GroupManifold{ùîΩ,M<:AbstractManifold{ùîΩ},O<:AbstractGroupOperation} <: AbstractDecoratorManifold{ùîΩ} Concrete decorator for a smooth manifold that equips the manifold with a group operation, thus making it a Lie group. See  IsGroupManifold  for more details. Group manifolds by default forward metric-related operations to the wrapped manifold. Constructor GroupManifold(\n    manifold::AbstractManifold,\n    op::AbstractGroupOperation,\n    vectors::AbstractGroupVectorRepresentation=LeftInvariantRepresentation(),\n) Define the group operation  op  acting on the manifold  manifold , hence if  op  acts smoothly, this forms a Lie group. source"},{"id":1023,"pagetitle":"Group manifold","title":"Base.rand","ref":"/manifolds/stable/manifolds/group/#Base.rand-Tuple{GroupManifold}","content":" Base.rand  ‚Äî  Method rand(::GroupManifold; vector_at=nothing, œÉ=1.0)\nrand!(::GroupManifold, pX; vector_at=nothing, kwargs...)\nrand(::TraitList{<:IsGroupManifold}, M; vector_at=nothing, œÉ=1.0)\nrand!(TraitList{<:IsGroupManifold}, M, pX; vector_at=nothing, kwargs...) Compute a random point or tangent vector on a Lie group. For points this just means to generate a random point on the underlying manifold itself. For tangent vectors, an element in the Lie Algebra is generated. source"},{"id":1024,"pagetitle":"Group manifold","title":"Manifolds.vector_representation","ref":"/manifolds/stable/manifolds/group/#Manifolds.vector_representation-Tuple{GroupManifold}","content":" Manifolds.vector_representation  ‚Äî  Method vector_representation(M::GroupManifold) Get the  AbstractGroupVectorRepresentation  of  GroupManifold M . source"},{"id":1025,"pagetitle":"Group manifold","title":"Default Representation of Tangent Vectors","ref":"/manifolds/stable/manifolds/group/#Default-Representation-of-Tangent-Vectors","content":" Default Representation of Tangent Vectors In most groups, the representation of a tangent vector   $X$  at the point  $p ‚àà \\mathcal{G}$  is stored  as a vector  $Y ‚àà \\mathfrak{g}$ . This helps to compute the derivatives of the composition and inverse. To explain this, let us assume that  the group consists of matrices  (this is always possible). The representation of a tangent vector   $X$  at the point  $p ‚àà \\mathcal{G}$  is stored  as the vector  $Y ‚àà \\mathfrak{g}$  given by \\[X = pY\\]"},{"id":1026,"pagetitle":"Group manifold","title":"Derivative of the Group Composition on the Left","ref":"/manifolds/stable/manifolds/group/#Derivative-of-the-Group-Composition-on-the-Left","content":" Derivative of the Group Composition on the Left The derivative of the composition  $pq$  with respect to  $p$  in the direction  $X$ , tangent at  $p$  is given by \\[Xq = pYq = pq(q^{-1}Yq)\\] We see that with this representation convention, this derivative is just the adjoint action of  $q^{-1}$  on the vector  $Y$ ."},{"id":1027,"pagetitle":"Group manifold","title":"Derivative of the Group Composition on the Right","ref":"/manifolds/stable/manifolds/group/#Derivative-of-the-Group-Composition-on-the-Right","content":" Derivative of the Group Composition on the Right For the derivative with respect to  $q$  of the composition  $pq$  at a tangent vector  $X$  at  $q$  stored as  $Y$  with  $X = qY$ , we have similarly \\[pX = pqY\\] With the representation convention above, this derivative is just the identity."},{"id":1028,"pagetitle":"Group manifold","title":"Derivative of the Group Inverse","ref":"/manifolds/stable/manifolds/group/#Derivative-of-the-Group-Inverse","content":" Derivative of the Group Inverse Finally, we look at the derivative of the inverse  $p^{-1}$  at a point  $p$  in a tangent direction  $X$  at  $p$  with  $X = pY$ . The result is a tangent vector at  $p^{-1}$  given by \\[-p^{-1}Xp^{-1} = - Yp^{-1} = -p^{-1}(p Y p^{-1})\\] With the representation convention above, this derivative is thus  $-pYp^{-1}$ , that is, the opposite of the adjoint action of  $p$  on the vector  $Y$ ."},{"id":1029,"pagetitle":"Group manifold","title":"Implication for Creating New Groups","ref":"/manifolds/stable/manifolds/group/#Implication-for-Creating-New-Groups","content":" Implication for Creating New Groups When you create a new group, defining the adjoint action alone ( adjoint_action ) automatically defines all the relevant derivatives above."},{"id":1030,"pagetitle":"Group manifold","title":"Generic Operations","ref":"/manifolds/stable/manifolds/group/#Generic-Operations","content":" Generic Operations For groups based on an addition operation or a group operation, several default implementations are provided."},{"id":1031,"pagetitle":"Group manifold","title":"Addition Operation","ref":"/manifolds/stable/manifolds/group/#Addition-Operation","content":" Addition Operation"},{"id":1032,"pagetitle":"Group manifold","title":"Manifolds.AdditionOperation","ref":"/manifolds/stable/manifolds/group/#Manifolds.AdditionOperation","content":" Manifolds.AdditionOperation  ‚Äî  Type AdditionOperation <: AbstractGroupOperation Group operation that consists of simple addition. source"},{"id":1033,"pagetitle":"Group manifold","title":"Manifolds.adjoint_inv_diff","ref":"/manifolds/stable/manifolds/group/#Manifolds.adjoint_inv_diff-Tuple{ManifoldsBase.TraitList{<:IsGroupManifold{AdditionOperation}}, AbstractDecoratorManifold, Any, Any}","content":" Manifolds.adjoint_inv_diff  ‚Äî  Method adjoint_inv_diff(::AdditionGroupTrait, G::AbstractDecoratorManifold, p, X) Compute the value of pullback of additive matrix inversion  $p ‚Ü¶ -p$  at  $X$ , i.e.  $-X$ . source"},{"id":1034,"pagetitle":"Group manifold","title":"Manifolds.inv_diff","ref":"/manifolds/stable/manifolds/group/#Manifolds.inv_diff-Tuple{ManifoldsBase.TraitList{<:IsGroupManifold{AdditionOperation}}, AbstractDecoratorManifold, Any, Any}","content":" Manifolds.inv_diff  ‚Äî  Method inv_diff(::AdditionGroupTrait, G::AbstractDecoratorManifold, p, X) Compute the value of differential of additive matrix inversion  $p ‚Ü¶ -p$  at  $X$ , i.e.  $-X$ . source"},{"id":1035,"pagetitle":"Group manifold","title":"Multiplication Operation","ref":"/manifolds/stable/manifolds/group/#Multiplication-Operation","content":" Multiplication Operation"},{"id":1036,"pagetitle":"Group manifold","title":"Manifolds.MultiplicationOperation","ref":"/manifolds/stable/manifolds/group/#Manifolds.MultiplicationOperation","content":" Manifolds.MultiplicationOperation  ‚Äî  Type MultiplicationOperation <: AbstractGroupOperation Group operation that consists of multiplication. source"},{"id":1037,"pagetitle":"Group manifold","title":"Manifolds.adjoint_inv_diff","ref":"/manifolds/stable/manifolds/group/#Manifolds.adjoint_inv_diff-Tuple{ManifoldsBase.TraitList{<:IsGroupManifold{<:MultiplicationOperation}}, AbstractDecoratorManifold, Any, Any}","content":" Manifolds.adjoint_inv_diff  ‚Äî  Method adjoint_inv_diff(::MultiplicationGroupTrait, G::AbstractDecoratorManifold, p, X) Compute the value of differential of matrix inversion  $p ‚Ü¶ p^{-1}$  at  $X$ . When tangent vectors are represented in Lie algebra in a left-invariant way, the formula reads  $-p^\\mathrm{T}X(p^{-1})^\\mathrm{T}$ . For matrix groups with ambient space tangent vectors, the formula would read  $-(p^{-1})^\\mathrm{T}X(p^{-1})^\\mathrm{T}$ . See the section about matrix inverse in [ Gil08 ]. source"},{"id":1038,"pagetitle":"Group manifold","title":"Manifolds.inv_diff","ref":"/manifolds/stable/manifolds/group/#Manifolds.inv_diff-Tuple{ManifoldsBase.TraitList{<:IsGroupManifold{<:MultiplicationOperation}}, AbstractDecoratorManifold, Any, Any}","content":" Manifolds.inv_diff  ‚Äî  Method inv_diff(::MultiplicationGroupTrait, G::AbstractDecoratorManifold, p, X) Compute the value of differential of matrix inversion  $p ‚Ü¶ p^{-1}$  at  $X$ . When tangent vectors are represented in Lie algebra in a left-invariant way, the formula reads  $-pXp^{-1}$ . For matrix groups with ambient space tangent vectors, the formula would read  $-p^{-1}Xp^{-1}$ . See the section about matrix inverse in [ Gil08 ]. source"},{"id":1039,"pagetitle":"Group manifold","title":"Circle group","ref":"/manifolds/stable/manifolds/group/#Circle-group","content":" Circle group"},{"id":1040,"pagetitle":"Group manifold","title":"Manifolds.CircleGroup","ref":"/manifolds/stable/manifolds/group/#Manifolds.CircleGroup","content":" Manifolds.CircleGroup  ‚Äî  Type CircleGroup <: GroupManifold{Circle{‚ÑÇ},MultiplicationOperation} The circle group is the complex circle ( Circle(‚ÑÇ) ) equipped with the group operation of complex multiplication ( MultiplicationOperation ). source"},{"id":1041,"pagetitle":"Group manifold","title":"Manifolds.RealCircleGroup","ref":"/manifolds/stable/manifolds/group/#Manifolds.RealCircleGroup","content":" Manifolds.RealCircleGroup  ‚Äî  Type RealCircleGroup <: GroupManifold{Circle{‚Ñù},AdditionOperation} The real circle group is the real circle ( Circle(‚Ñù) ) equipped with the group operation of addition ( AdditionOperation ). source"},{"id":1042,"pagetitle":"Group manifold","title":"General linear group","ref":"/manifolds/stable/manifolds/group/#General-linear-group","content":" General linear group"},{"id":1043,"pagetitle":"Group manifold","title":"Manifolds.GeneralLinear","ref":"/manifolds/stable/manifolds/group/#Manifolds.GeneralLinear","content":" Manifolds.GeneralLinear  ‚Äî  Type GeneralLinear{T,ùîΩ} <: AbstractDecoratorManifold{ùîΩ} The general linear group, that is, the group of all invertible matrices in  $ùîΩ^{n√ón}$ . The default metric is the left- $\\mathrm{GL}(n)$ -right- $\\mathrm{O}(n)$ -invariant metric whose inner product is \\[‚ü®X_p,Y_p‚ü©_p = ‚ü®p^{-1}X_p,p^{-1}Y_p‚ü©_\\mathrm{F} = ‚ü®X_e, Y_e‚ü©_\\mathrm{F},\\] where  $X_p, Y_p ‚àà T_p \\mathrm{GL}(n, ùîΩ)$ ,  $X_e = p^{-1}X_p ‚àà ùî§ùî©(n) = T_e \\mathrm{GL}(n, ùîΩ) = ùîΩ^{n√ón}$  is the corresponding vector in the Lie algebra, and  $‚ü®‚ãÖ,‚ãÖ‚ü©_\\mathrm{F}$  denotes the Frobenius inner product. By default, tangent vectors  $X_p$  are represented with their corresponding Lie algebra vectors  $X_e = p^{-1}X_p$ . source"},{"id":1044,"pagetitle":"Group manifold","title":"Base.exp","ref":"/manifolds/stable/manifolds/group/#Base.exp-Tuple{GeneralLinear, Any, Any}","content":" Base.exp  ‚Äî  Method exp(G::GeneralLinear, p, X) Compute the exponential map on the  GeneralLinear  group. The exponential map is \\[\\exp_p \\colon X ‚Ü¶ p \\operatorname{Exp}(X^\\mathrm{H}) \\operatorname{Exp}(X - X^\\mathrm{H}),\\] where  $\\operatorname{Exp}(‚ãÖ)$  denotes the matrix exponential, and  $‚ãÖ^\\mathrm{H}$  is the conjugate transpose [ ALRV14 ] [ NM16 ]. source"},{"id":1045,"pagetitle":"Group manifold","title":"Base.log","ref":"/manifolds/stable/manifolds/group/#Base.log-Tuple{GeneralLinear, Any, Any}","content":" Base.log  ‚Äî  Method log(G::GeneralLinear, p, q) Compute the logarithmic map on the  GeneralLinear(n)  group. The algorithm proceeds in two stages. First, the point  $r = p^{-1} q$  is projected to the nearest element (under the Frobenius norm) of the direct product subgroup  $\\mathrm{O}(n) √ó S^+$ , whose logarithmic map is exactly computed using the matrix logarithm. This initial tangent vector is then refined using the  NLSolveInverseRetraction . For  GeneralLinear(n, ‚ÑÇ) , the logarithmic map is instead computed on the realified supergroup  GeneralLinear(2n)  and the resulting tangent vector is then complexified. Note that this implementation is experimental. source"},{"id":1046,"pagetitle":"Group manifold","title":"Base.rand","ref":"/manifolds/stable/manifolds/group/#Base.rand-Tuple{GeneralLinear}","content":" Base.rand  ‚Äî  Method Random.rand(G::GeneralLinear; vector_at=nothing, kwargs...) If  vector_at  is  nothing , return a random point on the  GeneralLinear  group  G  by using  rand  in the embedding. If  vector_at  is not  nothing , return a random tangent vector from the tangent space of the point  vector_at  on the  GeneralLinear  by using by using  rand  in the embedding. source"},{"id":1047,"pagetitle":"Group manifold","title":"ManifoldDiff.riemannian_gradient","ref":"/manifolds/stable/manifolds/group/#ManifoldDiff.riemannian_gradient-Tuple{GeneralLinear, Any, Any}","content":" ManifoldDiff.riemannian_gradient  ‚Äî  Method riemannian_gradient(G::GeneralLinear, p, X)\nriemannian_gradient!(G::GeneralLinear, Y, p, X) Let  $f: ùîΩ^{n √ó n} ‚Üí ‚Ñù$  be a function in the embedding,  $p ‚àà \\mathrm{GL}(n, ùîΩ)$  and denote by  $X = \\operatorname{grad} f(p)$  its Euclidean gradient. Then, any  $Z ‚àà T_p \\mathrm{GL}(n, ùîΩ)$  has two representations, namely as  $X$  in the Lie algebra as a tangent vector for the Lie group and as  $pZ$  in the embedding. When we now look for the Riemannian gradient  $Y$  if  $f$  at  $p$  we need that for any  $Z ‚àà T_p \\mathrm{GL}(n, ùîΩ)$  it holds \\[‚ü®X, pZ‚ü© = Df(p)[pZ] = g_p(Y, Z),\\] where we have to use  $pX$  whenever we are in the embedding and where  $g_p$  denotes the left-invariant metric on General linear interpreted on the Lie algebra. Both metrics have the formula of the Frobenius inner product for matrices, so we obtain \\[‚ü®X, pZ‚ü© = \\operatorname{tr}(X^\\mathrm{H} pZ)\n  = \\operatorname{tr}\\bigl( (p^{\\mathrm{H}}X)^\\mathrm{H} Z)\n  = g_p\\bigl( p^{\\mathrm{H}}X, Z \\bigr).\\] Hence the Riemannian gradient is given by  $Y = p^{\\mathrm{H}}X$ . This can be computed in-place of  Y . source"},{"id":1048,"pagetitle":"Group manifold","title":"ManifoldsBase.embed!","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.embed!-Tuple{GeneralLinear, Any, Any, Any}","content":" ManifoldsBase.embed!  ‚Äî  Method embed(G::GeneralLinear, p, X)\nembed!(G::GeneralLinear, Y, p, X) Embedding a tangent vector  X  at  p  would usually be the identity, but on  GeneralLinear  the tangent vectors are represented in the Lie algebra, hence embedding this tangent vector means we have to transport it back to the right tangent space which is done by  $Y = pX$ . This can be done in-place of  Y . source"},{"id":1049,"pagetitle":"Group manifold","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.embed-Tuple{GeneralLinear, Any, Any}","content":" ManifoldsBase.embed  ‚Äî  Method embed(G::GeneralLinear, p, X)\nembed!(G::GeneralLinear, Y, p, X) Embedding a tangent vector  X  at  p  would usually be the identity, but on  GeneralLinear  the tangent vectors are represented in the Lie algebra, hence embedding this tangent vector means we have to transport it back to the right tangent space which is done by  $Y = pX$ . This can be done in-place of  Y . source"},{"id":1050,"pagetitle":"Group manifold","title":"ManifoldsBase.project!","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.project!-Tuple{GeneralLinear, Any, Any, Any}","content":" ManifoldsBase.project!  ‚Äî  Method project(G::GeneralLinear, p, X)\nproject!(G::GeneralLinear, Y, p, X) Project a tangent vector  X  from the embedding, that is the space of  $n√ón$  matrices. While the tangent space at every point of the  GeneralLinear  would yield the identity operation here, tangent vectors on  GeneralLinear  are represented in the Lie Algebra, such that this projection has to solve  $pY = X$ . source"},{"id":1051,"pagetitle":"Group manifold","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.project-Tuple{GeneralLinear, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(G::GeneralLinear, p, X)\nproject!(G::GeneralLinear, Y, p, X) Project a tangent vector  X  from the embedding, that is the space of  $n√ón$  matrices. While the tangent space at every point of the  GeneralLinear  would yield the identity operation here, tangent vectors on  GeneralLinear  are represented in the Lie Algebra, such that this projection has to solve  $pY = X$ . source"},{"id":1052,"pagetitle":"Group manifold","title":"Heisenberg group","ref":"/manifolds/stable/manifolds/group/#Heisenberg-group","content":" Heisenberg group"},{"id":1053,"pagetitle":"Group manifold","title":"Manifolds.HeisenbergGroup","ref":"/manifolds/stable/manifolds/group/#Manifolds.HeisenbergGroup","content":" Manifolds.HeisenbergGroup  ‚Äî  Type HeisenbergGroup{T} <: AbstractDecoratorManifold{‚Ñù} Heisenberg group  HeisenbergGroup(n)  is the group of  $(n+2)√ó(n+2)$  matrices [ BP08 ] \\[\\begin{bmatrix} 1 & \\mathbf{a} & c \\\\\n\\mathbf{0} & I_n & \\mathbf{b} \\\\\n0 & \\mathbf{0} & 1 \\end{bmatrix}\\] where  $I_n$  is the  $n√ón$  unit matrix,  $\\mathbf{a}$  is a row vector of length  $n$ ,  $\\mathbf{b}$  is a column vector of length  $n$  and  $c$  is a real number. The group operation is matrix multiplication. The left-invariant metric on the manifold is used. source"},{"id":1054,"pagetitle":"Group manifold","title":"Base.exp","ref":"/manifolds/stable/manifolds/group/#Base.exp-Tuple{HeisenbergGroup, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::HeisenbergGroup, p, X) Exponential map on the  HeisenbergGroup M  with the left-invariant metric. The expression reads \\[\\exp_{\\begin{bmatrix} 1 & \\mathbf{a}_p & c_p \\\\\n\\mathbf{0} & I_n & \\mathbf{b}_p \\\\\n0 & \\mathbf{0} & 1 \\end{bmatrix}}\\left(\\begin{bmatrix} 0 & \\mathbf{a}_X & c_X \\\\\n\\mathbf{0} & 0_n & \\mathbf{b}_X \\\\\n0 & \\mathbf{0} & 0 \\end{bmatrix}\\right) =\n\\begin{bmatrix} 1 & \\mathbf{a}_p + \\mathbf{a}_X & c_p + c_X + \\mathbf{a}_X‚ãÖ\\mathbf{b}_X/2 + \\mathbf{a}_p‚ãÖ\\mathbf{b}_X \\\\\n\\mathbf{0} & I_n & \\mathbf{b}_p + \\mathbf{b}_X \\\\\n0 & \\mathbf{0} & 1 \\end{bmatrix}\\] where  $I_n$  is the  $n√ón$  identity matrix,  $0_n$  is the  $n√ón$  zero matrix and  $\\mathbf{a}‚ãÖ\\mathbf{b}$  is dot product of vectors. source"},{"id":1055,"pagetitle":"Group manifold","title":"Base.log","ref":"/manifolds/stable/manifolds/group/#Base.log-Tuple{HeisenbergGroup, Any, Any}","content":" Base.log  ‚Äî  Method log(G::HeisenbergGroup, p, q) Compute the logarithmic map on the  HeisenbergGroup  group. The formula reads \\[\\log_{\\begin{bmatrix} 1 & \\mathbf{a}_p & c_p \\\\\n\\mathbf{0} & I_n & \\mathbf{b}_p \\\\\n0 & \\mathbf{0} & 1 \\end{bmatrix}}\\left(\\begin{bmatrix} 1 & \\mathbf{a}_q & c_q \\\\\n\\mathbf{0} & I_n & \\mathbf{b}_q \\\\\n0 & \\mathbf{0} & 1 \\end{bmatrix}\\right) =\n\\begin{bmatrix} 0 & \\mathbf{a}_q - \\mathbf{a}_p & c_q - c_p + \\mathbf{a}_p‚ãÖ\\mathbf{b}_p - \\mathbf{a}_q‚ãÖ\\mathbf{b}_q - (\\mathbf{a}_q - \\mathbf{a}_p)‚ãÖ(\\mathbf{b}_q - \\mathbf{b}_p) / 2 \\\\\n\\mathbf{0} & 0_n & \\mathbf{b}_q - \\mathbf{b}_p \\\\\n0 & \\mathbf{0} & 0 \\end{bmatrix}\\] where  $I_n$  is the  $n√ón$  identity matrix,  $0_n$  is the  $n√ón$  zero matrix and  $\\mathbf{a}‚ãÖ\\mathbf{b}$  is dot product of vectors. source"},{"id":1056,"pagetitle":"Group manifold","title":"Base.rand","ref":"/manifolds/stable/manifolds/group/#Base.rand-Tuple{HeisenbergGroup}","content":" Base.rand  ‚Äî  Method Random.rand(M::HeisenbergGroup; vector_at = nothing, œÉ::Real=1.0) If  vector_at  is  nothing , return a random point on the  HeisenbergGroup M  by sampling elements of the first row and the last column from the normal distribution with mean 0 and standard deviation  œÉ . If  vector_at  is not  nothing , return a random tangent vector from the tangent space of the point  vector_at  on the  HeisenbergGroup  by using a normal distribution with mean 0 and standard deviation  œÉ . source"},{"id":1057,"pagetitle":"Group manifold","title":"Manifolds.exp_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.exp_lie-Tuple{HeisenbergGroup, Any}","content":" Manifolds.exp_lie  ‚Äî  Method exp_lie(M::HeisenbergGroup, X) Lie group exponential for the  HeisenbergGroup M  of the vector  X . The formula reads \\[\\exp\\left(\\begin{bmatrix} 0 & \\mathbf{a} & c \\\\\n\\mathbf{0} & 0_n & \\mathbf{b} \\\\\n0 & \\mathbf{0} & 0 \\end{bmatrix}\\right) = \\begin{bmatrix} 1 & \\mathbf{a} & c + \\mathbf{a}‚ãÖ\\mathbf{b}/2 \\\\\n\\mathbf{0} & I_n & \\mathbf{b} \\\\\n0 & \\mathbf{0} & 1 \\end{bmatrix}\\] where  $I_n$  is the  $n√ón$  identity matrix,  $0_n$  is the  $n√ón$  zero matrix and  $\\mathbf{a}‚ãÖ\\mathbf{b}$  is dot product of vectors. source"},{"id":1058,"pagetitle":"Group manifold","title":"Manifolds.log_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.log_lie-Tuple{HeisenbergGroup, Any}","content":" Manifolds.log_lie  ‚Äî  Method log_lie(M::HeisenbergGroup, p) Lie group logarithm for the  HeisenbergGroup M  of the point  p . The formula reads \\[\\log\\left(\\begin{bmatrix} 1 & \\mathbf{a} & c \\\\\n\\mathbf{0} & I_n & \\mathbf{b} \\\\\n0 & \\mathbf{0} & 1 \\end{bmatrix}\\right) =\n\\begin{bmatrix} 0 & \\mathbf{a} & c - \\mathbf{a}‚ãÖ\\mathbf{b}/2 \\\\\n\\mathbf{0} & 0_n & \\mathbf{b} \\\\\n0 & \\mathbf{0} & 0 \\end{bmatrix}\\] where  $I_n$  is the  $n√ón$  identity matrix,  $0_n$  is the  $n√ón$  zero matrix and  $\\mathbf{a}‚ãÖ\\mathbf{b}$  is dot product of vectors. source"},{"id":1059,"pagetitle":"Group manifold","title":"ManifoldsBase.get_coordinates","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.get_coordinates-Tuple{HeisenbergGroup, Any, Any, DefaultOrthonormalBasis{‚Ñù, TangentSpaceType}}","content":" ManifoldsBase.get_coordinates  ‚Äî  Method get_coordinates(M::HeisenbergGroup, p, X, ::DefaultOrthonormalBasis{‚Ñù,TangentSpaceType}) Get coordinates of tangent vector  X  at point  p  from the  HeisenbergGroup M . Given a matrix \\[\\begin{bmatrix} 1 & \\mathbf{a} & c \\\\\n\\mathbf{0} & I_n & \\mathbf{b} \\\\\n0 & \\mathbf{0} & 1 \\end{bmatrix}\\] the coordinates are concatenated vectors  $\\mathbf{a}$ ,  $\\mathbf{b}$ , and number  $c$ . source"},{"id":1060,"pagetitle":"Group manifold","title":"ManifoldsBase.get_vector","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.get_vector-Tuple{HeisenbergGroup, Any, Any, DefaultOrthonormalBasis{‚Ñù, TangentSpaceType}}","content":" ManifoldsBase.get_vector  ‚Äî  Method get_vector(M::HeisenbergGroup, p, X‚Å±, ::DefaultOrthonormalBasis{‚Ñù,TangentSpaceType}) Get tangent vector with coordinates  X‚Å±  at point  p  from the  HeisenbergGroup M . Given a vector of coordinates  $\\begin{bmatrix}\\mathbb{a} & \\mathbb{b} & c\\end{bmatrix}$  the tangent vector is equal to \\[\\begin{bmatrix} 1 & \\mathbf{a} & c \\\\\n\\mathbf{0} & I_n & \\mathbf{b} \\\\\n0 & \\mathbf{0} & 1 \\end{bmatrix}\\] source"},{"id":1061,"pagetitle":"Group manifold","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.injectivity_radius-Tuple{HeisenbergGroup}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::HeisenbergGroup) Return the injectivity radius on the  HeisenbergGroup M , which is  $‚àû$ . source"},{"id":1062,"pagetitle":"Group manifold","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.project-Tuple{HeisenbergGroup, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::HeisenbergGroup, p, X) Project a matrix  X  in the Euclidean embedding onto the Lie algebra of  HeisenbergGroup M . Sets the diagonal elements to 0 and all non-diagonal elements except the first row and the last column to 0. source"},{"id":1063,"pagetitle":"Group manifold","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.project-Tuple{HeisenbergGroup, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::HeisenbergGroup, p) Project a matrix  p  in the Euclidean embedding onto the  HeisenbergGroup M . Sets the diagonal elements to 1 and all non-diagonal elements except the first row and the last column to 0. source"},{"id":1064,"pagetitle":"Group manifold","title":"(Special) Orthogonal and (Special) Unitary group","ref":"/manifolds/stable/manifolds/group/#(Special)-Orthogonal-and-(Special)-Unitary-group","content":" (Special) Orthogonal and (Special) Unitary group Since the orthogonal, unitary and special orthogonal and special unitary groups share many common functions, these are also implemented on a common level."},{"id":1065,"pagetitle":"Group manifold","title":"Common functions","ref":"/manifolds/stable/manifolds/group/#Common-functions","content":" Common functions"},{"id":1066,"pagetitle":"Group manifold","title":"Manifolds.GeneralUnitaryMultiplicationGroup","ref":"/manifolds/stable/manifolds/group/#Manifolds.GeneralUnitaryMultiplicationGroup","content":" Manifolds.GeneralUnitaryMultiplicationGroup  ‚Äî  Type GeneralUnitaryMultiplicationGroup{T,ùîΩ,S} <: AbstractDecoratorManifold{ùîΩ} A generic type for Lie groups based on a unitary property and matrix multiplication, see e.g.  Orthogonal ,  SpecialOrthogonal ,  Unitary , and  SpecialUnitary source"},{"id":1067,"pagetitle":"Group manifold","title":"Manifolds.exp_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.exp_lie-Tuple{Manifolds.GeneralUnitaryMultiplicationGroup{ManifoldsBase.TypeParameter{Tuple{2}}, ‚Ñù}, Any}","content":" Manifolds.exp_lie  ‚Äî  Method  exp_lie(G::Orthogonal{TypeParameter{Tuple{2}}}, X)\n exp_lie(G::SpecialOrthogonal{TypeParameter{Tuple{2}}}, X) Compute the Lie group exponential map on the  Orthogonal (2)  or  SpecialOrthogonal (2)  group. Given  $X = \\begin{pmatrix} 0 & -Œ∏ \\\\ Œ∏ & 0 \\end{pmatrix}$ , the group exponential is \\[\\exp_e \\colon X ‚Ü¶ \\begin{pmatrix} \\cos Œ∏ & -\\sin Œ∏ \\\\ \\sin Œ∏ & \\cos Œ∏ \\end{pmatrix}.\\] source"},{"id":1068,"pagetitle":"Group manifold","title":"Manifolds.exp_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.exp_lie-Tuple{Manifolds.GeneralUnitaryMultiplicationGroup{ManifoldsBase.TypeParameter{Tuple{4}}, ‚Ñù}, Any}","content":" Manifolds.exp_lie  ‚Äî  Method  exp_lie(G::Orthogonal{TypeParameter{Tuple{4}}}, X)\n exp_lie(G::SpecialOrthogonal{TypeParameter{Tuple{4}}}, X) Compute the group exponential map on the  Orthogonal (4)  or the  SpecialOrthogonal  group. The algorithm used is a more numerically stable form of those proposed in [ GX02 ], [ AR13 ]. source"},{"id":1069,"pagetitle":"Group manifold","title":"Orthogonal group","ref":"/manifolds/stable/manifolds/group/#Orthogonal-group","content":" Orthogonal group"},{"id":1070,"pagetitle":"Group manifold","title":"Manifolds.Orthogonal","ref":"/manifolds/stable/manifolds/group/#Manifolds.Orthogonal","content":" Manifolds.Orthogonal  ‚Äî  Type Orthogonal{T} = GeneralUnitaryMultiplicationGroup{T,‚Ñù,AbsoluteDeterminantOneMatrixType} Orthogonal group  $\\mathrm{O}(n)$  represented by  OrthogonalMatrices . Constructor Orthogonal(n::Int; parameter::Symbol=:type) source"},{"id":1071,"pagetitle":"Group manifold","title":"Special orthogonal group","ref":"/manifolds/stable/manifolds/group/#Special-orthogonal-group","content":" Special orthogonal group"},{"id":1072,"pagetitle":"Group manifold","title":"Manifolds.SpecialOrthogonal","ref":"/manifolds/stable/manifolds/group/#Manifolds.SpecialOrthogonal","content":" Manifolds.SpecialOrthogonal  ‚Äî  Type SpecialOrthogonal{T} = GeneralUnitaryMultiplicationGroup{T,‚Ñù,DeterminantOneMatrixType} Special orthogonal group  $\\mathrm{SO}(n)$  represented by rotation matrices, see  Rotations . Constructor SpecialOrthogonal(n) source"},{"id":1073,"pagetitle":"Group manifold","title":"Manifolds.adjoint_matrix","ref":"/manifolds/stable/manifolds/group/#Manifolds.adjoint_matrix-Tuple{SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{2}}}, Any}","content":" Manifolds.adjoint_matrix  ‚Äî  Method adjoint_matrix(::SpecialOrthogonal{TypeParameter{Tuple{2}}}, p) Compte the adjoint matrix for  SpecialOrthogonal (2)  at point  p , which is equal to  1 . See [ SDA21 ], Appendix A. source"},{"id":1074,"pagetitle":"Group manifold","title":"Manifolds.adjoint_matrix","ref":"/manifolds/stable/manifolds/group/#Manifolds.adjoint_matrix-Tuple{SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{3}}}, Any}","content":" Manifolds.adjoint_matrix  ‚Äî  Method adjoint_matrix(::SpecialOrthogonal{TypeParameter{Tuple{3}}}, p) Compte the adjoint matrix for  SpecialOrthogonal (3)  at point  p , which is equal to  p . See [ Chi12 ], Section 10.6.6. source"},{"id":1075,"pagetitle":"Group manifold","title":"Special unitary group","ref":"/manifolds/stable/manifolds/group/#Special-unitary-group","content":" Special unitary group"},{"id":1076,"pagetitle":"Group manifold","title":"Manifolds.SpecialUnitary","ref":"/manifolds/stable/manifolds/group/#Manifolds.SpecialUnitary","content":" Manifolds.SpecialUnitary  ‚Äî  Type SpecialUnitary{n} = GeneralUnitaryMultiplicationGroup{n,‚Ñù,GeneralUnitaryMatrices{n,‚ÑÇ,DeterminantOneMatrixType}} The special unitary group  $\\mathrm{SU}(n)$  represented by unitary matrices of determinant +1. The tangent spaces are of the form \\[T_p\\mathrm{SU}(x) = \\bigl\\{ X \\in \\mathbb C^{n√ón} \\big| X = pY \\text{ where } Y = -Y^{\\mathrm{H}} \\bigr\\}\\] and we represent tangent vectors by just storing the  SkewHermitianMatrices $Y$ , or in other words we represent the tangent spaces employing the Lie algebra  $\\mathfrak{su}(n)$ . Constructor SpecialUnitary(n) Generate the Lie group of  $n√ón$  unitary matrices with determinant +1. source"},{"id":1077,"pagetitle":"Group manifold","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.project-Tuple{SpecialUnitary, Vararg{Any}}","content":" ManifoldsBase.project  ‚Äî  Method project(G::SpecialUnitary, p) Project  p  to the nearest point on the  SpecialUnitary  group  G . Given the singular value decomposition  $p = U S V^\\mathrm{H}$ , with the singular values sorted in descending order, the projection is \\[\\operatorname{proj}_{\\mathrm{SU}(n)}(p) =\nU\\operatorname{diag}\\left[1,1,‚Ä¶,\\det(U V^\\mathrm{H})\\right] V^\\mathrm{H}.\\] The diagonal matrix ensures that the determinant of the result is  $+1$ . source"},{"id":1078,"pagetitle":"Group manifold","title":"Unitary group","ref":"/manifolds/stable/manifolds/group/#Unitary-group","content":" Unitary group"},{"id":1079,"pagetitle":"Group manifold","title":"Manifolds.Unitary","ref":"/manifolds/stable/manifolds/group/#Manifolds.Unitary","content":" Manifolds.Unitary  ‚Äî  Type  Unitary{n,ùîΩ} = GeneralUnitaryMultiplicationGroup{n,ùîΩ,AbsoluteDeterminantOneMatrixType} The group of unitary matrices  $\\mathrm{U}(n, ùîΩ)$ , either complex (when ùîΩ=‚ÑÇ) or quaternionic (when ùîΩ=‚Ñç) The group consists of all points  $p ‚àà ùîΩ^{n√ón}$  where  $p^{\\mathrm{H}}p = pp^{\\mathrm{H}} = I$ . The tangent spaces are if the form \\[T_p\\mathrm{U}(n) = \\bigl\\{ X \\in ùîΩ^{n√ón} \\big| X = pY \\text{ where } Y = -Y^{\\mathrm{H}} \\bigr\\}\\] and we represent tangent vectors by just storing the  SkewHermitianMatrices $Y$ , or in other words we represent the tangent spaces employing the Lie algebra  $\\mathfrak{u}(n, ùîΩ)$ . Quaternionic unitary group is isomorphic to the compact symplectic group of the same dimension. Constructor Unitary(n, ùîΩ::AbstractNumbers=‚ÑÇ) Construct  $\\mathrm{U}(n, ùîΩ)$ . See also  Orthogonal(n)  for the real-valued case. source"},{"id":1080,"pagetitle":"Group manifold","title":"Manifolds.exp_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.exp_lie-Tuple{Unitary{ManifoldsBase.TypeParameter{Tuple{2}}, ‚ÑÇ}, Any}","content":" Manifolds.exp_lie  ‚Äî  Method exp_lie(G::Unitary{TypeParameter{Tuple{2}},‚ÑÇ}, X) Compute the group exponential map on the  Unitary(2)  group, which is \\[\\exp_e \\colon X ‚Ü¶ e^{\\operatorname{tr}(X) / 2} \\left(\\cos Œ∏ I + \\frac{\\sin Œ∏}{Œ∏} \\left(X - \\frac{\\operatorname{tr}(X)}{2} I\\right)\\right),\\] where  $Œ∏ = \\frac{1}{2} \\sqrt{4\\det(X) - \\operatorname{tr}(X)^2}$ . source"},{"id":1081,"pagetitle":"Group manifold","title":"Power group","ref":"/manifolds/stable/manifolds/group/#Power-group","content":" Power group"},{"id":1082,"pagetitle":"Group manifold","title":"Manifolds.PowerGroup","ref":"/manifolds/stable/manifolds/group/#Manifolds.PowerGroup-Tuple{AbstractPowerManifold}","content":" Manifolds.PowerGroup  ‚Äî  Method PowerGroup{ùîΩ,T} <: GroupManifold{ùîΩ,<:AbstractPowerManifold{ùîΩ,M,RPT},ProductOperation} Decorate a power manifold with a  ProductOperation . Constituent manifold of the power manifold must also have a  IsGroupManifold  or a decorated instance of one. This type is mostly useful for equipping the direct product of group manifolds with an  Identity  element. Constructor PowerGroup(manifold::AbstractPowerManifold) source"},{"id":1083,"pagetitle":"Group manifold","title":"Manifolds.PowerGroupNested","ref":"/manifolds/stable/manifolds/group/#Manifolds.PowerGroupNested","content":" Manifolds.PowerGroupNested  ‚Äî  Type PowerGroupNested Alias to  PowerGroup  with  NestedPowerRepresentation  representation. source"},{"id":1084,"pagetitle":"Group manifold","title":"Manifolds.PowerGroupNestedReplacing","ref":"/manifolds/stable/manifolds/group/#Manifolds.PowerGroupNestedReplacing","content":" Manifolds.PowerGroupNestedReplacing  ‚Äî  Type PowerGroupNestedReplacing Alias to  PowerGroup  with  NestedReplacingPowerRepresentation  representation. source"},{"id":1085,"pagetitle":"Group manifold","title":"Product group","ref":"/manifolds/stable/manifolds/group/#Product-group","content":" Product group"},{"id":1086,"pagetitle":"Group manifold","title":"Manifolds.ProductGroup","ref":"/manifolds/stable/manifolds/group/#Manifolds.ProductGroup-Tuple{ProductManifold, AbstractGroupVectorRepresentation}","content":" Manifolds.ProductGroup  ‚Äî  Method ProductGroup{ùîΩ,T} <: GroupManifold{ùîΩ,ProductManifold{T},ProductOperation} Decorate a product manifold with a  ProductOperation . Each submanifold must also have a  IsGroupManifold  or a decorated instance of one. This type is mostly useful for equipping the direct product of group manifolds with an  Identity  element. Constructor ProductGroup(manifold::ProductManifold) source"},{"id":1087,"pagetitle":"Group manifold","title":"Manifolds.ProductOperation","ref":"/manifolds/stable/manifolds/group/#Manifolds.ProductOperation","content":" Manifolds.ProductOperation  ‚Äî  Type ProductOperation <: AbstractGroupOperation Direct product group operation. source"},{"id":1088,"pagetitle":"Group manifold","title":"Semidirect product group","ref":"/manifolds/stable/manifolds/group/#Semidirect-product-group","content":" Semidirect product group"},{"id":1089,"pagetitle":"Group manifold","title":"Manifolds.HybridTangentRepresentation","ref":"/manifolds/stable/manifolds/group/#Manifolds.HybridTangentRepresentation","content":" Manifolds.HybridTangentRepresentation  ‚Äî  Type struct HybridTangentRepresentation <: AbstractGroupVectorRepresentation end Tangent vector representation on  SemidirectProductGroup  such as  SpecialEuclidean  that corresponds to simple product structure of underlying groups. source"},{"id":1090,"pagetitle":"Group manifold","title":"Manifolds.SemidirectProductGroup","ref":"/manifolds/stable/manifolds/group/#Manifolds.SemidirectProductGroup-Union{Tuple{ùîΩ}, Tuple{AbstractDecoratorManifold{ùîΩ}, AbstractDecoratorManifold{ùîΩ}, AbstractGroupAction, AbstractGroupVectorRepresentation}} where ùîΩ","content":" Manifolds.SemidirectProductGroup  ‚Äî  Method SemidirectProductGroup(N::GroupManifold, H::GroupManifold, A::AbstractGroupAction) A group that is the semidirect product of a normal group  $\\mathcal{N}$  and a subgroup  $\\mathcal{H}$ , written  $\\mathcal{G} = \\mathcal{N} ‚ãä_Œ∏ \\mathcal{H}$ , where  $Œ∏: \\mathcal{H} √ó \\mathcal{N} ‚Üí \\mathcal{N}$  is an automorphism action of  $\\mathcal{H}$  on  $\\mathcal{N}$ . The group  $\\mathcal{G}$  has the composition rule \\[g \\circ g' = (n, h) \\circ (n', h') = (n \\circ Œ∏_h(n'), h \\circ h')\\] and the inverse \\[g^{-1} = (n, h)^{-1} = (Œ∏_{h^{-1}}(n^{-1}), h^{-1}).\\] source"},{"id":1091,"pagetitle":"Group manifold","title":"Manifolds.SemidirectProductOperation","ref":"/manifolds/stable/manifolds/group/#Manifolds.SemidirectProductOperation","content":" Manifolds.SemidirectProductOperation  ‚Äî  Type SemidirectProductOperation(action::AbstractGroupAction) Group operation of a semidirect product group. The operation consists of the operation  opN  on a normal subgroup  N , the operation  opH  on a subgroup  H , and an automorphism  action  of elements of  H  on  N . Only the action is stored. source"},{"id":1092,"pagetitle":"Group manifold","title":"Manifolds.identity_element","ref":"/manifolds/stable/manifolds/group/#Manifolds.identity_element-Tuple{SemidirectProductGroup}","content":" Manifolds.identity_element  ‚Äî  Method identity_element(G::SemidirectProductGroup) Get the identity element of  SemidirectProductGroup G . Uses  ArrayPartition  from  RecursiveArrayTools.jl  to represent the point. source"},{"id":1093,"pagetitle":"Group manifold","title":"Manifolds.translate_diff","ref":"/manifolds/stable/manifolds/group/#Manifolds.translate_diff-Tuple{SemidirectProductGroup{ùîΩ, N, H, A, HybridTangentRepresentation} where {ùîΩ, N, H, A<:AbstractGroupAction}, Any, Any, Any, Tuple{LeftAction, LeftSide}}","content":" Manifolds.translate_diff  ‚Äî  Method translate_diff(G::SemidirectProductGroupHVR, p, q, X, conX::LeftForwardAction) Perform differential of the left translation on the semidirect product group  G  with  HybridTangentRepresentation . Since the left translation is defined as (cf.  SemidirectProductGroup ): \\[L_{(n', h')} (n, h) = ( L_{n'} Œ∏_{h'}(n), L_{h'} h)\\] then its differential can be computed as \\[\\mathrm{d}L_{(n', h')}(X_n, X_h) = ( \\mathrm{d}L_{n'} (\\mathrm{d}Œ∏_{h'}(X_n)), \\mathrm{d}L_{h'} X_h).\\] source"},{"id":1094,"pagetitle":"Group manifold","title":"Special Euclidean group","ref":"/manifolds/stable/manifolds/group/#Special-Euclidean-group","content":" Special Euclidean group"},{"id":1095,"pagetitle":"Group manifold","title":"Manifolds.SpecialEuclidean","ref":"/manifolds/stable/manifolds/group/#Manifolds.SpecialEuclidean","content":" Manifolds.SpecialEuclidean  ‚Äî  Type SpecialEuclidean(\n    n::Int;\n    vectors::AbstractGroupVectorRepresentation=LeftInvariantRepresentation()\n) Special Euclidean group  $\\mathrm{SE}(n)$ , the group of rigid motions. $\\mathrm{SE}(n)$  is the semidirect product of the  TranslationGroup  on  $‚Ñù^n$  and  SpecialOrthogonal (n) \\[\\mathrm{SE}(n) ‚âê \\mathrm{T}(n) ‚ãä_Œ∏ \\mathrm{SO}(n),\\] where  $Œ∏$  is the canonical action of  $\\mathrm{SO}(n)$  on  $\\mathrm{T}(n)$  by vector rotation. This constructor is equivalent to calling Tn = TranslationGroup(n)\nSOn = SpecialOrthogonal(n)\nSemidirectProductGroup(Tn, SOn, RotationAction(Tn, SOn), vectors) Points on  $\\mathrm{SE}(n)$  may be represented as points on the underlying product manifold  $\\mathrm{T}(n) √ó \\mathrm{SO}(n)$ . For group-specific functions, they may also be represented as affine matrices with size  (n + 1, n + 1)  (see  affine_matrix ), for which the group operation is  MultiplicationOperation . There are two supported conventions for tangent vector storage, which can be selected using the  vectors  keyword argument: LeftInvariantRepresentation  (default one), which corresponds to left-invariant storage commonly used in other Lie groups. HybridTangentRepresentation  which corresponds to the representation implied by product manifold structure of underlying groups. source"},{"id":1096,"pagetitle":"Group manifold","title":"Manifolds.SpecialEuclideanInGeneralLinear","ref":"/manifolds/stable/manifolds/group/#Manifolds.SpecialEuclideanInGeneralLinear","content":" Manifolds.SpecialEuclideanInGeneralLinear  ‚Äî  Type SpecialEuclideanInGeneralLinear An explicit isometric and homomorphic embedding of  $\\mathrm{SE}(n)$  in  $\\mathrm{GL}(n+1)$  and  $ùî∞ùî¢(n)$  in  $ùî§ùî©(n+1)$ . Note that this is  not  a transparently isometric embedding. Constructor SpecialEuclideanInGeneralLinear(\n    n::Int;\n    se_vectors::AbstractGroupVectorRepresentation=LeftInvariantVectorRepresentation(),\n) Where  se_vectors  is the tangent vector representation of the  SpecialEuclidean  group to be used. source"},{"id":1097,"pagetitle":"Group manifold","title":"Manifolds._get_parameter","ref":"/manifolds/stable/manifolds/group/#Manifolds._get_parameter-Tuple{AbstractManifold}","content":" Manifolds._get_parameter  ‚Äî  Method _get_parameter(M::AbstractManifold) Similar to  get_parameter  but it can be specialized for manifolds without breaking manifolds being parametrized by other manifolds. source"},{"id":1098,"pagetitle":"Group manifold","title":"Manifolds.adjoint_action","ref":"/manifolds/stable/manifolds/group/#Manifolds.adjoint_action-Tuple{GroupManifold{‚Ñù, ProductManifold{‚Ñù, Tuple{TranslationGroup{ManifoldsBase.TypeParameter{Tuple{3}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{3}}}}}, Manifolds.SemidirectProductOperation{RotationAction{LeftAction, TranslationGroup{ManifoldsBase.TypeParameter{Tuple{3}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{3}}}}}, <:HybridTangentRepresentation}, Any, TFVector{<:Any, VeeOrthogonalBasis{‚Ñù}}}","content":" Manifolds.adjoint_action  ‚Äî  Method adjoint_action(\n    ::SpecialEuclidean{TypeParameter{Tuple{3}},<:HybridTangentRepresentation},\n    p,\n    fX::TFVector{<:Any,VeeOrthogonalBasis{‚Ñù}},\n) Adjoint action of the  SpecialEuclidean  group on the vector with coefficients  fX  tangent at point  p . The formula for the coefficients reads  $t√ó(R‚ãÖœâ) + R‚ãÖr$  for the translation part and  $R‚ãÖœâ$  for the rotation part, where  t  is the translation part of  p ,  R  is the rotation matrix part of  p ,  r  is the translation part of  fX  and  œâ  is the rotation part of  fX ,  $√ó$  is the cross product and  $‚ãÖ$  is the matrix product. source"},{"id":1099,"pagetitle":"Group manifold","title":"Manifolds.adjoint_matrix","ref":"/manifolds/stable/manifolds/group/#Manifolds.adjoint_matrix-Tuple{GroupManifold{‚Ñù, ProductManifold{‚Ñù, Tuple{TranslationGroup{ManifoldsBase.TypeParameter{Tuple{2}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{2}}}}}, Manifolds.SemidirectProductOperation{RotationAction{LeftAction, TranslationGroup{ManifoldsBase.TypeParameter{Tuple{2}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{2}}}}}}, Any}","content":" Manifolds.adjoint_matrix  ‚Äî  Method adjoint_matrix(::SpecialEuclidean{TypeParameter{Tuple{2}}}, p) Compute the adjoint matrix for the group  SpecialEuclidean (2)  at point  p  in default coordinates. The formula follows Section 10.6.2 in [ Chi12 ] but with additional scaling by  $\\sqrt{2}$  due to a different choice of inner product. The formula reads \\[\\begin{pmatrix}\nR_{1,1} & R_{1,2} & t_2 \\\\\nR_{2,1} & R_{2,2} & -t_1 \\\\\n0 & 0 & 1\n\\end{pmatrix},\\] where  $R$  is the rotation matrix part of  p  and  $[t_1, t_2]$  is the translation part of  p . source"},{"id":1100,"pagetitle":"Group manifold","title":"Manifolds.adjoint_matrix","ref":"/manifolds/stable/manifolds/group/#Manifolds.adjoint_matrix-Tuple{GroupManifold{‚Ñù, ProductManifold{‚Ñù, Tuple{TranslationGroup{ManifoldsBase.TypeParameter{Tuple{3}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{3}}}}}, Manifolds.SemidirectProductOperation{RotationAction{LeftAction, TranslationGroup{ManifoldsBase.TypeParameter{Tuple{3}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{3}}}}}}, Any}","content":" Manifolds.adjoint_matrix  ‚Äî  Method adjoint_matrix(::SpecialEuclidean{TypeParameter{Tuple{3}}}, p) Compute the adjoint matrix for the group  SpecialEuclidean (3)  at point  p  in default coordinates. The formula follows Section 10.6.9 in [ Chi12 ] with changes due to different conventions. The formula reads \\[\\begin{pmatrix}\nR & UR/\\sqrt{2} \\\\\n0_{3√ó3} & R\n\\end{pmatrix}.\\] where  $R$  is the rotation matrix of  p  and  $U$  is the matrix \\[\\begin{pmatrix}\n0 & -t_3 & t_2 \\\\\nt_3 & 0 & -t_1 \\\\\n-t_2 & t_1 & 0\n\\end{pmatrix}\\] where  $[t_1, t_2, t_3]$  is the translation vector of  p . source"},{"id":1101,"pagetitle":"Group manifold","title":"Manifolds.affine_matrix","ref":"/manifolds/stable/manifolds/group/#Manifolds.affine_matrix-Tuple{SpecialEuclidean, Any}","content":" Manifolds.affine_matrix  ‚Äî  Method affine_matrix(G::SpecialEuclidean, p) -> AbstractMatrix Represent the point  $p ‚àà \\mathrm{SE}(n)$  as an affine matrix. For  $p = (t, R) ‚àà \\mathrm{SE}(n)$ , where  $t ‚àà \\mathrm{T}(n), R ‚àà \\mathrm{SO}(n)$ , the affine representation is the  $n + 1 √ó n + 1$  matrix \\[\\begin{pmatrix}\nR & t \\\\\n0^\\mathrm{T} & 1\n\\end{pmatrix}.\\] This function embeds  $\\mathrm{SE}(n)$  in the general linear group  $\\mathrm{GL}(n+1)$ . It is an isometric embedding and group homomorphism [ Ric88 ]. See also  screw_matrix  for matrix representations of the Lie algebra. source"},{"id":1102,"pagetitle":"Group manifold","title":"Manifolds.exp_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.exp_lie-Tuple{GroupManifold{‚Ñù, ProductManifold{‚Ñù, Tuple{TranslationGroup{ManifoldsBase.TypeParameter{Tuple{2}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{2}}}}}, Manifolds.SemidirectProductOperation{RotationAction{LeftAction, TranslationGroup{ManifoldsBase.TypeParameter{Tuple{2}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{2}}}}}}, Any}","content":" Manifolds.exp_lie  ‚Äî  Method exp_lie(G::SpecialEuclidean{TypeParameter{Tuple{2}}}, X) Compute the group exponential of  $X = (b, Œ©) ‚àà ùî∞ùî¢(2)$ , where  $b ‚àà ùî±(2)$  and  $Œ© ‚àà ùî∞ùî¨(2)$ : \\[\\exp X = (t, R) = (U(Œ∏) b, \\exp Œ©),\\] where  $t ‚àà \\mathrm{T}(2)$ ,  $R = \\exp Œ©$  is the group exponential on  $\\mathrm{SO}(2)$ , \\[U(Œ∏) = \\frac{\\sin Œ∏}{Œ∏} I_2 + \\frac{1 - \\cos Œ∏}{Œ∏^2} Œ©,\\] and  $Œ∏ = \\frac{1}{\\sqrt{2}} \\lVert Œ© \\rVert_e$  (see  norm ) is the angle of the rotation. source"},{"id":1103,"pagetitle":"Group manifold","title":"Manifolds.exp_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.exp_lie-Tuple{GroupManifold{‚Ñù, ProductManifold{‚Ñù, Tuple{TranslationGroup{ManifoldsBase.TypeParameter{Tuple{3}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{3}}}}}, Manifolds.SemidirectProductOperation{RotationAction{LeftAction, TranslationGroup{ManifoldsBase.TypeParameter{Tuple{3}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{3}}}}}}, Any}","content":" Manifolds.exp_lie  ‚Äî  Method exp_lie(G::SpecialEuclidean{TypeParameter{Tuple{3}}}, X) Compute the group exponential of  $X = (b, Œ©) ‚àà ùî∞ùî¢(3)$ , where  $b ‚àà ùî±(3)$  and  $Œ© ‚àà ùî∞ùî¨(3)$ : \\[\\exp X = (t, R) = (U(Œ∏) b, \\exp Œ©),\\] where  $t ‚àà \\mathrm{T}(3)$ ,  $R = \\exp Œ©$  is the group exponential on  $\\mathrm{SO}(3)$ , \\[U(Œ∏) = I_3 + \\frac{1 - \\cos Œ∏}{Œ∏^2} Œ© + \\frac{Œ∏ - \\sin Œ∏}{Œ∏^3} Œ©^2,\\] and  $Œ∏ = \\frac{1}{\\sqrt{2}} \\lVert Œ© \\rVert_e$  (see  norm ) is the angle of the rotation. source"},{"id":1104,"pagetitle":"Group manifold","title":"Manifolds.exp_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.exp_lie-Tuple{SpecialEuclidean, Any}","content":" Manifolds.exp_lie  ‚Äî  Method exp_lie(G::SpecialEuclidean{n}, X) Compute the group exponential of  $X = (b, Œ©) ‚àà ùî∞ùî¢(n)$ , where  $b ‚àà ùî±(n)$  and  $Œ© ‚àà ùî∞ùî¨(n)$ : \\[\\exp X = (t, R),\\] where  $t ‚àà \\mathrm{T}(n)$  and  $R = \\exp Œ©$  is the group exponential on  $\\mathrm{SO}(n)$ . In the  screw_matrix  representation, the group exponential is the matrix exponential (see  exp_lie ). source"},{"id":1105,"pagetitle":"Group manifold","title":"Manifolds.jacobian_exp_inv_argument","ref":"/manifolds/stable/manifolds/group/#Manifolds.jacobian_exp_inv_argument-Tuple{GroupManifold{‚Ñù, ProductManifold{‚Ñù, Tuple{TranslationGroup{ManifoldsBase.TypeParameter{Tuple{2}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{2}}}}}, Manifolds.SemidirectProductOperation{RotationAction{LeftAction, TranslationGroup{ManifoldsBase.TypeParameter{Tuple{2}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{2}}}}}}, Any, Any}","content":" Manifolds.jacobian_exp_inv_argument  ‚Äî  Method jacobian_exp_inv_argument(\n    M::SpecialEuclidean{TypeParameter{Tuple{2}}},\n    p,\n    X,\n) Compute Jacobian matrix of the invariant exponential map on  SpecialEuclidean (2) . The formula reads \\[\\begin{pmatrix}\n\\frac{1}{Œ∏}\\sin(Œ∏) & \\frac{1}{Œ∏} (1-\\cos(Œ∏)) & \\frac{1}{\\sqrt{2} Œ∏^2}(t_1(\\sin(Œ∏) - Œ∏) + t_2(\\cos(Œ∏) - 1)) \\\\\n\\frac{1}{Œ∏}(-1+\\cos(Œ∏)) & \\frac{1}{Œ∏}\\sin(Œ∏) & \\frac{1}{\\sqrt{2} Œ∏^2}(t_2(\\sin(Œ∏) - Œ∏) + t_1(-\\cos(Œ∏) + 1)) \\\\\n0 & 0 & 1\n\\end{pmatrix}.\\] where  $Œ∏$  is the norm of  X  and  $[t_1, t_2]$  is the translation part of  X . It is adapted from [ Chi12 ], Section 10.6.2, to  Manifolds.jl  conventions. source"},{"id":1106,"pagetitle":"Group manifold","title":"Manifolds.jacobian_exp_inv_argument","ref":"/manifolds/stable/manifolds/group/#Manifolds.jacobian_exp_inv_argument-Tuple{GroupManifold{‚Ñù, ProductManifold{‚Ñù, Tuple{TranslationGroup{ManifoldsBase.TypeParameter{Tuple{3}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{3}}}}}, Manifolds.SemidirectProductOperation{RotationAction{LeftAction, TranslationGroup{ManifoldsBase.TypeParameter{Tuple{3}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{3}}}}}}, Any, Any}","content":" Manifolds.jacobian_exp_inv_argument  ‚Äî  Method jacobian_exp_inv_argument(\n    M::SpecialEuclidean{TypeParameter{Tuple{3}}},\n    p,\n    X,\n) Compute Jacobian matrix of the invariant exponential map on  SpecialEuclidean (3) . The formula reads \\[\\begin{pmatrix}\nR & Q \\\\\n0_{3√ó3} & R\n\\end{pmatrix},\\] where  $R$  is the Jacobian of exponential map on  Rotations (3)  with respect to the argument, and  $Q$  is \\[\\begin{align*}\nQ = &\\frac{1}{2} T \\\\\n    &- \\frac{Œ∏ - \\sin(Œ∏)}{Œ∏^3} (X_r T + T X_r + X_r T X_r) \\\\\n    & + \\frac{1 - \\frac{Œ∏^2}{2} - \\cos(Œ∏)}{Œ∏^4} (X_r^2 T + T X_r^2 - 3 X_r T X_r)\\\\\n    & + \\frac{1}{2}\\left(\\frac{1 - \\frac{Œ∏^2}{2} - \\cos(Œ∏)}{Œ∏^4} - 3 \\frac{Œ∏ - \\sin(Œ∏) - \\frac{Œ∏^3}{6}}{Œ∏^5}\\right) (X_r T X_r^2 + X_r^2 T X_r)\n\\end{align*}\\] where  $X_r$  is the rotation part of  $X$  and  $T$  is \\[\\frac{1}{\\sqrt{2}}\\begin{pmatrix}\n0 & -t_3 & t_2 \\\\\nt_3 & 0 & -t_1 \\\\\n-t_2 & t_1 & 0\n\\end{pmatrix},\\] where  $[t_1, t_2, t_3]$  is the translation part of  X . It is adapted from [ BF14 ], Eq. (102), to  Manifolds.jl  conventions. source"},{"id":1107,"pagetitle":"Group manifold","title":"Manifolds.lie_bracket","ref":"/manifolds/stable/manifolds/group/#Manifolds.lie_bracket-Tuple{SpecialEuclidean, AbstractMatrix, AbstractMatrix}","content":" Manifolds.lie_bracket  ‚Äî  Method lie_bracket(G::SpecialEuclidean, X::ArrayPartition, Y::ArrayPartition)\nlie_bracket(G::SpecialEuclidean, X::AbstractMatrix, Y::AbstractMatrix) Calculate the Lie bracket between elements  X  and  Y  of the special Euclidean Lie algebra. For the matrix representation (which can be obtained using  screw_matrix ) the formula is  $[X, Y] = XY-YX$ , while in the  ArrayPartition  representation the formula reads  $[X, Y] = [(t_1, R_1), (t_2, R_2)] = (R_1 t_2 - R_2 t_1, R_1 R_2 - R_2 R_1)$ . source"},{"id":1108,"pagetitle":"Group manifold","title":"Manifolds.log_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.log_lie-Tuple{GroupManifold{‚Ñù, ProductManifold{‚Ñù, Tuple{TranslationGroup{ManifoldsBase.TypeParameter{Tuple{2}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{2}}}}}, Manifolds.SemidirectProductOperation{RotationAction{LeftAction, TranslationGroup{ManifoldsBase.TypeParameter{Tuple{2}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{2}}}}}}, Any}","content":" Manifolds.log_lie  ‚Äî  Method log_lie(G::SpecialEuclidean{TypeParameter{Tuple{2}}}, p) Compute the group logarithm of  $p = (t, R) ‚àà \\mathrm{SE}(2)$ , where  $t ‚àà \\mathrm{T}(2)$  and  $R ‚àà \\mathrm{SO}(2)$ : \\[\\log p = (b, Œ©) = (U(Œ∏)^{-1} t, \\log R),\\] where  $b ‚àà ùî±(2)$ ,  $Œ© = \\log R ‚àà ùî∞ùî¨(2)$  is the group logarithm on  $\\mathrm{SO}(2)$ , \\[U(Œ∏) = \\frac{\\sin Œ∏}{Œ∏} I_2 + \\frac{1 - \\cos Œ∏}{Œ∏^2} Œ©,\\] and  $Œ∏ = \\frac{1}{\\sqrt{2}} \\lVert Œ© \\rVert_e$  (see  norm ) is the angle of the rotation. source"},{"id":1109,"pagetitle":"Group manifold","title":"Manifolds.log_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.log_lie-Tuple{GroupManifold{‚Ñù, ProductManifold{‚Ñù, Tuple{TranslationGroup{ManifoldsBase.TypeParameter{Tuple{3}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{3}}}}}, Manifolds.SemidirectProductOperation{RotationAction{LeftAction, TranslationGroup{ManifoldsBase.TypeParameter{Tuple{3}}, ‚Ñù}, SpecialOrthogonal{ManifoldsBase.TypeParameter{Tuple{3}}}}}}, Any}","content":" Manifolds.log_lie  ‚Äî  Method log_lie(G::SpecialEuclidean{TypeParameter{Tuple{3}}}, p) Compute the group logarithm of  $p = (t, R) ‚àà \\mathrm{SE}(3)$ , where  $t ‚àà \\mathrm{T}(3)$  and  $R ‚àà \\mathrm{SO}(3)$ : \\[\\log p = (b, Œ©) = (U(Œ∏)^{-1} t, \\log R),\\] where  $b ‚àà ùî±(3)$ ,  $Œ© = \\log R ‚àà ùî∞ùî¨(3)$  is the group logarithm on  $\\mathrm{SO}(3)$ , \\[U(Œ∏) = I_3 + \\frac{1 - \\cos Œ∏}{Œ∏^2} Œ© + \\frac{Œ∏ - \\sin Œ∏}{Œ∏^3} Œ©^2,\\] and  $Œ∏ = \\frac{1}{\\sqrt{2}} \\lVert Œ© \\rVert_e$  (see  norm ) is the angle of the rotation. source"},{"id":1110,"pagetitle":"Group manifold","title":"Manifolds.log_lie","ref":"/manifolds/stable/manifolds/group/#Manifolds.log_lie-Tuple{SpecialEuclidean, Any}","content":" Manifolds.log_lie  ‚Äî  Method log_lie(G::SpecialEuclidean, p) Compute the group logarithm of  $p = (t, R) ‚àà \\mathrm{SE}(n)$ , where  $t ‚àà \\mathrm{T}(n)$  and  $R ‚àà \\mathrm{SO}(n)$ : \\[\\log p = (b, Œ©),\\] where  $b ‚àà ùî±(n)$  and  $Œ© = \\log R ‚àà ùî∞ùî¨(n)$  is the group logarithm on  $\\mathrm{SO}(n)$ . In the  affine_matrix  representation, the group logarithm is the matrix logarithm (see  log_lie ): source"},{"id":1111,"pagetitle":"Group manifold","title":"Manifolds.screw_matrix","ref":"/manifolds/stable/manifolds/group/#Manifolds.screw_matrix-Tuple{SpecialEuclidean, Any}","content":" Manifolds.screw_matrix  ‚Äî  Method screw_matrix(G::SpecialEuclidean, X) -> AbstractMatrix Represent the Lie algebra element  $X ‚àà ùî∞ùî¢(n) = T_e \\mathrm{SE}(n)$  as a screw matrix. For  $X = (b, Œ©) ‚àà ùî∞ùî¢(n)$ , where  $Œ© ‚àà ùî∞ùî¨(n) = T_e \\mathrm{SO}(n)$ , the screw representation is the  $n + 1 √ó n + 1$  matrix \\[\\begin{pmatrix}\nŒ© & b \\\\\n0^\\mathrm{T} & 0\n\\end{pmatrix}.\\] This function embeds  $ùî∞ùî¢(n)$  in the general linear Lie algebra  $ùî§ùî©(n+1)$  but it's not a homomorphic embedding (see  SpecialEuclideanInGeneralLinear  for a homomorphic one). See also  affine_matrix  for matrix representations of the Lie group. source"},{"id":1112,"pagetitle":"Group manifold","title":"Manifolds.translate_diff","ref":"/manifolds/stable/manifolds/group/#Manifolds.translate_diff-Tuple{SpecialEuclidean, Any, Any, Any, Tuple{RightAction, RightSide}}","content":" Manifolds.translate_diff  ‚Äî  Method translate_diff(G::SpecialEuclidean, p, q, X, ::RightBackwardAction) Differential of the right action of the  SpecialEuclidean  group on itself. The formula for the rotation part is the differential of the right rotation action, while the formula for the translation part reads \\[R_q‚ãÖX_R‚ãÖt_p + X_t\\] where  $R_q$  is the rotation part of  q ,  $X_R$  is the rotation part of  X ,  $t_p$  is the translation part of  p  and  $X_t$  is the translation part of  X . source"},{"id":1113,"pagetitle":"Group manifold","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.embed-Tuple{EmbeddedManifold{‚Ñù, <:SpecialEuclidean, <:GeneralLinear}, Any, Any}","content":" ManifoldsBase.embed  ‚Äî  Method embed(M::SpecialEuclideanInGeneralLinear, p, X) Embed the tangent vector  X at point p on [ SpecialEuclidean ](@ref) in the [ GeneralLinear ](@ref) group. Point p can use any representation valid for SpecialEuclidean . The embedding is similar from the one defined by [ screw_matrix`](@ref). source"},{"id":1114,"pagetitle":"Group manifold","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.embed-Tuple{EmbeddedManifold{‚Ñù, <:SpecialEuclidean, <:GeneralLinear}, Any}","content":" ManifoldsBase.embed  ‚Äî  Method embed(M::SpecialEuclideanInGeneralLinear, p) Embed the point  p  on  SpecialEuclidean  in the  GeneralLinear  group. The embedding is calculated using  affine_matrix . source"},{"id":1115,"pagetitle":"Group manifold","title":"Special linear group","ref":"/manifolds/stable/manifolds/group/#Special-linear-group","content":" Special linear group"},{"id":1116,"pagetitle":"Group manifold","title":"Manifolds.SpecialLinear","ref":"/manifolds/stable/manifolds/group/#Manifolds.SpecialLinear","content":" Manifolds.SpecialLinear  ‚Äî  Type SpecialLinear{T,ùîΩ} <: AbstractDecoratorManifold The special linear group  $\\mathrm{SL}(n,ùîΩ)$  that is, the group of all invertible matrices with unit determinant in  $ùîΩ^{n√ón}$ . The Lie algebra  $ùî∞ùî©(n, ùîΩ) = T_e \\mathrm{SL}(n,ùîΩ)$  is the set of all matrices in  $ùîΩ^{n√ón}$  with trace of zero. By default, tangent vectors  $X_p ‚àà T_p \\mathrm{SL}(n,ùîΩ)$  for  $p ‚àà \\mathrm{SL}(n,ùîΩ)$  are represented with their corresponding Lie algebra vector  $X_e = p^{-1}X_p ‚àà ùî∞ùî©(n, ùîΩ)$ . The default metric is the same left- $\\mathrm{GL}(n)$ -right- $\\mathrm{O}(n)$ -invariant metric used for  GeneralLinear(n, ùîΩ) . The resulting geodesic on  $\\mathrm{GL}(n,ùîΩ)$  emanating from an element of  $\\mathrm{SL}(n,ùîΩ)$  in the direction of an element of  $ùî∞ùî©(n, ùîΩ)$  is a closed subgroup of  $\\mathrm{SL}(n,ùîΩ)$ . As a result, most metric functions forward to  GeneralLinear . source"},{"id":1117,"pagetitle":"Group manifold","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.project-Tuple{SpecialLinear, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(G::SpecialLinear, p, X) Orthogonally project  $X ‚àà ùîΩ^{n√ón}$  onto the tangent space of  $p$  to the  SpecialLinear $G = \\mathrm{SL}(n, ùîΩ)$ . The formula reads \\[\\operatorname{proj}_{p}\n    = (\\mathrm{d}L_p)_e ‚àò \\operatorname{proj}_{ùî∞ùî©(n, ùîΩ)} ‚àò (\\mathrm{d}L_p^{-1})_p\n    \\colon X ‚Ü¶ X - \\frac{\\operatorname{tr}(X)}{n} I,\\] where the last expression uses the tangent space representation as the Lie algebra. source"},{"id":1118,"pagetitle":"Group manifold","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.project-Tuple{SpecialLinear, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(G::SpecialLinear, p) Project  $p ‚àà \\mathrm{GL}(n, ùîΩ)$  to the  SpecialLinear  group  $G=\\mathrm{SL}(n, ùîΩ)$ . Given the singular value decomposition of  $p$ , written  $p = U S V^\\mathrm{H}$ , the formula for the projection is \\[\\operatorname{proj}_{\\mathrm{SL}(n, ùîΩ)}(p) = U S D V^\\mathrm{H},\\] where \\[D_{ij} = Œ¥_{ij} \\begin{cases}\n    1            & \\text{ if } i ‚â† n \\\\\n    \\det(p)^{-1} & \\text{ if } i = n\n\\end{cases}.\\] source"},{"id":1119,"pagetitle":"Group manifold","title":"Translation group","ref":"/manifolds/stable/manifolds/group/#Translation-group","content":" Translation group"},{"id":1120,"pagetitle":"Group manifold","title":"Manifolds.TranslationGroup","ref":"/manifolds/stable/manifolds/group/#Manifolds.TranslationGroup","content":" Manifolds.TranslationGroup  ‚Äî  Type TranslationGroup{T,ùîΩ} <: GroupManifold{Euclidean{T,ùîΩ},AdditionOperation,LeftInvariantRepresentation} Translation group  $\\mathrm{T}(n)$  represented by translation arrays. Constructor TranslationGroup(n‚ÇÅ,...,n·µ¢; field=ùîΩ, parameter::Symbol=:type) Generate the translation group on  $ùîΩ^{n‚ÇÅ,‚Ä¶,n·µ¢}$  =  Euclidean(n‚ÇÅ,...,n·µ¢; field=ùîΩ) , which is isomorphic to the group itself. source"},{"id":1121,"pagetitle":"Group manifold","title":"Metrics on groups","ref":"/manifolds/stable/manifolds/group/#Metrics-on-groups","content":" Metrics on groups Lie groups by default typically forward all metric-related operations like exponential or logarithmic map to the underlying manifold, for example  SpecialOrthogonal  uses methods for  Rotations  (which is, incidentally, bi-invariant), or  SpecialEuclidean  uses product metric of the translation and rotation parts (which is not invariant under group operation). It is, however, possible to change the metric used by a group by wrapping it in a  MetricManifold  decorator."},{"id":1122,"pagetitle":"Group manifold","title":"Invariant metrics","ref":"/manifolds/stable/manifolds/group/#Invariant-metrics","content":" Invariant metrics"},{"id":1123,"pagetitle":"Group manifold","title":"Manifolds.LeftInvariantMetric","ref":"/manifolds/stable/manifolds/group/#Manifolds.LeftInvariantMetric","content":" Manifolds.LeftInvariantMetric  ‚Äî  Type LeftInvariantMetric <: AbstractMetric An  AbstractMetric  that changes the metric of a Lie group to the left-invariant metric obtained by left-translations to the identity. Adds the  HasLeftInvariantMetric  trait. source"},{"id":1124,"pagetitle":"Group manifold","title":"Manifolds.RightInvariantMetric","ref":"/manifolds/stable/manifolds/group/#Manifolds.RightInvariantMetric","content":" Manifolds.RightInvariantMetric  ‚Äî  Type RightInvariantMetric <: AbstractMetric An  AbstractMetric  that changes the metric of a Lie group to the right-invariant metric obtained by right-translations to the identity. Adds the  HasRightInvariantMetric  trait. source"},{"id":1125,"pagetitle":"Group manifold","title":"Manifolds.direction","ref":"/manifolds/stable/manifolds/group/#Manifolds.direction-Tuple{AbstractDecoratorManifold}","content":" Manifolds.direction  ‚Äî  Method direction(::AbstractDecoratorManifold) -> AD Get the direction of the action a certain Lie group with its implicit metric has. source"},{"id":1126,"pagetitle":"Group manifold","title":"Manifolds.has_approx_invariant_metric","ref":"/manifolds/stable/manifolds/group/#Manifolds.has_approx_invariant_metric-Tuple{AbstractDecoratorManifold, Any, Any, Any, Any, Tuple{ActionDirection, Manifolds.GroupActionSide}}","content":" Manifolds.has_approx_invariant_metric  ‚Äî  Method has_approx_invariant_metric(\n    G::AbstractDecoratorManifold,\n    p,\n    X,\n    Y,\n    qs::AbstractVector,\n    conv::ActionDirectionAndSide = LeftForwardAction();\n    kwargs...,\n) -> Bool Check whether the metric on the group  $\\mathcal{G}$  is (approximately) invariant using a set of predefined points. Namely, for  $p ‚àà \\mathcal{G}$ ,  $X,Y ‚àà T_p \\mathcal{G}$ , a metric  $g$ , and a translation map  $œÑ_q$  in the specified direction, check for each  $q ‚àà \\mathcal{G}$  that the following condition holds: \\[g_p(X, Y) ‚âà g_{œÑ_q p}((\\mathrm{d}œÑ_q)_p X, (\\mathrm{d}œÑ_q)_p Y).\\] This is necessary but not sufficient for invariance. Optionally,  kwargs  passed to  isapprox  may be provided. source"},{"id":1127,"pagetitle":"Group manifold","title":"Cartan-Schouten connections","ref":"/manifolds/stable/manifolds/group/#Cartan-Schouten-connections","content":" Cartan-Schouten connections"},{"id":1128,"pagetitle":"Group manifold","title":"Manifolds.AbstractCartanSchoutenConnection","ref":"/manifolds/stable/manifolds/group/#Manifolds.AbstractCartanSchoutenConnection","content":" Manifolds.AbstractCartanSchoutenConnection  ‚Äî  Type AbstractCartanSchoutenConnection Abstract type for Cartan-Schouten connections, that is connections whose geodesics going through group identity are one-parameter subgroups. See [ PL20 ] for details. source"},{"id":1129,"pagetitle":"Group manifold","title":"Manifolds.CartanSchoutenMinus","ref":"/manifolds/stable/manifolds/group/#Manifolds.CartanSchoutenMinus","content":" Manifolds.CartanSchoutenMinus  ‚Äî  Type CartanSchoutenMinus The unique Cartan-Schouten connection such that all left-invariant vector fields are globally defined by their value at identity. It is biinvariant with respect to the group operation. source"},{"id":1130,"pagetitle":"Group manifold","title":"Manifolds.CartanSchoutenPlus","ref":"/manifolds/stable/manifolds/group/#Manifolds.CartanSchoutenPlus","content":" Manifolds.CartanSchoutenPlus  ‚Äî  Type CartanSchoutenPlus The unique Cartan-Schouten connection such that all right-invariant vector fields are globally defined by their value at identity. It is biinvariant with respect to the group operation. source"},{"id":1131,"pagetitle":"Group manifold","title":"Manifolds.CartanSchoutenZero","ref":"/manifolds/stable/manifolds/group/#Manifolds.CartanSchoutenZero","content":" Manifolds.CartanSchoutenZero  ‚Äî  Type CartanSchoutenZero The unique torsion-free Cartan-Schouten connection. It is biinvariant with respect to the group operation. If the metric on the underlying manifold is bi-invariant then it is equivalent to the Levi-Civita connection of that metric. source"},{"id":1132,"pagetitle":"Group manifold","title":"Base.exp","ref":"/manifolds/stable/manifolds/group/#Base.exp-Union{Tuple{ùîΩ}, Tuple{ConnectionManifold{ùîΩ, <:AbstractDecoratorManifold{ùîΩ}, <:AbstractCartanSchoutenConnection}, Any, Any}} where ùîΩ","content":" Base.exp  ‚Äî  Method exp(M::ConnectionManifold{ùîΩ,<:AbstractDecoratorManifold{ùîΩ},<:AbstractCartanSchoutenConnection}, p, X) where {ùîΩ} Compute the exponential map on the  ConnectionManifold M  with a Cartan-Schouten connection. See Sections 5.3.2 and 5.3.3 of [ PL20 ] for details. source"},{"id":1133,"pagetitle":"Group manifold","title":"Base.log","ref":"/manifolds/stable/manifolds/group/#Base.log-Union{Tuple{ùîΩ}, Tuple{ConnectionManifold{ùîΩ, <:AbstractDecoratorManifold{ùîΩ}, <:AbstractCartanSchoutenConnection}, Any, Any}} where ùîΩ","content":" Base.log  ‚Äî  Method log(M::ConnectionManifold{ùîΩ,<:AbstractDecoratorManifold{ùîΩ},<:AbstractCartanSchoutenConnection}, p, q) where {ùîΩ} Compute the logarithmic map on the  ConnectionManifold M  with a Cartan-Schouten connection. See Sections 5.3.2 and 5.3.3 of [ PL20 ] for details. source"},{"id":1134,"pagetitle":"Group manifold","title":"ManifoldsBase.parallel_transport_direction","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.parallel_transport_direction-Tuple{ConnectionManifold{ùîΩ, M, CartanSchoutenZero} where {ùîΩ, M}, Identity, Any, Any}","content":" ManifoldsBase.parallel_transport_direction  ‚Äî  Method parallel_transport_direction(M::CartanSchoutenZeroGroup, ::Identity, X, d) Transport tangent vector  X  at identity on the group manifold with the  CartanSchoutenZero  connection in the direction  d . See [ PL20 ] for details. source"},{"id":1135,"pagetitle":"Group manifold","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.parallel_transport_to-Tuple{ConnectionManifold{ùîΩ, M, CartanSchoutenMinus} where {ùîΩ, M}, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::CartanSchoutenMinusGroup, p, X, q) Transport tangent vector  X  at point  p  on the group manifold  M  with the  CartanSchoutenMinus  connection to point  q . See [ PL20 ] for details. source"},{"id":1136,"pagetitle":"Group manifold","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.parallel_transport_to-Tuple{ConnectionManifold{ùîΩ, M, CartanSchoutenPlus} where {ùîΩ, M}, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method vector_transport_to(M::CartanSchoutenPlusGroup, p, X, q) Transport tangent vector  X  at point  p  on the group manifold  M  with the  CartanSchoutenPlus  connection to point  q . See [ PL20 ] for details. source"},{"id":1137,"pagetitle":"Group manifold","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/group/#ManifoldsBase.parallel_transport_to-Tuple{ConnectionManifold{ùîΩ, M, CartanSchoutenZero} where {ùîΩ, M}, Identity, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::CartanSchoutenZeroGroup, p::Identity, X, q) Transport vector  X  at identity of group  M  equipped with the  CartanSchoutenZero  connection to point  q  using parallel transport. source"},{"id":1140,"pagetitle":"Hamiltonian","title":"Hamiltonian matrices","ref":"/manifolds/stable/manifolds/hamiltonian/#Hamiltonian-matrices","content":" Hamiltonian matrices"},{"id":1141,"pagetitle":"Hamiltonian","title":"Manifolds.Hamiltonian","ref":"/manifolds/stable/manifolds/hamiltonian/#Manifolds.Hamiltonian","content":" Manifolds.Hamiltonian  ‚Äî  Type Hamiltonian{T,S<:AbstractMatrix{<:T}} <: AbstractMatrix{T} A type to store a Hamiltonian matrix, that is a square matrix for which  $A^+ = -A$  where \\[A^+ = J_{2n}A^{\\mathrm{T}}J_{2n}, \\qquad J_{2n} \\begin{pmatrix} 0 & I_n\\\\-I_n & 0 \\end{pmatrix},\\] and  $I_n$  denotes the  $n√ón$ source"},{"id":1142,"pagetitle":"Hamiltonian","title":"Manifolds.HamiltonianMatrices","ref":"/manifolds/stable/manifolds/hamiltonian/#Manifolds.HamiltonianMatrices","content":" Manifolds.HamiltonianMatrices  ‚Äî  Type HamiltonianMatrices{T,ùîΩ} <: AbstractDecoratorManifold{ùîΩ} The  AbstractManifold  consisting of (real-valued) hamiltonian matrices of size  $n√ón$ , i.e. the set \\[\\mathfrak{sp}(2n,ùîΩ) = \\bigl\\{p  ‚àà ùîΩ^{2n√ó2n}\\ \\big|\\ p^+ = p \\bigr\\},\\] where  $‚ãÖ^{+}$  denotes the  symplectic_inverse , and  $ùîΩ ‚àà \\{ ‚Ñù, ‚ÑÇ\\}$ . Though it is slightly redundant, usually the matrices are stored as  $2n√ó2n$  arrays. The symbol  $\\mathfak{sp}$  refers to the main usage within  Manifolds.jl  that is the Lie algebra to the  SymplecticMatrices  interpreted as a Lie group with the matrix multiplication as group operation. Constructor HamiltonianMatrices(2n::Int, field::AbstractNumbers=‚Ñù) Generate the manifold of  $2n√ó2n$  Hamiltonian matrices. source"},{"id":1143,"pagetitle":"Hamiltonian","title":"Base.:^","ref":"/manifolds/stable/manifolds/hamiltonian/#Base.:^-Tuple{Hamiltonian, typeof(+)}","content":" Base.:^  ‚Äî  Method ^(A::Hamiltonian, ::typeof(+)) Compute the  symplectic_inverse  of a Hamiltonian (A) source"},{"id":1144,"pagetitle":"Hamiltonian","title":"Base.rand","ref":"/manifolds/stable/manifolds/hamiltonian/#Base.rand-Tuple{HamiltonianMatrices}","content":" Base.rand  ‚Äî  Method pX = rand(M::HamiltonianMatrices; œÉ::Real=1.0, vector_at=nothing)\nrand!(M::HamiltonianMatrices, pX; œÉ::Real=1.0, vector_at=nothing) Generate a random Hamiltonian matrix. Since these are a submanifold of  $‚Ñù^{2n√ó2n}$ , the same method applies for points and tangent vectors. This can also be done in-place of  pX . The construction is based on generating one normally-distributed  $n√ón$  matrix  $A$  and two symmetric  $n√ón$  matrices  $B, C$  which are then stacked: \\[p = \\begin{pmatrix} A & B\\\\ C & -A^{\\mathrm{T}} \\end{pmatrix}\\] source"},{"id":1145,"pagetitle":"Hamiltonian","title":"Manifolds.is_hamiltonian","ref":"/manifolds/stable/manifolds/hamiltonian/#Manifolds.is_hamiltonian-Tuple{AbstractMatrix}","content":" Manifolds.is_hamiltonian  ‚Äî  Method is_hamiltonian(A::AbstractMatrix; kwargs...) Test whether a matrix  A  is hamiltonian. The test consists of verifying whether \\[A^+ = -A\\] where  $A^+$  denotes the  symplectic_inverse  of  A . The passed keyword arguments are passed on to  isapprox  check within source"},{"id":1146,"pagetitle":"Hamiltonian","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/hamiltonian/#ManifoldsBase.check_point-Tuple{HamiltonianMatrices, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::HamiltonianMatrices{n,ùîΩ}, p; kwargs...) Check whether  p  is a valid manifold point on the  HamiltonianMatrices M , i.e. whether  p is_hamiltonian . The tolerance for the test of  p  can be set using  kwargs... . source"},{"id":1147,"pagetitle":"Hamiltonian","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/hamiltonian/#ManifoldsBase.check_vector-Tuple{HamiltonianMatrices, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::HamiltonianMatrices{n,ùîΩ}, p, X; kwargs... ) Check whether  X  is a tangent vector to manifold point  p  on the  HamiltonianMatrices M , i.e.  X  has to be a Hamiltonian matrix The tolerance for  is_hamiltonian X  can be set using  kwargs... . source"},{"id":1148,"pagetitle":"Hamiltonian","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/hamiltonian/#ManifoldsBase.is_flat-Tuple{HamiltonianMatrices}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::HamiltonianMatrices) Return true.  HamiltonianMatrices  is a flat manifold. source"},{"id":1151,"pagetitle":"Heisenberg matrices","title":"Heisenberg matrices","ref":"/manifolds/stable/manifolds/heisenberg/#Heisenberg-matrices","content":" Heisenberg matrices"},{"id":1152,"pagetitle":"Heisenberg matrices","title":"Manifolds.HeisenbergMatrices","ref":"/manifolds/stable/manifolds/heisenberg/#Manifolds.HeisenbergMatrices","content":" Manifolds.HeisenbergMatrices  ‚Äî  Type HeisenbergMatrices{T} <: AbstractDecoratorManifold{ùîΩ} Heisenberg matrices  HeisenbergMatrices(n)  is the manifold of  $(n+2)√ó(n+2)$  matrices [ BP08 ] \\[\\begin{bmatrix} 1 & \\mathbf{a} & c \\\\\n\\mathbf{0}_n & I_n & \\mathbf{b} \\\\\n0 & \\mathbf{0}_n^\\mathrm{T} & 1 \\end{bmatrix}\\] where  $I_n$  is the  $n√ón$  unit matrix,  $\\mathbf{a}$  is a row vector of length  $n$ ,  $\\mathbf{b}$  is a column vector of length  $n$ ,  $\\mathbf{0}_n$  is the column zero vector of length  $n$ , and  $c$  is a real number. It is a submanifold of  Euclidean (n+2, n+2)  and the manifold of the  HeisenbergGroup . Constructor HeisenbergMatrices(n::Int; parameter::Symbol=:type) Generate the manifold of  $(n+2)√ó(n+2)$  Heisenberg matrices. source"},{"id":1153,"pagetitle":"Heisenberg matrices","title":"Base.rand","ref":"/manifolds/stable/manifolds/heisenberg/#Base.rand-Tuple{HeisenbergMatrices}","content":" Base.rand  ‚Äî  Method Random.rand(M::HeisenbergMatrices; vector_at = nothing, œÉ::Real=1.0) If  vector_at  is  nothing , return a random point on the  HeisenbergMatrices M  by sampling elements of the first row and the last column from the normal distribution with mean 0 and standard deviation  œÉ . If  vector_at  is not  nothing , return a random tangent vector from the tangent space of the point  vector_at  on the  HeisenbergMatrices  by using a normal distribution with mean 0 and standard deviation  œÉ . source"},{"id":1154,"pagetitle":"Heisenberg matrices","title":"ManifoldsBase.Weingarten","ref":"/manifolds/stable/manifolds/heisenberg/#ManifoldsBase.Weingarten-Tuple{HeisenbergMatrices, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Y = Weingarten(M::HeisenbergMatrices, p, X, V)\nWeingarten!(M::HeisenbergMatrices, Y, p, X, V) Compute the Weingarten map  $\\mathcal W_p$  at  p  on the  HeisenbergMatrices M  with respect to the tangent vector  $X \\in T_p\\mathcal M$  and the normal vector  $V \\in N_p\\mathcal M$ . Since this a flat space by itself, the result is always the zero tangent vector. source"},{"id":1155,"pagetitle":"Heisenberg matrices","title":"ManifoldsBase.get_coordinates","ref":"/manifolds/stable/manifolds/heisenberg/#ManifoldsBase.get_coordinates-Tuple{HeisenbergMatrices, Any, Any, DefaultOrthonormalBasis{‚Ñù, TangentSpaceType}}","content":" ManifoldsBase.get_coordinates  ‚Äî  Method get_coordinates(M::HeisenbergMatrices, p, X, ::DefaultOrthonormalBasis{‚Ñù,TangentSpaceType}) Get coordinates of tangent vector  X  at point  p  from the  HeisenbergMatrices M . Given a matrix \\[\\begin{bmatrix} 1 & \\mathbf{a} & c \\\\\n\\mathbf{0} & I_n & \\mathbf{b} \\\\\n0 & \\mathbf{0} & 1 \\end{bmatrix}\\] the coordinates are concatenated vectors  $\\mathbf{a}$ ,  $\\mathbf{b}$ , and number  $c$ . source"},{"id":1156,"pagetitle":"Heisenberg matrices","title":"ManifoldsBase.get_vector","ref":"/manifolds/stable/manifolds/heisenberg/#ManifoldsBase.get_vector-Tuple{HeisenbergMatrices, Any, Any, DefaultOrthonormalBasis{‚Ñù, TangentSpaceType}}","content":" ManifoldsBase.get_vector  ‚Äî  Method get_vector(M::HeisenbergMatrices, p, X‚Å±, ::DefaultOrthonormalBasis{‚Ñù,TangentSpaceType}) Get tangent vector with coordinates  X‚Å±  at point  p  from the  HeisenbergMatrices M . Given a vector of coordinates  $\\begin{bmatrix}\\mathbb{a} & \\mathbb{b} & c\\end{bmatrix}$  the tangent vector is equal to \\[\\begin{bmatrix} 1 & \\mathbf{a} & c \\\\\n\\mathbf{0} & I_n & \\mathbf{b} \\\\\n0 & \\mathbf{0} & 1 \\end{bmatrix}\\] source"},{"id":1157,"pagetitle":"Heisenberg matrices","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/heisenberg/#ManifoldsBase.is_flat-Tuple{HeisenbergMatrices}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::HeisenbergMatrices) Return true.  HeisenbergMatrices  is a flat manifold. source"},{"id":1158,"pagetitle":"Heisenberg matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/heisenberg/#ManifoldsBase.manifold_dimension-Tuple{HeisenbergMatrices}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::HeisenbergMatrices) Return the dimension of  HeisenbergMatrices (n) , which is equal to  $2n+1$ . source"},{"id":1161,"pagetitle":"Hyperbolic space","title":"Hyperbolic space","ref":"/manifolds/stable/manifolds/hyperbolic/#HyperbolicSpace","content":" Hyperbolic space The hyperbolic space can be represented in three different models. Hyperboloid  which is the default model, i.e. is used when using arbitrary array types for points and tangent vectors Poincar√© ball  with separate types for points and tangent vectors and a  visualization  for the two-dimensional case Poincar√© half space  with separate types for points and tangent vectors and a  visualization  for the two-dimensional cae. In the following the common functions are collected. A function in this general section uses vectors interpreted as if in the  hyperboloid model , and other representations usually just convert to this representation to use these general functions."},{"id":1162,"pagetitle":"Hyperbolic space","title":"Manifolds.Hyperbolic","ref":"/manifolds/stable/manifolds/hyperbolic/#Manifolds.Hyperbolic","content":" Manifolds.Hyperbolic  ‚Äî  Type Hyperbolic{T} <: AbstractDecoratorManifold{‚Ñù} The hyperbolic space  $\\mathcal H^n$  represented by  $n+1$ -Tuples, i.e. embedded in the  Lorentz ian manifold equipped with the  MinkowskiMetric $‚ü®‚ãÖ,‚ãÖ‚ü©_{\\mathrm{M}}$ . The space is defined as \\[\\mathcal H^n = \\Bigl\\{p ‚àà ‚Ñù^{n+1}\\ \\Big|\\ ‚ü®p,p‚ü©_{\\mathrm{M}}= -p_{n+1}^2\n  + \\displaystyle\\sum_{k=1}^n p_k^2 = -1, p_{n+1} > 0\\Bigr\\},.\\] The tangent space  $T_p \\mathcal H^n$  is given by \\[T_p \\mathcal H^n := \\bigl\\{\nX ‚àà ‚Ñù^{n+1} : ‚ü®p,X‚ü©_{\\mathrm{M}} = 0\n\\bigr\\}.\\] Note that while the  MinkowskiMetric  renders the  Lorentz  manifold (only) pseudo-Riemannian, on the tangent bundle of the Hyperbolic space it induces a Riemannian metric. The corresponding sectional curvature is  $-1$ . If  p  and  X  are  Vector s of length  n+1  they are assumed to be a  HyperboloidPoint  and a  HyperboloidTangentVector , respectively Other models are the Poincar√© ball model, see  PoincareBallPoint  and  PoincareBallTangentVector , respectively and the Poincar√© half space model, see  PoincareHalfSpacePoint  and  PoincareHalfSpaceTangentVector , respectively. Constructor Hyperbolic(n::Int; parameter::Symbol=:type) Generate the Hyperbolic manifold of dimension  n . source"},{"id":1163,"pagetitle":"Hyperbolic space","title":"Manifolds.HyperboloidPoint","ref":"/manifolds/stable/manifolds/hyperbolic/#Manifolds.HyperboloidPoint","content":" Manifolds.HyperboloidPoint  ‚Äî  Type HyperboloidPoint <: AbstractManifoldPoint In the Hyperboloid model of the  Hyperbolic $\\mathcal H^n$  points are represented as vectors in  $‚Ñù^{n+1}$  with  MinkowskiMetric  equal to  $-1$ . This representation is the default, i.e.  AbstractVector s are assumed to have this representation. source"},{"id":1164,"pagetitle":"Hyperbolic space","title":"Manifolds.HyperboloidTangentVector","ref":"/manifolds/stable/manifolds/hyperbolic/#Manifolds.HyperboloidTangentVector","content":" Manifolds.HyperboloidTangentVector  ‚Äî  Type HyperboloidTangentVector <: AbstractTangentVector In the Hyperboloid model of the  Hyperbolic $\\mathcal H^n$  tangent vctors are represented as vectors in  $‚Ñù^{n+1}$  with  MinkowskiMetric $‚ü®p,X‚ü©_{\\mathrm{M}}=0$  to their base point  $p$ . This representation is the default, i.e. vectors are assumed to have this representation. source"},{"id":1165,"pagetitle":"Hyperbolic space","title":"Manifolds.PoincareBallPoint","ref":"/manifolds/stable/manifolds/hyperbolic/#Manifolds.PoincareBallPoint","content":" Manifolds.PoincareBallPoint  ‚Äî  Type PoincareBallPoint <: AbstractManifoldPoint A point on the  Hyperbolic  manifold  $\\mathcal H^n$  can be represented as a vector of norm less than one in  $\\mathbb R^n$ . source"},{"id":1166,"pagetitle":"Hyperbolic space","title":"Manifolds.PoincareBallTangentVector","ref":"/manifolds/stable/manifolds/hyperbolic/#Manifolds.PoincareBallTangentVector","content":" Manifolds.PoincareBallTangentVector  ‚Äî  Type PoincareBallTangentVector <: AbstractTangentVector In the Poincar√© ball model of the  Hyperbolic $\\mathcal H^n$  tangent vectors are represented as vectors in  $‚Ñù^{n}$ . source"},{"id":1167,"pagetitle":"Hyperbolic space","title":"Manifolds.PoincareHalfSpacePoint","ref":"/manifolds/stable/manifolds/hyperbolic/#Manifolds.PoincareHalfSpacePoint","content":" Manifolds.PoincareHalfSpacePoint  ‚Äî  Type PoincareHalfSpacePoint <: AbstractManifoldPoint A point on the  Hyperbolic  manifold  $\\mathcal H^n$  can be represented as a vector in the half plane, i.e.  $x ‚àà ‚Ñù^n$  with  $x_d > 0$ . source"},{"id":1168,"pagetitle":"Hyperbolic space","title":"Manifolds.PoincareHalfSpaceTangentVector","ref":"/manifolds/stable/manifolds/hyperbolic/#Manifolds.PoincareHalfSpaceTangentVector","content":" Manifolds.PoincareHalfSpaceTangentVector  ‚Äî  Type PoincareHalfPlaneTangentVector <: AbstractTangentVector In the Poincar√© half plane model of the  Hyperbolic $\\mathcal H^n$  tangent vectors are represented as vectors in  $‚Ñù^{n}$ . source"},{"id":1169,"pagetitle":"Hyperbolic space","title":"Base.exp","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.exp-Tuple{Hyperbolic, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::Hyperbolic, p, X) Compute the exponential map on the  Hyperbolic  space  $\\mathcal H^n$  emanating from  p  towards  X . The formula reads \\[\\exp_p X = \\cosh(\\sqrt{‚ü®X,X‚ü©_{\\mathrm{M}}})p\n+ \\sinh(\\sqrt{‚ü®X,X‚ü©_{\\mathrm{M}}})\\frac{X}{\\sqrt{‚ü®X,X‚ü©_{\\mathrm{M}}}},\\] where  $‚ü®‚ãÖ,‚ãÖ‚ü©_{\\mathrm{M}}$  denotes the  MinkowskiMetric  on the embedding, the  Lorentz ian manifold, see for example the extended version [ BPS15 ] of the paper [ BPS16 ]. source"},{"id":1170,"pagetitle":"Hyperbolic space","title":"Base.log","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.log-Tuple{Hyperbolic, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::Hyperbolic, p, q) Compute the logarithmic map on the  Hyperbolic  space  $\\mathcal H^n$ , the tangent vector representing the  geodesic  starting from  p  reaches  q  after time 1. The formula reads for  $p ‚â† q$ \\[\\log_p q = d_{\\mathcal H^n}(p,q)\n\\frac{q-‚ü®p,q‚ü©_{\\mathrm{M}} p}{\\lVert q-‚ü®p,q‚ü©_{\\mathrm{M}} p \\rVert_2},\\] where  $‚ü®‚ãÖ,‚ãÖ‚ü©_{\\mathrm{M}}$  denotes the  MinkowskiMetric  on the embedding, the  Lorentz ian manifold. For  $p=q$  the logarithmic map is equal to the zero vector For more details, see for example the extended version [ BPS15 ] of the paper [ BPS16 ]. source"},{"id":1171,"pagetitle":"Hyperbolic space","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/hyperbolic/#Manifolds.manifold_volume-Tuple{Hyperbolic}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_dimension(M::Hyperbolic) Return the volume of the hyperbolic space manifold  $\\mathcal H^n$ , i.e. infinity. source"},{"id":1172,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.check_point-Tuple{Hyperbolic, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::Hyperbolic, p; kwargs...) Check whether  p  is a valid point on the  Hyperbolic M . For the  HyperboloidPoint  or plain vectors this means that,  p  is a vector of length  $n+1$  with inner product in the embedding of -1, see  MinkowskiMetric . The tolerance for the last test can be set using the  kwargs... . For the  PoincareBallPoint  a valid point is a vector  $p ‚àà ‚Ñù^n$  with a norm strictly less than 1. For the  PoincareHalfSpacePoint  a valid point is a vector from  $p ‚àà ‚Ñù^n$  with a positive last entry, i.e.  $p_n>0$ source"},{"id":1173,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.check_vector-Tuple{Hyperbolic, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::Hyperbolic, p, X; kwargs... ) Check whether  X  is a tangent vector to  p  on the  Hyperbolic M , i.e. after  check_point (M,p) ,  X  has to be of the same dimension as  p . The tolerance for the last test can be set using the  kwargs... . For a the hyperboloid model or vectors,  X  has to be  orthogonal to  p  with respect to the inner product from the embedding, see  MinkowskiMetric . For a the Poincar√© ball as well as the Poincar√© half plane model,  X  has to be a vector from  $‚Ñù^{n}$ . source"},{"id":1174,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.injectivity_radius-Tuple{Hyperbolic}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::Hyperbolic)\ninjectivity_radius(M::Hyperbolic, p) Return the injectivity radius on the  Hyperbolic , which is  $‚àû$ . source"},{"id":1175,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.is_flat-Tuple{Hyperbolic}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::Hyperbolic) Return false.  Hyperbolic  is not a flat manifold. source"},{"id":1176,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.manifold_dimension-Tuple{Hyperbolic}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::Hyperbolic) Return the dimension of the hyperbolic space manifold  $\\mathcal H^n$ , i.e.  $\\dim(\\mathcal H^n) = n$ . source"},{"id":1177,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.parallel_transport_to-Tuple{Hyperbolic, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::Hyperbolic, p, X, q) Compute the parallel transport of the  X  from the tangent space at  p  on the  Hyperbolic  space  $\\mathcal H^n$  to the tangent at  q  along the  geodesic  connecting  p  and  q . The formula reads \\[\\mathcal P_{q‚Üêp}X = X - \\frac{‚ü®\\log_p q,X‚ü©_p}{d^2_{\\mathcal H^n}(p,q)}\n\\bigl(\\log_p q + \\log_qp \\bigr),\\] where  $‚ü®‚ãÖ,‚ãÖ‚ü©_p$  denotes the inner product in the tangent space at  p . source"},{"id":1178,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.project-Tuple{Hyperbolic, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Hyperbolic, p, X) Perform an orthogonal projection with respect to the Minkowski inner product of  X  onto the tangent space at  p  of the  Hyperbolic  space  M . The formula reads \\[Y = X + ‚ü®p,X‚ü©_{\\mathrm{M}} p,\\] where  $‚ü®‚ãÖ, ‚ãÖ‚ü©_{\\mathrm{M}}$  denotes the  MinkowskiMetric  on the embedding, the  Lorentz ian manifold. Note Projection is only available for the (default)  HyperboloidTangentVector  representation, the others don't have such an embedding source"},{"id":1179,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.riemann_tensor","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.riemann_tensor-Tuple{Hyperbolic, Vararg{Any, 4}}","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(M::Hyperbolic{n}, p, X, Y, Z) Compute the Riemann tensor  $R(X,Y)Z$  at point  p  on  Hyperbolic M . The formula reads (see e.g., [ Lee19 ] Proposition 8.36) \\[R(X,Y)Z = - (\\langle Z, Y \\rangle X - \\langle Z, X \\rangle Y)\\] source"},{"id":1180,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.sectional_curvature","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.sectional_curvature-Tuple{Hyperbolic, Any, Any, Any}","content":" ManifoldsBase.sectional_curvature  ‚Äî  Method sectional_curvature(::Hyperbolic, p, X, Y) Sectional curvature of  Hyperbolic M  is -1 if dimension is > 1 and 0 otherwise. source"},{"id":1181,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.sectional_curvature_max","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.sectional_curvature_max-Tuple{Hyperbolic}","content":" ManifoldsBase.sectional_curvature_max  ‚Äî  Method sectional_curvature_max(::Hyperbolic) Sectional curvature of  Hyperbolic M  is -1 if dimension is > 1 and 0 otherwise. source"},{"id":1182,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.sectional_curvature_min","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.sectional_curvature_min-Tuple{Hyperbolic}","content":" ManifoldsBase.sectional_curvature_min  ‚Äî  Method sectional_curvature_min(M::Hyperbolic) Sectional curvature of  Hyperbolic M  is -1 if dimension is > 1 and 0 otherwise. source"},{"id":1183,"pagetitle":"Hyperbolic space","title":"Statistics.mean","ref":"/manifolds/stable/manifolds/hyperbolic/#Statistics.mean-Tuple{Hyperbolic, Vararg{Any}}","content":" Statistics.mean  ‚Äî  Method mean(\n    M::Hyperbolic,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method = CyclicProximalPointEstimation();\n    kwargs...,\n) Compute the Riemannian  mean  of  x  on the  Hyperbolic  space using  CyclicProximalPointEstimation . source"},{"id":1184,"pagetitle":"Hyperbolic space","title":"hyperboloid model","ref":"/manifolds/stable/manifolds/hyperbolic/#hyperboloid_model","content":" hyperboloid model"},{"id":1185,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{HyperboloidPoint}, PoincareBallPoint}","content":" Base.convert  ‚Äî  Method convert(::Type{HyperboloidPoint}, p::PoincareBallPoint)\nconvert(::Type{AbstractVector}, p::PoincareBallPoint) convert a point  PoincareBallPoint x  (from  $‚Ñù^n$ ) from the Poincar√© ball model of the  Hyperbolic  manifold  $\\mathcal H^n$  to a  HyperboloidPoint $œÄ(p) ‚àà ‚Ñù^{n+1}$ . The isometry is defined by \\[œÄ(p) = \\frac{1}{1-\\lVert p \\rVert^2}\n\\begin{pmatrix}2p_1\\\\‚ãÆ\\\\2p_n\\\\1+\\lVert p \\rVert^2\\end{pmatrix}\\] Note that this is also used, when the type to convert to is a vector. source"},{"id":1186,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{HyperboloidPoint}, PoincareHalfSpacePoint}","content":" Base.convert  ‚Äî  Method convert(::Type{HyperboloidPoint}, p::PoincareHalfSpacePoint)\nconvert(::Type{AbstractVector}, p::PoincareHalfSpacePoint) convert a point  PoincareHalfSpacePoint p  (from  $‚Ñù^n$ ) from the Poincar√© half plane model of the  Hyperbolic  manifold  $\\mathcal H^n$  to a  HyperboloidPoint $œÄ(p) ‚àà ‚Ñù^{n+1}$ . This is done in two steps, namely transforming it to a Poincare ball point and from there further on to a Hyperboloid point. source"},{"id":1187,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{HyperboloidTangentVector}, PoincareBallPoint, PoincareBallTangentVector}","content":" Base.convert  ‚Äî  Method convert(::Type{HyperboloidTangentVector}, p::PoincareBallPoint, X::PoincareBallTangentVector)\nconvert(::Type{AbstractVector}, p::PoincareBallPoint, X::PoincareBallTangentVector) Convert the  PoincareBallTangentVector X  from the tangent space at  p  to a  HyperboloidTangentVector  by computing the push forward of the isometric map, cf.  convert(::Type{HyperboloidPoint}, p::PoincareBallPoint) . The push forward  $œÄ_*(p)$  maps from  $‚Ñù^n$  to a subspace of  $‚Ñù^{n+1}$ , the formula reads \\[œÄ_*(p)[X] = \\begin{pmatrix}\n    \\frac{2X_1}{1-\\lVert p \\rVert^2} + \\frac{4}{(1-\\lVert p \\rVert^2)^2}‚ü®X,p‚ü©p_1\\\\\n    ‚ãÆ\\\\\n    \\frac{2X_n}{1-\\lVert p \\rVert^2} + \\frac{4}{(1-\\lVert p \\rVert^2)^2}‚ü®X,p‚ü©p_n\\\\\n    \\frac{4}{(1-\\lVert p \\rVert^2)^2}‚ü®X,p‚ü©\n\\end{pmatrix}.\\] source"},{"id":1188,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{HyperboloidTangentVector}, PoincareHalfSpacePoint, PoincareHalfSpaceTangentVector}","content":" Base.convert  ‚Äî  Method convert(::Type{HyperboloidTangentVector}, p::PoincareHalfSpacePoint, X::PoincareHalfSpaceTangentVector)\nconvert(::Type{AbstractVector}, p::PoincareHalfSpacePoint, X::PoincareHalfSpaceTangentVector) convert a point  PoincareHalfSpaceTangentVector X  (from  $‚Ñù^n$ ) at  p  from the Poincar√© half plane model of the  Hyperbolic  manifold  $\\mathcal H^n$  to a  HyperboloidTangentVector $œÄ(p) ‚àà ‚Ñù^{n+1}$ . This is done in two steps, namely transforming it to a Poincare ball point and from there further on to a Hyperboloid point. source"},{"id":1189,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{Tuple{HyperboloidPoint, HyperboloidTangentVector}}, Tuple{PoincareBallPoint, PoincareBallTangentVector}}","content":" Base.convert  ‚Äî  Method convert(\n    ::Type{Tuple{HyperboloidPoint,HyperboloidTangentVector}}.\n    (p,X)::Tuple{PoincareBallPoint,PoincareBallTangentVector}\n)\nconvert(\n    ::Type{Tuple{P,T}},\n    (p, X)::Tuple{PoincareBallPoint,PoincareBallTangentVector},\n) where {P<:AbstractVector, T <: AbstractVector} Convert a  PoincareBallPoint p  and a  PoincareBallTangentVector X  to a  HyperboloidPoint  and a  HyperboloidTangentVector  simultaneously, see  convert(::Type{HyperboloidPoint}, ::PoincareBallPoint)  and  convert(::Type{HyperboloidTangentVector}, ::PoincareBallPoint, ::PoincareBallTangentVector)  for the formulae. source"},{"id":1190,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{Tuple{HyperboloidPoint, HyperboloidTangentVector}}, Tuple{PoincareHalfSpacePoint, PoincareHalfSpaceTangentVector}}","content":" Base.convert  ‚Äî  Method convert(\n    ::Type{Tuple{HyperboloidPoint,HyperboloidTangentVector},\n    (p,X)::Tuple{PoincareHalfSpacePoint, PoincareHalfSpaceTangentVector}\n)\nconvert(\n    ::Type{Tuple{T,T},\n    (p,X)::Tuple{PoincareHalfSpacePoint, PoincareHalfSpaceTangentVector}\n) where {T<:AbstractVector} convert a point  PoincareHalfSpaceTangentVector X  (from  $‚Ñù^n$ ) at  p  from the Poincar√© half plane model of the  Hyperbolic  manifold  $\\mathcal H^n$  to a tuple of a  HyperboloidPoint  and a  HyperboloidTangentVector $œÄ(p) ‚àà ‚Ñù^{n+1}$  simultaneously. This is done in two steps, namely transforming it to the Poincare ball model and from there further on to a Hyperboloid. source"},{"id":1191,"pagetitle":"Hyperbolic space","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldDiff.riemannian_Hessian-Tuple{Hyperbolic, Vararg{Any, 4}}","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method Y = riemannian_Hessian(M::Hyperbolic, p, G, H, X)\nriemannian_Hessian!(M::Hyperbolic, Y, p, G, H, X) Compute the Riemannian Hessian  $\\operatorname{Hess} f(p)[X]$  given the Euclidean gradient  $‚àá f(\\tilde p)$  in  G  and the Euclidean Hessian  $‚àá^2 f(\\tilde p)[\\tilde X]$  in  H , where  $\\tilde p, \\tilde X$  are the representations of  $p,X$  in the embedding,. Let  $\\mathbf{g} = \\mathbf{g}^{-1} = \\operatorname{diag}(1,...,1,-1)$ . Then using Remark 4.1 [ Ngu23 ] the formula reads \\[\\operatorname{Hess}f(p)[X]\n=\n\\operatorname{proj}_{T_p\\mathcal M}\\bigl(\n    \\mathbf{g}^{-1}\\nabla^2f(p)[X] + X‚ü®p,\\mathbf{g}^{-1}‚àáf(p)‚ü©_p\n\\bigr).\\] source"},{"id":1192,"pagetitle":"Hyperbolic space","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/hyperbolic/#Manifolds.volume_density-Tuple{Hyperbolic, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(M::Hyperbolic, p, X) Compute volume density function of the hyperbolic manifold. The formula reads  $(\\sinh(\\lVert X\\rVert)/\\lVert X\\rVert)^(n-1)$  where  n  is the dimension of  M . It is derived from Eq. (4.1) in[ CLLD22 ]. source"},{"id":1193,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.change_representer","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.change_representer-Tuple{Hyperbolic, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(M::Hyperbolic, ::EuclideanMetric, p, X) Change the Eucliden representer  X  of a cotangent vector at point  p . We only have to correct for the metric, which means that the sign of the last entry changes, since for the result  $Y$   we are looking for a tangent vector such that \\[    g_p(Y,Z) = -y_{n+1}z_{n+1} + \\sum_{i=1}^n y_iz_i = \\sum_{i=1}^{n+1} z_ix_i\\] holds, which directly yields  $y_i=x_i$  for  $i=1,\\ldots,n$  and  $y_{n+1}=-x_{n+1}$ . source"},{"id":1194,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.distance-Tuple{Hyperbolic, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::Hyperbolic, p, q)\ndistance(M::Hyperbolic, p::HyperboloidPoint, q::HyperboloidPoint) Compute the distance on the  Hyperbolic M , which reads \\[d_{\\mathcal H^n}(p,q) = \\operatorname{acosh}( - ‚ü®p, q‚ü©_{\\mathrm{M}}),\\] where  $‚ü®‚ãÖ,‚ãÖ‚ü©_{\\mathrm{M}}$  denotes the  MinkowskiMetric  on the embedding, the  Lorentz ian manifold, see for example the extended version [ BPS15 ] of the paper [ BPS16 ]. source"},{"id":1195,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.get_coordinates","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.get_coordinates-Tuple{Hyperbolic, Any, Any, DefaultOrthonormalBasis}","content":" ManifoldsBase.get_coordinates  ‚Äî  Method get_coordinates(M::Hyperbolic, p, X, ::DefaultOrthonormalBasis) Compute the coordinates of the vector  X  with respect to the orthogonalized version of the unit vectors from  $‚Ñù^n$ , where  $n$  is the manifold dimension of the  Hyperbolic M , putting them into the tangent space at  p  and orthonormalizing them. source"},{"id":1196,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.get_vector","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.get_vector-Tuple{Hyperbolic, Any, Any, DefaultOrthonormalBasis}","content":" ManifoldsBase.get_vector  ‚Äî  Method get_vector(M::Hyperbolic, p, c, ::DefaultOrthonormalBasis) Compute the vector from the coordinates with respect to the orthogonalized version of the unit vectors from  $‚Ñù^n$ , where  $n$  is the manifold dimension of the  Hyperbolic M , putting them into the tangent space at  p  and orthonormalizing them. source"},{"id":1197,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.inner-Tuple{Hyperbolic, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::Hyperbolic, p, X, Y)\ninner(M::Hyperbolic, p::HyperboloidPoint, X::HyperboloidTangentVector, Y::HyperboloidTangentVector) Cmpute the inner product in the Hyperboloid model, i.e. the  minkowski_metric  in the embedding. The formula reads \\[g_p(X,Y) = ‚ü®X,Y‚ü©_{\\mathrm{M}} = -X_{n}Y_{n} + \\displaystyle\\sum_{k=1}^{n-1} X_kY_k.\\] This employs the metric of the embedding, see  Lorentz  space, see for example the extended version [ BPS15 ] of the paper [ BPS16 ]. source"},{"id":1198,"pagetitle":"Hyperbolic space","title":"Visualization of the Hyperboloid","ref":"/manifolds/stable/manifolds/hyperbolic/#hyperboloid_plot","content":" Visualization of the Hyperboloid For the case of  Hyperbolic (2)  there is plotting available based on a  PlottingRecipe . You can easily plot points, connecting geodesics as well as tangent vectors. Note The recipes are only loaded if  Plots.jl  or  RecipesBase.jl  is loaded. If we consider a set of points, we can first plot these and their connecting geodesics using the  geodesic_interpolation  for the points. This variable specifies with how many points a geodesic between two successive points is sampled (per default it's  -1 , which deactivates geodesics) and the line style is set to be a path. In general you can plot the surface of the hyperboloid either as wireframe ( wireframe=true ) additionally specifying  wires  (or  wires_x  and  wires_y ) to change the density of wires and a  wireframe_color . The same holds for the plot as a  surface  (which is  false  by default) and its  surface_resolution  (or  surface_resolution_x  or  surface_resolution_y ) and a  surface_color . using Manifolds, Plots\nM = Hyperbolic(2)\npts =  [ [0.85*cos(œÜ), 0.85*sin(œÜ), sqrt(0.85^2+1)] for œÜ ‚àà range(0,2œÄ,length=11) ]\nscene = plot(M, pts; geodesic_interpolation=100) To just plot the points atop, we can just omit the  geodesic_interpolation  parameter to obtain a scatter plot. Note that we avoid redrawing the wireframe in the following  plot!  calls. plot!(scene, M, pts; wireframe=false) We can further generate tangent vectors in these spaces and use a plot for there. Keep in mind that a tangent vector in plotting always requires its base point. pts2 = [ [0.45 .*cos(œÜ + 6œÄ/11), 0.45 .*sin(œÜ + 6œÄ/11), sqrt(0.45^2+1) ] for œÜ ‚àà range(0,2œÄ,length=11)]\nvecs = log.(Ref(M),pts,pts2)\nplot!(scene, M, pts, vecs; wireframe=false) Just to illustrate, for the first point the tangent vector is pointing along the following geodesic plot!(scene, M, [pts[1], pts2[1]]; geodesic_interpolation=100, wireframe=false)"},{"id":1199,"pagetitle":"Hyperbolic space","title":"Internal functions","ref":"/manifolds/stable/manifolds/hyperbolic/#Internal-functions","content":" Internal functions The following functions are available for internal use to construct points in the hyperboloid model"},{"id":1200,"pagetitle":"Hyperbolic space","title":"Manifolds._hyperbolize","ref":"/manifolds/stable/manifolds/hyperbolic/#Manifolds._hyperbolize-Tuple{Hyperbolic, Any, Any}","content":" Manifolds._hyperbolize  ‚Äî  Method _hyperbolize(M, p, Y) Given the  Hyperbolic (n)  manifold using the hyperboloid model and a point  p  thereon, we can put a vector  $Y\\in ‚Ñù^n$   into the tangent space by computing its last component such that for the resulting  p  we have that its  minkowski_metric  is  $‚ü®p,X‚ü©_{\\mathrm{M}} = 0$ , i.e.  $X_{n+1} = \\frac{‚ü®\\tilde p, Y‚ü©}{p_{n+1}}$ , where  $\\tilde p = (p_1,\\ldots,p_n)$ . source"},{"id":1201,"pagetitle":"Hyperbolic space","title":"Manifolds._hyperbolize","ref":"/manifolds/stable/manifolds/hyperbolic/#Manifolds._hyperbolize-Tuple{Hyperbolic, Any}","content":" Manifolds._hyperbolize  ‚Äî  Method _hyperbolize(M, q) Given the  Hyperbolic (n)  manifold using the hyperboloid model, a point from the  $q\\in ‚Ñù^n$  can be set onto the manifold by computing its last component such that for the resulting  p  we have that its  minkowski_metric  is  $‚ü®p,p‚ü©_{\\mathrm{M}} = - 1$ , i.e.  $p_{n+1} = \\sqrt{\\lVert q \\rVert^2 - 1}$ source"},{"id":1202,"pagetitle":"Hyperbolic space","title":"Poincar√© ball model","ref":"/manifolds/stable/manifolds/hyperbolic/#poincare_ball","content":" Poincar√© ball model"},{"id":1203,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{PoincareBallPoint}, Any}","content":" Base.convert  ‚Äî  Method convert(::Type{PoincareBallPoint}, p::HyperboloidPoint)\nconvert(::Type{PoincareBallPoint}, p::T) where {T<:AbstractVector} convert a  HyperboloidPoint $p‚àà‚Ñù^{n+1}$  from the hyperboloid model of the  Hyperbolic  manifold  $\\mathcal H^n$  to a  PoincareBallPoint $œÄ(p)‚àà‚Ñù^{n}$  in the Poincar√© ball model. The isometry is defined by \\[œÄ(p) = \\frac{1}{1+p_{n+1}} \\begin{pmatrix}p_1\\\\‚ãÆ\\\\p_n\\end{pmatrix}\\] Note that this is also used, when  x  is a vector. source"},{"id":1204,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{PoincareBallPoint}, PoincareHalfSpacePoint}","content":" Base.convert  ‚Äî  Method convert(::Type{PoincareBallPoint}, p::PoincareHalfSpacePoint) convert a point  PoincareHalfSpacePoint p  (from  $‚Ñù^n$ ) from the Poincar√© half plane model of the  Hyperbolic  manifold  $\\mathcal H^n$  to a  PoincareBallPoint $œÄ(p) ‚àà ‚Ñù^n$ . Denote by  $\\tilde p = (p_1,\\ldots,p_{d-1})^{\\mathrm{T}}$ . Then the isometry is defined by \\[œÄ(p) = \\frac{1}{\\lVert \\tilde p \\rVert^2 + (p_n+1)^2}\n\\begin{pmatrix}2p_1\\\\‚ãÆ\\\\2p_{n-1}\\\\\\lVert p\\rVert^2 - 1\\end{pmatrix}.\\] source"},{"id":1205,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{PoincareBallTangentVector}, Any}","content":" Base.convert  ‚Äî  Method convert(::Type{PoincareBallTangentVector}, p::HyperboloidPoint, X::HyperboloidTangentVector)\nconvert(::Type{PoincareBallTangentVector}, p::P, X::T) where {P<:AbstractVector, T<:AbstractVector} convert a  HyperboloidTangentVector X  at  p  to a  PoincareBallTangentVector  on the  Hyperbolic  manifold  $\\mathcal H^n$  by computing the push forward  $œÄ_*(p)[X]$  of the isometry  $œÄ$  that maps from the Hyperboloid to the Poincar√© ball, cf.  convert(::Type{PoincareBallPoint}, ::HyperboloidPoint) . The formula reads \\[œÄ_*(p)[X] = \\frac{1}{p_{n+1}+1}\\Bigl(\\tilde X - \\frac{X_{n+1}}{p_{n+1}+1}\\tilde p \\Bigl),\\] where  $\\tilde X = \\begin{pmatrix}X_1\\\\‚ãÆ\\\\X_n\\end{pmatrix}$  and  $\\tilde p = \\begin{pmatrix}p_1\\\\‚ãÆ\\\\p_n\\end{pmatrix}$ . source"},{"id":1206,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{PoincareBallTangentVector}, PoincareHalfSpacePoint, PoincareHalfSpaceTangentVector}","content":" Base.convert  ‚Äî  Method convert(\n    ::Type{PoincareBallTangentVector},\n    p::PoincareHalfSpacePoint,\n    X::PoincareHalfSpaceTangentVector\n) convert a  PoincareHalfSpaceTangentVector X  at  p  to a  PoincareBallTangentVector  on the  Hyperbolic  manifold  $\\mathcal H^n$  by computing the push forward  $œÄ_*(p)[X]$  of the isometry  $œÄ$  that maps from the Poincar√© half space to the Poincar√© ball, cf.  convert(::Type{PoincareBallPoint}, ::PoincareHalfSpacePoint) . The formula reads \\[œÄ_*(p)[X] =\n\\frac{1}{\\lVert \\tilde p\\rVert^2 + (1+p_n)^2}\n\\begin{pmatrix}\n2X_1\\\\\n‚ãÆ\\\\\n2X_{n-1}\\\\\n2‚ü®X,p‚ü©\n\\end{pmatrix}\n-\n\\frac{2}{(\\lVert \\tilde p\\rVert^2 + (1+p_n)^2)^2}\n\\begin{pmatrix}\n2p_1(‚ü®X,p‚ü©+X_n)\\\\\n‚ãÆ\\\\\n2p_{n-1}(‚ü®X,p‚ü©+X_n)\\\\\n(\\lVert p \\rVert^2-1)(‚ü®X,p‚ü©+X_n)\n\\end{pmatrix}\\] where  $\\tilde p = \\begin{pmatrix}p_1\\\\‚ãÆ\\\\p_{n-1}\\end{pmatrix}$ . source"},{"id":1207,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{Tuple{PoincareBallPoint, PoincareBallTangentVector}}, Tuple{HyperboloidPoint, HyperboloidTangentVector}}","content":" Base.convert  ‚Äî  Method convert(\n    ::Type{Tuple{PoincareBallPoint,PoincareBallTangentVector}},\n    (p,X)::Tuple{HyperboloidPoint,HyperboloidTangentVector}\n)\nconvert(\n    ::Type{Tuple{PoincareBallPoint,PoincareBallTangentVector}},\n    (p, X)::Tuple{P,T},\n) where {P<:AbstractVector, T <: AbstractVector} Convert a  HyperboloidPoint p  and a  HyperboloidTangentVector X  to a  PoincareBallPoint  and a  PoincareBallTangentVector  simultaneously, see  convert(::Type{PoincareBallPoint}, ::HyperboloidPoint)  and  convert(::Type{PoincareBallTangentVector}, ::HyperboloidPoint, ::HyperboloidTangentVector)  for the formulae. source"},{"id":1208,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{Tuple{PoincareBallPoint, PoincareBallTangentVector}}, Tuple{PoincareHalfSpacePoint, PoincareHalfSpaceTangentVector}}","content":" Base.convert  ‚Äî  Method convert(\n    ::Type{Tuple{PoincareBallPoint,PoincareBallTangentVector}},\n    (p,X)::Tuple{HyperboloidPoint,HyperboloidTangentVector}\n)\nconvert(\n    ::Type{Tuple{PoincareBallPoint,PoincareBallTangentVector}},\n    (p, X)::Tuple{T,T},\n) where {T <: AbstractVector} Convert a  PoincareHalfSpacePoint p  and a  PoincareHalfSpaceTangentVector X  to a  PoincareBallPoint  and a  PoincareBallTangentVector  simultaneously, see  convert(::Type{PoincareBallPoint}, ::PoincareHalfSpacePoint)  and  convert(::Type{PoincareBallTangentVector}, ::PoincareHalfSpacePoint, ::PoincareHalfSpaceTangentVector)  for the formulae. source"},{"id":1209,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.change_metric","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.change_metric-Tuple{Hyperbolic, EuclideanMetric, PoincareBallPoint, PoincareBallTangentVector}","content":" ManifoldsBase.change_metric  ‚Äî  Method change_metric(M::Hyperbolic, ::EuclideanMetric, p::PoincareBallPoint, X::PoincareBallTangentVector) Since in the metric we always have the term  $Œ± = \\frac{2}{1-\\sum_{i=1}^n p_i^2}$  per element, the correction for the metric reads  $Z = \\frac{1}{Œ±}X$ . source"},{"id":1210,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.change_representer","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.change_representer-Tuple{Hyperbolic, EuclideanMetric, PoincareBallPoint, PoincareBallTangentVector}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(M::Hyperbolic, ::EuclideanMetric, p::PoincareBallPoint, X::PoincareBallTangentVector) Since in the metric we have the term  $Œ± = \\frac{2}{1-\\sum_{i=1}^n p_i^2}$  per element, the correction for the gradient reads  $Y = \\frac{1}{Œ±^2}X$ . source"},{"id":1211,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.distance-Tuple{Hyperbolic, PoincareBallPoint, PoincareBallPoint}","content":" ManifoldsBase.distance  ‚Äî  Method distance(::Hyperbolic, p::PoincareBallPoint, q::PoincareBallPoint) Compute the distance on the  Hyperbolic  manifold  $\\mathcal H^n$  represented in the Poincar√© ball model. The formula reads \\[d_{\\mathcal H^n}(p,q) =\n\\operatorname{acosh}\\Bigl(\n  1 + \\frac{2\\lVert p - q \\rVert^2}{(1-\\lVert p\\rVert^2)(1-\\lVert q\\rVert^2)}\n\\Bigr)\\] source"},{"id":1212,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.inner-Tuple{Hyperbolic, PoincareBallPoint, PoincareBallTangentVector, PoincareBallTangentVector}","content":" ManifoldsBase.inner  ‚Äî  Method inner(::Hyperbolic, p::PoincareBallPoint, X::PoincareBallTangentVector, Y::PoincareBallTangentVector) Compute the inner product in the Poincar√© ball model. The formula reads \\[g_p(X,Y) = \\frac{4}{(1-\\lVert p \\rVert^2)^2}  ‚ü®X, Y‚ü© .\\] source"},{"id":1213,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.project-Tuple{Hyperbolic, PoincareBallPoint, PoincareBallTangentVector}","content":" ManifoldsBase.project  ‚Äî  Method project(::Hyperbolic, ::PoincareBallPoint, ::PoincareBallTangentVector) projection of tangent vectors in the Poincar√© ball model is just the identity, since the tangent space consists of all  $‚Ñù^n$ . source"},{"id":1214,"pagetitle":"Hyperbolic space","title":"Visualization of the Poincar√© ball","ref":"/manifolds/stable/manifolds/hyperbolic/#poincare_ball_plot","content":" Visualization of the Poincar√© ball For the case of  Hyperbolic (2)  there is a plotting available based on a  PlottingRecipe  you can easily plot points, connecting geodesics as well as tangent vectors. Note The recipes are only loaded if  Plots.jl  or  RecipesBase.jl  is loaded. If we consider a set of points, we can first plot these and their connecting geodesics using the  geodesic_interpolation  For the points. This variable specifies with how many points a geodesic between two successive points is sampled (per default it's  -1 , which deactivates geodesics) and the line style is set to be a path. Another keyword argument added is the border of the Poincar√© disc, namely  circle_points = 720  resolution of the drawn boundary (every hlaf angle) as well as its color,  hyperbolic_border_color = RGBA(0.0, 0.0, 0.0, 1.0) . using Manifolds, Plots\nM = Hyperbolic(2)\npts = PoincareBallPoint.( [0.85 .* [cos(œÜ), sin(œÜ)] for œÜ ‚àà range(0,2œÄ,length=11)])\nscene = plot(M, pts, geodesic_interpolation = 100) To just plot the points atop, we can just omit the  geodesic_interpolation  parameter to obtain a scatter plot plot!(scene, M, pts) We can further generate tangent vectors in these spaces and use a plot for there. Keep in mind, that a tangent vector in plotting always requires its base point pts2 = PoincareBallPoint.( [0.45 .* [cos(œÜ + 6œÄ/11), sin(œÜ + 6œÄ/11)] for œÜ ‚àà range(0,2œÄ,length=11)])\nvecs = log.(Ref(M),pts,pts2)\nplot!(scene, M, pts,vecs) Just to illustrate, for the first point the tangent vector is pointing along the following geodesic plot!(scene, M, [pts[1], pts2[1]], geodesic_interpolation=100)"},{"id":1215,"pagetitle":"Hyperbolic space","title":"Poincar√© half space model","ref":"/manifolds/stable/manifolds/hyperbolic/#poincare_halfspace","content":" Poincar√© half space model"},{"id":1216,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{PoincareHalfSpacePoint}, Any}","content":" Base.convert  ‚Äî  Method convert(::Type{PoincareHalfSpacePoint}, p::Hyperboloid)\nconvert(::Type{PoincareHalfSpacePoint}, p) convert a  HyperboloidPoint  or  Vector p  (from  $‚Ñù^{n+1}$ ) from the Hyperboloid model of the  Hyperbolic  manifold  $\\mathcal H^n$  to a  PoincareHalfSpacePoint $œÄ(x) ‚àà ‚Ñù^{n}$ . This is done in two steps, namely transforming it to a Poincare ball point and from there further on to a PoincareHalfSpacePoint point. source"},{"id":1217,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{PoincareHalfSpacePoint}, PoincareBallPoint}","content":" Base.convert  ‚Äî  Method convert(::Type{PoincareHalfSpacePoint}, p::PoincareBallPoint) convert a point  PoincareBallPoint p  (from  $‚Ñù^n$ ) from the Poincar√© ball model of the  Hyperbolic  manifold  $\\mathcal H^n$  to a  PoincareHalfSpacePoint $œÄ(p) ‚àà ‚Ñù^n$ . Denote by  $\\tilde p = (p_1,\\ldots,p_{n-1})$ . Then the isometry is defined by \\[œÄ(p) = \\frac{1}{\\lVert \\tilde p \\rVert^2 - (p_n-1)^2}\n\\begin{pmatrix}2p_1\\\\‚ãÆ\\\\2p_{n-1}\\\\1-\\lVert p\\rVert^2\\end{pmatrix}.\\] source"},{"id":1218,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{PoincareHalfSpaceTangentVector}, Any}","content":" Base.convert  ‚Äî  Method convert(::Type{PoincareHalfSpaceTangentVector}, p::HyperboloidPoint, ::HyperboloidTangentVector)\nconvert(::Type{PoincareHalfSpaceTangentVector}, p::P, X::T) where {P<:AbstractVector, T<:AbstractVector} convert a  HyperboloidTangentVector X  at  p  to a  PoincareHalfSpaceTangentVector  on the  Hyperbolic  manifold  $\\mathcal H^n$  by computing the push forward  $œÄ_*(p)[X]$  of the isometry  $œÄ$  that maps from the Hyperboloid to the Poincar√© half space, cf.  convert(::Type{PoincareHalfSpacePoint}, ::HyperboloidPoint) . This is done similarly to the approach there, i.e. by using the Poincar√© ball model as an intermediate step. source"},{"id":1219,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{PoincareHalfSpaceTangentVector}, PoincareBallPoint, PoincareBallTangentVector}","content":" Base.convert  ‚Äî  Method convert(::Type{PoincareHalfSpaceTangentVector}, p::PoincareBallPoint, X::PoincareBallTangentVector) convert a  PoincareBallTangentVector X  at  p  to a  PoincareHalfSpacePoint  on the  Hyperbolic  manifold  $\\mathcal H^n$  by computing the push forward  $œÄ_*(p)[X]$  of the isometry  $œÄ$  that maps from the Poincar√© ball to the Poincar√© half space, cf.  convert(::Type{PoincareHalfSpacePoint}, ::PoincareBallPoint) . The formula reads \\[œÄ_*(p)[X] =\n\\frac{1}{\\lVert \\tilde p\\rVert^2 + (1-p_n)^2}\n\\begin{pmatrix}\n2X_1\\\\\n‚ãÆ\\\\\n2X_{n-1}\\\\\n-2‚ü®X,p‚ü©\n\\end{pmatrix}\n-\n\\frac{2}{(\\lVert \\tilde p\\rVert^2 + (1-p_n)^2)^2}\n\\begin{pmatrix}\n2p_1(‚ü®X,p‚ü©-X_n)\\\\\n‚ãÆ\\\\\n2p_{n-1}(‚ü®X,p‚ü©-X_n)\\\\\n(\\lVert p \\rVert^2-1)(‚ü®X,p‚ü©-X_n)\n\\end{pmatrix}\\] where  $\\tilde p = \\begin{pmatrix}p_1\\\\‚ãÆ\\\\p_{n-1}\\end{pmatrix}$ . source"},{"id":1220,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{Tuple{PoincareHalfSpacePoint, PoincareHalfSpaceTangentVector}}, Tuple{HyperboloidPoint, HyperboloidTangentVector}}","content":" Base.convert  ‚Äî  Method convert(\n    ::Type{Tuple{PoincareHalfSpacePoint,PoincareHalfSpaceTangentVector}},\n    (p,X)::Tuple{HyperboloidPoint,HyperboloidTangentVector}\n)\nconvert(\n    ::Type{Tuple{PoincareHalfSpacePoint,PoincareHalfSpaceTangentVector}},\n    (p, X)::Tuple{P,T},\n) where {P<:AbstractVector, T <: AbstractVector} Convert a  HyperboloidPoint p  and a  HyperboloidTangentVector X  to a  PoincareHalfSpacePoint  and a  PoincareHalfSpaceTangentVector  simultaneously, see  convert(::Type{PoincareHalfSpacePoint}, ::HyperboloidPoint)  and  convert(::Type{PoincareHalfSpaceTangentVector}, ::Tuple{HyperboloidPoint,HyperboloidTangentVector})  for the formulae. source"},{"id":1221,"pagetitle":"Hyperbolic space","title":"Base.convert","ref":"/manifolds/stable/manifolds/hyperbolic/#Base.convert-Tuple{Type{Tuple{PoincareHalfSpacePoint, PoincareHalfSpaceTangentVector}}, Tuple{PoincareBallPoint, PoincareBallTangentVector}}","content":" Base.convert  ‚Äî  Method convert(\n    ::Type{Tuple{PoincareHalfSpacePoint,PoincareHalfSpaceTangentVector}},\n    (p,X)::Tuple{PoincareBallPoint,PoincareBallTangentVector}\n) Convert a  PoincareBallPoint p  and a  PoincareBallTangentVector X  to a  PoincareHalfSpacePoint  and a  PoincareHalfSpaceTangentVector  simultaneously, see  convert(::Type{PoincareHalfSpacePoint}, ::PoincareBallPoint)  and  convert(::Type{PoincareHalfSpaceTangentVector}, ::PoincareBallPoint,::PoincareBallTangentVector)  for the formulae. source"},{"id":1222,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.distance-Tuple{Hyperbolic, PoincareHalfSpacePoint, PoincareHalfSpacePoint}","content":" ManifoldsBase.distance  ‚Äî  Method distance(::Hyperbolic, p::PoincareHalfSpacePoint, q::PoincareHalfSpacePoint) Compute the distance on the  Hyperbolic  manifold  $\\mathcal H^n$  represented in the Poincar√© half space model. The formula reads \\[d_{\\mathcal H^n}(p,q) = \\operatorname{acosh}\\Bigl( 1 + \\frac{\\lVert p - q \\rVert^2}{2 p_n q_n} \\Bigr)\\] source"},{"id":1223,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.inner-Tuple{Hyperbolic, PoincareHalfSpacePoint, PoincareHalfSpaceTangentVector, PoincareHalfSpaceTangentVector}","content":" ManifoldsBase.inner  ‚Äî  Method inner(\n    ::Hyperbolic,\n    p::PoincareHalfSpacePoint,\n    X::PoincareHalfSpaceTangentVector,\n    Y::PoincareHalfSpaceTangentVector\n) Compute the inner product in the Poincar√© half space model. The formula reads \\[g_p(X,Y) = \\frac{‚ü®X,Y‚ü©}{p_n^2}.\\] source"},{"id":1224,"pagetitle":"Hyperbolic space","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/hyperbolic/#ManifoldsBase.project-Tuple{Hyperbolic, PoincareHalfSpaceTangentVector}","content":" ManifoldsBase.project  ‚Äî  Method project(::Hyperbolic, ::PoincareHalfSpacePoint ::PoincareHalfSpaceTangentVector) projection of tangent vectors in the Poincar√© half space model is just the identity, since the tangent space consists of all  $‚Ñù^n$ . source"},{"id":1225,"pagetitle":"Hyperbolic space","title":"Visualization on the Poincar√© half plane","ref":"/manifolds/stable/manifolds/hyperbolic/#poincare_half_plane_plot","content":" Visualization on the Poincar√© half plane For the case of  Hyperbolic (2)  there is a plotting available based on a  PlottingRecipe  you can easily plot points, connecting geodesics as well as tangent vectors. Note The recipes are only loaded if  Plots.jl  or  RecipesBase.jl  is loaded. We again have two different recipes, one for points, one for tangent vectors, where the first one again can be equipped with geodesics between the points. In the following example we generate 7 points on an ellipse in the  Hyperboloid model . using Manifolds, Plots\nM = Hyperbolic(2)\npre_pts = [2.0 .* [5.0*cos(œÜ), sin(œÜ)] for œÜ ‚àà range(0,2œÄ,length=7)]\npts = convert.(\n    Ref(PoincareHalfSpacePoint),\n    Manifolds._hyperbolize.(Ref(M), pre_pts)\n)\nscene = plot(M, pts, geodesic_interpolation = 100) To just plot the points atop, we can just omit the  geodesic_interpolation  parameter to obtain a scatter plot plot!(scene, M, pts) We can further generate tangent vectors in these spaces and use a plot for there. Keep in mind, that a tangent vector in plotting always requires its base point. Here we would like to look at the tangent vectors pointing to the  origin origin = PoincareHalfSpacePoint([0.0,1.0])\nvecs = [log(M,p,origin) for p ‚àà pts]\nscene = plot!(scene, M, pts, vecs) And we can again look at the corresponding geodesics, for example plot!(scene, M, [pts[1], origin], geodesic_interpolation=100)\nplot!(scene, M, [pts[2], origin], geodesic_interpolation=100)"},{"id":1226,"pagetitle":"Hyperbolic space","title":"Literature","ref":"/manifolds/stable/manifolds/hyperbolic/#Literature","content":" Literature [BPS15] R.¬†Bergmann, J.¬†Persch and G.¬†Steidl.  A parallel Douglas Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds , arXiv¬†Preprint (2015),  arXiv:1512.02814 . [BPS16] R.¬†Bergmann, J.¬†Persch and G.¬†Steidl.  A parallel Douglas Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  9 , 901‚Äì937  (2016). [CLLD22] E.¬†Chevallier, D.¬†Li, Y.¬†Lu and D.¬†B.¬†Dunson.  Exponential-wrapped distributions on symmetric spaces . ArXiv¬†Preprint (2022). [Lee19] J.¬†M.¬†Lee.  Introduction to Riemannian Manifolds  (Springer Cham, 2019). [Ngu23] D.¬†Nguyen.  Operator-Valued Formulas for Riemannian Gradient and Hessian and Families of Tractable Metrics in Riemannian Optimization .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  198 , 135‚Äì164  (2023),  arXiv:2009.10159 ."},{"id":1229,"pagetitle":"Hyperrectangle","title":"Hyperrectangle","ref":"/manifolds/stable/manifolds/hyperrectangle/#HyperrectangleSection","content":" Hyperrectangle Hyperrectangle is a manifold with corners [ Joy10 ], and also a subset of the real  Euclidean  manifold. It is useful for box-constrained optimization, for example it is implicitly used in the classic L-BFGS-B algorithm. Note This is a manifold with corners. Some parts of its interface specific to this property are experimental and may change without a breaking release."},{"id":1230,"pagetitle":"Hyperrectangle","title":"Manifolds.Hyperrectangle","ref":"/manifolds/stable/manifolds/hyperrectangle/#Manifolds.Hyperrectangle","content":" Manifolds.Hyperrectangle  ‚Äî  Type Hyperrectangle{T} <: AbstractManifold{‚Ñù} Hyperrectangle, also known as orthotope or box. This is a manifold with corners [ Joy10 ] with the standard Euclidean metric. Constructor Hyperrectangle(lb::AbstractArray, ub::AbstractArray) Generate the hyperrectangle of arrays such that each element of the array is between lower and upper bound with the same index. source"},{"id":1231,"pagetitle":"Hyperrectangle","title":"Base.exp","ref":"/manifolds/stable/manifolds/hyperrectangle/#Base.exp-Tuple{Hyperrectangle, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::Hyperrectangle, p, X) Compute the exponential map on the  Hyperrectangle  manifold  M  from  p  in direction  X , which in this case is just \\[\\exp_p X = p + X.\\] source"},{"id":1232,"pagetitle":"Hyperrectangle","title":"Base.log","ref":"/manifolds/stable/manifolds/hyperrectangle/#Base.log-Tuple{Hyperrectangle, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::Hyperrectangle, p, q) Compute the logarithmic map on the  Hyperrectangle M  from  p  to  q , which in this case is just \\[\\log_p q = q-p.\\] source"},{"id":1233,"pagetitle":"Hyperrectangle","title":"LinearAlgebra.norm","ref":"/manifolds/stable/manifolds/hyperrectangle/#LinearAlgebra.norm-Tuple{Hyperrectangle, Any, Any}","content":" LinearAlgebra.norm  ‚Äî  Method norm(M::Hyperrectangle, p, X) Compute the norm of a tangent vector  X  at  p  on the  Hyperrectangle M , i.e. since every tangent space can be identified with  M  itself in this case, just the (Frobenius) norm of  X . source"},{"id":1234,"pagetitle":"Hyperrectangle","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/hyperrectangle/#Manifolds.manifold_volume-Tuple{Hyperrectangle}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(::Hyperrectangle) Return volume of the  Hyperrectangle  manifold, i.e. infinity. source"},{"id":1235,"pagetitle":"Hyperrectangle","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/hyperrectangle/#Manifolds.volume_density-Tuple{Hyperrectangle, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(M::Hyperrectangle, p, X) Return volume density function of  Hyperrectangle  manifold  M , i.e. 1. source"},{"id":1236,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.Weingarten","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.Weingarten-Tuple{Hyperrectangle, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Y = Weingarten(M::Hyperrectangle, p, X, V)\nWeingarten!(M::Hyperrectangle, Y, p, X, V) Compute the Weingarten map  $\\mathcal W_p$  at  p  on the  Hyperrectangle M  with respect to the tangent vector  $X \\in T_p\\mathcal M$  and the normal vector  $V \\in N_p\\mathcal M$ . Since this a flat space by itself, the result is always the zero tangent vector. source"},{"id":1237,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.default_retraction_method","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.default_retraction_method-Tuple{Hyperrectangle}","content":" ManifoldsBase.default_retraction_method  ‚Äî  Method default_retraction_method(M::Hyperrectangle) Return  ProjectionRetraction  as the default retraction for the  Hyperrectangle  manifold. source"},{"id":1238,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.distance-Tuple{Hyperrectangle, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::Hyperrectangle, p, q) Compute the euclidean distance between two points on the  Hyperrectangle  manifold  M , i.e. for vectors it's just the norm of the difference, for matrices and higher order arrays, the matrix and tensor Frobenius norm, respectively. source"},{"id":1239,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.embed-Tuple{Hyperrectangle, Any, Any}","content":" ManifoldsBase.embed  ‚Äî  Method embed(M::Hyperrectangle, p, X) Embed the tangent vector  X  at point  p  in  M . Equivalent to an identity map. source"},{"id":1240,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.embed-Tuple{Hyperrectangle, Any}","content":" ManifoldsBase.embed  ‚Äî  Method embed(M::Hyperrectangle, p) Embed the point  p  in  M . Equivalent to an identity map. source"},{"id":1241,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.injectivity_radius-Tuple{Hyperrectangle, Any}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::Hyperrectangle, p) Return the injectivity radius on the  Hyperrectangle M  at point  p , which is the distance to the nearest boundary the point is not on. source"},{"id":1242,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.inner-Tuple{Hyperrectangle, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::Hyperrectangle, p, X, Y) Compute the inner product on the  Hyperrectangle M , which is just the inner product on the real-valued vector space of arrays (or tensors) of size  $n_1 √ó n_2  √ó  ‚Ä¶  √ó n_i$ , i.e. \\[g_p(X,Y) = \\sum_{k ‚àà I} X_{k} Y_{k},\\] where  $I$  is the set of vectors  $k ‚àà ‚Ñï^i$ , such that for all $i ‚â§ j ‚â§ i$  it holds  $1 ‚â§ k_j ‚â§ n_j$ . For the special case of  $i ‚â§ 2$ , i.e. matrices and vectors, this simplifies to \\[g_p(X,Y) = \\operatorname{tr}(X^{\\mathrm{T}}Y),\\] where  $‚ãÖ^{\\mathrm{T}}$  denotes transposition. source"},{"id":1243,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.is_flat-Tuple{Hyperrectangle}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::Hyperrectangle) Return true.  Hyperrectangle  is a flat manifold. source"},{"id":1244,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.manifold_dimension-Tuple{Hyperrectangle}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::Hyperrectangle) Return the manifold dimension of the  Hyperrectangle M , i.e. the product of all array dimensions. source"},{"id":1245,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.parallel_transport_direction","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.parallel_transport_direction-Tuple{Hyperrectangle, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_direction  ‚Äî  Method parallel_transport_direction(M::Hyperrectangle, p, X, d) the parallel transport on  Hyperrectangle  is the identity, i.e. returns  X . source"},{"id":1246,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.parallel_transport_to-Tuple{Hyperrectangle, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::Hyperrectangle, p, X, q) the parallel transport on  Hyperrectangle  is the identity, i.e. returns  X . source"},{"id":1247,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.project-Tuple{Hyperrectangle, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Hyperrectangle, p, X) Project an arbitrary vector  X  into the tangent space of a point  p  on the  Hyperrectangle M , which is just the identity, since any tangent space of  M  can be identified with all of  M . source"},{"id":1248,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.project-Tuple{Hyperrectangle, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Hyperrectangle, p) Project an arbitrary point  p  onto the  Hyperrectangle  manifold  M , which is of course just the identity map. source"},{"id":1249,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.representation_size-Tuple{Hyperrectangle}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::Hyperrectangle) Return the array dimensions required to represent an element on the  Hyperrectangle M , i.e. the vector of all array dimensions. source"},{"id":1250,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.riemann_tensor","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.riemann_tensor-Tuple{Hyperrectangle, Vararg{Any, 4}}","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(M::Hyperrectangle, p, X, Y, Z) Compute the Riemann tensor  $R(X,Y)Z$  at point  p  on  Hyperrectangle  manifold  M . Its value is always the zero tangent vector. source"},{"id":1251,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.sectional_curvature","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.sectional_curvature-Tuple{Hyperrectangle, Any, Any, Any}","content":" ManifoldsBase.sectional_curvature  ‚Äî  Method sectional_curvature(::Hyperrectangle, p, X, Y) Sectional curvature of  Hyperrectangle  manifold  M  is 0. source"},{"id":1252,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.sectional_curvature_max","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.sectional_curvature_max-Tuple{Hyperrectangle}","content":" ManifoldsBase.sectional_curvature_max  ‚Äî  Method sectional_curvature_max(::Hyperrectangle) Sectional curvature of  Hyperrectangle  manifold  M  is 0. source"},{"id":1253,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.sectional_curvature_min","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.sectional_curvature_min-Tuple{Hyperrectangle}","content":" ManifoldsBase.sectional_curvature_min  ‚Äî  Method sectional_curvature_min(M::Hyperrectangle) Sectional curvature of  Hyperrectangle  manifold  M  is 0. source"},{"id":1254,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.vector_transport_to","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.vector_transport_to-Tuple{Hyperrectangle, Any, Any, Any, AbstractVectorTransportMethod}","content":" ManifoldsBase.vector_transport_to  ‚Äî  Method vector_transport_to(M::Hyperrectangle, p, X, q, ::AbstractVectorTransportMethod) Transport the vector  X  from the tangent space at  p  to the tangent space at  q  on the  Hyperrectangle M , which simplifies to the identity. source"},{"id":1255,"pagetitle":"Hyperrectangle","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/hyperrectangle/#ManifoldsBase.zero_vector-Tuple{Hyperrectangle, Vararg{Any}}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(M::Hyperrectangle, p) Return the zero vector in the tangent space of  p  on the  Hyperrectangle M , which here is just a zero filled array the same size as  p . source"},{"id":1256,"pagetitle":"Hyperrectangle","title":"Literature","ref":"/manifolds/stable/manifolds/hyperrectangle/#Literature","content":" Literature [Joy10] D.¬†Joyce.  On manifolds with corners  (2010),  arXiv:0910.3518 ."},{"id":1259,"pagetitle":"Invertible matrices","title":"Invertible matrices","ref":"/manifolds/stable/manifolds/invertible/#Invertible-matrices","content":" Invertible matrices"},{"id":1260,"pagetitle":"Invertible matrices","title":"Manifolds.InvertibleMatrices","ref":"/manifolds/stable/manifolds/invertible/#Manifolds.InvertibleMatrices","content":" Manifolds.InvertibleMatrices  ‚Äî  Type InvertibleMatrices{ùîΩ,T} <: AbstractDecoratorManifold{ùîΩ} The  AbstractManifold  consisting of the real- or complex-valued invertible matrices, that is the set \\[\\bigl\\{p  ‚àà ùîΩ^{n√ón}\\ \\big|\\ \\det(p) \\neq 0 \\bigr\\},\\] where the field  $ùîΩ ‚àà \\{ ‚Ñù, ‚ÑÇ\\}$ . Constructor InvertibleMatrices(n::Int, field::AbstractNumbers=‚Ñù) Generate the manifold of  $n√ón$  invertible matrices. source"},{"id":1261,"pagetitle":"Invertible matrices","title":"Base.rand","ref":"/manifolds/stable/manifolds/invertible/#Base.rand-Tuple{InvertibleMatrices}","content":" Base.rand  ‚Äî  Method Random.rand(M::InvertibleMatrices; vector_at=nothing, kwargs...) If  vector_at  is  nothing , return a random point on the  InvertibleMatrices  manifold  M  by using  rand  in the embedding. If  vector_at  is not  nothing , return a random tangent vector from the tangent space of the point  vector_at  on the  InvertibleMatrices  by using by using  rand  in the embedding. source"},{"id":1262,"pagetitle":"Invertible matrices","title":"ManifoldsBase.Weingarten","ref":"/manifolds/stable/manifolds/invertible/#ManifoldsBase.Weingarten-Tuple{InvertibleMatrices, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Y = Weingarten(M::InvertibleMatrices, p, X, V)\nWeingarten!(M::InvertibleMatrices, Y, p, X, V) Compute the Weingarten map  $\\mathcal W_p$  at  p  on the  InvertibleMatrices M  with respect to the tangent vector  $X \\in T_p\\mathcal M$  and the normal vector  $V \\in N_p\\mathcal M$ . Since this a flat space by itself, the result is always the zero tangent vector. source"},{"id":1263,"pagetitle":"Invertible matrices","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/invertible/#ManifoldsBase.check_point-Tuple{InvertibleMatrices, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::InvertibleMatrices{n,ùîΩ}, p; kwargs...) Check whether  p  is a valid manifold point on the  InvertibleMatrices M , i.e. whether  p  is an invertible matrix of size  (n,n)  with values from the corresponding  AbstractNumbers ùîΩ . source"},{"id":1264,"pagetitle":"Invertible matrices","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/invertible/#ManifoldsBase.check_vector-Tuple{InvertibleMatrices, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::InvertibleMatrices{n,ùîΩ}, p, X; kwargs... ) Check whether  X  is a tangent vector to manifold point  p  on the  InvertibleMatrices M , which are all matrices of size  $n√ón$  its values have to be from the correct  AbstractNumbers . source"},{"id":1265,"pagetitle":"Invertible matrices","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/invertible/#ManifoldsBase.is_flat-Tuple{InvertibleMatrices}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::InvertibleMatrices) Return true.  InvertibleMatrices  is a flat manifold. source"},{"id":1266,"pagetitle":"Invertible matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/invertible/#ManifoldsBase.manifold_dimension-Union{Tuple{InvertibleMatrices{<:Any, ùîΩ}}, Tuple{ùîΩ}} where ùîΩ","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::InvertibleMatrices{n,ùîΩ}) Return the dimension of the  InvertibleMatrices  matrix  M  over the number system  ùîΩ , which is the same dimension as its embedding, the  Euclidean (n, n; field=ùîΩ) . source"},{"id":1269,"pagetitle":"Lorentzian manifold","title":"Lorentzian Manifold","ref":"/manifolds/stable/manifolds/lorentz/#Lorentzian-Manifold","content":" Lorentzian Manifold The  Lorentz manifold  is a  pseudo-Riemannian manifold . It is named after the Dutch physicist  Hendrik Lorentz  (1853‚Äì1928). The default  LorentzMetric  is the  MinkowskiMetric  named after the German mathematician  Hermann Minkowski  (1864‚Äì1909). Within  Manifolds.jl  it is used as the embedding of the  Hyperbolic  space."},{"id":1270,"pagetitle":"Lorentzian manifold","title":"Manifolds.Lorentz","ref":"/manifolds/stable/manifolds/lorentz/#Manifolds.Lorentz","content":" Manifolds.Lorentz  ‚Äî  Type Lorentz{T} = MetricManifold{Euclidean{T,‚Ñù},LorentzMetric} The Lorentz manifold (or Lorentzian) is a pseudo-Riemannian manifold. Constructor Lorentz(n[, metric=MinkowskiMetric()]) Generate the Lorentz manifold of dimension  n  with the  LorentzMetric m , which is by default set to the  MinkowskiMetric . source"},{"id":1271,"pagetitle":"Lorentzian manifold","title":"Manifolds.LorentzMetric","ref":"/manifolds/stable/manifolds/lorentz/#Manifolds.LorentzMetric","content":" Manifolds.LorentzMetric  ‚Äî  Type LorentzMetric <: AbstractMetric Abstract type for Lorentz metrics, which have a single time dimension. These metrics assume the spacelike convention with the time dimension being last, giving the signature  $(++...+-)$ . source"},{"id":1272,"pagetitle":"Lorentzian manifold","title":"Manifolds.MinkowskiMetric","ref":"/manifolds/stable/manifolds/lorentz/#Manifolds.MinkowskiMetric","content":" Manifolds.MinkowskiMetric  ‚Äî  Type MinkowskiMetric <: LorentzMetric As a special metric of signature   $(++...+-)$ , i.e. a  LorentzMetric , see  minkowski_metric  for the formula. source"},{"id":1273,"pagetitle":"Lorentzian manifold","title":"Manifolds.minkowski_metric","ref":"/manifolds/stable/manifolds/lorentz/#Manifolds.minkowski_metric-Tuple{Any, Any}","content":" Manifolds.minkowski_metric  ‚Äî  Method minkowski_metric(a, b) Compute the minkowski metric on  $\\mathbb R^n$  is given by \\[‚ü®a,b‚ü©_{\\mathrm{M}} = -a_{n}b_{n} +\n\\displaystyle\\sum_{k=1}^{n-1} a_kb_k.\\] source"},{"id":1276,"pagetitle":"Metric manifold","title":"Metric manifold","ref":"/manifolds/stable/manifolds/metric/#Metric-manifold","content":" Metric manifold A Riemannian manifold always consists of a  topological manifold  together with a smoothly varying metric  $g$ . However, often there is an implicitly assumed (default) metric, like the usual inner product on  Euclidean  space. This decorator takes this into account. It is not necessary to use this decorator if you implement just one (or the first) metric. If you later introduce a second, the old (first) metric can be used with the (non  MetricManifold )  AbstractManifold , i.e. without an explicitly stated metric. This manifold decorator serves two purposes: to implement different metrics (e.g. in closed form) for one  AbstractManifold to provide a way to compute geodesics on manifolds, where this  AbstractMetric  does not yield closed formula. Metric manifold Types Implement Different Metrics on the same Manifold Implementation of Metrics Metrics, charts and bases of vector spaces Note that a metric manifold is has a  IsConnectionManifold  trait referring to the  LeviCivitaConnection  of the metric  $g$ , and thus a large part of metric manifold's functionality relies on this. Let's first look at the provided types."},{"id":1277,"pagetitle":"Metric manifold","title":"Types","ref":"/manifolds/stable/manifolds/metric/#Types","content":" Types"},{"id":1278,"pagetitle":"Metric manifold","title":"Manifolds.IsDefaultMetric","ref":"/manifolds/stable/manifolds/metric/#Manifolds.IsDefaultMetric","content":" Manifolds.IsDefaultMetric  ‚Äî  Type IsDefaultMetric{G<:AbstractMetric} Specify that a certain  AbstractMetric  is the default metric for a manifold. This way the corresponding  MetricManifold  falls back to the default methods of the manifold it decorates. source"},{"id":1279,"pagetitle":"Metric manifold","title":"Manifolds.IsMetricManifold","ref":"/manifolds/stable/manifolds/metric/#Manifolds.IsMetricManifold","content":" Manifolds.IsMetricManifold  ‚Äî  Type IsMetricManifold <: AbstractTrait Specify that a certain decorated Manifold is a metric manifold in the sence that it provides explicit metric properties, extending/changing the default metric properties of a manifold. source"},{"id":1280,"pagetitle":"Metric manifold","title":"Manifolds.MetricManifold","ref":"/manifolds/stable/manifolds/metric/#Manifolds.MetricManifold","content":" Manifolds.MetricManifold  ‚Äî  Type MetricManifold{ùîΩ,M<:AbstractManifold{ùîΩ},G<:AbstractMetric} <: AbstractDecoratorManifold{ùîΩ} Equip a  AbstractManifold  explicitly with an  AbstractMetric G . For a Metric AbstractManifold, by default, assumes, that you implement the linear form from  local_metric  in order to evaluate the exponential map. If the corresponding  AbstractMetric G  yields closed form formulae for e.g. the exponential map and this is implemented directly (without solving the ode), you can of course still implement that directly. Constructor MetricManifold(M, G) Generate the  AbstractManifold M  as a manifold with the  AbstractMetric G . source"},{"id":1281,"pagetitle":"Metric manifold","title":"Implement Different Metrics on the same Manifold","ref":"/manifolds/stable/manifolds/metric/#Implement-Different-Metrics-on-the-same-Manifold","content":" Implement Different Metrics on the same Manifold In order to distinguish different metrics on one manifold, one can introduce two  AbstractMetric s and use this type to dispatch on the metric, see  SymmetricPositiveDefinite . To avoid overhead, one  AbstractMetric  can then be marked as being the default, i.e. the one that is used, when no  MetricManifold  decorator is present. This avoids reimplementation of the first existing metric, access to the metric-dependent functions that were implemented using the undecorated manifold, as well as the transparent fallback of the corresponding  MetricManifold  with default metric to the undecorated implementations. This does not cause any runtime overhead. Introducing a default  AbstractMetric  serves a better readability of the code when working with different metrics."},{"id":1282,"pagetitle":"Metric manifold","title":"Implementation of Metrics","ref":"/manifolds/stable/manifolds/metric/#Implementation-of-Metrics","content":" Implementation of Metrics For the case that a  local_metric  is implemented as a bilinear form that is positive definite, the following further functions are provided, unless the corresponding  AbstractMetric  is marked as default ‚Äì then the fallbacks mentioned in the last section are used for e.g. the exponential map."},{"id":1283,"pagetitle":"Metric manifold","title":"Base.log","ref":"/manifolds/stable/manifolds/metric/#Base.log-Tuple{MetricManifold, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(N::MetricManifold{M,G}, p, q) Compute the logarithmic map on the  AbstractManifold M  equipped with the  AbstractMetric G . If the metric was declared the default metric using the  IsDefaultMetric  trait or  is_default_metric , this method falls back to  log(M,p,q) . Otherwise, you have to provide an implementation for the non-default  AbstractMetric G  metric within its  MetricManifold {M,G} . source"},{"id":1284,"pagetitle":"Metric manifold","title":"Manifolds.connection","ref":"/manifolds/stable/manifolds/metric/#Manifolds.connection-Tuple{MetricManifold}","content":" Manifolds.connection  ‚Äî  Method connection(::MetricManifold) Return the  LeviCivitaConnection  for a metric manifold. source"},{"id":1285,"pagetitle":"Metric manifold","title":"Manifolds.det_local_metric","ref":"/manifolds/stable/manifolds/metric/#Manifolds.det_local_metric-Tuple{AbstractManifold, Any, AbstractBasis}","content":" Manifolds.det_local_metric  ‚Äî  Method det_local_metric(M::AbstractManifold, p, B::AbstractBasis) Return the determinant of local matrix representation of the metric tensor  $g$ , i.e. of the matrix  $G(p)$  representing the metric in the tangent space at  $p$  with as a matrix. See also  local_metric source"},{"id":1286,"pagetitle":"Metric manifold","title":"Manifolds.einstein_tensor","ref":"/manifolds/stable/manifolds/metric/#Manifolds.einstein_tensor-Tuple{AbstractManifold, Any, AbstractBasis}","content":" Manifolds.einstein_tensor  ‚Äî  Method einstein_tensor(M::AbstractManifold, p, B::AbstractBasis; backend::AbstractDiffBackend = diff_badefault_differential_backendckend()) Compute the Einstein tensor of the manifold  M  at the point  p , see  https://en.wikipedia.org/wiki/Einstein_tensor source"},{"id":1287,"pagetitle":"Metric manifold","title":"Manifolds.flat","ref":"/manifolds/stable/manifolds/metric/#Manifolds.flat-Tuple{MetricManifold, Any, TFVector}","content":" Manifolds.flat  ‚Äî  Method flat(N::MetricManifold{M,G}, p, X::TFVector) Compute the musical isomorphism to transform the tangent vector  X  from the  AbstractManifold M  equipped with  AbstractMetric G  to a cotangent by computing \\[X^‚ô≠= G_p X,\\] where  $G_p$  is the local matrix representation of  G , see  local_metric source"},{"id":1288,"pagetitle":"Metric manifold","title":"Manifolds.inverse_local_metric","ref":"/manifolds/stable/manifolds/metric/#Manifolds.inverse_local_metric-Tuple{AbstractManifold, Any, AbstractBasis}","content":" Manifolds.inverse_local_metric  ‚Äî  Method inverse_local_metric(M::AbstractcManifold{ùîΩ}, p, B::AbstractBasis) Return the local matrix representation of the inverse metric (cometric) tensor of the tangent space at  p  on the  AbstractManifold M  with respect to the  AbstractBasis  basis  B . The metric tensor (see  local_metric ) is usually denoted by  $G = (g_{ij}) ‚àà ùîΩ^{d√ód}$ , where  $d$  is the dimension of the manifold. Then the inverse local metric is denoted by  $G^{-1} = g^{ij}$ . source"},{"id":1289,"pagetitle":"Metric manifold","title":"Manifolds.is_default_metric","ref":"/manifolds/stable/manifolds/metric/#Manifolds.is_default_metric-Tuple{AbstractManifold, AbstractMetric}","content":" Manifolds.is_default_metric  ‚Äî  Method is_default_metric(M::AbstractManifold, G::AbstractMetric) returns whether an  AbstractMetric  is the default metric on the manifold  M  or not. This can be set by defining this function, or setting the  IsDefaultMetric  trait for an  AbstractDecoratorManifold . source"},{"id":1290,"pagetitle":"Metric manifold","title":"Manifolds.local_metric","ref":"/manifolds/stable/manifolds/metric/#Manifolds.local_metric-Tuple{AbstractManifold, Any, AbstractBasis}","content":" Manifolds.local_metric  ‚Äî  Method local_metric(M::AbstractManifold{ùîΩ}, p, B::AbstractBasis) Return the local matrix representation at the point  p  of the metric tensor  $g$  with respect to the  AbstractBasis B  on the  AbstractManifold M . Let  $d$ denote the dimension of the manifold and  $b_1,\\ldots,b_d$  the basis vectors. Then the local matrix representation is a matrix  $G\\in ùîΩ^{n√ón}$  whose entries are given by  $g_{ij} = g_p(b_i,b_j), i,j\\in\\{1,‚Ä¶,d\\}$ . This yields the property for two tangent vectors (using Einstein summation convention)  $X = X^ib_i, Y=Y^ib_i \\in T_p\\mathcal M$  we get  $g_p(X, Y) = g_{ij} X^i Y^j$ . source"},{"id":1291,"pagetitle":"Metric manifold","title":"Manifolds.local_metric_jacobian","ref":"/manifolds/stable/manifolds/metric/#Manifolds.local_metric_jacobian-Tuple{AbstractManifold, Any, AbstractBasis, Any}","content":" Manifolds.local_metric_jacobian  ‚Äî  Method local_metric_jacobian(\n    M::AbstractManifold,\n    p,\n    B::AbstractBasis;\n    backend::AbstractDiffBackend,\n) Get partial derivatives of the local metric of  M  at  p  in basis  B  with respect to the coordinates of  p ,  $\\frac{‚àÇ}{‚àÇ p^k} g_{ij} = g_{ij,k}$ . The dimensions of the resulting multi-dimensional array are ordered  $(i,j,k)$ . source"},{"id":1292,"pagetitle":"Metric manifold","title":"Manifolds.log_local_metric_density","ref":"/manifolds/stable/manifolds/metric/#Manifolds.log_local_metric_density-Tuple{AbstractManifold, Any, AbstractBasis}","content":" Manifolds.log_local_metric_density  ‚Äî  Method log_local_metric_density(M::AbstractManifold, p, B::AbstractBasis) Return the natural logarithm of the metric density  $œÅ$  of  M  at  p , which is given by  $œÅ = \\log \\sqrt{|\\det [g_{ij}]|}$  for the metric tensor expressed in basis  B . source"},{"id":1293,"pagetitle":"Metric manifold","title":"Manifolds.metric","ref":"/manifolds/stable/manifolds/metric/#Manifolds.metric-Tuple{MetricManifold}","content":" Manifolds.metric  ‚Äî  Method metric(M::MetricManifold) Get the metric  $g$  of the manifold  M . source"},{"id":1294,"pagetitle":"Metric manifold","title":"Manifolds.ricci_curvature","ref":"/manifolds/stable/manifolds/metric/#Manifolds.ricci_curvature-Tuple{AbstractManifold, Any, AbstractBasis}","content":" Manifolds.ricci_curvature  ‚Äî  Method ricci_curvature(M::AbstractManifold, p, B::AbstractBasis; backend::AbstractDiffBackend = default_differential_backend()) Compute the Ricci scalar curvature of the manifold  M  at the point  p  using basis  B . The curvature is computed as the trace of the Ricci curvature tensor with respect to the metric, that is  $R=g^{ij}R_{ij}$  where  $R$  is the scalar Ricci curvature at  p ,  $g^{ij}$  is the inverse local metric (see  inverse_local_metric ) at  p  and  $R_{ij}$  is the Riccie curvature tensor, see  ricci_tensor . Both the tensor and inverse local metric are expressed in local coordinates defined by  B , and the formula uses the Einstein summation convention. source"},{"id":1295,"pagetitle":"Metric manifold","title":"Manifolds.sharp","ref":"/manifolds/stable/manifolds/metric/#Manifolds.sharp-Tuple{MetricManifold, Any, CoTFVector}","content":" Manifolds.sharp  ‚Äî  Method sharp(N::MetricManifold{M,G}, p, Œæ::CoTFVector) Compute the musical isomorphism to transform the cotangent vector  Œæ  from the  AbstractManifold M  equipped with  AbstractMetric G  to a tangent by computing \\[Œæ^‚ôØ = G_p^{-1} Œæ,\\] where  $G_p$  is the local matrix representation of  G , i.e. one employs  inverse_local_metric  here to obtain  $G_p^{-1}$ . source"},{"id":1296,"pagetitle":"Metric manifold","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/metric/#ManifoldsBase.inner-Tuple{MetricManifold, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(N::MetricManifold{M,G}, p, X, Y) Compute the inner product of  X  and  Y  from the tangent space at  p  on the  AbstractManifold M  using the  AbstractMetric G . If  M  has  G  as its  IsDefaultMetric  trait, this is done using  inner(M, p, X, Y) , otherwise the  local_metric (M, p)  is employed as \\[g_p(X, Y) = ‚ü®X, G_p Y‚ü©,\\] where  $G_p$  is the loal matrix representation of the  AbstractMetric G . source"},{"id":1297,"pagetitle":"Metric manifold","title":"Metrics, charts and bases of vector spaces","ref":"/manifolds/stable/manifolds/metric/#Metrics,-charts-and-bases-of-vector-spaces","content":" Metrics, charts and bases of vector spaces Metric-related functions, similarly to connection-related functions, need to operate in a basis of a vector space, see  here . Metric-related functions can take bases of associated tangent spaces as arguments. For example  local_metric  can take the basis of the tangent space it is supposed to operate on instead of a custom basis of the space of symmetric bilinear operators."},{"id":1300,"pagetitle":"Multinomial matrices","title":"Multinomial matrices","ref":"/manifolds/stable/manifolds/multinomial/#Multinomial-matrices","content":" Multinomial matrices"},{"id":1301,"pagetitle":"Multinomial matrices","title":"Manifolds.MultinomialMatrices","ref":"/manifolds/stable/manifolds/multinomial/#Manifolds.MultinomialMatrices","content":" Manifolds.MultinomialMatrices  ‚Äî  Type MultinomialMatrices{n,m} <: AbstractPowerManifold{‚Ñù} The multinomial manifold consists of  m  column vectors, where each column is of length  n  and unit norm, i.e. \\[\\mathcal{MN}(n,m) \\coloneqq \\bigl\\{\n    p ‚àà ‚Ñù^{n√óm}\\ \\big|\\ p_{i,j} > 0 \\text{ for all } i=1,‚Ä¶,n, j=1,‚Ä¶,m\n    \\text{ and } p^{\\mathrm{T}}\\mathbb{1}_m = \\mathbb{1}_n\\bigr\\},\\] where  $\\mathbb{1}_k$  is the vector of length  $k$  containing ones. This yields exactly the same metric as considering the product metric of the probablity vectors, i.e.  PowerManifold  of the  $(n-1)$ -dimensional  ProbabilitySimplex . The  ProbabilitySimplex  is stored internally within  M.manifold , such that all functions of  AbstractPowerManifold   can be used directly. Constructor MultinomialMatrices(n::Int, m::Int; parameter::Symbol=:type) Generate the manifold of matrices  $‚Ñù^{n√óm}$  such that the  $m$  columns are discrete probability distributions, i.e. sum up to one. parameter : whether a type parameter should be used to store  n  and  m . By default size is stored in type. Value can either be  :field  or  :type . source"},{"id":1302,"pagetitle":"Multinomial matrices","title":"Functions","ref":"/manifolds/stable/manifolds/multinomial/#Functions","content":" Functions Most functions are directly implemented for an  AbstractPowerManifold   with  ArrayPowerRepresentation  except the following special cases:"},{"id":1303,"pagetitle":"Multinomial matrices","title":"ManifoldDiff.riemannian_gradient","ref":"/manifolds/stable/manifolds/multinomial/#ManifoldDiff.riemannian_gradient-Tuple{MultinomialMatrices, Any, Any}","content":" ManifoldDiff.riemannian_gradient  ‚Äî  Method riemannian_gradient(M::MultinomialMatrices, p, Y; kwargs...) Let  $Y$  denote the Euclidean gradient of a function  $\\tilde f$  defined in the embedding neighborhood of  M , then the Riemannian gradient is given by Equation 5 of [ DH19 ] as \\[  \\operatorname{grad} f(p) = \\proj_{T_p\\mathcal M}(Y‚äôp)\\] where  $‚äô$  denotes the Hadamard or elementwise product. source"},{"id":1304,"pagetitle":"Multinomial matrices","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/multinomial/#ManifoldsBase.check_point-Tuple{MultinomialMatrices, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::MultinomialMatrices, p) Checks whether  p  is a valid point on the  MultinomialMatrices (m,n) M , i.e. is a matrix of  m  discrete probability distributions as columns from  $‚Ñù^n$ , i.e. each column is a point from  ProbabilitySimplex (n-1) . source"},{"id":1305,"pagetitle":"Multinomial matrices","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/multinomial/#ManifoldsBase.check_vector-Tuple{MultinomialMatrices, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::MultinomialMatrices p, X; kwargs...) Checks whether  X  is a valid tangent vector to  p  on the  MultinomialMatrices M . This means, that  p  is valid, that  X  is of correct dimension and columnswise a tangent vector to the columns of  p  on the  ProbabilitySimplex . source"},{"id":1308,"pagetitle":"Multinomial doubly stochastic matrices","title":"Multinomial doubly stochastic matrices","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#Multinomial-doubly-stochastic-matrices","content":" Multinomial doubly stochastic matrices"},{"id":1309,"pagetitle":"Multinomial doubly stochastic matrices","title":"Manifolds.AbstractMultinomialDoublyStochastic","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#Manifolds.AbstractMultinomialDoublyStochastic","content":" Manifolds.AbstractMultinomialDoublyStochastic  ‚Äî  Type AbstractMultinomialDoublyStochastic <: AbstractDecoratorManifold{‚Ñù} A common type for manifolds that are doubly stochastic, for example by direct constraint  MultinomialDoubleStochastic  or by symmetry  MultinomialSymmetric , or additionally by symmetric positive definiteness  MultinomialSymmetricPositiveDefinite  as long as they are also modeled as  IsIsometricEmbeddedManifold . That way they share the inner product (just by restriction), and even the Riemannian gradient source"},{"id":1310,"pagetitle":"Multinomial doubly stochastic matrices","title":"Manifolds.MultinomialDoubleStochastic","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#Manifolds.MultinomialDoubleStochastic","content":" Manifolds.MultinomialDoubleStochastic  ‚Äî  Type MultinomialDoublyStochastic{T} <: AbstractMultinomialDoublyStochastic The set of doubly stochastic multinomial matrices consists of all  $n√ón$  matrices with stochastic columns and rows, i.e. \\[\\begin{aligned}\n\\mathcal{DP}(n) \\coloneqq \\bigl\\{p ‚àà ‚Ñù^{n√ón}\\ \\big|\\ &p_{i,j} > 0 \\text{ for all } i=1,‚Ä¶,n, j=1,‚Ä¶,m,\\\\\n& p\\mathbf{1}_n = p^{\\mathrm{T}}\\mathbf{1}_n = \\mathbf{1}_n\n\\bigr\\},\n\\end{aligned}\\] where  $\\mathbf{1}_n$  is the vector of length  $n$  containing ones. The tangent space can be written as \\[T_p\\mathcal{DP}(n) \\coloneqq \\bigl\\{\nX ‚àà ‚Ñù^{n√ón}\\ \\big|\\ X = X^{\\mathrm{T}} \\text{ and }\nX\\mathbf{1}_n = X^{\\mathrm{T}}\\mathbf{1}_n = \\mathbf{0}_n\n\\bigr\\},\\] where  $\\mathbf{0}_n$  is the vector of length  $n$  containing zeros. More details can be found in Section III [ DH19 ]. Constructor MultinomialDoubleStochastic(n::Int; parameter::Symbol=:type) Generate the manifold of matrices  $‚Ñù^{n√ón}$  that are doubly stochastic and symmetric. source"},{"id":1311,"pagetitle":"Multinomial doubly stochastic matrices","title":"Base.rand","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#Base.rand-Tuple{MultinomialDoubleStochastic}","content":" Base.rand  ‚Äî  Method rand(::MultinomialDoubleStochastic; vector_at=nothing, œÉ::Real=1.0, kwargs...) Generate random points on the  MultinomialDoubleStochastic  manifold or tangent vectors at the point  vector_at  if that is not  nothing . Let  $n√ón$  denote the matrix dimension of the  MultinomialDoubleStochastic . When  vector_at  is nothing, this is done by generating a random matrix rand(n,n)  with positive entries and projecting it onto the manifold. The  kwargs...  are passed to this projection. When  vector_at  is not  nothing , a random matrix in the ambient space is generated and projected onto the tangent space source"},{"id":1312,"pagetitle":"Multinomial doubly stochastic matrices","title":"ManifoldDiff.riemannian_gradient","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#ManifoldDiff.riemannian_gradient-Tuple{Manifolds.AbstractMultinomialDoublyStochastic, Any, Any}","content":" ManifoldDiff.riemannian_gradient  ‚Äî  Method riemannian_gradient(M::AbstractMultinomialDoublyStochastic, p, Y; kwargs...) Let  $Y$  denote the Euclidean gradient of a function  $\\tilde f$  defined in the embedding neighborhood of  M , then the Riemannian gradient is given by Lemma 1 [ DH19 ] as \\[  \\operatorname{grad} f(p) = \\proj_{T_p\\mathcal M}(Y‚äôp)\\] where  $‚äô$  denotes the Hadamard or elementwise product, and the projection is the projection onto the tangent space of the corresponding manifold. source"},{"id":1313,"pagetitle":"Multinomial doubly stochastic matrices","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#ManifoldsBase.check_point-Tuple{MultinomialDoubleStochastic, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::MultinomialDoubleStochastic, p) Checks whether  p  is a valid point on the  MultinomialDoubleStochastic (n) M , i.e. is a  matrix with positive entries whose rows and columns sum to one. source"},{"id":1314,"pagetitle":"Multinomial doubly stochastic matrices","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#ManifoldsBase.check_vector-Tuple{MultinomialDoubleStochastic, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::MultinomialDoubleStochastic p, X; kwargs...) Checks whether  X  is a valid tangent vector to  p  on the  MultinomialDoubleStochastic M . This means, that  p  is valid, that  X  is of correct dimension and sums to zero along any column or row. source"},{"id":1315,"pagetitle":"Multinomial doubly stochastic matrices","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#ManifoldsBase.is_flat-Tuple{MultinomialDoubleStochastic}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::MultinomialDoubleStochastic) Return false.  MultinomialDoubleStochastic  is not a flat manifold. source"},{"id":1316,"pagetitle":"Multinomial doubly stochastic matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#ManifoldsBase.manifold_dimension-Tuple{MultinomialDoubleStochastic}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::MultinomialDoubleStochastic) returns the dimension of the  MultinomialDoubleStochastic  manifold namely \\[\\operatorname{dim}_{\\mathcal{DP}(n)} = (n-1)^2.\\] source"},{"id":1317,"pagetitle":"Multinomial doubly stochastic matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#ManifoldsBase.project-Tuple{Manifolds.AbstractMultinomialDoublyStochastic, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(\n    M::AbstractMultinomialDoublyStochastic,\n    p;\n    maxiter = 100,\n    tolerance = eps(eltype(p))\n) project a matrix  p  with positive entries applying Sinkhorn's algorithm. Note that this project method ‚Äì different from the usual case, accepts keywords. source"},{"id":1318,"pagetitle":"Multinomial doubly stochastic matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#ManifoldsBase.project-Tuple{MultinomialDoubleStochastic, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::MultinomialDoubleStochastic, p, Y) Project  Y  onto the tangent space at  p  on the  MultinomialDoubleStochastic M , return the result in  X . The formula reads \\[    \\operatorname{proj}_p(Y) = Y - (Œ±\\mathbf{1}_n^{\\mathrm{T}} + \\mathbf{1}_nŒ≤^{\\mathrm{T}}) ‚äô p,\\] where  $‚äô$  denotes the Hadamard or elementwise product and  $\\mathbb{1}_n$  is the vector of length  $n$  containing ones. The two vectors  $Œ±,Œ≤ ‚àà ‚Ñù^{n√ón}$  are computed as a solution (typically using the left pseudo inverse) of \\[    \\begin{pmatrix} I_n & p\\\\p^{\\mathrm{T}} & I_n \\end{pmatrix}\n    \\begin{pmatrix} Œ±\\\\ Œ≤\\end{pmatrix}\n    =\n    \\begin{pmatrix} Y\\mathbf{1}\\\\Y^{\\mathrm{T}}\\mathbf{1}\\end{pmatrix},\\] where  $I_n$  is the  $n√ón$  unit matrix and  $\\mathbf{1}_n$  is the vector of length  $n$  containing ones. source"},{"id":1319,"pagetitle":"Multinomial doubly stochastic matrices","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#ManifoldsBase.representation_size-Tuple{Manifolds.AbstractMultinomialDoublyStochastic}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::AbstractMultinomialDoublyStochastic) return the representation size of doubly stochastic matrices, which are embedded in the  $‚Ñù^{n√ón}$  matrices and hence the answer here is `` source"},{"id":1320,"pagetitle":"Multinomial doubly stochastic matrices","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#ManifoldsBase.retract-Tuple{MultinomialDoubleStochastic, Any, Any, ProjectionRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::MultinomialDoubleStochastic, p, X, ::ProjectionRetraction) compute a projection based retraction by projecting  $p\\odot\\exp(X‚®∏p)$  back onto the manifold, where  $‚äô,‚®∏$  are elementwise multiplication and division, respectively. Similarly,  $\\exp$  refers to the elementwise exponentiation. source"},{"id":1321,"pagetitle":"Multinomial doubly stochastic matrices","title":"Literature","ref":"/manifolds/stable/manifolds/multinomialdoublystochastic/#Literature","content":" Literature [DH19] A.¬†Douik and B.¬†Hassibi.  Manifold Optimization Over the Set of Doubly Stochastic Matrices: A Second-Order Geometry .  IEEE¬†Transactions¬†on¬†Signal¬†Processing  67 , 5761‚Äì5774  (2019),  arXiv:1802.02628 ."},{"id":1324,"pagetitle":"Multinomial symmetric matrices","title":"Multinomial symmetric matrices","ref":"/manifolds/stable/manifolds/multinomialsymmetric/#Multinomial-symmetric-matrices","content":" Multinomial symmetric matrices"},{"id":1325,"pagetitle":"Multinomial symmetric matrices","title":"Manifolds.MultinomialSymmetric","ref":"/manifolds/stable/manifolds/multinomialsymmetric/#Manifolds.MultinomialSymmetric","content":" Manifolds.MultinomialSymmetric  ‚Äî  Type MultinomialSymmetric{T} <: AbstractMultinomialDoublyStochastic The multinomial symmetric matrices manifold consists of all symmetric  $n√ón$  matrices with positive entries such that each column sums to one, i.e. \\[\\begin{aligned}\n\\mathcal{SP}(n) \\coloneqq \\bigl\\{p ‚àà ‚Ñù^{n√ón}\\ \\big|\\ &p_{i,j} > 0 \\text{ for all } i=1,‚Ä¶,n, j=1,‚Ä¶,m,\\\\\n& p^\\mathrm{T} = p,\\\\\n& p\\mathbf{1}_n = \\mathbf{1}_n\n\\bigr\\},\n\\end{aligned}\\] where  $\\mathbf{1}_n$  is the vector of length  $n$  containing ones. It is modeled as  IsIsometricEmbeddedManifold . via the  AbstractMultinomialDoublyStochastic  type, since it shares a few functions also with  AbstractMultinomialDoublyStochastic , most and foremost projection of a point from the embedding onto the manifold. The tangent space can be written as \\[T_p\\mathcal{SP}(n) \\coloneqq \\bigl\\{\nX ‚àà ‚Ñù^{n√ón}\\ \\big|\\ X = X^{\\mathrm{T}} \\text{ and }\nX\\mathbf{1}_n = \\mathbf{0}_n\n\\bigr\\},\\] where  $\\mathbf{0}_n$  is the vector of length  $n$  containing zeros. More details can be found in Section IV [ DH19 ]. Constructor MultinomialSymmetric(n) Generate the manifold of matrices  $‚Ñù^{n√ón}$  that are doubly stochastic and symmetric. source"},{"id":1326,"pagetitle":"Multinomial symmetric matrices","title":"Base.rand","ref":"/manifolds/stable/manifolds/multinomialsymmetric/#Base.rand-Tuple{MultinomialSymmetric}","content":" Base.rand  ‚Äî  Method rand(::MultinomialSymmetric; vector_at=nothing, œÉ::Real=1.0, kwargs...) Generate random points on the  MultinomialSymmetric  manifold or tangent vectors at the point  vector_at  if that is not  nothing . Let  $n√ón$  denote the matrix dimension of the  MultinomialSymmetric . When  vector_at  is nothing, this is done by generating a random matrix  rand(n, n)  with positive entries and projecting it onto the manifold. The  kwargs...  are passed to this projection. When  vector_at  is not  nothing , a random matrix in the ambient space is generated and projected onto the tangent space source"},{"id":1327,"pagetitle":"Multinomial symmetric matrices","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolds/stable/manifolds/multinomialsymmetric/#ManifoldDiff.riemannian_Hessian-Tuple{MultinomialSymmetric, Vararg{Any, 4}}","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method Y = riemannian_Hessian(M::MultinomialSymmetric, p, G, H, X)\nriemannian_Hessian!(M::MultinomialSymmetric, Y, p, G, H, X) Compute the Riemannian Hessian  $\\operatorname{Hess} f(p)[X]$  given the Euclidean gradient  $‚àá f(\\tilde p)$  in  G  and the Euclidean Hessian  $‚àá^2 f(\\tilde p)[\\tilde X]$  in  H , where  $\\tilde p, \\tilde X$  are the representations of  $p,X$  in the embedding,. The Riemannian Hessian can be computed as stated in Corollary 3 [ DH19 ]. source"},{"id":1328,"pagetitle":"Multinomial symmetric matrices","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/multinomialsymmetric/#ManifoldsBase.check_point-Tuple{MultinomialSymmetric, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::MultinomialSymmetric, p) Checks whether  p  is a valid point on the  MultinomialSymmetric (m,n) M , i.e. is a symmetric matrix with positive entries whose rows sum to one. source"},{"id":1329,"pagetitle":"Multinomial symmetric matrices","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/multinomialsymmetric/#ManifoldsBase.check_vector-Tuple{MultinomialSymmetric, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::MultinomialSymmetric p, X; kwargs...) Checks whether  X  is a valid tangent vector to  p  on the  MultinomialSymmetric M . This means, that  p  is valid, that  X  is of correct dimension, symmetric, and sums to zero along any row. source"},{"id":1330,"pagetitle":"Multinomial symmetric matrices","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/multinomialsymmetric/#ManifoldsBase.is_flat-Tuple{MultinomialSymmetric}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::MultinomialSymmetric) Return false.  MultinomialSymmetric  is not a flat manifold. source"},{"id":1331,"pagetitle":"Multinomial symmetric matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/multinomialsymmetric/#ManifoldsBase.manifold_dimension-Tuple{MultinomialSymmetric}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::MultinomialSymmetric) returns the dimension of the  MultinomialSymmetric  manifold namely \\[\\operatorname{dim}_{\\mathcal{SP}(n)} = \\frac{n(n-1)}{2}.\\] source"},{"id":1332,"pagetitle":"Multinomial symmetric matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/multinomialsymmetric/#ManifoldsBase.project-Tuple{MultinomialSymmetric, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::MultinomialSymmetric, p, Y) Project  Y  onto the tangent space at  p  on the  MultinomialSymmetric M , return the result in  X . The formula from [ DH19 ], Sec. VI reads \\[    \\operatorname{proj}_p(Y) = Y - (Œ±\\mathbf{1}_n^{\\mathrm{T}} + \\mathbf{1}_n Œ±^{\\mathrm{T}}) ‚äô p,\\] where  $‚äô$  denotes the Hadamard or elementwise product and  $\\mathbb{1}_n$  is the vector of length  $n$  containing ones. The two vector  $Œ± ‚àà ‚Ñù^{n√ón}$  is given by solving \\[    (I_n+p)Œ± =  Y\\mathbf{1},\\] where  $I_n$  is teh  $n√ón$  unit matrix and  $\\mathbf{1}_n$  is the vector of length  $n$  containing ones. source"},{"id":1333,"pagetitle":"Multinomial symmetric matrices","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/multinomialsymmetric/#ManifoldsBase.retract-Tuple{MultinomialSymmetric, Any, Any, ProjectionRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::MultinomialSymmetric, p, X, ::ProjectionRetraction) compute a projection based retraction by projecting  $p‚äô\\exp(X‚®∏p)$  back onto the manifold, where  $‚äô,‚®∏$  are elementwise multiplication and division, respectively. Similarly,  $\\exp$  refers to the elementwise exponentiation. source"},{"id":1334,"pagetitle":"Multinomial symmetric matrices","title":"Literature","ref":"/manifolds/stable/manifolds/multinomialsymmetric/#Literature","content":" Literature [DH19] A.¬†Douik and B.¬†Hassibi.  Manifold Optimization Over the Set of Doubly Stochastic Matrices: A Second-Order Geometry .  IEEE¬†Transactions¬†on¬†Signal¬†Processing  67 , 5761‚Äì5774  (2019),  arXiv:1802.02628 ."},{"id":1337,"pagetitle":"Multinomial symmetric positive definite matrices","title":"Multinomial symmetric positive definite matrices","ref":"/manifolds/stable/manifolds/multinomialsymmetricpositivedefinite/#Multinomial-symmetric-positive-definite-matrices","content":" Multinomial symmetric positive definite matrices"},{"id":1338,"pagetitle":"Multinomial symmetric positive definite matrices","title":"Manifolds.MultinomialSymmetricPositiveDefinite","ref":"/manifolds/stable/manifolds/multinomialsymmetricpositivedefinite/#Manifolds.MultinomialSymmetricPositiveDefinite","content":" Manifolds.MultinomialSymmetricPositiveDefinite  ‚Äî  Type MultinomialSymmetricPositiveDefinite <: AbstractMultinomialDoublyStochastic The symmetric positive definite multinomial matrices manifold consists of all symmetric  $n√ón$  matrices with positive eigenvalues, and positive entries such that each column sums to one, i.e. \\[\\begin{aligned}\n\\mathcal{SP}^+(n) \\coloneqq \\bigl\\{\n    p ‚àà ‚Ñù^{n√ón}\\ \\big|\\ &p_{i,j} > 0 \\text{ for all } i=1,‚Ä¶,n, j=1,‚Ä¶,m,\\\\\n& p^\\mathrm{T} = p,\\\\\n& p\\mathbf{1}_n = \\mathbf{1}_n\\\\\na^\\mathrm{T}pa > 0 \\text{ for all } a ‚àà ‚Ñù^{n}\\backslash\\{\\mathbf{0}_n\\}\n\\bigr\\},\n\\end{aligned}\\] where  $\\mathbf{1}_n$  and  $\\mathbr{0}_n$  are the vectors of length  $n$  containing ones and zeros, respectively. More details about this manifold can be found in [ DH19 ]. Constructor MultinomialSymmetricPositiveDefinite(n) Generate the manifold of matrices  $\\mathbb R^{n√ón}$  that are symmetric, positive definite, and doubly stochastic. source"},{"id":1339,"pagetitle":"Multinomial symmetric positive definite matrices","title":"Random.rand!","ref":"/manifolds/stable/manifolds/multinomialsymmetricpositivedefinite/#Random.rand!-Tuple{Random.AbstractRNG, MultinomialSymmetricPositiveDefinite, AbstractMatrix}","content":" Random.rand!  ‚Äî  Method Random.rand!(\n    rng::AbstractRNG,\n    M::MultinomialSymmetricPositiveDefinite,\n    p::AbstractMatrix,\n) Generate a random point on  MultinomialSymmetricPositiveDefinite  manifold. The steps are as follows: Generate a random  totally positive matrix   a. Construct a vector  L  of  n  random positive increasing real numbers.  b. Construct the  Vandermonde matrix V  based on the sequence  L .  c. Perform LU factorization of  V  in such way that both L and U components have     positive elements.  d. Convert the LU factorization into LDU factorization by taking the diagonal of U     and dividing U by it,  V=LDU .  e. Construct a new matrix  R = UDL  which is totally positive. Project the totally positive matrix  R  onto the manifold of  MultinomialDoubleStochastic  matrices. Symmetrize the projected matrix and return the result. This method roughly follows the procedure described in https://math.stackexchange.com/questions/2773460/how-to-generate-a-totally-positive-matrix-randomly-using-software-like-maple source"},{"id":1340,"pagetitle":"Multinomial symmetric positive definite matrices","title":"Literature","ref":"/manifolds/stable/manifolds/multinomialsymmetricpositivedefinite/#Literature","content":" Literature [DH19] A.¬†Douik and B.¬†Hassibi.  Manifold Optimization Over the Set of Doubly Stochastic Matrices: A Second-Order Geometry .  IEEE¬†Transactions¬†on¬†Signal¬†Processing  67 , 5761‚Äì5774  (2019),  arXiv:1802.02628 ."},{"id":1343,"pagetitle":"Oblique manifold","title":"Oblique manifold","ref":"/manifolds/stable/manifolds/oblique/#Oblique-manifold","content":" Oblique manifold The oblique manifold  $\\mathcal{OB}(n,m)$  is modeled as an  AbstractPowerManifold   of the (real-valued)  Sphere  and uses  ArrayPowerRepresentation . Points on the torus are hence matrices,  $x ‚àà ‚Ñù^{n,m}$ ."},{"id":1344,"pagetitle":"Oblique manifold","title":"Manifolds.Oblique","ref":"/manifolds/stable/manifolds/oblique/#Manifolds.Oblique","content":" Manifolds.Oblique  ‚Äî  Type Oblique{T,ùîΩ,S} <: AbstractPowerManifold{ùîΩ} The oblique manifold  $\\mathcal{OB}(n,m)$  is the set of ùîΩ-valued matrices with unit norm column endowed with the metric from the embedding. This yields exactly the same metric as considering the product metric of the unit norm vectors, i.e.  PowerManifold  of the  $(n-1)$ -dimensional  Sphere . The  Sphere  is stored internally within  M.manifold , such that all functions of  AbstractPowerManifold   can be used directly. Constructor Oblique(n::Int, m::Int, field::AbstractNumbers=‚Ñù; parameter::Symbol=:type) Generate the manifold of matrices  $\\mathbb R^{n√óm}$  such that the  $m$  columns are unit vectors, i.e. from the  Sphere (n-1) . source"},{"id":1345,"pagetitle":"Oblique manifold","title":"Functions","ref":"/manifolds/stable/manifolds/oblique/#Functions","content":" Functions Most functions are directly implemented for an  AbstractPowerManifold   with  ArrayPowerRepresentation  except the following special cases:"},{"id":1346,"pagetitle":"Oblique manifold","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/oblique/#ManifoldsBase.check_point-Tuple{Oblique, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::Oblique, p) Checks whether  p  is a valid point on the  Oblique {m,n} M , i.e. is a matrix of  m  unit columns from  $\\mathbb R^{n}$ , i.e. each column is a point from  Sphere (n-1) . source"},{"id":1347,"pagetitle":"Oblique manifold","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/oblique/#ManifoldsBase.check_vector-Tuple{Oblique, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::Oblique p, X; kwargs...) Checks whether  X  is a valid tangent vector to  p  on the  Oblique M . This means, that  p  is valid, that  X  is of correct dimension and columnswise a tangent vector to the columns of  p  on the  Sphere . source"},{"id":1348,"pagetitle":"Oblique manifold","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/oblique/#ManifoldsBase.parallel_transport_to-Tuple{Oblique, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::Oblique, p, X, q) Compute the parallel transport on the  Oblique  manifold by doing a column wise parallel transport on the  Sphere source"},{"id":1351,"pagetitle":"Positive numbers","title":"Positive Numbers","ref":"/manifolds/stable/manifolds/positivenumbers/#Positive-Numbers","content":" Positive Numbers The manifold  PositiveNumbers  represents positive numbers with hyperbolic geometry. Additionally, there are also short forms for its corresponding  PowerManifold s, i.e.  PositiveVectors ,  PositiveMatrices , and  PositiveArrays ."},{"id":1352,"pagetitle":"Positive numbers","title":"Manifolds.PositiveNumbers","ref":"/manifolds/stable/manifolds/positivenumbers/#Manifolds.PositiveNumbers","content":" Manifolds.PositiveNumbers  ‚Äî  Type PositiveNumbers <: AbstractManifold{‚Ñù} The hyperbolic manifold of positive numbers  $H^1$  is a the hyperbolic manifold represented by just positive numbers. Constructor PositiveNumbers() Generate the  ‚Ñù -valued hyperbolic model represented by positive positive numbers. To use this with arrays (1-element arrays), please use  SymmetricPositiveDefinite (1) . source"},{"id":1353,"pagetitle":"Positive numbers","title":"Base.exp","ref":"/manifolds/stable/manifolds/positivenumbers/#Base.exp-Tuple{PositiveNumbers, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::PositiveNumbers, p, X) Compute the exponential map on the  PositiveNumbers M . \\[\\exp_p X = p\\operatorname{exp}(X/p).\\] source"},{"id":1354,"pagetitle":"Positive numbers","title":"Base.log","ref":"/manifolds/stable/manifolds/positivenumbers/#Base.log-Tuple{PositiveNumbers, Any, Any}","content":" Base.log  ‚Äî  Method log(M::PositiveNumbers, p, q) Compute the logarithmic map on the  PositiveNumbers M . \\[\\log_p q = p\\log\\frac{q}{p}.\\] source"},{"id":1355,"pagetitle":"Positive numbers","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldDiff.riemannian_Hessian-Tuple{PositiveNumbers, Vararg{Any, 4}}","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method riemannian_Hessian(M::SymmetricPositiveDefinite, p, G, H, X) The Riemannian Hessian can be computed as stated in Eq. (7.3) [ Ngu23 ]. Let  $\\nabla f(p)$  denote the Euclidean gradient  G ,  $\\nabla^2 f(p)[X]$  the Euclidean Hessian  H . Then the formula reads \\[    \\operatorname{Hess}f(p)[X] = p\\bigl(‚àá^2 f(p)[X]\\bigr)p + X\\bigl(‚àáf(p)\\bigr)p\\] source"},{"id":1356,"pagetitle":"Positive numbers","title":"Manifolds.PositiveArrays","ref":"/manifolds/stable/manifolds/positivenumbers/#Manifolds.PositiveArrays-Union{NTuple{I, Int64}, Tuple{I}} where I","content":" Manifolds.PositiveArrays  ‚Äî  Method PositiveArrays(n‚ÇÅ, n‚ÇÇ, ..., n·µ¢; parameter::Symbol=:type) Generate the manifold of  i -dimensional arrays with positive entries. This manifold is modeled as a  PowerManifold  of  PositiveNumbers . parameter : whether a type parameter should be used to store  n . By default size is stored in a type parameter. Value can either be  :field  or  :type . source"},{"id":1357,"pagetitle":"Positive numbers","title":"Manifolds.PositiveMatrices","ref":"/manifolds/stable/manifolds/positivenumbers/#Manifolds.PositiveMatrices-Tuple{Integer, Integer}","content":" Manifolds.PositiveMatrices  ‚Äî  Method PositiveMatrices(m::Integer, n::Integer; parameter::Symbol=:type) Generate the manifold of matrices with positive entries. This manifold is modeled as a  PowerManifold  of  PositiveNumbers . parameter : whether a type parameter should be used to store  n . By default size is stored in a type parameter. Value can either be  :field  or  :type . source"},{"id":1358,"pagetitle":"Positive numbers","title":"Manifolds.PositiveVectors","ref":"/manifolds/stable/manifolds/positivenumbers/#Manifolds.PositiveVectors-Tuple{Integer}","content":" Manifolds.PositiveVectors  ‚Äî  Method PositiveVectors(n::Integer; parameter::Symbol=:type) Generate the manifold of vectors with positive entries. This manifold is modeled as a  PowerManifold  of  PositiveNumbers . parameter : whether a type parameter should be used to store  n . By default size is stored in a type parameter. Value can either be  :field  or  :type . source"},{"id":1359,"pagetitle":"Positive numbers","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/positivenumbers/#Manifolds.manifold_volume-Tuple{PositiveNumbers}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(M::PositiveNumbers) Return volume of  PositiveNumbers M , i.e.  Inf . source"},{"id":1360,"pagetitle":"Positive numbers","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/positivenumbers/#Manifolds.volume_density-Tuple{PositiveNumbers, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(M::PositiveNumbers, p, X) Compute volume density function of  PositiveNumbers . The formula reads \\[\\theta_p(X) = \\exp(X / p)\\] source"},{"id":1361,"pagetitle":"Positive numbers","title":"ManifoldsBase.change_metric","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldsBase.change_metric-Tuple{PositiveNumbers, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_metric  ‚Äî  Method change_metric(M::PositiveNumbers, E::EuclideanMetric, p, X) Given a tangent vector  $X ‚àà T_p\\mathcal M$  representing a linear function with respect to the  EuclideanMetric g_E , this function changes the representer into the one with respect to the positivity metric of  PositiveNumbers M . For all  $Z,Y$  we are looking for the function  $c$  on the tangent space at  $p$  such that \\[    ‚ü®Z,Y‚ü© = XY = \\frac{c(Z)c(Y)}{p^2} = g_p(c(Y),c(Z))\\] and hence  $C(X) = pX$ . source"},{"id":1362,"pagetitle":"Positive numbers","title":"ManifoldsBase.change_representer","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldsBase.change_representer-Tuple{PositiveNumbers, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(M::PositiveNumbers, E::EuclideanMetric, p, X) Given a tangent vector  $X ‚àà T_p\\mathcal M$  representing a linear function with respect to the  EuclideanMetric g_E , this function changes the representer into the one with respect to the positivity metric representation of  PositiveNumbers M . For all tangent vectors  $Y$  the result  $Z$  has to fulfill \\[    ‚ü®X,Y‚ü© = XY = \\frac{ZY}{p^2} = g_p(YZ)\\] and hence  $Z = p^2X$ source"},{"id":1363,"pagetitle":"Positive numbers","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldsBase.check_point-Tuple{PositiveNumbers, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::PositiveNumbers, p) Check whether  p  is a point on the  PositiveNumbers M , i.e.  $p>0$ . source"},{"id":1364,"pagetitle":"Positive numbers","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldsBase.check_vector-Tuple{PositiveNumbers, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::PositiveNumbers, p, X; kwargs...) Check whether  X  is a tangent vector in the tangent space of  p  on the  PositiveNumbers M . For the real-valued case represented by positive numbers, all  X  are valid, since the tangent space is the whole real line. For the complex-valued case  X  [...] source"},{"id":1365,"pagetitle":"Positive numbers","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldsBase.distance-Tuple{PositiveNumbers, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::PositiveNumbers, p, q) Compute the distance on the  PositiveNumbers M , which is \\[d(p,q) = \\Bigl\\lvert \\log \\frac{p}{q} \\Bigr\\rvert = \\lvert \\log p - \\log q\\rvert.\\] source"},{"id":1366,"pagetitle":"Positive numbers","title":"ManifoldsBase.get_coordinates","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldsBase.get_coordinates-Tuple{PositiveNumbers, Any, Any, DefaultOrthonormalBasis{‚Ñù}}","content":" ManifoldsBase.get_coordinates  ‚Äî  Method get_coordinates(::PositiveNumbers, p, X, ::DefaultOrthonormalBasis{‚Ñù}) Compute the coordinate of vector  X  which is tangent to  p  on the  PositiveNumbers  manifold. The formula is  $X / p$ . source"},{"id":1367,"pagetitle":"Positive numbers","title":"ManifoldsBase.get_vector","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldsBase.get_vector-Tuple{PositiveNumbers, Any, Any, DefaultOrthonormalBasis{‚Ñù}}","content":" ManifoldsBase.get_vector  ‚Äî  Method get_vector(::PositiveNumbers, p, c, ::DefaultOrthonormalBasis{‚Ñù}) Compute the vector with coordinate  c  which is tangent to  p  on the  PositiveNumbers  manifold. The formula is  $p * c$ . source"},{"id":1368,"pagetitle":"Positive numbers","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldsBase.injectivity_radius-Tuple{PositiveNumbers}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::PositiveNumbers[, p]) Return the injectivity radius on the  PositiveNumbers M , i.e.  $\\infty$ . source"},{"id":1369,"pagetitle":"Positive numbers","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldsBase.inner-Tuple{PositiveNumbers, Vararg{Any}}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::PositiveNumbers, p, X, Y) Compute the inner product of the two tangent vectors  X,Y  from the tangent plane at  p  on the  PositiveNumbers M , i.e. \\[g_p(X,Y) = \\frac{XY}{p^2}.\\] source"},{"id":1370,"pagetitle":"Positive numbers","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldsBase.is_flat-Tuple{PositiveNumbers}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::PositiveNumbers) Return false.  PositiveNumbers  is not a flat manifold. source"},{"id":1371,"pagetitle":"Positive numbers","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldsBase.manifold_dimension-Tuple{PositiveNumbers}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::PositiveNumbers) Return the dimension of the  PositiveNumbers M , i.e. of the 1-dimensional hyperbolic space, \\[\\dim(H^1) = 1\\] source"},{"id":1372,"pagetitle":"Positive numbers","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldsBase.parallel_transport_to-Tuple{PositiveNumbers, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::PositiveNumbers, p, X, q) Compute the parallel transport of  X  from the tangent space at  p  to the tangent space at  q  on the  PositiveNumbers M . \\[\\mathcal P_{q\\gets p}(X) = X‚ãÖ\\frac{q}{p}.\\] source"},{"id":1373,"pagetitle":"Positive numbers","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/positivenumbers/#ManifoldsBase.project-Tuple{PositiveNumbers, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::PositiveNumbers, p, X) Project a value  X  onto the tangent space of the point  p  on the  PositiveNumbers M , which is just the identity. source"},{"id":1376,"pagetitle":"Power manifold","title":"Power manifold","ref":"/manifolds/stable/manifolds/power/#PowerManifoldSection","content":" Power manifold A power manifold is based on a  AbstractManifold $\\mathcal M$  to build a  $\\mathcal M^{n_1√ón_2 √ó‚ãØ√ón_m}$ . In the case where  $m=1$  we can represent a manifold-valued vector of data of length  $n_1$ , for example a time series. The case where  $m=2$  is useful for representing manifold-valued matrices of data of size  $n_1√ón_2$ , for example certain types of images. There are three available representations for points and vectors on a power manifold: ArrayPowerRepresentation  (the default one), very efficient but only applicable when points on the underlying manifold are represented using plain  AbstractArray s. NestedPowerRepresentation , applicable to any manifold. It assumes that points on the underlying manifold are represented using mutable data types. NestedReplacingPowerRepresentation , applicable to any manifold. It does not mutate points on the underlying manifold, replacing them instead when appropriate. Below are some examples of usage of these representations."},{"id":1377,"pagetitle":"Power manifold","title":"Example","ref":"/manifolds/stable/manifolds/power/#Example","content":" Example There are two ways to store the data: in a multidimensional array or in a nested array. Let's look at an example for both. Let  $\\mathcal M$  be  Sphere(2)  the 2-sphere and we want to look at vectors of length 4."},{"id":1378,"pagetitle":"Power manifold","title":"ArrayPowerRepresentation","ref":"/manifolds/stable/manifolds/power/#ArrayPowerRepresentation","content":" ArrayPowerRepresentation For the default, the  ArrayPowerRepresentation , we store the data in a multidimensional array, using Manifolds\nM = PowerManifold(Sphere(2), 4)\np = cat([1.0, 0.0, 0.0],\n        [1/sqrt(2.0), 1/sqrt(2.0), 0.0],\n        [1/sqrt(2.0), 0.0, 1/sqrt(2.0)],\n        [0.0, 1.0, 0.0]\n    ,dims=2) 3√ó4 Matrix{Float64}:\n 1.0  0.707107  0.707107  0.0\n 0.0  0.707107  0.0       1.0\n 0.0  0.0       0.707107  0.0 which is a valid point i.e. is_point(M, p) true This can also be used in combination with  HybridArrays.jl  and  StaticArrays.jl , by setting using HybridArrays, StaticArrays\nq = HybridArray{Tuple{3,StaticArrays.Dynamic()},Float64,2}(p) 3√ó4 HybridArrays.HybridMatrix{3, StaticArraysCore.Dynamic(), Float64, 2, Matrix{Float64}} with indices SOneTo(3)√óBase.OneTo(4):\n 1.0  0.707107  0.707107  0.0\n 0.0  0.707107  0.0       1.0\n 0.0  0.0       0.707107  0.0 which is still a valid point on  M  and  PowerManifold  works with these, too. An advantage of this representation is that it is quite efficient, especially when a  HybridArray  (from the  HybridArrays.jl  package) is used to represent a point on the power manifold. A disadvantage is not being able to easily identify parts of the multidimensional array that correspond to a single point on the base manifold. Another problem is, that accessing a single point is  p[:, 1]  which might be unintuitive."},{"id":1379,"pagetitle":"Power manifold","title":"NestedPowerRepresentation","ref":"/manifolds/stable/manifolds/power/#NestedPowerRepresentation","content":" NestedPowerRepresentation For the  NestedPowerRepresentation  we can now do using Manifolds\nM = PowerManifold(Sphere(2), NestedPowerRepresentation(), 4)\np = [ [1.0, 0.0, 0.0],\n      [1/sqrt(2.0), 1/sqrt(2.0), 0.0],\n      [1/sqrt(2.0), 0.0, 1/sqrt(2.0)],\n      [0.0, 1.0, 0.0],\n    ] 4-element Vector{Vector{Float64}}:\n [1.0, 0.0, 0.0]\n [0.7071067811865475, 0.7071067811865475, 0.0]\n [0.7071067811865475, 0.0, 0.7071067811865475]\n [0.0, 1.0, 0.0] which is again a valid point so  is_point(M, p)  here also yields true. A disadvantage might be that with nested arrays one loses a little bit of performance. The data however is nicely encapsulated. Accessing the first data item is just  p[1] . For accessing points on power manifolds in both representations you can use  get_component  and  set_component!  functions. They work work both point representations. using Manifolds\nM = PowerManifold(Sphere(2), NestedPowerRepresentation(), 4)\np = [ [1.0, 0.0, 0.0],\n      [1/sqrt(2.0), 1/sqrt(2.0), 0.0],\n      [1/sqrt(2.0), 0.0, 1/sqrt(2.0)],\n      [0.0, 1.0, 0.0],\n    ]\nset_component!(M, p, [0.0, 0.0, 1.0], 4)\nget_component(M, p, 4) 3-element Vector{Float64}:\n 0.0\n 0.0\n 1.0"},{"id":1380,"pagetitle":"Power manifold","title":"NestedReplacingPowerRepresentation","ref":"/manifolds/stable/manifolds/power/#NestedReplacingPowerRepresentation","content":" NestedReplacingPowerRepresentation The final representation is the  NestedReplacingPowerRepresentation . It is similar to the  NestedPowerRepresentation  but it does not perform in-place operations on the points on the underlying manifold. The example below uses this representation to store points on a power manifold of the  SpecialEuclidean  group in-line in an  Vector  for improved efficiency. When having a mixture of both, i.e. an array structure that is nested (like  ¬¥NestedPowerRepresentation ) in the sense that the elements of the main vector are immutable, then changing the elements can not be done in an in-place way and hence  NestedReplacingPowerRepresentation  has to be used. using Manifolds, StaticArrays, RecursiveArrayTools\nR2 = Rotations(2)\n\nG = SpecialEuclidean(2)\nN = 5\nGN = PowerManifold(G, NestedReplacingPowerRepresentation(), N)\n\nq = [1.0 0.0; 0.0 1.0]\np1 = [ArrayPartition(SVector{2,Float64}([i - 0.1, -i]), SMatrix{2,2,Float64}(exp(R2, q, hat(R2, q, i)))) for i in 1:N]\np2 = [ArrayPartition(SVector{2,Float64}([i - 0.1, -i]), SMatrix{2,2,Float64}(exp(R2, q, hat(R2, q, -i)))) for i in 1:N]\n\nX = similar(p1);\n\nlog!(GN, X, p1, p2) 5-element Vector{RecursiveArrayTools.ArrayPartition{Float64, Tuple{StaticArraysCore.SVector{2, Float64}, StaticArraysCore.SMatrix{2, 2, Float64, 4}}}}:\n ([0.0, 0.0], [0.0 2.0; -2.0 0.0])\n ([0.0, 0.0], [0.0 -2.2831853071795862; 2.2831853071795862 0.0])\n ([0.0, 0.0], [0.0 -0.28318530717958645; 0.28318530717958645 0.0])\n ([0.0, 0.0], [0.0 1.7168146928204135; -1.7168146928204135 0.0])\n ([0.0, 0.0], [0.0 -2.566370614359173; 2.566370614359173 0.0])"},{"id":1381,"pagetitle":"Power manifold","title":"Types and Functions","ref":"/manifolds/stable/manifolds/power/#Types-and-Functions","content":" Types and Functions"},{"id":1382,"pagetitle":"Power manifold","title":"Manifolds.ArrayPowerRepresentation","ref":"/manifolds/stable/manifolds/power/#Manifolds.ArrayPowerRepresentation","content":" Manifolds.ArrayPowerRepresentation  ‚Äî  Type ArrayPowerRepresentation Representation of points and tangent vectors on a power manifold using multidimensional arrays where first dimensions are equal to  representation_size  of the wrapped manifold and the following ones are equal to the number of elements in each direction. Torus  uses this representation. source"},{"id":1383,"pagetitle":"Power manifold","title":"Manifolds.PowerMetric","ref":"/manifolds/stable/manifolds/power/#Manifolds.PowerMetric","content":" Manifolds.PowerMetric  ‚Äî  Type PowerMetric <: AbstractMetric Represent the  AbstractMetric  on an  AbstractPowerManifold , i.e. the inner product on the tangent space is the sum of the inner product of each elements tangent space of the power manifold. source"},{"id":1384,"pagetitle":"Power manifold","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolds/stable/manifolds/power/#ManifoldDiff.riemannian_Hessian-Tuple{AbstractPowerManifold, Vararg{Any, 4}}","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method Y = riemannian_Hessian(M::AbstractPowerManifold, p, G, H, X)\nriemannian_Hessian!(M::AbstractPowerManifold, Y, p, G, H, X) Compute the Riemannian Hessian  $\\operatorname{Hess} f(p)[X]$  given the Euclidean gradient  $‚àá f(\\tilde p)$  in  G  and the Euclidean Hessian  $‚àá^2 f(\\tilde p)[\\tilde X]$  in  H , where  $\\tilde p, \\tilde X$  are the representations of  $p,X$  in the embedding,. On an abstract power manifold, this decouples and can be computed elementwise. source"},{"id":1385,"pagetitle":"Power manifold","title":"Manifolds.flat","ref":"/manifolds/stable/manifolds/power/#Manifolds.flat-Tuple{AbstractPowerManifold, Vararg{Any}}","content":" Manifolds.flat  ‚Äî  Method flat(M::AbstractPowerManifold, p, X) use the musical isomorphism to transform the tangent vector  X  from the tangent space at  p  on an  AbstractPowerManifold M  to a cotangent vector. This can be done elementwise for each entry of  X  (and  p ). source"},{"id":1386,"pagetitle":"Power manifold","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/power/#Manifolds.manifold_volume-Tuple{PowerManifold}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(M::PowerManifold) Return the manifold volume of an  PowerManifold M . source"},{"id":1387,"pagetitle":"Power manifold","title":"Manifolds.sharp","ref":"/manifolds/stable/manifolds/power/#Manifolds.sharp-Tuple{AbstractPowerManifold, Vararg{Any}}","content":" Manifolds.sharp  ‚Äî  Method sharp(M::AbstractPowerManifold, p, Œæ::RieszRepresenterCotangentVector) Use the musical isomorphism to transform the cotangent vector  Œæ  from the tangent space at  p  on an  AbstractPowerManifold M  to a tangent vector. This can be done elementwise for every entry of  Œæ  (and  p ). source"},{"id":1388,"pagetitle":"Power manifold","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/power/#Manifolds.volume_density-Tuple{PowerManifold, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(M::PowerManifold, p, X) Return volume density on the  PowerManifold M , i.e. product of constituent volume densities. source"},{"id":1391,"pagetitle":"Probability simplex","title":"The probability simplex","ref":"/manifolds/stable/manifolds/probabilitysimplex/#The-probability-simplex","content":" The probability simplex"},{"id":1392,"pagetitle":"Probability simplex","title":"Manifolds.FisherRaoMetric","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Manifolds.FisherRaoMetric","content":" Manifolds.FisherRaoMetric  ‚Äî  Type FisherRaoMetric <: AbstractMetric The Fisher-Rao metric or Fisher information metric is a particular Riemannian metric which can be defined on a smooth statistical manifold, i.e., a smooth manifold whose points are probability measures defined on a common probability space. See for example the  ProbabilitySimplex . source"},{"id":1393,"pagetitle":"Probability simplex","title":"Manifolds.ProbabilitySimplex","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Manifolds.ProbabilitySimplex","content":" Manifolds.ProbabilitySimplex  ‚Äî  Type ProbabilitySimplex{T,boundary} <: AbstractDecoratorManifold{ùîΩ} The (relative interior of) the probability simplex is the set \\[Œî^n := \\biggl\\{ p ‚àà ‚Ñù^{n+1}\\ \\big|\\ p_i > 0 \\text{ for all } i=1,‚Ä¶,n+1,\n\\text{ and } ‚ü®\\mathbb{1},p‚ü© = \\sum_{i=1}^{n+1} p_i = 1\\biggr\\},\\] where  $\\mathbb{1}=(1,‚Ä¶,1)^{\\mathrm{T}}‚àà ‚Ñù^{n+1}$  denotes the vector containing only ones. If  boundary  is set to  :open , then the object represents an open simplex. Otherwise, that is when  boundary  is set to  :closed , the boundary is also included: \\[\\hat{Œî}^n := \\biggl\\{ p ‚àà ‚Ñù^{n+1}\\ \\big|\\ p_i \\geq 0 \\text{ for all } i=1,‚Ä¶,n+1,\n\\text{ and } ‚ü®\\mathbb{1},p‚ü© = \\sum_{i=1}^{n+1} p_i = 1\\biggr\\},\\] This set is also called the unit simplex or standard simplex. The tangent space is given by \\[T_pŒî^n = \\biggl\\{ X ‚àà ‚Ñù^{n+1}\\ \\big|\\ ‚ü®\\mathbb{1},X‚ü© = \\sum_{i=1}^{n+1} X_i = 0 \\biggr\\}\\] The manifold is implemented assuming the Fisher-Rao metric for the multinomial distribution, which is equivalent to the induced metric from isometrically embedding the probability simplex in the  $n$ -sphere of radius 2. The corresponding diffeomorphism  $\\varphi: \\mathbb Œî^n ‚Üí \\mathcal N$ , where  $\\mathcal N \\subset 2ùïä^n$  is given by  $\\varphi(p) = 2\\sqrt{p}$ . This implementation follows the notation in [ APSS17 ]. Constructor ProbabilitySimplex(n::Int; boundary::Symbol=:open) source"},{"id":1394,"pagetitle":"Probability simplex","title":"Base.exp","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Base.exp-Tuple{ProbabilitySimplex, Vararg{Any}}","content":" Base.exp  ‚Äî  Method exp(M::ProbabilitySimplex, p, X) Compute the exponential map on the probability simplex. \\[\\exp_pX = \\frac{1}{2}\\Bigl(p+\\frac{X_p^2}{\\lVert X_p \\rVert^2}\\Bigr)\n+ \\frac{1}{2}\\Bigl(p - \\frac{X_p^2}{\\lVert X_p \\rVert^2}\\Bigr)\\cos(\\lVert X_p\\rVert)\n+ \\frac{1}{\\lVert X_p \\rVert}\\sqrt{p}\\sin(\\lVert X_p\\rVert),\\] where  $X_p = \\frac{X}{\\sqrt{p}}$ , with its division meant elementwise, as well as for the operations  $X_p^2$  and  $\\sqrt{p}$ . source"},{"id":1395,"pagetitle":"Probability simplex","title":"Base.log","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Base.log-Tuple{ProbabilitySimplex, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::ProbabilitySimplex, p, q) Compute the logarithmic map of  p  and  q  on the  ProbabilitySimplex M . \\[\\log_pq = \\frac{d_{Œî^n}(p,q)}{\\sqrt{1-‚ü®\\sqrt{p},\\sqrt{q}‚ü©}}(\\sqrt{pq} - ‚ü®\\sqrt{p},\\sqrt{q}‚ü©p),\\] where  $pq$  and  $\\sqrt{p}$  is meant elementwise. source"},{"id":1396,"pagetitle":"Probability simplex","title":"Base.rand","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Base.rand-Tuple{ProbabilitySimplex}","content":" Base.rand  ‚Äî  Method rand(::ProbabilitySimplex; vector_at=nothing, œÉ::Real=1.0) When  vector_at  is  nothing , return a random (uniform over the Fisher-Rao metric; that is, uniform with respect to the  n -sphere whose positive orthant is mapped to the simplex). point  x  on the  ProbabilitySimplex  manifold  M  according to the isometric embedding into the  n -sphere by normalizing the vector length of a sample from a multivariate Gaussian. See [ Mar72 ]. When  vector_at  is not  nothing , return a (Gaussian) random vector from the tangent space  $T_{p}\\mathrm{\\Delta}^n$ by shifting a multivariate Gaussian with standard deviation  œÉ  to have a zero component sum. source"},{"id":1397,"pagetitle":"Probability simplex","title":"ManifoldDiff.riemannian_gradient","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldDiff.riemannian_gradient-Tuple{ProbabilitySimplex, Any, Any}","content":" ManifoldDiff.riemannian_gradient  ‚Äî  Method X = riemannian_gradient(M::ProbabilitySimplex, p, Y)\nriemannian_gradient!(M::ProbabilitySimplex, X, p, Y) Given a gradient  $Y = \\operatorname{grad} \\tilde f(p)$  in the embedding  $‚Ñù^{n+1}$  of the  ProbabilitySimplex $Œî^n$ , this function computes the Riemannian gradient  $X = \\operatorname{grad} f(p)$  where  $f$  is the function  $\\tilde f$  restricted to the manifold. The formula reads \\[    X = p ‚äô Y - ‚ü®p, Y‚ü©p,\\] where  $‚äô$  denotes the emelementwise product. source"},{"id":1398,"pagetitle":"Probability simplex","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Manifolds.manifold_volume-Tuple{ProbabilitySimplex}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(::ProbabilitySimplex) Return the volume of the  ProbabilitySimplex , i.e. volume of the  n -dimensional  Sphere  divided by  $2^{n+1}$ , corresponding to the volume of its positive orthant. source"},{"id":1399,"pagetitle":"Probability simplex","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Manifolds.volume_density-Tuple{ProbabilitySimplex, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(M::ProbabilitySimplex, p, X) Compute the volume density at point  p  on  ProbabilitySimplex M  for tangent vector  X . It is computed using isometry with positive orthant of a sphere. source"},{"id":1400,"pagetitle":"Probability simplex","title":"ManifoldsBase.change_metric","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.change_metric-Tuple{ProbabilitySimplex, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_metric  ‚Äî  Method change_metric(M::ProbabilitySimplex, ::EuclideanMetric, p, X) To change the metric, we are looking for a function  $c\\colon T_pŒî^n ‚Üí T_pŒî^n$  such that for all  $X,Y ‚àà T_pŒî^n$  This can be achieved by rewriting representer change in matrix form as  (Diagonal(p) - p * p') * X  and taking square root of the matrix source"},{"id":1401,"pagetitle":"Probability simplex","title":"ManifoldsBase.change_representer","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.change_representer-Tuple{ProbabilitySimplex, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(M::ProbabilitySimplex, ::EuclideanMetric, p, X) Given a tangent vector with respect to the metric from the embedding, the  EuclideanMetric , the representer of a linear functional on the tangent space is adapted as  $Z = p .* X .- p .* dot(p, X)$ . The first part ‚Äúcompensates‚Äù for the division by  $p$  in the Riemannian metric on the  ProbabilitySimplex  and the second part performs appropriate projection to keep the vector tangent. For details see Proposition 2.3 in [ APSS17 ]. source"},{"id":1402,"pagetitle":"Probability simplex","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.check_point-Union{Tuple{boundary}, Tuple{ProbabilitySimplex{<:Any, boundary}, Any}} where boundary","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::ProbabilitySimplex, p; kwargs...) Check whether  p  is a valid point on the  ProbabilitySimplex M , i.e. is a point in the embedding with positive entries that sum to one The tolerance for the last test can be set using the  kwargs... . source"},{"id":1403,"pagetitle":"Probability simplex","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.check_vector-Union{Tuple{T}, Tuple{ProbabilitySimplex, Any, T}} where T","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::ProbabilitySimplex, p, X; kwargs... ) Check whether  X  is a tangent vector to  p  on the  ProbabilitySimplex M , i.e. after  check_point (M,p) ,  X  has to be of same dimension as  p  and its elements have to sum to one. The tolerance for the last test can be set using the  kwargs... . source"},{"id":1404,"pagetitle":"Probability simplex","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.distance-Tuple{ProbabilitySimplex, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M, p, q) Compute the distance between two points on the  ProbabilitySimplex M . The formula reads \\[d_{Œî^n}(p,q) = 2\\arccos \\biggl( \\sum_{i=1}^{n+1} \\sqrt{p_i q_i} \\biggr)\\] source"},{"id":1405,"pagetitle":"Probability simplex","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.injectivity_radius-Tuple{ProbabilitySimplex, Any}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::ProbabilitySimplex, p) Compute the injectivity radius on the  ProbabilitySimplex M  at the point  p , i.e. the distanceradius to a point near/on the boundary, that could be reached by following the geodesic. source"},{"id":1406,"pagetitle":"Probability simplex","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.inner-Union{Tuple{boundary}, Tuple{ProbabilitySimplex{<:Any, boundary}, Any, Any, Any}} where boundary","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::ProbabilitySimplex, p, X, Y) Compute the inner product of two tangent vectors  X ,  Y  from the tangent space  $T_pŒî^n$  at  p . The formula reads \\[g_p(X,Y) = \\sum_{i=1}^{n+1}\\frac{X_iY_i}{p_i}\\] When  M  includes boundary, we can just skip coordinates where  $p_i$  is equal to 0, see Proposition 2.1 in [ AJLS17 ]. source"},{"id":1407,"pagetitle":"Probability simplex","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.inverse_retract-Tuple{ProbabilitySimplex, Any, Any, SoftmaxInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::ProbabilitySimplex, p, q, ::SoftmaxInverseRetraction) Compute a first order approximation by projection. The formula reads \\[\\operatorname{retr}^{-1}_p q = \\bigl( I_{n+1} - \\frac{1}{n}\\mathbb{1}^{n+1,n+1} \\bigr)(\\log(q)-\\log(p))\\] where  $\\mathbb{1}^{m,n}$  is the size  (m,n)  matrix containing ones, and  $\\log$  is applied elementwise. source"},{"id":1408,"pagetitle":"Probability simplex","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.is_flat-Tuple{ProbabilitySimplex}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::ProbabilitySimplex) Return false.  ProbabilitySimplex  is not a flat manifold. source"},{"id":1409,"pagetitle":"Probability simplex","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.manifold_dimension-Tuple{ProbabilitySimplex}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::ProbabilitySimplex) Returns the manifold dimension of the probability simplex in  $‚Ñù^{n+1}$ , i.e. \\[    \\dim_{Œî^n} = n.\\] source"},{"id":1410,"pagetitle":"Probability simplex","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.project-Tuple{ProbabilitySimplex, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::ProbabilitySimplex, p, Y) Project  Y  from the embedding onto the tangent space at  p  on the  ProbabilitySimplex M . The formula reads ` math \\operatorname{proj}_{Œî^n}(p,Y) = Y - \\bar{Y}  where  $\\bar{Y}$  denotes mean of  $Y$ . source"},{"id":1411,"pagetitle":"Probability simplex","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.project-Tuple{ProbabilitySimplex, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::ProbabilitySimplex, p) project  p  from the embedding onto the  ProbabilitySimplex M . The formula reads \\[\\operatorname{proj}_{Œî^n}(p) = \\frac{1}{‚ü®\\mathbb 1,p‚ü©}p,\\] where  $\\mathbb 1 ‚àà ‚Ñù$  denotes the vector of ones. Not that this projection is only well-defined if  $p$  has positive entries. source"},{"id":1412,"pagetitle":"Probability simplex","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.representation_size-Tuple{ProbabilitySimplex}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(::ProbabilitySimplex) Return the representation size of points in the  $n$ -dimensional probability simplex, i.e. an array size of  (n+1,) . source"},{"id":1413,"pagetitle":"Probability simplex","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.retract-Tuple{ProbabilitySimplex, Any, Any, SoftmaxRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::ProbabilitySimplex, p, X, ::SoftmaxRetraction) Compute a first order approximation by applying the softmax function. The formula reads \\[\\operatorname{retr}_p X = \\frac{p\\mathrm{e}^X}{‚ü®p,\\mathrm{e}^X‚ü©},\\] where multiplication, exponentiation and division are meant elementwise. source"},{"id":1414,"pagetitle":"Probability simplex","title":"ManifoldsBase.riemann_tensor","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.riemann_tensor-Tuple{ProbabilitySimplex, Vararg{Any, 4}}","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(::ProbabilitySimplex, p, X, Y, Z) Compute the Riemann tensor  $R(X,Y)Z$  at point  p  on  ProbabilitySimplex M . It is computed using isometry with positive orthant of a sphere. source"},{"id":1415,"pagetitle":"Probability simplex","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/probabilitysimplex/#ManifoldsBase.zero_vector-Tuple{ProbabilitySimplex, Any}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(M::ProbabilitySimplex, p) Return the zero tangent vector in the tangent space of the point  p   from the  ProbabilitySimplex M , i.e. its representation by the zero vector in the embedding. source"},{"id":1416,"pagetitle":"Probability simplex","title":"Statistics.mean","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Statistics.mean-Tuple{ProbabilitySimplex, Vararg{Any}}","content":" Statistics.mean  ‚Äî  Method mean(\n    M::ProbabilitySimplex,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method = GeodesicInterpolation();\n    kwargs...,\n) Compute the Riemannian  mean  of  x  using  GeodesicInterpolation . source"},{"id":1417,"pagetitle":"Probability simplex","title":"Euclidean metric","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Euclidean-metric","content":" Euclidean metric"},{"id":1418,"pagetitle":"Probability simplex","title":"Base.rand","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Base.rand-Tuple{MetricManifold{‚Ñù, <:ProbabilitySimplex, <:EuclideanMetric}}","content":" Base.rand  ‚Äî  Method rand(::MetricManifold{‚Ñù,<:ProbabilitySimplex,<:EuclideanMetric}; vector_at=nothing, œÉ::Real=1.0) When  vector_at  is  nothing , return a random (uniform) point  x  on the  ProbabilitySimplex  with the Euclidean metric manifold  M  by normalizing independent exponential draws to unit sum, see [ Dev86 ], Theorems 2.1 and 2.2 on p. 207 and 208, respectively. When  vector_at  is not  nothing , return a (Gaussian) random vector from the tangent space  $T_{p}\\mathrm{\\Delta}^n$ by shifting a multivariate Gaussian with standard deviation  œÉ  to have a zero component sum. source"},{"id":1419,"pagetitle":"Probability simplex","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Manifolds.manifold_volume-Tuple{MetricManifold{‚Ñù, <:ProbabilitySimplex, <:EuclideanMetric}}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(::MetricManifold{‚Ñù,<:ProbabilitySimplex{n},<:EuclideanMetric})) where {n} Return the volume of the  ProbabilitySimplex  with the Euclidean metric. The formula reads  $\\frac{\\sqrt{n+1}}{n!}$ source"},{"id":1420,"pagetitle":"Probability simplex","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Manifolds.volume_density-Tuple{MetricManifold{‚Ñù, <:ProbabilitySimplex, <:EuclideanMetric}, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(::MetricManifold{‚Ñù,<:ProbabilitySimplex,<:EuclideanMetric}, p, X) Compute the volume density at point  p  on  ProbabilitySimplex M  for tangent vector  X . It is equal to 1. source"},{"id":1421,"pagetitle":"Probability simplex","title":"Real probability amplitudes","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Real-probability-amplitudes","content":" Real probability amplitudes An isometric embedding of interior of  ProbabilitySimplex  in positive orthant of the  Sphere  is established through functions  simplex_to_amplitude  and  amplitude_to_simplex . Some properties extend to the boundary but not all. This embedding isometrically maps the Fisher-Rao metric on the open probability simplex to the sphere of radius 1 with Euclidean metric. More details can be found in Section 2.2 of [ AJLS17 ]. The name derives from the notion of probability amplitudes in quantum mechanics. They are complex-valued and their squared norm corresponds to probability. This construction restricted to real valued amplitudes results in this embedding."},{"id":1422,"pagetitle":"Probability simplex","title":"Manifolds.amplitude_to_simplex","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Manifolds.amplitude_to_simplex-Tuple{ProbabilitySimplex, Any}","content":" Manifolds.amplitude_to_simplex  ‚Äî  Method amplitude_to_simplex(M::ProbabilitySimplex, p) Convert point (real) probability amplitude  p  on to a point on  ProbabilitySimplex . The formula reads  $(p_1^2, p_2^2, ‚Ä¶, p_{N+1}^2)$ . This is an isometry from the interior of the positive orthant of a sphere to interior of the probability simplex. source"},{"id":1423,"pagetitle":"Probability simplex","title":"Manifolds.amplitude_to_simplex_diff","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Manifolds.amplitude_to_simplex_diff-Tuple{ProbabilitySimplex, Any, Any}","content":" Manifolds.amplitude_to_simplex_diff  ‚Äî  Method amplitude_to_simplex_diff(M::ProbabilitySimplex, p, X) Compute differential of  amplitude_to_simplex  of a point  p  on  ProbabilitySimplex  at tangent vector  X  from the tangent space at  p  from a sphere. source"},{"id":1424,"pagetitle":"Probability simplex","title":"Manifolds.simplex_to_amplitude","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Manifolds.simplex_to_amplitude-Tuple{ProbabilitySimplex, Any}","content":" Manifolds.simplex_to_amplitude  ‚Äî  Method simplex_to_amplitude(M::ProbabilitySimplex, p) Convert point  p  on  ProbabilitySimplex  to (real) probability amplitude. The formula reads  $(\\sqrt{p_1}, \\sqrt{p_2}, ‚Ä¶, \\sqrt{p_{N+1}})$ . This is an isometry from the interior of the probability simplex to the interior of the positive orthant of a sphere. source"},{"id":1425,"pagetitle":"Probability simplex","title":"Manifolds.simplex_to_amplitude_diff","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Manifolds.simplex_to_amplitude_diff-Tuple{ProbabilitySimplex, Any, Any}","content":" Manifolds.simplex_to_amplitude_diff  ‚Äî  Method simplex_to_amplitude_diff(M::ProbabilitySimplex, p, X) Compute differential of  simplex_to_amplitude  of a point on  p  one  ProbabilitySimplex  at tangent vector  X  from the tangent space at  p  from a sphere. source"},{"id":1426,"pagetitle":"Probability simplex","title":"Literature","ref":"/manifolds/stable/manifolds/probabilitysimplex/#Literature","content":" Literature [AJLS17] N.¬†Ay, J.¬†Jost, H.¬†V.¬†L√™ and L.¬†Schwachh√∂fer.  Information Geometry  (Springer Cham, 2017). [Dev86] L.¬†Devroye.  Non-Uniform Random Variate Generation  (Springer New York, NY, 1986). [Mar72] G.¬†Marsaglia.  Choosing a Point from the Surface of a Sphere .  Annals¬†of¬†Mathematical¬†Statistics  43 , 645‚Äì646  (1972). [APSS17] F.¬†√Östr√∂m, S.¬†Petra, B.¬†Schmitzer and C.¬†Schn√∂rr.  Image Labeling by Assignment .  Journal¬†of¬†Mathematical¬†Imaging¬†and¬†Vision  58 , 211‚Äì238  (2017),  arXiv:1603.05285 ."},{"id":1429,"pagetitle":"Product manifold","title":"Product manifold","ref":"/manifolds/stable/manifolds/product/#ProductManifoldSection","content":" Product manifold Product manifold  $\\mathcal M = \\mathcal{M}_1 √ó \\mathcal{M}_2 √ó ‚Ä¶ √ó \\mathcal{M}_n$  of manifolds  $\\mathcal{M}_1, \\mathcal{M}_2, ‚Ä¶, \\mathcal{M}_n$ . Points on the product manifold can be constructed using  ArrayPartition  (from  RecursiveArrayTools.jl ) with canonical projections  $Œ†_i : \\mathcal{M} ‚Üí \\mathcal{M}_i$  for  $i ‚àà 1, 2, ‚Ä¶, n$  provided by  submanifold_component ."},{"id":1430,"pagetitle":"Product manifold","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolds/stable/manifolds/product/#ManifoldDiff.riemannian_Hessian-Tuple{ProductManifold, Vararg{Any, 4}}","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method Y = riemannian_Hessian(M::ProductManifold, p, G, H, X)\nriemannian_Hessian!(M::ProductManifold, Y, p, G, H, X) Compute the Riemannian Hessian  $\\operatorname{Hess} f(p)[X]$  given the Euclidean gradient  $‚àá f(\\tilde p)$  in  G  and the Euclidean Hessian  $‚àá^2 f(\\tilde p)[\\tilde X]$  in  H , where  $\\tilde p, \\tilde X$  are the representations of  $p,X$  in the embedding,. On a product manifold, this decouples and can be computed elementwise. source"},{"id":1431,"pagetitle":"Product manifold","title":"Manifolds.flat","ref":"/manifolds/stable/manifolds/product/#Manifolds.flat-Tuple{ProductManifold, Vararg{Any}}","content":" Manifolds.flat  ‚Äî  Method flat(M::ProductManifold, p, X::FVector{TangentSpaceType}) use the musical isomorphism to transform the tangent vector  X  from the tangent space at  p  on the  ProductManifold M  to a cotangent vector. This can be done elementwise for every entry of  X  (with respect to the corresponding entry in  p ) separately. source"},{"id":1432,"pagetitle":"Product manifold","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/product/#Manifolds.manifold_volume-Tuple{ProductManifold}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(M::ProductManifold) Return the volume of  ProductManifold M , i.e. product of volumes of the manifolds  M  is constructed from. source"},{"id":1433,"pagetitle":"Product manifold","title":"Manifolds.sharp","ref":"/manifolds/stable/manifolds/product/#Manifolds.sharp-Tuple{ProductManifold, Vararg{Any}}","content":" Manifolds.sharp  ‚Äî  Method sharp(M::ProductManifold, p, Œæ::FVector{CotangentSpaceType}) Use the musical isomorphism to transform the cotangent vector  Œæ  from the tangent space at  p  on the  ProductManifold M  to a tangent vector. This can be done elementwise for every entry of  Œæ  (and  p ) separately source"},{"id":1434,"pagetitle":"Product manifold","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/product/#Manifolds.volume_density-Tuple{ProductManifold, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(M::ProductManifold, p, X) Return volume density on the  ProductManifold M , i.e. product of constituent volume densities. source"},{"id":1437,"pagetitle":"Projective space","title":"Projective space","ref":"/manifolds/stable/manifolds/projectivespace/#Projective-space","content":" Projective space"},{"id":1438,"pagetitle":"Projective space","title":"Manifolds.AbstractProjectiveSpace","ref":"/manifolds/stable/manifolds/projectivespace/#Manifolds.AbstractProjectiveSpace","content":" Manifolds.AbstractProjectiveSpace  ‚Äî  Type AbstractProjectiveSpace{ùîΩ} <: AbstractDecoratorManifold{ùîΩ} An abstract type to represent a projective space over  ùîΩ  that is represented isometrically in the embedding. source"},{"id":1439,"pagetitle":"Projective space","title":"Manifolds.ArrayProjectiveSpace","ref":"/manifolds/stable/manifolds/projectivespace/#Manifolds.ArrayProjectiveSpace","content":" Manifolds.ArrayProjectiveSpace  ‚Äî  Type ArrayProjectiveSpace{T<:Tuple,ùîΩ} <: AbstractProjectiveSpace{ùîΩ} The projective space  $ùîΩ‚Ñô^{n‚ÇÅ,n‚ÇÇ,‚Ä¶,n·µ¢}$  is the manifold of all lines in  $ùîΩ^{n‚ÇÅ,n‚ÇÇ,‚Ä¶,n·µ¢}$ . The default representation is in the embedding, i.e. as unit (Frobenius) norm matrices in  $ùîΩ^{n‚ÇÅ,n‚ÇÇ,‚Ä¶,n·µ¢}$ : \\[ùîΩ‚Ñô^{n_1, n_2, ‚Ä¶, n_i} := \\bigl\\{ [p] ‚äÇ ùîΩ^{n_1, n_2, ‚Ä¶, n_i} \\ \\big|\\ \\lVert p \\rVert_{\\mathrm{F}} = 1, Œª ‚àà ùîΩ, |Œª| = 1, p ‚àº p Œª \\bigr\\}.\\] where  $[p]$  is an equivalence class of points  $p$ ,  $‚àº$  indicates equivalence, and  $\\lVert ‚ãÖ \\rVert_{\\mathrm{F}}$  is the Frobenius norm. Note that unlike  ProjectiveSpace , the argument for  ArrayProjectiveSpace  is given by the size of the embedding. This means that  ProjectiveSpace(2)  and  ArrayProjectiveSpace(3)  are the same manifold. Additionally,  ArrayProjectiveSpace(n,1;field=ùîΩ)  and  Grassmann(n,1;field=ùîΩ)  are the same. The tangent space at point  $p$  is given by \\[T_p ùîΩ‚Ñô^{n_1, n_2, ‚Ä¶, n_i} := \\bigl\\{ X ‚àà ùîΩ^{n_1, n_2, ‚Ä¶, n_i}\\ |\\ ‚ü®p,X‚ü©_{\\mathrm{F}} = 0 \\bigr \\},\\] where  $‚ü®‚ãÖ,‚ãÖ‚ü©_{\\mathrm{F}}$  denotes the (Frobenius) inner product in the embedding  $ùîΩ^{n_1, n_2, ‚Ä¶, n_i}$ . Constructor ArrayProjectiveSpace(n‚ÇÅ,n‚ÇÇ,...,n·µ¢; field=‚Ñù) Generate the projective space  $ùîΩ‚Ñô^{n_1, n_2, ‚Ä¶, n_i}$ , defaulting to the real projective space, where  field  can also be used to generate the complex- and right-quaternionic projective spaces. source"},{"id":1440,"pagetitle":"Projective space","title":"Manifolds.ProjectiveSpace","ref":"/manifolds/stable/manifolds/projectivespace/#Manifolds.ProjectiveSpace","content":" Manifolds.ProjectiveSpace  ‚Äî  Type ProjectiveSpace{n,ùîΩ} <: AbstractProjectiveSpace{ùîΩ} The projective space  $ùîΩ‚Ñô^n$  is the manifold of all lines in  $ùîΩ^{n+1}$ . The default representation is in the embedding, i.e. as unit norm vectors in  $ùîΩ^{n+1}$ : \\[ùîΩ‚Ñô^n := \\bigl\\{ [p] ‚äÇ ùîΩ^{n+1} \\ \\big|\\ \\lVert p \\rVert = 1, Œª ‚àà ùîΩ, |Œª| = 1, p ‚àº p Œª \\bigr\\},\\] where  $[p]$  is an equivalence class of points  $p$ , and  $‚àº$  indicates equivalence. For example, the real projective space  $‚Ñù‚Ñô^n$  is represented as the unit sphere  $ùïä^n$ , where antipodal points are considered equivalent. The tangent space at point  $p$  is given by \\[T_p ùîΩ‚Ñô^{n} := \\bigl\\{ X ‚àà ùîΩ^{n+1}\\ \\big|\\ ‚ü®p,X‚ü© = 0 \\bigr \\},\\] where  $‚ü®‚ãÖ,‚ãÖ‚ü©$  denotes the inner product in the embedding  $ùîΩ^{n+1}$ . When  $ùîΩ = ‚Ñç$ , this implementation of  $‚Ñç‚Ñô^n$  is the right-quaternionic projective space. Constructor ProjectiveSpace(n[, field=‚Ñù]) Generate the projective space  $ùîΩ‚Ñô^{n} ‚äÇ ùîΩ^{n+1}$ , defaulting to the real projective space  $‚Ñù‚Ñô^n$ , where  field  can also be used to generate the complex- and right-quaternionic projective spaces. source"},{"id":1441,"pagetitle":"Projective space","title":"Base.log","ref":"/manifolds/stable/manifolds/projectivespace/#Base.log-Tuple{AbstractProjectiveSpace, Any, Any}","content":" Base.log  ‚Äî  Method log(M::AbstractProjectiveSpace, p, q) Compute the logarithmic map on  AbstractProjectiveSpace M $= ùîΩ‚Ñô^n$ , i.e. the tangent vector whose corresponding  geodesic  starting from  p  reaches  q  after time 1 on  M . The formula reads \\[\\log_p q = (q Œª - \\cos Œ∏ p) \\frac{Œ∏}{\\sin Œ∏},\\] where  $Œ∏ = \\arccos|‚ü®q, p‚ü©_{\\mathrm{F}}|$  is the  distance  between  $p$  and  $q$ ,  $‚ü®‚ãÖ, ‚ãÖ‚ü©_{\\mathrm{F}}$  is the Frobenius inner product, and  $Œª = \\frac{‚ü®q, p‚ü©_{\\mathrm{F}}}{|‚ü®q, p‚ü©_{\\mathrm{F}}|} ‚àà ùîΩ$  is the unit scalar that minimizes  $d_{ùîΩ^{n+1}}(p - q Œª)$ . That is,  $q Œª$  is the member of the equivalence class  $[q]$  that is closest to  $p$  in the embedding. As a result,  $\\exp_p \\circ \\log_p \\colon q ‚Ü¶ q Œª$ . The logarithmic maps for the real  AbstractSphere $ùïä^n$  and the real projective space  $‚Ñù‚Ñô^n$  are identical when  $p$  and  $q$  are in the same hemisphere. source"},{"id":1442,"pagetitle":"Projective space","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/projectivespace/#Manifolds.manifold_volume-Tuple{AbstractProjectiveSpace{‚Ñù}}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(M::AbstractProjectiveSpace{‚Ñù}) Volume of the  $n$ -dimensional  AbstractProjectiveSpace M . The formula reads: \\[\\frac{\\pi^{(n+1)/2}}{Œì((n+1)/2)},\\] where  $Œì$  denotes the  Gamma function . For details see [ BST03 ]. source"},{"id":1443,"pagetitle":"Projective space","title":"ManifoldsBase._isapprox","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase._isapprox-Tuple{AbstractProjectiveSpace, Any, Any}","content":" ManifoldsBase._isapprox  ‚Äî  Method isapprox(M::AbstractProjectiveSpace, p, q; kwargs...) Check that points  p  and  q  on the  AbstractProjectiveSpace M $=ùîΩ‚Ñô^n$  are members of the same equivalence class, i.e. that  $p = q Œª$  for some element  $Œª ‚àà ùîΩ$  with unit absolute value, that is,  $|Œª| = 1$ . This is equivalent to the Riemannian  distance  being 0. source"},{"id":1444,"pagetitle":"Projective space","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.check_point-Tuple{AbstractProjectiveSpace, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::AbstractProjectiveSpace, p; kwargs...) Check whether  p  is a valid point on the  AbstractProjectiveSpace M , i.e. that it has the same size as elements of the embedding and has unit Frobenius norm. The tolerance for the norm check can be set using the  kwargs... . source"},{"id":1445,"pagetitle":"Projective space","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.check_vector-Union{Tuple{T}, Tuple{AbstractProjectiveSpace, Any, T}} where T","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::AbstractProjectiveSpace, p, X; kwargs... ) Check whether  X  is a tangent vector in the tangent space of  p  on the  AbstractProjectiveSpace M , i.e. that  X  has the same size as elements of the tangent space of the embedding and that the Frobenius inner product  $‚ü®p, X‚ü©_{\\mathrm{F}} = 0$ . source"},{"id":1446,"pagetitle":"Projective space","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.distance-Tuple{AbstractProjectiveSpace, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::AbstractProjectiveSpace, p, q) Compute the Riemannian distance on  AbstractProjectiveSpace M $=ùîΩ‚Ñô^n$  between points  p  and  q , i.e. \\[d_{ùîΩ‚Ñô^n}(p, q) = \\arccos\\bigl| ‚ü®p, q‚ü©_{\\mathrm{F}} \\bigr|.\\] Note that this definition is similar to that of the  AbstractSphere . However, the absolute value ensures that all equivalent  p  and  q  have the same pairwise distance. source"},{"id":1447,"pagetitle":"Projective space","title":"ManifoldsBase.get_coordinates","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.get_coordinates-Tuple{AbstractProjectiveSpace{‚Ñù}, Any, Any, DefaultOrthonormalBasis}","content":" ManifoldsBase.get_coordinates  ‚Äî  Method get_coordinates(M::AbstractProjectiveSpace, p, X, B::DefaultOrthonormalBasis{‚Ñù}) Represent the tangent vector  $X$  at point  $p$  from the  AbstractProjectiveSpace $M = ùîΩ‚Ñô^n$  in an orthonormal basis by unitarily transforming the hyperplane containing  $X$ , whose normal is  $p$ , to the hyperplane whose normal is the  $x$ -axis. Given  $q = p \\overline{Œª} + x$ , where  $Œª = \\frac{‚ü®x, p‚ü©_{\\mathrm{F}}}{|‚ü®x, p‚ü©_{\\mathrm{F}}|}$ ,  $‚ü®‚ãÖ, ‚ãÖ‚ü©_{\\mathrm{F}}$  denotes the Frobenius inner product, and  $\\overline{‚ãÖ}$  denotes complex or quaternionic conjugation, the formula for  $Y$  is \\[\\begin{pmatrix}0 \\\\ Y\\end{pmatrix} = \\left(X - q\\frac{2 ‚ü®q, X‚ü©_{\\mathrm{F}}}{‚ü®q, q‚ü©_{\\mathrm{F}}}\\right)\\overline{Œª}.\\] source"},{"id":1448,"pagetitle":"Projective space","title":"ManifoldsBase.get_vector","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.get_vector-Tuple{AbstractProjectiveSpace, Any, Any, DefaultOrthonormalBasis{‚Ñù}}","content":" ManifoldsBase.get_vector  ‚Äî  Method get_vector(M::AbstractProjectiveSpace, p, X, B::DefaultOrthonormalBasis{‚Ñù}) Convert a one-dimensional vector of coefficients  $X$  in the basis  B  of the tangent space at  $p$  on the  AbstractProjectiveSpace $M=ùîΩ‚Ñô^n$  to a tangent vector  $Y$  at  $p$  by unitarily transforming the hyperplane containing  $X$ , whose normal is the  $x$ -axis, to the hyperplane whose normal is  $p$ . Given  $q = p \\overline{Œª} + x$ , where  $Œª = \\frac{‚ü®x, p‚ü©_{\\mathrm{F}}}{|‚ü®x, p‚ü©_{\\mathrm{F}}|}$ ,  $‚ü®‚ãÖ, ‚ãÖ‚ü©_{\\mathrm{F}}$  denotes the Frobenius inner product, and  $\\overline{‚ãÖ}$  denotes complex or quaternionic conjugation, the formula for  $Y$  is \\[Y = \\left(X - q\\frac{2 \\left\\langle q, \\begin{pmatrix}0 \\\\ X\\end{pmatrix}\\right\\rangle_{\\mathrm{F}}}{‚ü®q, q‚ü©_{\\mathrm{F}}}\\right) Œª.\\] source"},{"id":1449,"pagetitle":"Projective space","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.inverse_retract-Tuple{AbstractProjectiveSpace, Any, Any, Union{PolarInverseRetraction, ProjectionInverseRetraction, QRInverseRetraction}}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::AbstractProjectiveSpace, p, q, method::ProjectionInverseRetraction)\ninverse_retract(M::AbstractProjectiveSpace, p, q, method::PolarInverseRetraction)\ninverse_retract(M::AbstractProjectiveSpace, p, q, method::QRInverseRetraction) Compute the equivalent inverse retraction  ProjectionInverseRetraction ,  PolarInverseRetraction  on the  AbstractProjectiveSpace  manifold  M $=ùîΩ‚Ñô^n$ , i.e. \\[\\operatorname{retr}_p^{-1} q = q \\frac{1}{‚ü®p, q‚ü©_{\\mathrm{F}}} - p,\\] where  $‚ü®‚ãÖ, ‚ãÖ‚ü©_{\\mathrm{F}}$  is the Frobenius inner product. Note that this inverse retraction is equivalent to the three corresponding inverse retractions on  Grassmann(n+1,1,ùîΩ) , where the three inverse retractions in this case coincide. For  $‚Ñù‚Ñô^n$ , it is the same as the  ProjectionInverseRetraction  on the real  Sphere . source"},{"id":1450,"pagetitle":"Projective space","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.is_flat-Tuple{AbstractProjectiveSpace}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(M::AbstractProjectiveSpace) Return true if  AbstractProjectiveSpace  is of dimension 1 and false otherwise. source"},{"id":1451,"pagetitle":"Projective space","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.manifold_dimension-Union{Tuple{AbstractProjectiveSpace{ùîΩ}}, Tuple{ùîΩ}} where ùîΩ","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::AbstractProjectiveSpace{ùîΩ}) where {ùîΩ} Return the real dimension of the  AbstractProjectiveSpace M , respectively i.e. the real dimension of the embedding minus the real dimension of the field  ùîΩ . source"},{"id":1452,"pagetitle":"Projective space","title":"ManifoldsBase.parallel_transport_direction","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.parallel_transport_direction-Tuple{AbstractProjectiveSpace, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_direction  ‚Äî  Method parallel_transport_direction(M::AbstractProjectiveSpace, p, X, d) Parallel transport a vector  X  from the tangent space at a point  p  on the  AbstractProjectiveSpace M  along the  geodesic  in the direction indicated by the tangent vector  d , i.e. \\[\\mathcal{P}_{\\exp_p (d) ‚Üê p}(X) = X - \\left(p \\frac{\\sin Œ∏}{Œ∏} + d \\frac{1 - \\cos Œ∏}{Œ∏^2}\\right) ‚ü®d, X‚ü©_p,\\] where  $Œ∏ = \\lVert d \\rVert$ , and  $‚ü®‚ãÖ, ‚ãÖ‚ü©_p$  is the  inner  product at the point  $p$ . For the real projective space, this is equivalent to the same vector transport on the real  AbstractSphere . source"},{"id":1453,"pagetitle":"Projective space","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.parallel_transport_to-Tuple{AbstractProjectiveSpace, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::AbstractProjectiveSpace, p, X, q) Parallel transport a vector  X  from the tangent space at a point  p  on the  AbstractProjectiveSpace M $=ùîΩ‚Ñô^n$  to the tangent space at another point  q . This implementation proceeds by transporting  $X$  to  $T_{q Œª} M$  using the same approach as  parallel_transport_direction , where  $Œª = \\frac{‚ü®q, p‚ü©_{\\mathrm{F}}}{|‚ü®q, p‚ü©_{\\mathrm{F}}|} ‚àà ùîΩ$  is the unit scalar that takes  $q$  to the member  $q Œª$  of its equivalence class  $[q]$  closest to  $p$  in the embedding. It then maps the transported vector from  $T_{q Œª} M$  to  $T_{q} M$ . The resulting transport to  $T_{q} M$  is \\[\\mathcal{P}_{q ‚Üê p}(X) = \\left(X - \\left(p \\frac{\\sin Œ∏}{Œ∏} + d \\frac{1 - \\cos Œ∏}{Œ∏^2}\\right) ‚ü®d, X‚ü©_p\\right) \\overline{Œª},\\] where  $d = \\log_p q$  is the direction of the transport,  $Œ∏ = \\lVert d \\rVert_p$  is the  distance  between  $p$  and  $q$ , and  $\\overline{‚ãÖ}$  denotes complex or quaternionic conjugation. source"},{"id":1454,"pagetitle":"Projective space","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.project-Tuple{AbstractProjectiveSpace, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::AbstractProjectiveSpace, p, X) Orthogonally project the point  X  onto the tangent space at  p  on the  AbstractProjectiveSpace M : \\[\\operatorname{proj}_p (X) = X - p‚ü®p, X‚ü©_{\\mathrm{F}},\\] where  $‚ü®‚ãÖ, ‚ãÖ‚ü©_{\\mathrm{F}}$  denotes the Frobenius inner product. For the real  AbstractSphere  and  AbstractProjectiveSpace , this projection is the same. source"},{"id":1455,"pagetitle":"Projective space","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.project-Tuple{AbstractProjectiveSpace, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::AbstractProjectiveSpace, p) Orthogonally project the point  p  from the embedding onto the  AbstractProjectiveSpace M : \\[\\operatorname{proj}(p) = \\frac{p}{\\lVert p \\rVert}_{\\mathrm{F}},\\] where  $\\lVert ‚ãÖ \\rVert_{\\mathrm{F}}$  denotes the Frobenius norm. This is identical to projection onto the  AbstractSphere . source"},{"id":1456,"pagetitle":"Projective space","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.representation_size-Tuple{ArrayProjectiveSpace}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::AbstractProjectiveSpace) Return the size points on the  AbstractProjectiveSpace M  are represented as, i.e., the representation size of the embedding. source"},{"id":1457,"pagetitle":"Projective space","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/projectivespace/#ManifoldsBase.retract-Tuple{AbstractProjectiveSpace, Any, Any, Union{PolarRetraction, ProjectionRetraction, QRRetraction}}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::AbstractProjectiveSpace, p, X, method::ProjectionRetraction)\nretract(M::AbstractProjectiveSpace, p, X, method::PolarRetraction)\nretract(M::AbstractProjectiveSpace, p, X, method::QRRetraction) Compute the equivalent retraction  ProjectionRetraction , and  QRRetraction  on the  AbstractProjectiveSpace  manifold  M $=ùîΩ‚Ñô^n$ , i.e. \\[\\operatorname{retr}_p X = \\operatorname{proj}_p(p + X).\\] Note that this retraction is equivalent to the three corresponding retractions on  Grassmann(n+1,1,ùîΩ) , where in this case they coincide. For  $‚Ñù‚Ñô^n$ , it is the same as the  ProjectionRetraction  on the real  Sphere . source"},{"id":1458,"pagetitle":"Projective space","title":"Statistics.mean","ref":"/manifolds/stable/manifolds/projectivespace/#Statistics.mean-Tuple{AbstractProjectiveSpace, Vararg{Any}}","content":" Statistics.mean  ‚Äî  Method mean(\n    M::AbstractProjectiveSpace,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method = GeodesicInterpolationWithinRadius(œÄ/4);\n    kwargs...,\n) Compute the Riemannian  mean  of points in vector  x  using  GeodesicInterpolationWithinRadius . source"},{"id":1461,"pagetitle":"Quotient manifold","title":"Quotient manifold","ref":"/manifolds/stable/manifolds/quotient/#QuotientManifoldSection","content":" Quotient manifold"},{"id":1462,"pagetitle":"Quotient manifold","title":"Manifolds.IsQuotientManifold","ref":"/manifolds/stable/manifolds/quotient/#Manifolds.IsQuotientManifold","content":" Manifolds.IsQuotientManifold  ‚Äî  Type IsQuotientManifold <: AbstractTrait Specify that a certain decorated manifold is a quotient manifold in the sense that it provides implicitly (or explicitly through  QuotientManifold   properties of a quotient manifold. See  QuotientManifold  for more details. source"},{"id":1463,"pagetitle":"Quotient manifold","title":"Manifolds.QuotientManifold","ref":"/manifolds/stable/manifolds/quotient/#Manifolds.QuotientManifold","content":" Manifolds.QuotientManifold  ‚Äî  Type QuotientManifold{M <: AbstractManifold{ùîΩ}, N} <: AbstractManifold{ùîΩ} Equip a manifold  $\\mathcal M$  explicitly with the property of being a quotient manifold. A manifold  $\\mathcal M$  is then a a quotient manifold of another manifold  $\\mathcal N$ , i.e. for an  equivalence relation $‚àº$  on  $\\mathcal N$  we have \\[    \\mathcal M = \\mathcal N / ‚àº = \\bigl\\{ [p] : p ‚àà \\mathcal N \\bigr\\},\\] where  $[p] ‚âî \\{ q ‚àà \\mathcal N : q ‚àº p\\}$  denotes the equivalence class containing  $p$ . For more details see Subsection 3.4.1 [ AMS08 ]. This manifold type models an explicit quotient structure. This should be done if either the default implementation of  $\\mathcal M$  uses another representation different from the quotient structure or if it provides a (default) quotient structure that is different from the one introduced here. Fields manifold  ‚Äì the manifold  $\\mathcal M$  in the introduction above. total_space  ‚Äì the manifold  $\\mathcal N$  in the introduction above. Constructor QuotientManifold(M,N) Create a manifold where  M  is the quotient manifold and  N is its total space. source"},{"id":1464,"pagetitle":"Quotient manifold","title":"Provided functions","ref":"/manifolds/stable/manifolds/quotient/#Provided-functions","content":" Provided functions"},{"id":1465,"pagetitle":"Quotient manifold","title":"Manifolds.canonical_project!","ref":"/manifolds/stable/manifolds/quotient/#Manifolds.canonical_project!-Tuple{AbstractManifold, Any, Any}","content":" Manifolds.canonical_project!  ‚Äî  Method canonical_project!(M, q, p) Compute the canonical projection  $œÄ$  on a manifold  $\\mathcal M$  that  IsQuotientManifold , e.g. a  QuotientManifold  in place of  q . See  canonical_project  for more details. source"},{"id":1466,"pagetitle":"Quotient manifold","title":"Manifolds.canonical_project","ref":"/manifolds/stable/manifolds/quotient/#Manifolds.canonical_project-Tuple{AbstractManifold, Any}","content":" Manifolds.canonical_project  ‚Äî  Method canonical_project(M, p) Compute the canonical projection  $œÄ$  on a manifold  $\\mathcal M$  that  IsQuotientManifold , e.g. a  QuotientManifold . The canonical (or natural) projection  $œÄ$  from the total space  $\\mathcal N$  onto  $\\mathcal M$  given by \\[    œÄ = œÄ_{\\mathcal N, \\mathcal M} : \\mathcal N ‚Üí \\mathcal M, p ‚Ü¶ œÄ_{\\mathcal N, \\mathcal M}(p) = [p].\\] in other words, this function implicitly assumes, that the total space  $\\mathcal N$  is given, for example explicitly when  M  is a  QuotientManifold  and  p  is a point on  N . source"},{"id":1467,"pagetitle":"Quotient manifold","title":"Manifolds.differential_canonical_project!","ref":"/manifolds/stable/manifolds/quotient/#Manifolds.differential_canonical_project!-Tuple{AbstractManifold, Any, Any}","content":" Manifolds.differential_canonical_project!  ‚Äî  Method differential_canonical_project!(M, Y, p, X) Compute the differential of the canonical projection  $œÄ$  on a manifold  $\\mathcal M$  that  IsQuotientManifold , e.g. a  QuotientManifold . See  differential_canonical_project  for details. source"},{"id":1468,"pagetitle":"Quotient manifold","title":"Manifolds.differential_canonical_project","ref":"/manifolds/stable/manifolds/quotient/#Manifolds.differential_canonical_project-Tuple{AbstractManifold, Any, Any}","content":" Manifolds.differential_canonical_project  ‚Äî  Method differential_canonical_project(M, p, X) Compute the differential of the canonical projection  $œÄ$  on a manifold  $\\mathcal M$  that  IsQuotientManifold , e.g. a  QuotientManifold . The canonical (or natural) projection  $œÄ$  from the total space  $\\mathcal N$  onto  $\\mathcal M$ , such that its differential \\[ DœÄ(p) : T_p\\mathcal N ‚Üí T_{œÄ(p)}\\mathcal M\\] where again the total space might be implicitly assumed, or explicitly when using a  QuotientManifold M . So here  p  is a point on  N  and  X  is from  $T_p\\mathcal N$ . source"},{"id":1469,"pagetitle":"Quotient manifold","title":"Manifolds.get_orbit_action","ref":"/manifolds/stable/manifolds/quotient/#Manifolds.get_orbit_action-Tuple{AbstractManifold}","content":" Manifolds.get_orbit_action  ‚Äî  Method get_orbit_action(M::AbstractDecoratorManifold) Return the group action that generates the orbit of an equivalence class of the quotient manifold  M  for which equivalence classes are orbits of an action of a Lie group. For the case that \\[\\mathcal M = \\mathcal N / \\mathcal O,\\] where  $\\mathcal O$  is a Lie group with its group action generating the orbit. source"},{"id":1470,"pagetitle":"Quotient manifold","title":"Manifolds.get_total_space","ref":"/manifolds/stable/manifolds/quotient/#Manifolds.get_total_space-Tuple{AbstractManifold}","content":" Manifolds.get_total_space  ‚Äî  Method get_total_space(M::AbstractDecoratorManifold) Return the total space of a manifold that  IsQuotientManifold , e.g. a  QuotientManifold . source"},{"id":1471,"pagetitle":"Quotient manifold","title":"Manifolds.horizontal_component","ref":"/manifolds/stable/manifolds/quotient/#Manifolds.horizontal_component-Tuple{AbstractManifold, Any, Any}","content":" Manifolds.horizontal_component  ‚Äî  Method horizontal_component(N::AbstractManifold, p, X)\nhorizontal_compontent(QuotientManifold{ùîΩ,M,N}, p, X) Compute the horizontal component of tangent vector  X  at point  p  in the total space of quotient manifold  N . This is often written as the space  $\\mathrm{Hor}_p^œÄ\\mathcal N$ . source"},{"id":1472,"pagetitle":"Quotient manifold","title":"Manifolds.horizontal_lift!","ref":"/manifolds/stable/manifolds/quotient/#Manifolds.horizontal_lift!-Tuple{AbstractManifold, Any, Any, Any}","content":" Manifolds.horizontal_lift!  ‚Äî  Method horizontal_lift!(N, Y, q, X)\nhorizontal_lift!(QuotientManifold{ùîΩ,M,N}, Y, p, X) Compute the horizontal lift of  X  from  $T_p\\mathcal M$ ,  $p=œÄ(q)$ . to ` T_q\\mathcal N  in place of  Y . source"},{"id":1473,"pagetitle":"Quotient manifold","title":"Manifolds.horizontal_lift","ref":"/manifolds/stable/manifolds/quotient/#Manifolds.horizontal_lift-Tuple{AbstractManifold, Any, Any}","content":" Manifolds.horizontal_lift  ‚Äî  Method horizontal_lift(N::AbstractManifold, q, X)\nhorizontal_lift(::QuotientManifold{ùîΩ,M,N}, p, X) Given a point  q  in total space of quotient manifold  N  such that  $p=œÄ(q)$  is a point on a quotient manifold  M  (implicitly given for the first case) and a tangent vector  X  this method computes a tangent vector  Y  on the horizontal space of  $T_q\\mathcal N$ , i.e. the subspace that is orthogonal to the kernel of  $DœÄ(q)$ . source"},{"id":1474,"pagetitle":"Quotient manifold","title":"Manifolds.vertical_component","ref":"/manifolds/stable/manifolds/quotient/#Manifolds.vertical_component-Tuple{AbstractManifold, Any, Any}","content":" Manifolds.vertical_component  ‚Äî  Method vertical_component(N::AbstractManifold, p, X)\nvertical_component(QuotientManifold{ùîΩ,M,N}, p, X) Compute the vertical component of tangent vector  X  at point  p  in the total space of quotient manifold  N . This is often written as the space  $\\mathrm{ver}_p^œÄ\\mathcal N$ . source"},{"id":1477,"pagetitle":"Rotations","title":"Rotations","ref":"/manifolds/stable/manifolds/rotations/#Rotations","content":" Rotations The manifold  $\\mathrm{SO}(n)$  of orthogonal matrices with determinant  $+1$  in  $‚Ñù^{n√ón}$ , i.e. \\[\\mathrm{SO}(n) = \\bigl\\{R ‚àà ‚Ñù^{n√ón} \\big| R R^{\\mathrm{T}} =\nR^{\\mathrm{T}}R = I_n, \\det(R) = 1 \\bigr\\}\\] The Lie group  $\\mathrm{SO}(n)$  is a subgroup of the orthogonal group  $\\mathrm{O}(n)$  and also known as the special orthogonal group or the set of rotations group. See also  SpecialOrthogonal , which is this manifold equipped with the group operation. The tangent space to a point  $p ‚àà \\mathrm{SO}(n)$  is given by \\[T_p\\mathrm{SO}(n) = \\{X : X=pY,\\qquad Y=-Y^{\\mathrm{T}}\\},\\] i.e. all vectors that are a product of a skew symmetric matrix multiplied with  $p$ . Since the orthogonal matrices  $\\mathrm{SO}(n)$  are a Lie group, tangent vectors can also be represented by elements of the corresponding Lie algebra, which is the tangent space at the identity element. In the notation above, this means we just store the component  $Y$  of  $X$ . This convention allows for more efficient operations on tangent vectors. Tangent spaces at different points are different vector spaces. Let  $L_R: \\mathrm{SO}(n) ‚Üí \\mathrm{SO}(n)$  where  $R ‚àà \\mathrm{SO}(n)$  be the left-multiplication by  $R$ , that is  $L_R(S) = RS$ . The tangent space at rotation  $R$ ,  $T_R \\mathrm{SO}(n)$ , is related to the tangent space at the identity rotation  $I_n$  by the differential of  $L_R$  at identity,  $(\\mathrm{d}L_R)_{I_n} : T_{I_n} \\mathrm{SO}(n) ‚Üí T_R \\mathrm{SO}(n)$ . To convert the tangent vector representation at the identity rotation  $X ‚àà T_{I_n} \\mathrm{SO}(n)$  (i.e., the default) to the matrix representation of the corresponding tangent vector  $Y$  at a rotation  $R$  use the  embed  which implements the following multiplication:  $Y = RX ‚àà T_R \\mathrm{SO}(n)$ . You can compare the functions  log  and  exp  to see how it works in practice. Several common functions are also implemented together with  orthogonal and unitary matrices ."},{"id":1478,"pagetitle":"Rotations","title":"Manifolds.Rotations","ref":"/manifolds/stable/manifolds/rotations/#Manifolds.Rotations","content":" Manifolds.Rotations  ‚Äî  Type Rotations{T} <: AbstractManifold{‚Ñù} The manifold of rotation matrices of size  $n√ón$ , i.e. real-valued orthogonal matrices with determinant  $+1$ . Constructor Rotations(n::Int; parameter::Symbol=:type) Generate the manifold of  $n√ón$  rotation matrices. source"},{"id":1479,"pagetitle":"Rotations","title":"ManifoldDiff.jacobian_exp_argument","ref":"/manifolds/stable/manifolds/rotations/#ManifoldDiff.jacobian_exp_argument-Tuple{Rotations{ManifoldsBase.TypeParameter{Tuple{2}}}, Any, Any}","content":" ManifoldDiff.jacobian_exp_argument  ‚Äî  Method jacobian_exp_argument(M::Rotations{TypeParameter{Tuple{2}}}, p, X) Compute Jacobian of the exponential map with respect to the argument  X  in orthonormal coordinates on the  Rotations (2)  manifold. It is equal to matrix  $[1]$ , see [ SDA21 ], Appendix A. source"},{"id":1480,"pagetitle":"Rotations","title":"ManifoldDiff.jacobian_exp_argument","ref":"/manifolds/stable/manifolds/rotations/#ManifoldDiff.jacobian_exp_argument-Tuple{Rotations{ManifoldsBase.TypeParameter{Tuple{3}}}, Any, Any}","content":" ManifoldDiff.jacobian_exp_argument  ‚Äî  Method jacobian_exp_argument(M::Rotations{TypeParameter{Tuple{3}}}, p, X) Compute Jacobian of the exponential map with respect to the argument  X  in orthonormal coordinates on the  Rotations (3)  manifold. The formula reads \\[ùïÄ + \\frac{\\cos(Œ∏) - 1}{Œ∏^2} X + \\frac{Œ∏ - \\sin(Œ∏)}{Œ∏^3} X^2,\\] where  $Œ∏$  is the norm of  X . It is adapted from [ Chi12 ], Eq. (10.86), to  Manifolds.jl  conventions. source"},{"id":1481,"pagetitle":"Rotations","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolds/stable/manifolds/rotations/#ManifoldDiff.riemannian_Hessian-Tuple{Rotations, Vararg{Any, 4}}","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method riemannian_Hessian(M::Rotations, p, G, H, X) The Riemannian Hessian can be computed by adopting Eq. (5.6) [ Ngu23 ], so very similar to the Stiefel manifold. The only difference is, that here the tangent vectors are stored in the Lie algebra, i.e. the update direction is actually  $pX$  instead of just  $X$  (in Stiefel). and that means the inverse has to be applied to the (Euclidean) Hessian to map it into the Lie algebra. source"},{"id":1482,"pagetitle":"Rotations","title":"Manifolds._exp_half","ref":"/manifolds/stable/manifolds/rotations/#Manifolds._exp_half-Tuple{Rotations, Any}","content":" Manifolds._exp_half  ‚Äî  Method _exp_half(::Rotations, d) Calculate  exp(d / 2) . source"},{"id":1483,"pagetitle":"Rotations","title":"Manifolds._exp_half","ref":"/manifolds/stable/manifolds/rotations/#Manifolds._exp_half-Tuple{Rotations{ManifoldsBase.TypeParameter{Tuple{3}}}, Any}","content":" Manifolds._exp_half  ‚Äî  Method _exp_half(::Rotations{TypeParameter{Tuple{3}}}, d) Calculate  exp(d / 2)  for the manifold of  Rotations(3)  based on the Rodrigues' rotation formula. source"},{"id":1484,"pagetitle":"Rotations","title":"Manifolds.angles_4d_skew_sym_matrix","ref":"/manifolds/stable/manifolds/rotations/#Manifolds.angles_4d_skew_sym_matrix-Tuple{Any}","content":" Manifolds.angles_4d_skew_sym_matrix  ‚Äî  Method angles_4d_skew_sym_matrix(A) The Lie algebra of  Rotations(4)  in  $‚Ñù^{4√ó4}$ ,  $ùî∞ùî¨(4)$ , consists of  $4√ó4$  skew-symmetric matrices. The unique imaginary components of their eigenvalues are the angles of the two plane rotations. This function computes these more efficiently than  eigvals . By convention, the returned values are sorted in decreasing order (corresponding to the same ordering of  angles  as  cos_angles_4d_rotation_matrix ). source"},{"id":1485,"pagetitle":"Rotations","title":"Manifolds.normal_rotation_distribution","ref":"/manifolds/stable/manifolds/rotations/#Manifolds.normal_rotation_distribution","content":" Manifolds.normal_rotation_distribution  ‚Äî  Function normal_rotation_distribution(M::Rotations, p, œÉ::Real) Return a random point on the manifold  Rotations M  by generating a (Gaussian) random orthogonal matrix with determinant  $+1$ . Let \\[QR = A\\] be the QR decomposition of a random matrix  $A$ , then the formula reads \\[p = QD\\] where  $D$  is a diagonal matrix with the signs of the diagonal entries of  $R$ , i.e. \\[D_{ij}=\\begin{cases}\n \\operatorname{sgn}(R_{ij}) & \\text{if} \\; i=j \\\\\n 0 & \\, \\text{otherwise}\n\\end{cases}.\\] It can happen that the matrix gets -1 as a determinant. In this case, the first and second columns are swapped. The argument  p  is used to determine the type of returned points. source"},{"id":1486,"pagetitle":"Rotations","title":"ManifoldsBase.Weingarten","ref":"/manifolds/stable/manifolds/rotations/#ManifoldsBase.Weingarten-Tuple{Rotations, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Weingarten(M::Rotations, p, X, V) Compute the Weingarten map  $\\mathcal W_p$  at  p  on the  Stiefel M  with respect to the tangent vector  $X \\in T_p\\mathcal M$  and the normal vector  $V \\in N_p\\mathcal M$ . The formula is due to [ AMT13 ] given by \\[\\mathcal W_p(X,V) = -\\frac{1}{2}p\\bigl(V^{\\mathrm{T}}X - X^\\mathrm{T}V\\bigr)\\] source"},{"id":1487,"pagetitle":"Rotations","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/rotations/#ManifoldsBase.injectivity_radius-Tuple{Rotations, PolarRetraction}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::Rotations, ::PolarRetraction) Return the radius of injectivity for the  PolarRetraction  on the  Rotations M  which is  $\\frac{œÄ}{\\sqrt{2}}$ . source"},{"id":1488,"pagetitle":"Rotations","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/rotations/#ManifoldsBase.inverse_retract-Tuple{Rotations, Any, Any, PolarInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M, p, q, ::PolarInverseRetraction) Compute a vector from the tangent space  $T_p\\mathrm{SO}(n)$  of the point  p  on the  Rotations  manifold  M  with which the point  q  can be reached by the  PolarRetraction  from the point  p  after time 1. The formula reads \\[\\operatorname{retr}^{-1}_p(q)\n= -\\frac{1}{2}(p^{\\mathrm{T}}qs - (p^{\\mathrm{T}}qs)^{\\mathrm{T}})\\] where  $s$  is the solution to the Sylvester equation $p^{\\mathrm{T}}qs + s(p^{\\mathrm{T}}q)^{\\mathrm{T}} + 2I_n = 0.$ source"},{"id":1489,"pagetitle":"Rotations","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/rotations/#ManifoldsBase.inverse_retract-Tuple{Rotations, Any, Any, QRInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::Rotations, p, q, ::QRInverseRetraction) Compute a vector from the tangent space  $T_p\\mathrm{SO}(n)$  of the point  p  on the  Rotations  manifold  M  with which the point  q  can be reached by the  QRRetraction  from the point  q  after time 1. source"},{"id":1490,"pagetitle":"Rotations","title":"ManifoldsBase.parallel_transport_direction","ref":"/manifolds/stable/manifolds/rotations/#ManifoldsBase.parallel_transport_direction-Tuple{Rotations, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_direction  ‚Äî  Method parallel_transport_direction(M::Rotations, p, X, d) Compute parallel transport of vector  X  tangent at  p  on the  Rotations  manifold in the direction  d . The formula, provided in [ Ren11 ], reads: \\[\\mathcal P_{q\\gets p}X = q^\\mathrm{T}p \\operatorname{Exp}(d/2) X \\operatorname{Exp}(d/2)\\] where  $q=\\exp_p d$ . The formula simplifies to identity for 2-D rotations. source"},{"id":1491,"pagetitle":"Rotations","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/rotations/#ManifoldsBase.project-Tuple{Rotations, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Rotations, p; check_det = true) Project  p  to the nearest point on manifold  M . Given the singular value decomposition  $p = U Œ£ V^\\mathrm{T}$ , with the singular values sorted in descending order, the projection is \\[\\operatorname{proj}_{\\mathrm{SO}(n)}(p) =\nU\\operatorname{diag}\\left[1,1,‚Ä¶,\\det(U V^\\mathrm{T})\\right] V^\\mathrm{T}\\] The diagonal matrix ensures that the determinant of the result is  $+1$ . If  p  is expected to be almost special orthogonal, then you may avoid this check with  check_det = false . source"},{"id":1492,"pagetitle":"Rotations","title":"ManifoldsBase.sectional_curvature_max","ref":"/manifolds/stable/manifolds/rotations/#ManifoldsBase.sectional_curvature_max-Tuple{Rotations}","content":" ManifoldsBase.sectional_curvature_max  ‚Äî  Method sectional_curvature_max(::Rotations) Sectional curvature of  Rotations M  is equal to 0 for  Rotations(1)  and  Rotations(2) , less than or equal to 1/8 for  Rotations(3)  and less than or equal to 1/4 for higher-dimensional rotations manifolds. For reference, see [ Ge14 ], Lemma 2.5 and [ CE08 ], Corollary 3.19. source"},{"id":1493,"pagetitle":"Rotations","title":"ManifoldsBase.sectional_curvature_min","ref":"/manifolds/stable/manifolds/rotations/#ManifoldsBase.sectional_curvature_min-Tuple{Rotations}","content":" ManifoldsBase.sectional_curvature_min  ‚Äî  Method sectional_curvature_min(M::Rotations) Sectional curvature of  Rotations M  is greater than or equal to 0. source"},{"id":1494,"pagetitle":"Rotations","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/rotations/#ManifoldsBase.zero_vector-Tuple{Rotations, Any}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(M::Rotations, p) Return the zero tangent vector from the tangent space art  p  on the  Rotations  as an element of the Lie group, i.e. the zero matrix. source"},{"id":1495,"pagetitle":"Rotations","title":"Literature","ref":"/manifolds/stable/manifolds/rotations/#Literature","content":" Literature [AMT13] P.¬†-.-A.¬†Absil, R.¬†Mahony and J.¬†Trumpf.  An Extrinsic Look at the Riemannian Hessian . In:  Geometric Science of Information , edited by F.¬†Nielsen and F.¬†Barbaresco (Springer Berlin Heidelberg, 2013); pp.¬†361‚Äì368. [CE08] J.¬†Cheeger and D.¬†G.¬†Ebin.  Comparison Theorems in Riemannian Geometry  (American Mathematical Society, Providence, R.I, 2008). [Chi12] G.¬†S.¬†Chirikjian.  Stochastic Models, Information Theory, and Lie Groups, Volume 2 . 1¬†Edition, Vol.¬†2 of  Applied and Numerical Harmonic Analysis  (Birkh√§user Boston, MA, 2012). [Ge14] J.¬†Ge.  DDVV-type inequality for skew-symmetric matrices and Simons-type inequality for Riemannian submersions .  Advances¬†in¬†Mathematics  251 , 62‚Äì86  (2014). [Ngu23] D.¬†Nguyen.  Operator-Valued Formulas for Riemannian Gradient and Hessian and Families of Tractable Metrics in Riemannian Optimization .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  198 , 135‚Äì164  (2023),  arXiv:2009.10159 . [Ren11] Q.¬†Rentmeesters.  A gradient method for geodesic data fitting on some symmetric Riemannian manifolds . In:  IEEE Conference on Decision and Control and European Control Conference  (2011); pp.¬†7141‚Äì7146. [SDA21] J.¬†Sol√†, J.¬†Deray and D.¬†Atchuthan.  A micro Lie theory for state estimation in robotics  (Dec 2021),  arXiv:1812.01537 [cs.RO] , arXiv: 1812.01537."},{"id":1498,"pagetitle":"Segre","title":"The Segre manifold","ref":"/manifolds/stable/manifolds/segre/#The-Segre-manifold","content":" The Segre manifold"},{"id":1499,"pagetitle":"Segre","title":"Manifolds.Segre","ref":"/manifolds/stable/manifolds/segre/#Manifolds.Segre","content":" Manifolds.Segre  ‚Äî  Type Segre{ùîΩ,V} <: AbstractManifold{ùîΩ} The Segre manifold \\[    \\mathcal{S} = \\operatorname{Seg}(ùîΩ^{n_1} \\times \\dots \\times ùîΩ^{n_d})\\] is the set of rank-one tensors in  $ùîΩ^{n_1} \\otimes \\dots \\otimes ùîΩ^{n_d}$ . When  $ùîΩ = ‚Ñù$ , the Segre manifold is a normal Riemannian covering of \\[    \\mathcal{P} = ‚Ñù^{+} \\times \\mathbb{S}^{n_1 - 1} \\times \\dots \\times \\mathbb{S}^{n_d - 1}\\] equipped with a  warped product metric . The tuple  $(n_1, \\dots, n_d)$  is called the  valence  of the manifold. The geometry of the Segre manifold is summarized in [ JSVV24 ]. It is named after  Corrado Segre (1863‚Äì1924). Constructor Segre(n::Int...; field::AbstractNumbers=‚Ñù) Generate a valence  (n, ...)  Segre manifold.  Segre(n)  is the same as  $\\mathbb{R}^{n} \\setminus \\{ 0 \\}$ . source"},{"id":1500,"pagetitle":"Segre","title":"Base.exp","ref":"/manifolds/stable/manifolds/segre/#Base.exp-Union{Tuple{V}, Tuple{Segre{‚Ñù, V}, Any, Any}} where V","content":" Base.exp  ‚Äî  Method exp(M::Segre{‚Ñù, V}, p, X) Exponential map on the Segre manifold. Let  $p ‚âê (Œª, x_1, ‚Ä¶, x_d) ‚àà \\mathcal{S}$  and  $X = (ŒΩ, u_1, ‚Ä¶, u_d) ‚àà T_p \\mathcal{S}$ . The exponential map is given by \\[    \\operatorname{exp}_p(X) ‚âê\n    \\left(\n        \\sqrt{(Œª + ŒΩ)^2 + (Œª m)^2},\\\\\n        x_1 \\cos\\mathopen{\\Big(} \\frac{f \\lVert u_1 \\rVert_{x_1}}{m} \\mathclose{\\Big)} + \\frac{u_1}{\\lVert u_1 \\rVert_{x_1}} \\sin\\mathopen{\\Big(} \\frac{f \\lVert u_1 \\rVert_{x_1}}{m} \\mathclose{\\Big)},\\\\\n        ‚Ä¶,\\\\\n        x_d \\cos\\mathopen{\\Big(} \\frac{f \\lVert u_d \\rVert_{x_d}}{m} \\mathclose{\\Big)} + \\frac{u_d}{\\lVert u_d \\rVert_{x_d}} \\sin\\mathopen{\\Big(} \\frac{f \\lVert u_d \\rVert_{x_d}}{m} \\mathclose{\\Big)}\n    \\right),\\] where \\[    \\begin{aligned}\n        f &= \\frac{\\pi}{2} - \\tan^{-1}\\mathopen{\\Big(} \\frac{Œª + ŒΩ}{Œª m} \\mathclose{\\Big)},\\\\\n        m &= \\sqrt{\\lVert u_1 \\rVert_{x_1}^2 + ‚Ä¶ + \\lVert u_d \\rVert_{x_d}^2}.\n    \\end{aligned}\\] If  $m = 0$  and  $-Œª < ŒΩ$ , then  $\\operatorname{exp}_p(v) = p + X$ . The formula is derived in proposition 3.1 in [ JSVV24 ]. source"},{"id":1501,"pagetitle":"Segre","title":"Base.log","ref":"/manifolds/stable/manifolds/segre/#Base.log-Union{Tuple{V}, Tuple{Segre{‚Ñù, V}, Any, Any}} where V","content":" Base.log  ‚Äî  Method log(M::Segre{‚Ñù, V}, p, q) Logarithmic map on the Segre manifold. Let  $p ‚âê (Œª, x_1, ‚Ä¶, x_d)$ ,  $q ‚âê (Œº, y_1, ‚Ä¶, y_d) ‚àà \\mathcal{S}$ . Assume  $p$  and  $q$  are connected by a geodesic. Let \\[    m = \\sqrt{\\sphericalangle(x_1, y_1)^2 + ‚Ä¶ + \\sphericalangle(x_d, y_d)^2}\\] and assume  $(Œº, y_1, ‚Ä¶, y_d)$  is the representative of  $q$  that minimizes  $m$ . Then \\[    \\operatorname{log}_p(q) =\n    \\left(\n        \\mu \\cos{m} - \\lambda,\n        (y_1 - ‚ü®x_1, y_1‚ü© x_1) \\frac{\\mu \\sphericalangle(x_1, y_1) \\sin{m}}{\\lambda m \\sin{\\sphericalangle(x_1, y_1)}},\n        \\dots,\n        (y_d - ‚ü®x_d, y_d‚ü© x_d) \\frac{\\mu \\sphericalangle(x_d, y_d) \\sin{m}}{\\lambda m \\sin{\\sphericalangle(x_d, y_d)}}\n    \\right).\\] The formula is derived in theorem 4.4 in [ JSVV24 ]. source"},{"id":1502,"pagetitle":"Segre","title":"Base.rand","ref":"/manifolds/stable/manifolds/segre/#Base.rand-Union{Tuple{Segre{‚Ñù, V}}, Tuple{V}} where V","content":" Base.rand  ‚Äî  Method rand(M::Segre{‚Ñù, V}; vector_at=nothing) If  vector_at  is  nothing , return a random point on \\[    ‚Ñù^{+} √ó \\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó \\mathbb{S}^{n_d - 1}\\] from a log-normal distribution on  $‚Ñù^{+}$  and a uniform distribution on  $\\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó \\mathbb{S}^{n_d - 1}$ . If  vector_at  is not  nothing , return a random tangent vector from a normal distribution on the tangent space. source"},{"id":1503,"pagetitle":"Segre","title":"Manifolds.closest_representative!","ref":"/manifolds/stable/manifolds/segre/#Manifolds.closest_representative!-Union{Tuple{V}, Tuple{Segre{‚Ñù, V}, Any, Any}} where V","content":" Manifolds.closest_representative!  ‚Äî  Method closest_representative!(M::Segre{‚Ñù, V}, p, q) $\\mathcal{S}$  is a  $2^d$ -sheeted Riemannian covering of \\[    \\mathcal{P} = ‚Ñù^{+} \\times \\mathbb{S}^{n_1 - 1} \\times \\dots \\times \\mathbb{S}^{n_d - 1}\\] with a warped product metric. Every equivalence class  $q \\in \\mathcal{S}$  has  $2^d$  representatives in  $\\mathcal{P}$ .  closest_representative!(M, q, p)  changes representative of  q  to the one that is closest to  p  in  $\\mathcal{P}$ . source"},{"id":1504,"pagetitle":"Segre","title":"Manifolds.connected_by_geodesic","ref":"/manifolds/stable/manifolds/segre/#Manifolds.connected_by_geodesic-Union{Tuple{V}, Tuple{Segre{‚Ñù, V}, Any, Any}} where V","content":" Manifolds.connected_by_geodesic  ‚Äî  Method connected_by_geodesic(M::Segre{‚Ñù, V}, p, q) $\\mathcal{S}$  is not a complete manifold, i.e. not every pair  p  and  q  of points are connected by a geodesic in  $\\mathcal{S}$ .  connected_by_geodesic(M, p, q)  returns  true  if two points,  p  and  q , are connected by a geodesic, and otherwise returns  false . source"},{"id":1505,"pagetitle":"Segre","title":"Manifolds.spherical_angle_sum","ref":"/manifolds/stable/manifolds/segre/#Manifolds.spherical_angle_sum-Union{Tuple{V}, Tuple{Segre{‚Ñù, V}, Any, Any}} where V","content":" Manifolds.spherical_angle_sum  ‚Äî  Method spherical_angle_sum(M::Segre{‚Ñù, V}, p, q) Let  $p ‚âê (Œª, x_1, ‚Ä¶, x_d)$ ,  $q ‚âê (Œº, y_1, ‚Ä¶, y_d) ‚àà \\mathcal{S}$ . Then this is \\[    \\sqrt{\\sphericalangle(x_1, y_1)^2 + ‚Ä¶ + \\sphericalangle(x_d, y_d)^2},\\] where  $\\sphericalangle(x_i, y_i)$  is the distance between  $x_i$  and  $y_i$  on the sphere  $\\mathbb{S}^{n_i - 1}$ . source"},{"id":1506,"pagetitle":"Segre","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.distance-Union{Tuple{V}, Tuple{Segre{‚Ñù, V}, Any, Any}} where V","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::Segre{‚Ñù, V}, p, q) Riemannian distance between two points  p  and  q  on the Segre manifold. Assume  $p ‚âê (Œª, x_1, ‚Ä¶, x_d)$ ,  $q ‚âê (Œº, y_1, ‚Ä¶, y_d) ‚àà \\mathcal{S}$  are connected by a geodesic. Let \\[    m = \\sqrt{\\sphericalangle(x_1, y_1)^2 + ‚Ä¶ + \\sphericalangle(x_d, y_d)^2}\\] and assume  $(Œº, y_1, ‚Ä¶, y_d)$  is the representation of  $q$  that minimizes  $m$ . Then \\[    \\operatorname{dist}_{\\mathcal{S}}(p, q) = \\sqrt{Œª^2 - 2 ŒªŒº\\cos(m) + Œº^2}.\\] source"},{"id":1507,"pagetitle":"Segre","title":"ManifoldsBase.embed!","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.embed!-Union{Tuple{V}, Tuple{ùîΩ}, Tuple{Segre{ùîΩ, V}, Any, Any, Any}} where {ùîΩ, V}","content":" ManifoldsBase.embed!  ‚Äî  Method embed!(M::Segre{ùîΩ, V}, p, X) Embed tangent vector  $X = (ŒΩ, u_1, ‚Ä¶, u_d)$  at  $p ‚âê (Œª, x_1, ‚Ä¶, x_d)$  in  $ùîΩ^{n_1 √ó‚ãØ√ó n_d}$  using the Kronecker product \\[    (ŒΩ, u_1, ‚Ä¶, u_d) ‚Ü¶ ŒΩ x_1 ‚äó‚ãØ‚äó x_d + Œª u_1 ‚äó x_2 ‚äó‚ãØ‚äó x_d + ‚Ä¶ + Œª x_1 ‚äó‚ãØ‚äó x_{d - 1} ‚äó u_d.\\] source"},{"id":1508,"pagetitle":"Segre","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.embed-Tuple{Segre, Any}","content":" ManifoldsBase.embed  ‚Äî  Method embed(M::Segre{ùîΩ, V}, p)\nembed!(M::Segre{ùîΩ, V}, q, p) Embed  $p ‚âê (Œª, x_1, ‚Ä¶, x_d)$  in  $ùîΩ^{n_1 √ó‚ãØ√ó n_d}$  using the Kronecker product \\[    (Œª, x_1, ‚Ä¶, x_d) ‚Ü¶ Œª x_1 ‚äó‚ãØ‚äó x_d.\\] source"},{"id":1509,"pagetitle":"Segre","title":"ManifoldsBase.get_coordinates","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.get_coordinates-Tuple{Segre, Any, Any, DefaultOrthonormalBasis}","content":" ManifoldsBase.get_coordinates  ‚Äî  Method get_coordinates(M::Segre{ùîΩ, V}, p, X, ::DefaultOrthonormalBasis; kwargs...) Get coordinates of  X  in the tangent space  $T_{(Œª, x_1, ‚Ä¶, x_d)} \\mathcal{S} = \\mathbb{R} √ó T_{x_1} \\mathbb{S}^{n_1 - 1} √ó‚Ä¶√ó T_{x_d} \\mathbb{S}^{n_d - 1}$  using a  DefaultOrthonormalBasis  on each factor. source"},{"id":1510,"pagetitle":"Segre","title":"ManifoldsBase.get_embedding","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.get_embedding-Union{Tuple{Segre{ùîΩ, V}}, Tuple{V}, Tuple{ùîΩ}} where {ùîΩ, V}","content":" ManifoldsBase.get_embedding  ‚Äî  Method get_embedding(M::Segre{ùîΩ,V}) Return the embedding of the  Segre  manifold  $\\mathcal{S}$ , which is  $ùîΩ^{n_1 √ó‚ãØ√ó n_d}$ . source"},{"id":1511,"pagetitle":"Segre","title":"ManifoldsBase.get_vector","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.get_vector-Tuple{Segre, Any, Any, DefaultOrthonormalBasis}","content":" ManifoldsBase.get_vector  ‚Äî  Method get_vector( M::Segre{ùîΩ, V}, p, c, DefaultOrthonormalBasis; kwargs...) Get tangent vector  X  from coordinates in the tangent space  $T_{(Œª, x_1, ‚Ä¶, x_d)} \\mathcal{S} = \\mathbb{R} √ó T_{x_1} \\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó T_{x_d} \\mathbb{S}^{n_d - 1}$  using a  DefaultOrthonormalBasis  on each factor. source"},{"id":1512,"pagetitle":"Segre","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.inner-Tuple{Segre{‚Ñù}, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::Segre{‚Ñù, V}, p, X, Y,) Inner product between two tangent vectors  $X = (ŒΩ, u_1, ‚Ä¶, u_d)$  and  $Y = (Œæ, v_1, ‚Ä¶, v_d)$  at  $p ‚âê (Œª, x_1, \\dots, x_d)$ . This inner product is obtained by embedding the Segre manifold in the space of tensors equipped with the Euclidean metric: \\[    \\langle X, Y \\rangle_{p} = \\nu \\xi + \\lambda^2 (\\langle u_1, v_1 \\rangle_{x_1} + \\dots + \\langle u_d, v_d \\rangle_{x_d}),\\] where  $ŒΩ, Œæ ‚àà T_{Œª} ‚Ñù^{+} = ‚Ñù$  and  $u_i$ ,  $v_i ‚àà T_{x_i} \\mathbb{S}^{n_i - 1} ‚äÇ ‚Ñù^{n_i}$ . source"},{"id":1513,"pagetitle":"Segre","title":"ManifoldsBase.is_point","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.is_point-Union{Tuple{V}, Tuple{Segre{‚Ñù, V}, Any}} where V","content":" ManifoldsBase.is_point  ‚Äî  Method is_point(M::Segre{‚Ñù, V}, p; kwargs...) Check whether  p  is a valid point on  M , i.e.  p[1]  is a singleton containing a positive number and  p[i + 1]  is a point on  Sphere(V[i]) . The tolerance can be set using the  kwargs... . source"},{"id":1514,"pagetitle":"Segre","title":"ManifoldsBase.is_vector","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.is_vector-Union{Tuple{V}, Tuple{Segre{‚Ñù, V}, Any, Any}} where V","content":" ManifoldsBase.is_vector  ‚Äî  Method is_vector(M::Segre{‚Ñù, V}, p, X, kwargs...) Check whether  X  is a tangent vector to  p  on  M , i.e.  X  has to be of same dimension as  p  and orthogonal to  p . The tolerance can be set using the  kwargs... . source"},{"id":1515,"pagetitle":"Segre","title":"ManifoldsBase.riemann_tensor","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.riemann_tensor-Union{Tuple{V}, Tuple{Segre{‚Ñù, V}, Vararg{Any, 4}}} where V","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(M::Segre{‚Ñù, V}, p, X, Y, Z) Riemann tensor of the Segre manifold at  $p$ . $\\mathcal{S}$  is locally a warped product of  $‚Ñù^{+}$  and  $\\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó \\mathbb{S}^{n_d - 1}$ . If  $p ‚âê (Œª, x_1, ‚Ä¶, x_d) ‚àà \\mathcal{S}$  and  $X$ ,  $Y$ ,  $Z ‚àà T_p (\\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó \\mathbb{S}^{n_d - 1}) ‚äÇ T_p \\mathcal{S}$ , then \\[    R_{\\mathcal{S}}(X, Y) Z = R_{\\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó \\mathbb{S}^{n_d - 1}}(X, Y) Z + Œª^{-2}(‚ü®X, Z‚ü©_p Y - ‚ü®Y, Z‚ü©_p X).\\] $R_{\\mathcal{S}}$  is zero in the remaining (orthogonal) directions. source"},{"id":1516,"pagetitle":"Segre","title":"ManifoldsBase.sectional_curvature","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.sectional_curvature-Union{Tuple{V}, Tuple{Segre{‚Ñù, V}, Any, Any, Any}} where V","content":" ManifoldsBase.sectional_curvature  ‚Äî  Method sectional_curvature(M::Segre{‚Ñù, V}, p, u, v) Sectional curvature of the Segre manifold at  $p$ . $\\mathcal{S}$  is locally a warped product of  $‚Ñù^{+}$  and  $\\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó \\mathbb{S}^{n_d - 1}$  If  $p ‚âê (Œª, x_1, ‚Ä¶, x_d) ‚àà \\mathcal{S}$ ,  $u_i ‚àà T_{x_i} \\mathbb{S}^{n_i - 1}$ , and  $v_j ‚àà T_{x_j} \\mathbb{S}^{n_j - 1}$ , then \\[    K_{\\mathcal{S}}(u_i, v_j) = \\frac{\\delta_{i j} - 1}{\\lambda^2}.\\] $K_{\\mathcal{S}}$  is zero in the remaining (orthogonal) directions. source"},{"id":1517,"pagetitle":"Segre","title":"A warped metric","ref":"/manifolds/stable/manifolds/segre/#segre-warped-metric-sec","content":" A warped metric"},{"id":1518,"pagetitle":"Segre","title":"Manifolds.WarpedMetric","ref":"/manifolds/stable/manifolds/segre/#Manifolds.WarpedMetric","content":" Manifolds.WarpedMetric  ‚Äî  Type WarpedMetric{A} <: AbstractMetric The  $A$ -warped metric on the Segre manifold  $\\mathcal{S}$  is a generalization of the Euclidean metric on  $\\mathcal{S}$ . We denote this manifold by  $\\mathcal{S}_A$ . Similarly to  $\\mathcal{S}$ , when  $ùîΩ = ‚Ñù$ ,  $\\mathcal{S}_A$  is a normal Riemannian covering of the product manifold \\[    ‚Ñù^{+} √ó \\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó \\mathbb{S}^{n_d - 1}\\] with a  warped product metric , but the warping function now depends on the  warping factor $A$ .  $A = 1$  corresponds to the usual Segre manifold. The Segre manifold is a cone in the sense that if  $p \\in \\mathcal{S}$ , then  $r p \\in \\mathcal{S}$  for all  $r \\neq 0$ . The tangent subspace at  $p$  defined  $\\mathrm{d} (r p) / \\mathrm{d} r$  is called the  radial  direction.  $A < 1$  puts less weight on the directions orthogonal to the radial direction compared to  $\\mathcal{S}$ , while  $A > 1$  puts more weight on those directions. The geometry is summarized in [ JSVV24 ]. Constructor WarpedMetric(A::Real) Generate a warped product metric with warping factor  A . source"},{"id":1519,"pagetitle":"Segre","title":"Base.exp","ref":"/manifolds/stable/manifolds/segre/#Base.exp-Union{Tuple{A}, Tuple{V}, Tuple{MetricManifold{‚Ñù, Segre{‚Ñù, V}, WarpedMetric{A}}, Any, Any}} where {V, A}","content":" Base.exp  ‚Äî  Method exp(M::MetricManifold{‚Ñù, Segre{‚Ñù, V}, WarpedMetric{A}}, p, X) Exponential map on the warped Segre manifold. Let  $p ‚âê (Œª, x_1,‚Ä¶, x_d) ‚àà \\mathcal{S}_A$  and  $X = (ŒΩ, u_1,‚Ä¶, u_d) ‚àà T_p \\mathcal{S}_A$ . Then the exponential map is given by \\[    \\operatorname{exp}_p(X) ‚âê\n    \\left(\n        \\sqrt{(Œª + ŒΩ)^2 + (Œª A m)^2},\\\\\n        x_1 \\cos\\mathopen{\\Big(} \\frac{f \\lVert u_1 \\rVert_{x_1}}{A m} \\mathclose{\\Big)} + \\frac{u_1}{\\lVert u_1 \\rVert_{x_1}} \\sin\\mathopen{\\Big(} \\frac{f \\lVert u_1 \\rVert_{x_1}}{A m} \\mathclose{\\Big)},\\\\\n        ‚Ä¶,\\\\\n        x_d \\cos\\mathopen{\\Big(} \\frac{f \\lVert u_d \\rVert_{x_d}}{A m} \\mathclose{\\Big)} + \\frac{u_d}{\\lVert u_d \\rVert_{x_d}} \\sin\\mathopen{\\Big(} \\frac{f \\lVert u_d \\rVert_{x_d}}{A m} \\mathclose{\\Big)}\n    \\right),\\] where \\[    \\begin{aligned}\n        f &= \\frac{\\pi}{2} - \\tan^{-1}\\mathopen{\\Big(} \\frac{Œª + ŒΩ}{Œª A m} \\mathclose{\\Big)},\\\\\n        m &= \\sqrt{\\lVert u_1 \\rVert_{x_1}^2 + ‚Ä¶ + \\lVert u_d \\rVert_{x_d}^2}.\n    \\end{aligned}\\] If  $m = 0$  and  $-Œª < ŒΩ$ , then  $\\operatorname{exp}_p(v) = p + X$ . The formula is derived in proposition 3.1 in [ JSVV24 ]. source"},{"id":1520,"pagetitle":"Segre","title":"Base.log","ref":"/manifolds/stable/manifolds/segre/#Base.log-Union{Tuple{A}, Tuple{V}, Tuple{MetricManifold{‚Ñù, Segre{‚Ñù, V}, WarpedMetric{A}}, Any, Any}} where {V, A}","content":" Base.log  ‚Äî  Method log(M::MetricManifold{‚Ñù, Segre{‚Ñù,V}, WarpedMetric{A}}, p, q) Logarithmic map on the warped Segre manifold. Let  $p ‚âê (Œª, x_1,‚Ä¶, x_d)$ ,  $q ‚âê (Œº, y_1,‚Ä¶, y_d) ‚àà \\mathcal{S}_A$ . Assume  $p$  and  $q$  are connected by a geodesic. Let \\[    m = \\sqrt{\\sphericalangle(x_1, y_1)^2 +‚Ä¶ + \\sphericalangle(x_d, y_d)^2}\\] and assume  $(Œº, y_1,‚Ä¶, y_d)$  is the representative of  $q$  that minimizes  $m$ . Then \\[    \\operatorname{log}_p(q) =\n    \\left(\n        \\mu \\cos{m} - \\lambda,\n        (y_1 - ‚ü®x_1, y_1‚ü© x_1) \\frac{\\mu \\sphericalangle(x_1, y_1) \\sin(A m)}{\\lambda A m \\sin{\\sphericalangle(x_1, y_1)}},\n        \\dots,\n        (y_d - ‚ü®x_d, y_d‚ü© x_d) \\frac{\\mu \\sphericalangle(x_d, y_d) \\sin(A m)}{\\lambda A m \\sin{\\sphericalangle(x_d, y_d)}}\n    \\right).\\] The formula is derived in theorem 4.4 in [ JSVV24 ]. source"},{"id":1521,"pagetitle":"Segre","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.distance-Union{Tuple{A}, Tuple{V}, Tuple{MetricManifold{‚Ñù, Segre{‚Ñù, V}, WarpedMetric{A}}, Any, Any}} where {V, A}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::MetricManifold{‚Ñù, Segre{‚Ñù,V}, WarpedMetric{A}}, p, q) Riemannian distance between two points  p  and  q  on the warped Segre manifold. Assume  $p ‚âê (Œª, x_1,‚Ä¶, x_d)$ ,  $q ‚âê (Œº, y_1,‚Ä¶, y_d) ‚àà \\mathcal{S}_A$  are connected by a geodesic. Let \\[    m = \\sqrt{\\sphericalangle(x_1, y_1)^2 +‚Ä¶ + \\sphericalangle(x_d, y_d)^2}\\] and assume  $(Œº, y_1,‚Ä¶, y_d)$  is the representation of  $q$  that minimizes  $m$ . Then \\[    \\operatorname{dist}_{\\mathcal{S}_A}(p, q) = \\sqrt{Œª^2 - 2 Œª Œº \\cos(A m) + Œº^2}.\\] source"},{"id":1522,"pagetitle":"Segre","title":"ManifoldsBase.get_coordinates","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.get_coordinates-Union{Tuple{A}, Tuple{V}, Tuple{ùîΩ}, Tuple{MetricManifold{ùîΩ, Segre{ùîΩ, V}, WarpedMetric{A}}, Any, Any, DefaultOrthonormalBasis}} where {ùîΩ, V, A}","content":" ManifoldsBase.get_coordinates  ‚Äî  Method get_coordinates(M::Segre{ùîΩ, V}, p, v, ::DefaultOrthonormalBasis; kwargs...) Get coordinates of  X  in the tangent space  $T_{(Œª, x_1,‚Ä¶, x_d)} \\mathcal{S}_A = \\mathbb{R} √ó T_{x_1} \\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó T_{x_d} \\mathbb{S}^{n_d - 1}$  using a  DefaultOrthonormalBasis  on each factor. source"},{"id":1523,"pagetitle":"Segre","title":"ManifoldsBase.get_vector","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.get_vector-Union{Tuple{ùîΩ}, Tuple{A}, Tuple{V}, Tuple{MetricManifold{ùîΩ, Segre{ùîΩ, V}, WarpedMetric{A}}, Any, Any, DefaultOrthonormalBasis}} where {V, A, ùîΩ}","content":" ManifoldsBase.get_vector  ‚Äî  Method get_vector(M::Segre{ùîΩ, V}, p, c, ::DefaultOrthonormalBasis; kwargs...) Get tangent vector  X  from coordinates in the tangent space  $T_{(Œª, x_1,‚Ä¶, x_d)} \\mathcal{S}_A = \\mathbb{R} √ó T_{x_1} \\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó T_{x_d} \\mathbb{S}^{n_d - 1}$  using a  DefaultOrthonormalBasis  on each factor. source"},{"id":1524,"pagetitle":"Segre","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.inner-Union{Tuple{A}, Tuple{V}, Tuple{MetricManifold{‚Ñù, Segre{‚Ñù, V}, WarpedMetric{A}}, Any, Any, Any}} where {V, A}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::MetricManifold{‚Ñù, Segre{‚Ñù, V}, WarpedMetric{A}}, p, X, Y) Inner product between two tangent vectors  $X = (ŒΩ, u_1,‚Ä¶, u_d)$  and  $Y = (Œæ, v_1,‚Ä¶, v_d)$  at  $p \\doteq (Œª, x_1,‚Ä¶, x_d)$ : \\[    ‚ü®X, Y‚ü©_{p} = ŒΩ Œæ + (A Œª)^2 (‚ü® u_1, v_1 ‚ü©_{x_1} +‚Ä¶ + ‚ü®u_d, v_d‚ü©_{x_d}),\\] where  $ŒΩ$ ,  $Œæ ‚àà T_{Œª} ‚Ñù^{+} = ‚Ñù$  and  $u_i$ ,  $v_i ‚àà T_{x_i} \\mathbb{S}^{n_i - 1} \\subset ‚Ñù^{n_i}$ . source"},{"id":1525,"pagetitle":"Segre","title":"ManifoldsBase.riemann_tensor","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.riemann_tensor-Union{Tuple{A}, Tuple{V}, Tuple{MetricManifold{‚Ñù, Segre{‚Ñù, V}, WarpedMetric{A}}, Vararg{Any, 4}}} where {V, A}","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(M::MetricManifold{‚Ñù, Segre{‚Ñù,V}, WarpedMetric{A}}, p, X, Y) Riemann tensor of the warped Segre manifold at  $p$ . $\\mathcal{S}_A$  is locally a warped product of  $‚Ñù^{+}$  and  $\\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó \\mathbb{S}^{n_d - 1}$ . If  $p ‚âê (Œª, x_1,‚Ä¶, x_d) ‚àà \\mathcal{S}_A$  and  $X$ ,  $Y$ ,  $Z ‚àà T_{(x_1,‚Ä¶, x_d)} (\\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó \\mathbb{S}^{n_d - 1}) \\subset T_p \\mathcal{S}_A$  then \\[    R_{\\mathcal{S}_A}(X, Y) Z = R_{\\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó \\mathbb{S}^{n_d - 1}}(X, Y) Z + Œª^{-2}(‚ü® X, Z ‚ü©_{p} Y - ‚ü® Y, Z ‚ü©_{p} X).\\] $R_{\\mathcal{S}_A}$  is zero in the remaining (orthogonal) directions. source"},{"id":1526,"pagetitle":"Segre","title":"ManifoldsBase.sectional_curvature","ref":"/manifolds/stable/manifolds/segre/#ManifoldsBase.sectional_curvature-Union{Tuple{A}, Tuple{V}, Tuple{MetricManifold{‚Ñù, Segre{‚Ñù, V}, WarpedMetric{A}}, Any, Any, Any}} where {V, A}","content":" ManifoldsBase.sectional_curvature  ‚Äî  Method sectional_curvature(M::MetricManifold{‚Ñù, Segre{‚Ñù,V}, WarpedMetric{A}}, p, X, Y) Sectional curvature of the warped Segre manifold at  $p$ . $\\mathcal{S}_A$  is locally a warped product of  $‚Ñù^{+}$  and  $\\mathbb{S}^{n_1 - 1} √ó‚ãØ√ó \\mathbb{S}^{n_d - 1}$  If  $p = (Œª, x_1,‚Ä¶, x_d) ‚àà \\mathcal{S}$ ,  $u_i ‚àà T_{x_i} \\mathbb{S}^{n_i - 1}$ , and  $v_j ‚àà T_{x_j} \\mathbb{S}^{n_j - 1}$ , then \\[    K_{\\mathcal{S}_A}(u_i, v_j) = \\frac{A^{-2} \\delta_{i j} - 1}{Œª^{2}}.\\] $K_{\\mathcal{S}_A}$  is zero in the remaining (orthogonal) directions. source"},{"id":1527,"pagetitle":"Segre","title":"Literature","ref":"/manifolds/stable/manifolds/segre/#Literature","content":" Literature [JSVV24] S.¬†Jacobsson, L.¬†Swijsen, J.¬†V.¬†Veken and N.¬†Vannieuwenhoven.  Warped geometries of Segre-Veronese manifolds  (2024),  arXiv:2410.00664 [math.NA] ."},{"id":1530,"pagetitle":"Shape spaces","title":"Shape spaces","ref":"/manifolds/stable/manifolds/shapespace/#Shape-spaces","content":" Shape spaces Shape spaces are spaces of  $k$  points in  $‚Ñù^n$  up to simultaneous action of a group on all points. The most commonly encountered are Kendall's pre-shape and shape spaces. In the case of the Kendall's pre-shape spaces the action is translation and scaling. In the case of the Kendall's shape spaces the action is translation, scaling and rotation. using Manifolds, Plots\n\nM = KendallsShapeSpace(2, 3)\n# two random point on the shape space\np = [\n    0.4385117672460505 -0.6877826444042382 0.24927087715818771\n    -0.3830259932279294 0.35347460720654283 0.029551386021386548\n]\nq = [\n    -0.42693314765896473 -0.3268567431952937 0.7537898908542584\n    0.3054740561061169 -0.18962848284149897 -0.11584557326461796\n]\n# let's plot them as triples of points on a plane\nfig = scatter(p[1,:], p[2,:], label=\"p\", aspect_ratio=:equal)\nscatter!(fig, q[1,:], q[2,:], label=\"q\")\n\n# aligning q to p\nA = get_orbit_action(M)\na = optimal_alignment(A, p, q)\nrot_q = apply(A, a, q)\nscatter!(fig, rot_q[1,:], rot_q[2,:], label=\"q aligned to p\") A more extensive usage example is available in the  hand_gestures.jl  tutorial."},{"id":1531,"pagetitle":"Shape spaces","title":"Manifolds.KendallsPreShapeSpace","ref":"/manifolds/stable/manifolds/shapespace/#Manifolds.KendallsPreShapeSpace","content":" Manifolds.KendallsPreShapeSpace  ‚Äî  Type KendallsPreShapeSpace{T} <: AbstractSphere{‚Ñù} Kendall's pre-shape space of  $k$  landmarks in  $‚Ñù^n$  represented by n√ók matrices. In each row the sum of elements of a matrix is equal to 0. The Frobenius norm of the matrix is equal to 1 [ Ken84 ][ Ken89 ]. The space can be interpreted as tuples of  $k$  points in  $‚Ñù^n$  up to simultaneous translation and scaling of all points, so this can be thought of as a quotient manifold. Constructor KendallsPreShapeSpace(n::Int, k::Int; parameter::Symbol=:type) See also KendallsShapeSpace , esp. for the references source"},{"id":1532,"pagetitle":"Shape spaces","title":"Manifolds.KendallsShapeSpace","ref":"/manifolds/stable/manifolds/shapespace/#Manifolds.KendallsShapeSpace","content":" Manifolds.KendallsShapeSpace  ‚Äî  Type KendallsShapeSpace{T} <: AbstractDecoratorManifold{‚Ñù} Kendall's shape space, defined as quotient of a  KendallsPreShapeSpace  (represented by n√ók matrices) by the action  ColumnwiseMultiplicationAction . The space can be interpreted as tuples of  $k$  points in  $‚Ñù^n$  up to simultaneous translation and scaling and rotation of all points [ Ken84 ][ Ken89 ]. This manifold possesses the  IsQuotientManifold  trait. Constructor KendallsShapeSpace(n::Int, k::Int; parameter::Symbol=:type) References source"},{"id":1533,"pagetitle":"Shape spaces","title":"Provided functions","ref":"/manifolds/stable/manifolds/shapespace/#Provided-functions","content":" Provided functions"},{"id":1534,"pagetitle":"Shape spaces","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/shapespace/#ManifoldsBase.check_point-Tuple{KendallsPreShapeSpace, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::KendallsPreShapeSpace, p; atol=sqrt(max_eps(X, Y)), kwargs...) Check whether  p  is a valid point on  KendallsPreShapeSpace , i.e. whether each row has zero mean. Other conditions are checked via embedding in  ArraySphere . source"},{"id":1535,"pagetitle":"Shape spaces","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/shapespace/#ManifoldsBase.check_vector-Tuple{KendallsPreShapeSpace, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::KendallsPreShapeSpace, p, X; kwargs... ) Check whether  X  is a valid tangent vector on  KendallsPreShapeSpace , i.e. whether each row has zero mean. Other conditions are checked via embedding in  ArraySphere . source"},{"id":1536,"pagetitle":"Shape spaces","title":"ManifoldsBase.get_embedding","ref":"/manifolds/stable/manifolds/shapespace/#ManifoldsBase.get_embedding-Tuple{KendallsPreShapeSpace}","content":" ManifoldsBase.get_embedding  ‚Äî  Method get_embedding(M::KendallsPreShapeSpace) Return the space  KendallsPreShapeSpace M  is embedded in, i.e.  ArraySphere  of matrices of the same shape. source"},{"id":1537,"pagetitle":"Shape spaces","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/shapespace/#ManifoldsBase.manifold_dimension-Tuple{KendallsPreShapeSpace}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::KendallsPreShapeSpace) Return the dimension of the  KendallsPreShapeSpace  manifold  M . The dimension is given by  $n(k - 1) - 1$ . source"},{"id":1538,"pagetitle":"Shape spaces","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/shapespace/#ManifoldsBase.project-Tuple{KendallsPreShapeSpace, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::KendallsPreShapeSpace, p, X) Project tangent vector  X  at point  p  from the embedding to  KendallsPreShapeSpace  by selecting the right element from the tangent space to orthogonal section representing the quotient manifold  M . See Section 3.7 of [ SK16 ] for details. source"},{"id":1539,"pagetitle":"Shape spaces","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/shapespace/#ManifoldsBase.project-Tuple{KendallsPreShapeSpace, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::KendallsPreShapeSpace, p) Project point  p  from the embedding to  KendallsPreShapeSpace  by selecting the right element from the orthogonal section representing the quotient manifold  M . See Section 3.7 of [ SK16 ] for details. The method computes the mean of the landmarks and moves them to make their mean zero; afterwards the Frobenius norm of the landmarks (as a matrix) is normalised to fix the scaling. source"},{"id":1540,"pagetitle":"Shape spaces","title":"Base.exp","ref":"/manifolds/stable/manifolds/shapespace/#Base.exp-Tuple{KendallsShapeSpace, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::KendallsShapeSpace, p, X) Compute the exponential map on  KendallsShapeSpace M . See [ GMTP21 ] for discussion about its computation. source"},{"id":1541,"pagetitle":"Shape spaces","title":"Base.log","ref":"/manifolds/stable/manifolds/shapespace/#Base.log-Tuple{KendallsShapeSpace, Any, Any}","content":" Base.log  ‚Äî  Method log(M::KendallsShapeSpace, p, q) Compute the logarithmic map on  KendallsShapeSpace M . See the [ exp ](@ref exp(::KendallsShapeSpace, ::Any, ::Any)onential map for more details source"},{"id":1542,"pagetitle":"Shape spaces","title":"Base.rand","ref":"/manifolds/stable/manifolds/shapespace/#Base.rand-Tuple{KendallsShapeSpace}","content":" Base.rand  ‚Äî  Method rand(::KendallsShapeSpace; vector_at=nothing) When  vector_at  is  nothing , return a random point  x  on the  KendallsShapeSpace  manifold  M  by generating a random point in the embedding. When  vector_at  is not  nothing , return a random vector from the tangent space with mean zero and standard deviation  œÉ . source"},{"id":1543,"pagetitle":"Shape spaces","title":"Manifolds.get_total_space","ref":"/manifolds/stable/manifolds/shapespace/#Manifolds.get_total_space-Tuple{KendallsShapeSpace}","content":" Manifolds.get_total_space  ‚Äî  Method get_total_space(::KendallsShapeSpace) Return the total space of the  KendallsShapeSpace  manifold, which is the  KendallsPreShapeSpace  manifold. source"},{"id":1544,"pagetitle":"Shape spaces","title":"Manifolds.horizontal_component","ref":"/manifolds/stable/manifolds/shapespace/#Manifolds.horizontal_component-Tuple{KendallsShapeSpace, Any, Any}","content":" Manifolds.horizontal_component  ‚Äî  Method horizontal_component(::KendallsShapeSpace, p, X) Compute the horizontal component of tangent vector  X  at  p  on  KendallsShapeSpace M . See [ GMTP21 ], Section 2.3 for details. source"},{"id":1545,"pagetitle":"Shape spaces","title":"ManifoldsBase.get_embedding","ref":"/manifolds/stable/manifolds/shapespace/#ManifoldsBase.get_embedding-Tuple{KendallsShapeSpace}","content":" ManifoldsBase.get_embedding  ‚Äî  Method get_embedding(M::KendallsShapeSpace) Get the manifold in which  KendallsShapeSpace M  is embedded, i.e.  KendallsPreShapeSpace  of matrices of the same shape. source"},{"id":1546,"pagetitle":"Shape spaces","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/shapespace/#ManifoldsBase.is_flat-Tuple{KendallsShapeSpace}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::KendallsShapeSpace) Return false.  KendallsShapeSpace  is not a flat manifold. source"},{"id":1547,"pagetitle":"Shape spaces","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/shapespace/#ManifoldsBase.manifold_dimension-Tuple{KendallsShapeSpace}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::KendallsShapeSpace) Return the dimension of the  KendallsShapeSpace  manifold  M . The dimension is given by  $n(k - 1) - 1 - n(n - 1)/2$  in the typical case where  $k \\geq n+1$ , and  $(k + 1)(k - 2) / 2$  otherwise, unless  $k$  is equal to 1, in which case the dimension is 0. See [ Ken84 ] for a discussion of the over-dimensioned case. source"},{"id":1548,"pagetitle":"Shape spaces","title":"Literature","ref":"/manifolds/stable/manifolds/shapespace/#Literature","content":" Literature [GMTP21] N.¬†Guigui, E.¬†Maignant, A.¬†Trouv√© and X.¬†Pennec.  Parallel Transport on Kendall Shape Spaces . In:  Geometric Science of Information  (SPringer Cham, 2021); pp.¬†103‚Äì110. [Ken84] D.¬†G.¬†Kendall.  Shape Manifolds, Procrustean Metrics, and Complex Projective Spaces .  Bulletin¬†of¬†the¬†London¬†Mathematical¬†Society  16 , 81‚Äì121  (1984). [Ken89] D.¬†G.¬†Kendall.  A Survey of the Statistical Theory of Shape .  Statistical¬†Sciences  4 , 87‚Äì99  (1989). [SK16] A.¬†Srivastava and E.¬†P.¬†Klassen.  Functional and Shape Data Analysis  (Springer New York, 2016)."},{"id":1551,"pagetitle":"Skew-Hermitian matrices","title":"Skew-hermitian matrices","ref":"/manifolds/stable/manifolds/skewhermitian/#Skew-hermitian-matrices","content":" Skew-hermitian matrices"},{"id":1552,"pagetitle":"Skew-Hermitian matrices","title":"Manifolds.SkewHermitianMatrices","ref":"/manifolds/stable/manifolds/skewhermitian/#Manifolds.SkewHermitianMatrices","content":" Manifolds.SkewHermitianMatrices  ‚Äî  Type SkewHermitianMatrices{T,ùîΩ} <: AbstractDecoratorManifold{ùîΩ} The  AbstractManifold $\\operatorname{SkewHerm}(n)$  consisting of the real- or complex-valued skew-hermitian matrices of size  $n√ón$ , i.e. the set \\[\\operatorname{SkewHerm}(n) = \\bigl\\{p  ‚àà ùîΩ^{n√ón}\\ \\big|\\ p^{\\mathrm{H}} = -p \\bigr\\},\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the Hermitian, i.e. complex conjugate transpose, and the field  $ùîΩ ‚àà \\{ ‚Ñù, ‚ÑÇ, ‚Ñç\\}$ . Though it is slightly redundant, usually the matrices are stored as  $n√ón$  arrays. Note that in this representation, the real-valued part of the diagonal must be zero, which is also reflected in the  manifold_dimension . Constructor SkewHermitianMatrices(n::Int, field::AbstractNumbers=‚Ñù; parameter::Symbol=:type) Generate the manifold of  $n√ón$  skew-hermitian matrices. source"},{"id":1553,"pagetitle":"Skew-Hermitian matrices","title":"Manifolds.SkewSymmetricMatrices","ref":"/manifolds/stable/manifolds/skewhermitian/#Manifolds.SkewSymmetricMatrices","content":" Manifolds.SkewSymmetricMatrices  ‚Äî  Type SkewSymmetricMatrices{T} Generate the manifold of  $n√ón$  real skew-symmetric matrices. This is equivalent to  SkewHermitianMatrices(n, ‚Ñù) . Constructor SkewSymmetricMatrices(n::Int) source"},{"id":1554,"pagetitle":"Skew-Hermitian matrices","title":"ManifoldsBase.Weingarten","ref":"/manifolds/stable/manifolds/skewhermitian/#ManifoldsBase.Weingarten-Tuple{SkewSymmetricMatrices, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Y = Weingarten(M::SkewSymmetricMatrices, p, X, V)\nWeingarten!(M::SkewSymmetricMatrices, Y, p, X, V) Compute the Weingarten map  $\\mathcal W_p$  at  p  on the  SkewSymmetricMatrices M  with respect to the tangent vector  $X \\in T_p\\mathcal M$  and the normal vector  $V \\in N_p\\mathcal M$ . Since this a flat space by itself, the result is always the zero tangent vector. source"},{"id":1555,"pagetitle":"Skew-Hermitian matrices","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/skewhermitian/#ManifoldsBase.check_point-Union{Tuple{ùîΩ}, Tuple{SkewHermitianMatrices{<:Any, ùîΩ}, Any}} where ùîΩ","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::SkewHermitianMatrices, p; kwargs...) Check whether  p  is a valid manifold point on the  SkewHermitianMatrices M , i.e. whether  p  is a skew-hermitian matrix of size  (n,n)  with values from the corresponding  AbstractNumbers ùîΩ . The tolerance for the skew-symmetry of  p  can be set using  kwargs... . source"},{"id":1556,"pagetitle":"Skew-Hermitian matrices","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/skewhermitian/#ManifoldsBase.check_vector-Tuple{SkewHermitianMatrices, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::SkewHermitianMatrices, p, X; kwargs... ) Check whether  X  is a tangent vector to manifold point  p  on the  SkewHermitianMatrices M , i.e.  X  must be a skew-hermitian matrix of size  (n,n)  and its values have to be from the correct  AbstractNumbers . The tolerance for the skew-symmetry of  p  and  X  can be set using  kwargs... . source"},{"id":1557,"pagetitle":"Skew-Hermitian matrices","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/skewhermitian/#ManifoldsBase.is_flat-Tuple{SkewHermitianMatrices}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::SkewHermitianMatrices) Return true.  SkewHermitianMatrices  is a flat manifold. source"},{"id":1558,"pagetitle":"Skew-Hermitian matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/skewhermitian/#ManifoldsBase.manifold_dimension-Union{Tuple{SkewHermitianMatrices{<:Any, ùîΩ}}, Tuple{ùîΩ}} where ùîΩ","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::SkewHermitianMatrices) Return the dimension of the  SkewHermitianMatrices  matrix  M  over the number system  ùîΩ , i.e. \\[\\dim \\mathrm{SkewHerm}(n,‚Ñù) = \\frac{n(n+1)}{2} \\dim_‚Ñù ùîΩ - n,\\] where  $\\dim_‚Ñù ùîΩ$  is the  real_dimension  of  $ùîΩ$ . The first term corresponds to only the upper triangular elements of the matrix being unique, and the second term corresponds to the constraint that the real part of the diagonal be zero. source"},{"id":1559,"pagetitle":"Skew-Hermitian matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/skewhermitian/#ManifoldsBase.project-Tuple{SkewHermitianMatrices, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::SkewHermitianMatrices, p, X) Project the matrix  X  onto the tangent space at  p  on the  SkewHermitianMatrices M , \\[\\operatorname{proj}_p(X) = \\frac{1}{2} \\bigl( X - X^{\\mathrm{H}} \\bigr),\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the Hermitian, i.e. complex conjugate transposed. source"},{"id":1560,"pagetitle":"Skew-Hermitian matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/skewhermitian/#ManifoldsBase.project-Tuple{SkewHermitianMatrices, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::SkewHermitianMatrices, p) Projects  p  from the embedding onto the  SkewHermitianMatrices M , i.e. \\[\\operatorname{proj}_{\\operatorname{SkewHerm}(n)}(p) = \\frac{1}{2} \\bigl( p - p^{\\mathrm{H}} \\bigr),\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the Hermitian, i.e. complex conjugate transposed. source"},{"id":1563,"pagetitle":"SPD, fixed determinant","title":"Symmetric positive definite matrices of fixed determinant","ref":"/manifolds/stable/manifolds/spdfixeddeterminant/#SPDFixedDeterminantSection","content":" Symmetric positive definite matrices of fixed determinant"},{"id":1564,"pagetitle":"SPD, fixed determinant","title":"Manifolds.SPDFixedDeterminant","ref":"/manifolds/stable/manifolds/spdfixeddeterminant/#Manifolds.SPDFixedDeterminant","content":" Manifolds.SPDFixedDeterminant  ‚Äî  Type SPDFixedDeterminant{T,D} <: AbstractDecoratorManifold{‚Ñù} The manifold of symmetric positive definite matrices of fixed determinant  $d > 0$ , i.e. \\[\\mathcal P_d(n) =\n\\bigl\\{\np ‚àà ‚Ñù^{n√ón} \\ \\big|\\ a^\\mathrm{T}pa > 0 \\text{ for all } a ‚àà ‚Ñù^{n}\\backslash\\{0\\}\n  \\text{ and } \\det(p) = d\n\\bigr\\}.\\] This manifold is modelled as a submanifold of  SymmetricPositiveDefinite (n) , see  IsEmbeddedSubmanifold  for the implications, but for example retractions and inverse retractions are all available These matrices are sometimes also called  isochoric , which refers to the interpretation of the matrix representing an ellipsoid. All ellipsoids that represent points on this manifold have the same volume. The tangent space is modelled the same as for  SymmetricPositiveDefinite (n)  and consists of all symmetric matrices with zero trace \\[    T_p\\mathcal P_d(n) =\n    \\bigl\\{\n        X \\in \\mathbb R^{n√ón} \\big|\\ X=X^\\mathrm{T} \\text{ and } \\operatorname{tr}(X) = 0\n    \\bigr\\},\\] since for a constant determinant we require that  $0 = D\\det(p)[Z] = \\det(p)\\operatorname{tr}(p^{-1}Z)$  for all tangent vectors  $Z$ . Additionally we store the tangent vectors as  $X=p^{-1}Z$ , i.e. symmetric matrices. Constructor SPDFixedDeterminant(n::Int, d::Real=1.0; parameter::Symbol=:type) Generate the manifold  $\\mathcal P_d(n) \\subset \\mathcal P(n)$  of determinant  $d$ , which defaults to  1.0 . parameter : whether a type parameter should be used to store  n . By default size is stored in type. Value can either be  :field  or  :type . source This manifold can is a submanifold of the  symmetric positive definite matrices  and hence inherits most properties therefrom. The differences are the functions"},{"id":1565,"pagetitle":"SPD, fixed determinant","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/spdfixeddeterminant/#ManifoldsBase.check_point-Tuple{SPDFixedDeterminant, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::SPDFixedDeterminant, p; kwargs...) Check whether  p  is a valid manifold point on the  SPDFixedDeterminant (n,d) M , i.e. whether  p  is a  SymmetricPositiveDefinite  matrix of size  (n, n) with determinant  $\\det(p) =$ M.d . The tolerance for the determinant of  p  can be set using  kwargs... . source"},{"id":1566,"pagetitle":"SPD, fixed determinant","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/spdfixeddeterminant/#ManifoldsBase.check_vector-Union{Tuple{T}, Tuple{SPDFixedDeterminant, Any, T}} where T","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::SPDFixedDeterminant, p, X; kwargs... ) Check whether  X  is a tangent vector to manifold point  p  on the  SPDFixedDeterminant M , i.e.  X  has to be a tangent vector on  SymmetricPositiveDefinite , so a symmetric matrix, and additionally fulfill  $\\operatorname{tr}(X) = 0$ . The tolerance for the trace check of  X  can be set using  kwargs... , which influences the  isapprox -check. source"},{"id":1567,"pagetitle":"SPD, fixed determinant","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/spdfixeddeterminant/#ManifoldsBase.project-Tuple{SPDFixedDeterminant, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method Y = project(M::SPDFixedDeterminant, p, X)\nproject!(M::SPDFixedDeterminant, Y, p, X) Project the symmetric matrix  X  onto the tangent space at  p  of the (sub-)manifold of s.p.d. matrices of determinant  M.d  (in place of  Y ), by setting its diagonal (and hence its trace) to zero. source"},{"id":1568,"pagetitle":"SPD, fixed determinant","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/spdfixeddeterminant/#ManifoldsBase.project-Tuple{SPDFixedDeterminant, Any}","content":" ManifoldsBase.project  ‚Äî  Method q = project(M::SPDFixedDeterminant, p)\nproject!(M::SPDFixedDeterminant, q, p) Project the symmetric positive definite (s.p.d.) matrix  p  from the embedding onto the (sub-)manifold of s.p.d. matrices of determinant  M.d  (in place of  q ). The formula reads \\[q = \\Bigl(\\frac{d}{\\det(p)}\\Bigr)^{\\frac{1}{n}}p\\] source"},{"id":1571,"pagetitle":"Spectrahedron","title":"Spectrahedron","ref":"/manifolds/stable/manifolds/spectrahedron/#Spectrahedron","content":" Spectrahedron"},{"id":1572,"pagetitle":"Spectrahedron","title":"Manifolds.Spectrahedron","ref":"/manifolds/stable/manifolds/spectrahedron/#Manifolds.Spectrahedron","content":" Manifolds.Spectrahedron  ‚Äî  Type Spectrahedron{T} <: AbstractDecoratorManifold{‚Ñù} The Spectrahedron manifold, also known as the set of correlation matrices (symmetric positive semidefinite matrices) of rank  $k$  with unit trace. \\[\\begin{aligned}\n\\mathcal S(n,k) =\n\\bigl\\{p ‚àà ‚Ñù^{n√ón}\\ \\big|\\ &a^\\mathrm{T}pa \\geq 0 \\text{ for all } a ‚àà ‚Ñù^{n},\\\\\n&\\operatorname{tr}(p) = \\sum_{i=1}^n p_{ii} = 1,\\\\\n&\\text{and } p = qq^{\\mathrm{T}} \\text{ for } q \\in  ‚Ñù^{n√ók}\n\\text{ with } \\operatorname{rank}(p) = \\operatorname{rank}(q) = k\n\\bigr\\}.\n\\end{aligned}\\] This manifold is working solely on the matrices  $q$ . Note that this  $q$  is not unique, indeed for any orthogonal matrix  $A$  we have  $(qA)(qA)^{\\mathrm{T}} = qq^{\\mathrm{T}} = p$ , so the manifold implemented here is the quotient manifold. The unit trace translates to unit frobenius norm of  $q$ . The tangent space at  $p$ , denoted  $T_p\\mathcal E(n,k)$ , is also represented by matrices  $Y\\in ‚Ñù^{n√ók}$  and reads as \\[T_p\\mathcal S(n,k) = \\bigl\\{\nX ‚àà ‚Ñù^{n√ón}\\,|\\,X = qY^{\\mathrm{T}} + Yq^{\\mathrm{T}}\n\\text{ with } \\operatorname{tr}(X) = \\sum_{i=1}^{n}X_{ii} = 0\n\\bigr\\}\\] endowed with the  Euclidean  metric from the embedding, i.e. from the  $‚Ñù^{n√ók}$ This manifold was for example investigated in [ JBAS10 ]. Constructor Spectrahedron(n::Int, k::Int; parameter::Symbol=:type) generates the manifold  $\\mathcal S(n,k) \\subset ‚Ñù^{n√ón}$ . source"},{"id":1573,"pagetitle":"Spectrahedron","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/spectrahedron/#ManifoldsBase.check_point-Tuple{Spectrahedron, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::Spectrahedron, q; kwargs...) checks, whether  q  is a valid representation of a point  $p=qq^{\\mathrm{T}}$  on the  Spectrahedron M , i.e. is a matrix of size  (N,K) , such that  $p$  is symmetric positive semidefinite and has unit trace, i.e.  $q$  has to have unit frobenius norm. Since by construction  $p$  is symmetric, this is not explicitly checked. Since  $p$  is by construction positive semidefinite, this is not checked. The tolerances for positive semidefiniteness and unit trace can be set using the  kwargs... . source"},{"id":1574,"pagetitle":"Spectrahedron","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/spectrahedron/#ManifoldsBase.check_vector-Union{Tuple{T}, Tuple{Spectrahedron, Any, T}} where T","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::Spectrahedron, q, Y; kwargs...) Check whether  $X = qY^{\\mathrm{T}} + Yq^{\\mathrm{T}}$  is a tangent vector to  $p=qq^{\\mathrm{T}}$  on the  Spectrahedron M , i.e. atfer  check_point  of  q ,  Y  has to be of same dimension as  q  and a  $X$  has to be a symmetric matrix with trace. The tolerance for the base point check and zero diagonal can be set using the  kwargs... . Note that symmetry of  $X$  holds by construction and is not explicitly checked. source"},{"id":1575,"pagetitle":"Spectrahedron","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/spectrahedron/#ManifoldsBase.is_flat-Tuple{Spectrahedron}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::Spectrahedron) Return false.  Spectrahedron  is not a flat manifold. source"},{"id":1576,"pagetitle":"Spectrahedron","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/spectrahedron/#ManifoldsBase.manifold_dimension-Tuple{Spectrahedron}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::Spectrahedron) returns the dimension of  Spectrahedron M $=\\mathcal S(n,k), n,k ‚àà ‚Ñï$ , i.e. \\[\\dim \\mathcal S(n,k) = nk - 1 - \\frac{k(k-1)}{2}.\\] source"},{"id":1577,"pagetitle":"Spectrahedron","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/spectrahedron/#ManifoldsBase.project-Tuple{Spectrahedron, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Spectrahedron, q) project  q  onto the manifold  Spectrahedron M , by normalizing w.r.t. the Frobenius norm source"},{"id":1578,"pagetitle":"Spectrahedron","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/spectrahedron/#ManifoldsBase.project-Tuple{Spectrahedron, Vararg{Any}}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Spectrahedron, q, Y) Project  Y  onto the tangent space at  q , i.e. row-wise onto the Spectrahedron manifold. source"},{"id":1579,"pagetitle":"Spectrahedron","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/spectrahedron/#ManifoldsBase.representation_size-Tuple{Spectrahedron}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::Spectrahedron) Return the size of an array representing an element on the  Spectrahedron  manifold  M , i.e.  $n√ók$ , the size of such factor of  $p=qq^{\\mathrm{T}}$  on  $\\mathcal M = \\mathcal S(n,k)$ . source"},{"id":1580,"pagetitle":"Spectrahedron","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/spectrahedron/#ManifoldsBase.retract-Tuple{Spectrahedron, Any, Any, ProjectionRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::Spectrahedron, q, Y, ::ProjectionRetraction) compute a projection based retraction by projecting  $q+Y$  back onto the manifold. source"},{"id":1581,"pagetitle":"Spectrahedron","title":"ManifoldsBase.vector_transport_to","ref":"/manifolds/stable/manifolds/spectrahedron/#ManifoldsBase.vector_transport_to-Tuple{Spectrahedron, Any, Any, Any, ProjectionTransport}","content":" ManifoldsBase.vector_transport_to  ‚Äî  Method vector_transport_to(M::Spectrahedron, p, X, q) transport the tangent vector  X  at  p  to  q  by projecting it onto the tangent space at  q . source"},{"id":1582,"pagetitle":"Spectrahedron","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/spectrahedron/#ManifoldsBase.zero_vector-Tuple{Spectrahedron, Vararg{Any}}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(M::Spectrahedron,p) returns the zero tangent vector in the tangent space of the symmetric positive definite matrix  p  on the  Spectrahedron  manifold  M . source"},{"id":1583,"pagetitle":"Spectrahedron","title":"Literature","ref":"/manifolds/stable/manifolds/spectrahedron/#Literature","content":" Literature"},{"id":1586,"pagetitle":"Sphere","title":"Sphere and unit norm arrays","ref":"/manifolds/stable/manifolds/sphere/#SphereSection","content":" Sphere and unit norm arrays"},{"id":1587,"pagetitle":"Sphere","title":"Manifolds.AbstractSphere","ref":"/manifolds/stable/manifolds/sphere/#Manifolds.AbstractSphere","content":" Manifolds.AbstractSphere  ‚Äî  Type AbstractSphere{ùîΩ} <: AbstractDecoratorManifold{ùîΩ} An abstract type to represent a unit sphere that is represented isometrically in the embedding. source The classical sphere, i.e. unit norm (real- or complex-valued) vectors can be generated as usual: to create the 2-dimensional sphere (in  $‚Ñù^3$ ), use  Sphere(2)  and  Sphere(2,‚ÑÇ) , respectively."},{"id":1588,"pagetitle":"Sphere","title":"Manifolds.Sphere","ref":"/manifolds/stable/manifolds/sphere/#Manifolds.Sphere","content":" Manifolds.Sphere  ‚Äî  Type Sphere{T,ùîΩ} <: AbstractSphere{ùîΩ} The (unit) sphere manifold  $ùïä^{n}$  is the set of all unit norm vectors in  $ùîΩ^{n+1}$ . The sphere is represented in the embedding, i.e. \\[ùïä^{n} := \\bigl\\{ p \\in ùîΩ^{n+1}\\ \\big|\\ \\lVert p \\rVert = 1 \\bigr\\}\\] where  $ùîΩ\\in\\{‚Ñù,‚ÑÇ,‚Ñç\\}$ . Note that compared to the  ArraySphere , here the argument  n  of the manifold is the dimension of the manifold, i.e.  $ùïä^{n} ‚äÇ ùîΩ^{n+1}$ ,  $n\\in ‚Ñï$ . The tangent space at point  $p$  is given by \\[T_pùïä^{n} := \\bigl\\{ X ‚àà ùîΩ^{n+1}\\ |\\ \\Re(‚ü®p,X‚ü©) = 0 \\bigr \\},\\] where  $ùîΩ\\in\\{‚Ñù,‚ÑÇ,‚Ñç\\}$  and  $‚ü®‚ãÖ,‚ãÖ‚ü©$  denotes the inner product in the embedding  $ùîΩ^{n+1}$ . For  $ùîΩ=‚ÑÇ$ , the manifold is the complex sphere, written  $‚ÑÇùïä^n$ , embedded in  $‚ÑÇ^{n+1}$ .  $‚ÑÇùïä^n$  is the complexification of the real sphere  $ùïä^{2n+1}$ . Likewise, the quaternionic sphere  $‚Ñçùïä^n$  is the quaternionification of the real sphere  $ùïä^{4n+3}$ . Consequently,  $‚ÑÇùïä^0$  is equivalent to  $ùïä^1$  and  Circle , while  $‚ÑÇùïä^1$  and  $‚Ñçùïä^0$  are equivalent to  $ùïä^3$ , though with different default representations. This manifold is modeled as a special case of the more general case, i.e. as an embedded manifold to the  Euclidean , and several functions like the  inner  product and the  zero_vector  are inherited from the embedding. Constructor Sphere(n[, field=‚Ñù]) Generate the (real-valued) sphere  $ùïä^{n} ‚äÇ ‚Ñù^{n+1}$ , where  field  can also be used to generate the complex- and quaternionic-valued sphere. source For the higher-dimensional arrays, for example unit (Frobenius) norm matrices, the manifold is generated using the size of the matrix. To create the unit sphere of  $3√ó2$  real-valued matrices, write  ArraySphere(3,2)  and the complex case is done ‚Äì as for the  Euclidean  case ‚Äì with an keyword argument  ArraySphere(3,2; field=‚ÑÇ) . This case also covers the classical sphere as a special case, but you specify the size of the vectors/embedding instead: The 2-sphere can here be generated  ArraySphere(3) ."},{"id":1589,"pagetitle":"Sphere","title":"Manifolds.ArraySphere","ref":"/manifolds/stable/manifolds/sphere/#Manifolds.ArraySphere","content":" Manifolds.ArraySphere  ‚Äî  Type ArraySphere{T<:Tuple,ùîΩ} <: AbstractSphere{ùîΩ} The (unit) sphere manifold  $ùïä^{n‚ÇÅ,n‚ÇÇ,...,n·µ¢}$  is the set of all unit (Frobenius) norm elements of  $ùîΩ^{n‚ÇÅ,n‚ÇÇ,...,n·µ¢}$ , where ``ùîΩ\\in{‚Ñù,‚ÑÇ,‚Ñç}. The generalized sphere is represented in the embedding, and supports arbitrary sized arrays or in other words arbitrary tensors of unit norm. The set formally reads \\[ùïä^{n_1, n_2, ‚Ä¶, n_i} := \\bigl\\{ p \\in ùîΩ^{n_1, n_2, ‚Ä¶, n_i}\\ \\big|\\ \\lVert p \\rVert = 1 \\bigr\\}\\] where  $ùîΩ\\in\\{‚Ñù,‚ÑÇ,‚Ñç\\}$ . Setting  $i=1$  and  $ùîΩ=‚Ñù$   this  simplifies to unit vectors in  $‚Ñù^n$ , see  Sphere  for this special case. Note that compared to this classical case, the argument for the generalized case here is given by the dimension of the embedding. This means that  Sphere(2)  and  ArraySphere(3)  are the same manifold. The tangent space at point  $p$  is given by \\[T_p ùïä^{n_1, n_2, ‚Ä¶, n_i} := \\bigl\\{ X ‚àà ùîΩ^{n_1, n_2, ‚Ä¶, n_i}\\ |\\ \\Re(‚ü®p,X‚ü©) = 0 \\bigr \\},\\] where  $ùîΩ\\in\\{‚Ñù,‚ÑÇ,‚Ñç\\}$  and  $‚ü®‚ãÖ,‚ãÖ‚ü©$  denotes the (Frobenius) inner product in the embedding  $ùîΩ^{n_1, n_2, ‚Ä¶, n_i}$ . This manifold is modeled as an embedded manifold to the  Euclidean , i.e. several functions like the  inner  product and the  zero_vector  are inherited from the embedding. Constructor ArraySphere(n‚ÇÅ,n‚ÇÇ,...,n·µ¢; field=‚Ñù, parameter::Symbol=:type) Generate sphere in  $ùîΩ^{n_1, n_2, ‚Ä¶, n_i}$ , where  $ùîΩ$  defaults to the real-valued case  $‚Ñù$ . source There is also one atlas available on the sphere."},{"id":1590,"pagetitle":"Sphere","title":"Manifolds.StereographicAtlas","ref":"/manifolds/stable/manifolds/sphere/#Manifolds.StereographicAtlas","content":" Manifolds.StereographicAtlas  ‚Äî  Type StereographicAtlas() The stereographic atlas of  $S^n$  with two charts: one with the singular point (-1, 0, ..., 0) (called  :north ) and one with the singular point (1, 0, ..., 0) (called  :south ). source"},{"id":1591,"pagetitle":"Sphere","title":"Functions on unit spheres","ref":"/manifolds/stable/manifolds/sphere/#Functions-on-unit-spheres","content":" Functions on unit spheres"},{"id":1592,"pagetitle":"Sphere","title":"Base.exp","ref":"/manifolds/stable/manifolds/sphere/#Base.exp-Tuple{AbstractSphere, Vararg{Any}}","content":" Base.exp  ‚Äî  Method exp(M::AbstractSphere, p, X) Compute the exponential map from  p  in the tangent direction  X  on the  AbstractSphere M  by following the great arc emanating from  p  in direction  X . \\[\\exp_p X = \\cos(\\lVert X \\rVert_p)p + \\sin(\\lVert X \\rVert_p)\\frac{X}{\\lVert X \\rVert_p},\\] where  $\\lVert X \\rVert_p$  is the  norm  on the tangent space at  p  of the  AbstractSphere M . source"},{"id":1593,"pagetitle":"Sphere","title":"Base.log","ref":"/manifolds/stable/manifolds/sphere/#Base.log-Tuple{AbstractSphere, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::AbstractSphere, p, q) Compute the logarithmic map on the  AbstractSphere M , i.e. the tangent vector, whose geodesic starting from  p  reaches  q  after time 1. The formula reads for  $x ‚â† -y$ \\[\\log_p q = d_{ùïä}(p,q) \\frac{q-\\Re(‚ü®p,q‚ü©) p}{\\lVert q-\\Re(‚ü®p,q‚ü©) p \\rVert_2},\\] and a deterministic choice from the set of tangent vectors is returned if  $x=-y$ , i.e. for opposite points. source"},{"id":1594,"pagetitle":"Sphere","title":"Manifolds.local_metric","ref":"/manifolds/stable/manifolds/sphere/#Manifolds.local_metric-Tuple{Sphere{Tuple{Int64}, ‚Ñù}, Any, DefaultOrthonormalBasis}","content":" Manifolds.local_metric  ‚Äî  Method local_metric(M::Sphere{n}, p, ::DefaultOrthonormalBasis) return the local representation of the metric in a  DefaultOrthonormalBasis , namely the diagonal matrix of size  $n√ón$  with ones on the diagonal, since the metric is obtained from the embedding by restriction to the tangent space  $T_p\\mathcal M$  at  $p$ . source"},{"id":1595,"pagetitle":"Sphere","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/sphere/#Manifolds.manifold_volume-Tuple{AbstractSphere{‚Ñù}}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(M::AbstractSphere{‚Ñù}) Volume of the  $n$ -dimensional  Sphere M . The formula reads \\[\\operatorname{Vol}(ùïä^{n}) = \\frac{2\\pi^{(n+1)/2}}{Œì((n+1)/2)},\\] where  $Œì$  denotes the  Gamma function . source"},{"id":1596,"pagetitle":"Sphere","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/sphere/#Manifolds.volume_density-Tuple{AbstractSphere{‚Ñù}, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(M::AbstractSphere{‚Ñù}, p, X) Compute volume density function of a sphere, i.e. determinant of the differential of exponential map  exp(M, p, X) . The formula reads  $(\\sin(\\lVert X\\rVert)/\\lVert X\\rVert)^(n-1)$  where  n  is the dimension of  M . It is derived from Eq. (4.1) in [ CLLD22 ]. source"},{"id":1597,"pagetitle":"Sphere","title":"ManifoldsBase.Weingarten","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.Weingarten-Tuple{Sphere, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Y = Weingarten(M::Sphere, p, X, V)\nWeingarten!(M::Sphere, Y, p, X, V) Compute the Weingarten map  $\\mathcal W_p$  at  p  on the  Sphere M  with respect to the tangent vector  $X \\in T_p\\mathcal M$  and the normal vector  $V \\in N_p\\mathcal M$ . The formula is due to [ AMT13 ] given by \\[\\mathcal W_p(X,V) = -Xp^{\\mathrm{T}}V\\] source"},{"id":1598,"pagetitle":"Sphere","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.check_point-Tuple{AbstractSphere, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::AbstractSphere, p; kwargs...) Check whether  p  is a valid point on the  AbstractSphere M , i.e. is a point in the embedding of unit length. The tolerance for the last test can be set using the  kwargs... . source"},{"id":1599,"pagetitle":"Sphere","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.check_vector-Union{Tuple{T}, Tuple{AbstractSphere, Any, T}} where T","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::AbstractSphere, p, X; kwargs... ) Check whether  X  is a tangent vector to  p  on the  AbstractSphere M , i.e. after  check_point (M,p) ,  X  has to be of same dimension as  p  and orthogonal to  p . The tolerance for the last test can be set using the  kwargs... . source"},{"id":1600,"pagetitle":"Sphere","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.distance-Tuple{AbstractSphere, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::AbstractSphere, p, q) Compute the geodesic distance between  p  and  q  on the  AbstractSphere M . The formula is given by the (shorter) great arc length on the (or a) great circle both  p  and  q  lie on. \\[d_{ùïä}(p,q) = \\arccos(\\Re(‚ü®p,q‚ü©)).\\] source"},{"id":1601,"pagetitle":"Sphere","title":"ManifoldsBase.get_coordinates","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.get_coordinates-Tuple{AbstractSphere{‚Ñù}, Any, Any, DefaultOrthonormalBasis}","content":" ManifoldsBase.get_coordinates  ‚Äî  Method get_coordinates(M::AbstractSphere{‚Ñù}, p, X, B::DefaultOrthonormalBasis) Represent the tangent vector  X  at point  p  from the  AbstractSphere M  in an orthonormal basis by rotating the hyperplane containing  X  to a hyperplane whose normal is the  $x$ -axis. Given  $q = p Œª + x$ , where  $Œª = \\operatorname{sgn}(‚ü®x, p‚ü©)$ , and  $‚ü®‚ãÖ, ‚ãÖ‚ü©_{\\mathrm{F}}$  denotes the Frobenius inner product, the formula for  $Y$  is \\[\\begin{pmatrix}0 \\\\ Y\\end{pmatrix} = X - q\\frac{2 ‚ü®q, X‚ü©_{\\mathrm{F}}}{‚ü®q, q‚ü©_{\\mathrm{F}}}.\\] source"},{"id":1602,"pagetitle":"Sphere","title":"ManifoldsBase.get_vector","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.get_vector-Tuple{AbstractSphere{‚Ñù}, Any, Any, DefaultOrthonormalBasis}","content":" ManifoldsBase.get_vector  ‚Äî  Method get_vector(M::AbstractSphere{‚Ñù}, p, X, B::DefaultOrthonormalBasis) Convert a one-dimensional vector of coefficients  X  in the basis  B  of the tangent space at  p  on the  AbstractSphere M  to a tangent vector  Y  at  p  by rotating the hyperplane containing  X , whose normal is the  $x$ -axis, to the hyperplane whose normal is  p . Given  $q = p Œª + x$ , where  $Œª = \\operatorname{sgn}(‚ü®x, p‚ü©)$ , and  $‚ü®‚ãÖ, ‚ãÖ‚ü©_{\\mathrm{F}}$  denotes the Frobenius inner product, the formula for  $Y$  is \\[Y = X - q\\frac{2 \\left\\langle q, \\begin{pmatrix}0 \\\\ X\\end{pmatrix}\\right\\rangle_{\\mathrm{F}}}{‚ü®q, q‚ü©_{\\mathrm{F}}}.\\] source"},{"id":1603,"pagetitle":"Sphere","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.injectivity_radius-Tuple{AbstractSphere}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::AbstractSphere[, p]) Return the injectivity radius for the  AbstractSphere M , which is globally  $œÄ$ . injectivity_radius(M::Sphere, x, ::ProjectionRetraction) Return the injectivity radius for the  ProjectionRetraction  on the  AbstractSphere , which is globally  $\\frac{œÄ}{2}$ . source"},{"id":1604,"pagetitle":"Sphere","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.inverse_retract-Tuple{AbstractSphere, Any, Any, ProjectionInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::AbstractSphere, p, q, ::ProjectionInverseRetraction) Compute the inverse of the projection based retraction on the  AbstractSphere M , i.e. rearranging  $p+X = q\\lVert p+X\\rVert_2$  yields since  $\\Re(‚ü®p,X‚ü©) = 0$  and when  $d_{ùïä^2}(p,q) ‚â§ \\frac{œÄ}{2}$  that \\[\\operatorname{retr}_p^{-1}(q) = \\frac{q}{\\Re(‚ü®p, q‚ü©)} - p.\\] source"},{"id":1605,"pagetitle":"Sphere","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.is_flat-Tuple{AbstractSphere}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(M::AbstractSphere) Return true if  AbstractSphere  is of dimension 1 and false otherwise. source"},{"id":1606,"pagetitle":"Sphere","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.manifold_dimension-Tuple{AbstractSphere}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::AbstractSphere) Return the dimension of the  AbstractSphere M , respectively i.e. the dimension of the embedding -1. source"},{"id":1607,"pagetitle":"Sphere","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.parallel_transport_to-Tuple{AbstractSphere, Vararg{Any, 4}}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::AbstractSphere, p, X, q) Compute the parallel transport on the  Sphere  of the tangent vector  X  at  p  to  q , provided, the  geodesic  between  p  and  q  is unique. The formula reads \\[P_{p‚Üêq}(X) = X - \\frac{\\Re(‚ü®\\log_p q,X‚ü©_p)}{d^2_ùïä(p,q)}\n\\bigl(\\log_p q + \\log_q p \\bigr).\\] source"},{"id":1608,"pagetitle":"Sphere","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.project-Tuple{AbstractSphere, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::AbstractSphere, p, X) Project the point  X  onto the tangent space at  p  on the  Sphere M . \\[\\operatorname{proj}_{p}(X) = X - \\Re(‚ü®p, X‚ü©)p\\] source"},{"id":1609,"pagetitle":"Sphere","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.project-Tuple{AbstractSphere, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::AbstractSphere, p) Project the point  p  from the embedding onto the  Sphere M . \\[\\operatorname{proj}(p) = \\frac{p}{\\lVert p \\rVert},\\] where  $\\lVert‚ãÖ\\rVert$  denotes the usual 2-norm for vectors if  $m=1$  and the Frobenius norm for the case  $m>1$ . source"},{"id":1610,"pagetitle":"Sphere","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.representation_size-Tuple{ArraySphere}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::AbstractSphere) Return the size points on the  AbstractSphere M  are represented as, i.e., the representation size of the embedding. source"},{"id":1611,"pagetitle":"Sphere","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.retract-Tuple{AbstractSphere, Any, Any, ProjectionRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::AbstractSphere, p, X, ::ProjectionRetraction) Compute the retraction that is based on projection, i.e. \\[\\operatorname{retr}_p(X) = \\frac{p+X}{\\lVert p+X \\rVert_2}\\] source"},{"id":1612,"pagetitle":"Sphere","title":"ManifoldsBase.riemann_tensor","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.riemann_tensor-Tuple{AbstractSphere{‚Ñù}, Vararg{Any, 4}}","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(M::AbstractSphere{‚Ñù}, p, X, Y, Z) Compute the Riemann tensor  $R(X,Y)Z$  at point  p  on  AbstractSphere M . The formula reads [ MF12 ] (though note that a different convention is used in that paper than in Manifolds.jl): \\[R(X,Y)Z = \\langle Z, Y \\rangle X - \\langle Z, X \\rangle Y\\] source"},{"id":1613,"pagetitle":"Sphere","title":"ManifoldsBase.sectional_curvature","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.sectional_curvature-Tuple{AbstractSphere, Any, Any, Any}","content":" ManifoldsBase.sectional_curvature  ‚Äî  Method sectional_curvature(::AbstractSphere, p, X, Y) Sectional curvature of  AbstractSphere M  is 1 if dimension is greater than 1 and 0 otherwise. source"},{"id":1614,"pagetitle":"Sphere","title":"ManifoldsBase.sectional_curvature_max","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.sectional_curvature_max-Tuple{AbstractSphere}","content":" ManifoldsBase.sectional_curvature_max  ‚Äî  Method sectional_curvature_max(::AbstractSphere) Sectional curvature of  AbstractSphere M  is 1 if dimension is greater than 1 and 0 otherwise. source"},{"id":1615,"pagetitle":"Sphere","title":"ManifoldsBase.sectional_curvature_min","ref":"/manifolds/stable/manifolds/sphere/#ManifoldsBase.sectional_curvature_min-Tuple{AbstractSphere}","content":" ManifoldsBase.sectional_curvature_min  ‚Äî  Method sectional_curvature_min(M::AbstractSphere) Sectional curvature of  AbstractSphere M  is 1 if dimension is greater than 1 and 0 otherwise. source"},{"id":1616,"pagetitle":"Sphere","title":"Statistics.mean","ref":"/manifolds/stable/manifolds/sphere/#Statistics.mean-Tuple{AbstractSphere, Vararg{Any}}","content":" Statistics.mean  ‚Äî  Method mean(\n    S::AbstractSphere,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method = GeodesicInterpolationWithinRadius(œÄ/2);\n    kwargs...,\n) Compute the Riemannian  mean  of  x  using  GeodesicInterpolationWithinRadius . source"},{"id":1617,"pagetitle":"Sphere","title":"Visualization on Sphere{2,‚Ñù}","ref":"/manifolds/stable/manifolds/sphere/#Visualization-on-Sphere{2,‚Ñù}","content":" Visualization on  Sphere{2,‚Ñù} You can visualize both points and tangent vectors on the sphere. Note There seems to be no unified way to draw spheres in the backends of  Plots.jl . This recipe currently uses the  seriestype wireframe  and  surface , which does not yet work with the default backend  GR . In general you can plot the surface of the hyperboloid either as wireframe ( wireframe=true ) additionally specifying  wires  (or  wires_x  and  wires_y ) to change the density of the wires and a  wireframe_color  for their color. The same holds for the plot as a  surface  (which is  false  by default) and its  surface_resolution  (or  surface_resolution_lat  or  surface_resolution_lon ) and a  surface_color . using Manifolds, Plots\npythonplot()\nM = Sphere(2)\npts = [ [1.0, 0.0, 0.0], [0.0, -1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, 0.0] ]\nscene = plot(M, pts; wireframe_color=colorant\"#CCCCCC\", markersize=10) which scatters our points. We can also draw connecting geodesics, which here is a geodesic triangle. Here we discretize each geodesic with 100 points along the geodesic. The default value is  geodesic_interpolation=-1  which switches to scatter plot of the data. plot!(scene, M, pts; wireframe=false, geodesic_interpolation=100, linewidth=2) And we can also add tangent vectors, for example tangents pointing towards the geometric center of given points. pts2 =  [ [1.0, 0.0, 0.0], [0.0, -1.0, 0.0], [0.0, 0.0, 1.0] ]\np3 = 1/sqrt(3) .* [1.0, -1.0, 1.0]\nvecs = log.(Ref(M), pts2, Ref(p3))\nplot!(scene, M, pts2, vecs; wireframe = false, linewidth=1.5)"},{"id":1618,"pagetitle":"Sphere","title":"Literature","ref":"/manifolds/stable/manifolds/sphere/#Literature","content":" Literature [AMT13] P.¬†-.-A.¬†Absil, R.¬†Mahony and J.¬†Trumpf.  An Extrinsic Look at the Riemannian Hessian . In:  Geometric Science of Information , edited by F.¬†Nielsen and F.¬†Barbaresco (Springer Berlin Heidelberg, 2013); pp.¬†361‚Äì368. [CLLD22] E.¬†Chevallier, D.¬†Li, Y.¬†Lu and D.¬†B.¬†Dunson.  Exponential-wrapped distributions on symmetric spaces . ArXiv¬†Preprint (2022). [MF12] P.¬†Muralidharan and P.¬†T.¬†Fletcher.  Sasaki metrics for analysis of longitudinal data on manifolds . In:  2012 IEEE Conference on Computer Vision and Pattern Recognition  (2012)."},{"id":1621,"pagetitle":"Unit-norm symmetric matrices","title":"Unit-norm symmetric matrices","ref":"/manifolds/stable/manifolds/spheresymmetricmatrices/#Unit-norm-symmetric-matrices","content":" Unit-norm symmetric matrices"},{"id":1622,"pagetitle":"Unit-norm symmetric matrices","title":"Manifolds.SphereSymmetricMatrices","ref":"/manifolds/stable/manifolds/spheresymmetricmatrices/#Manifolds.SphereSymmetricMatrices","content":" Manifolds.SphereSymmetricMatrices  ‚Äî  Type SphereSymmetricMatrices{T,ùîΩ} <: AbstractEmbeddedManifold{‚Ñù,TransparentIsometricEmbedding} The  AbstractManifold   consisting of the  $n√ón$  symmetric matrices of unit Frobenius norm, i.e. \\[\\mathcal{S}_{\\text{sym}} :=\\bigl\\{p  ‚àà ùîΩ^{n√ón}\\ \\big|\\ p^{\\mathrm{H}} = p, \\lVert p \\rVert = 1 \\bigr\\},\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the Hermitian, i.e. complex conjugate transpose, and the field  $ùîΩ ‚àà \\{ ‚Ñù, ‚ÑÇ\\}$ . Constructor SphereSymmetricMatrices(n[, field=‚Ñù]) Generate the manifold of  n -by- n  symmetric matrices of unit Frobenius norm. source"},{"id":1623,"pagetitle":"Unit-norm symmetric matrices","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/spheresymmetricmatrices/#ManifoldsBase.check_point-Union{Tuple{T}, Tuple{SphereSymmetricMatrices, T}} where T","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::SphereSymmetricMatrices, p; kwargs...) Check whether the matrix is a valid point on the  SphereSymmetricMatrices M , i.e. is an  n -by- n  symmetric matrix of unit Frobenius norm. The tolerance for the symmetry of  p  can be set using  kwargs... . source"},{"id":1624,"pagetitle":"Unit-norm symmetric matrices","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/spheresymmetricmatrices/#ManifoldsBase.check_vector-Union{Tuple{T}, Tuple{SphereSymmetricMatrices, Any, T}} where T","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::SphereSymmetricMatrices, p, X; kwargs... ) Check whether  X  is a tangent vector to manifold point  p  on the  SphereSymmetricMatrices M , i.e.  X  has to be a symmetric matrix of size  (n,n)  of unit Frobenius norm. The tolerance for the symmetry of  p  and  X  can be set using  kwargs... . source"},{"id":1625,"pagetitle":"Unit-norm symmetric matrices","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/spheresymmetricmatrices/#ManifoldsBase.is_flat-Tuple{SphereSymmetricMatrices}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::SphereSymmetricMatrices) Return false.  SphereSymmetricMatrices  is not a flat manifold. source"},{"id":1626,"pagetitle":"Unit-norm symmetric matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/spheresymmetricmatrices/#ManifoldsBase.manifold_dimension-Union{Tuple{SphereSymmetricMatrices{<:Any, ùîΩ}}, Tuple{ùîΩ}} where ùîΩ","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::SphereSymmetricMatrices{<:Any,ùîΩ}) Return the manifold dimension of the  SphereSymmetricMatrices n -by- n  symmetric matrix  M  of unit Frobenius norm over the number system  ùîΩ , i.e. \\[\\begin{aligned}\n\\dim(\\mathcal{S}_{\\text{sym}})(n,‚Ñù) &= \\frac{n(n+1)}{2} - 1,\\\\\n\\dim(\\mathcal{S}_{\\text{sym}})(n,‚ÑÇ) &= 2\\frac{n(n+1)}{2} - n -1.\n\\end{aligned}\\] source"},{"id":1627,"pagetitle":"Unit-norm symmetric matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/spheresymmetricmatrices/#ManifoldsBase.project-Tuple{SphereSymmetricMatrices, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::SphereSymmetricMatrices, p, X) Project the matrix  X  onto the tangent space at  p  on the  SphereSymmetricMatrices M , i.e. \\[\\operatorname{proj}_p(X) = \\frac{X + X^{\\mathrm{H}}}{2} - ‚ü®p, \\frac{X + X^{\\mathrm{H}}}{2}‚ü©p,\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the Hermitian, i.e. complex conjugate transposed. source"},{"id":1628,"pagetitle":"Unit-norm symmetric matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/spheresymmetricmatrices/#ManifoldsBase.project-Tuple{SphereSymmetricMatrices, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::SphereSymmetricMatrices, p) Projects  p  from the embedding onto the  SphereSymmetricMatrices M , i.e. \\[\\operatorname{proj}_{\\mathcal{S}_{\\text{sym}}}(p) = \\frac{1}{2} \\bigl( p + p^{\\mathrm{H}} \\bigr),\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the Hermitian, i.e. complex conjugate transposed. source"},{"id":1631,"pagetitle":"Stiefel","title":"Stiefel","ref":"/manifolds/stable/manifolds/stiefel/#Stiefel","content":" Stiefel"},{"id":1632,"pagetitle":"Stiefel","title":"Common and metric independent functions","ref":"/manifolds/stable/manifolds/stiefel/#Common-and-metric-independent-functions","content":" Common and metric independent functions"},{"id":1633,"pagetitle":"Stiefel","title":"Manifolds.Stiefel","ref":"/manifolds/stable/manifolds/stiefel/#Manifolds.Stiefel","content":" Manifolds.Stiefel  ‚Äî  Type Stiefel{T,ùîΩ} <: AbstractDecoratorManifold{ùîΩ} The Stiefel manifold consists of all  $n√ók$ ,  $n ‚â• k$  unitary matrices, i.e. \\[\\operatorname{St}(n,k) = \\bigl\\{ p ‚àà ùîΩ^{n√ók}\\ \\big|\\ p^{\\mathrm{H}}p = I_k \\bigr\\},\\] where  $ùîΩ ‚àà \\{‚Ñù, ‚ÑÇ\\}$ ,  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transpose or Hermitian, and  $I_k ‚àà ‚Ñù^{k√ók}$  denotes the  $k√ók$  identity matrix. The tangent space at a point  $p ‚àà \\mathcal M$  is given by \\[T_p \\mathcal M = \\{ X ‚àà ùîΩ^{n√ók} : p^{\\mathrm{H}}X + X^{\\mathrm{H}}p = 0_k\\},\\] where  $0_k$  is the  $k√ók$  zero matrix. This manifold is modeled as an embedded manifold to the  Euclidean , i.e. several functions like the  inner  product and the  zero_vector  are inherited from the embedding. The manifold is named after  Eduard L. Stiefel  (1909‚Äì1978). Constructor Stiefel(n, k, field=‚Ñù; parameter::Symbol=:type) Generate the (real-valued) Stiefel manifold of  $n√ók$  dimensional orthonormal matrices. source"},{"id":1634,"pagetitle":"Stiefel","title":"Base.rand","ref":"/manifolds/stable/manifolds/stiefel/#Base.rand-Tuple{Stiefel}","content":" Base.rand  ‚Äî  Method rand(::Stiefel; vector_at=nothing, œÉ::Real=1.0) When  vector_at  is  nothing , return a random (Gaussian) point  x  on the  Stiefel  manifold  M  by generating a (Gaussian) matrix with standard deviation  œÉ  and return the orthogonalized version, i.e. return the Q component of the QR decomposition of the random matrix of size  $n√ók$ . When  vector_at  is not  nothing , return a (Gaussian) random vector from the tangent space  $T_{vector\\_at}\\mathrm{St}(n,k)$  with mean zero and standard deviation  œÉ  by projecting a random Matrix onto the tangent vector at  vector_at . source"},{"id":1635,"pagetitle":"Stiefel","title":"ManifoldsBase.change_metric","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.change_metric-Tuple{Stiefel, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_metric  ‚Äî  Method change_metric(M::Stiefel, ::EuclideanMetric, p X) Change  X  to the corresponding vector with respect to the metric of the  Stiefel M , which is just the identity, since the manifold is isometrically embedded. source"},{"id":1636,"pagetitle":"Stiefel","title":"ManifoldsBase.change_representer","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.change_representer-Tuple{Stiefel, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(M::Stiefel, ::EuclideanMetric, p, X) Change  X  to the corresponding representer of a cotangent vector at  p . Since the  Stiefel  manifold  M , is isometrically embedded, this is the identity source"},{"id":1637,"pagetitle":"Stiefel","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.check_point-Tuple{Stiefel, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::Stiefel, p; kwargs...) Check whether  p  is a valid point on the  Stiefel M = $\\operatorname{St}(n,k)$ , i.e. that it has the right  AbstractNumbers  type and  $p^{\\mathrm{H}}p$  is (approximately) the identity, where  $‚ãÖ^{\\mathrm{H}}$  is the complex conjugate transpose. The settings for approximately can be set with  kwargs... . source"},{"id":1638,"pagetitle":"Stiefel","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.check_vector-Tuple{Stiefel, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::Stiefel, p, X; kwargs...) Checks whether  X  is a valid tangent vector at  p  on the  Stiefel M = $\\operatorname{St}(n,k)$ , i.e. the  AbstractNumbers  fits and it (approximately) holds that  $p^{\\mathrm{H}}X + X^{\\mathrm{H}}p = 0$ . The settings for approximately can be set with  kwargs... . source"},{"id":1639,"pagetitle":"Stiefel","title":"ManifoldsBase.default_inverse_retraction_method","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.default_inverse_retraction_method-Tuple{Stiefel}","content":" ManifoldsBase.default_inverse_retraction_method  ‚Äî  Method default_inverse_retraction_method(M::Stiefel) Return  PolarInverseRetraction  as the default inverse retraction for the  Stiefel  manifold. source"},{"id":1640,"pagetitle":"Stiefel","title":"ManifoldsBase.default_retraction_method","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.default_retraction_method-Tuple{Stiefel}","content":" ManifoldsBase.default_retraction_method  ‚Äî  Method default_retraction_method(M::Stiefel) Return  PolarRetraction  as the default retraction for the  Stiefel  manifold. source"},{"id":1641,"pagetitle":"Stiefel","title":"ManifoldsBase.default_vector_transport_method","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.default_vector_transport_method-Tuple{Stiefel}","content":" ManifoldsBase.default_vector_transport_method  ‚Äî  Method default_vector_transport_method(M::Stiefel) Return the  DifferentiatedRetractionVectorTransport  of the [ PolarRetraction ]( PolarRetraction  as the default vector transport method for the  Stiefel  manifold. source"},{"id":1642,"pagetitle":"Stiefel","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.inverse_retract-Tuple{Stiefel, Any, Any, PolarInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::Stiefel, p, q, ::PolarInverseRetraction) Compute the inverse retraction based on a singular value decomposition for two points  p ,  q  on the  Stiefel  manifold  M . This follows the following approach: From the Polar retraction we know that \\[\\operatorname{retr}_p^{-1}q = qs - t\\] if such a symmetric positive definite  $k√ók$  matrix exists. Since  $qs - t$  is also a tangent vector at  $p$  we obtain \\[p^{\\mathrm{H}}qs + s(p^{\\mathrm{H}}q)^{\\mathrm{H}} + 2I_k = 0,\\] which can either be solved by a Lyapunov approach or a continuous-time algebraic Riccati equation. This implementation follows the Lyapunov approach. source"},{"id":1643,"pagetitle":"Stiefel","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.inverse_retract-Tuple{Stiefel, Any, Any, QRInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::Stiefel, p, q, ::QRInverseRetraction) Compute the inverse retraction based on a qr decomposition for two points  p ,  q  on the  Stiefel  manifold  M  and return the resulting tangent vector in  X . The computation follows Algorithm 1 in [ KFT13 ]. source"},{"id":1644,"pagetitle":"Stiefel","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.is_flat-Tuple{Stiefel}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(M::Stiefel) Return true if  Stiefel M  is one-dimensional. source"},{"id":1645,"pagetitle":"Stiefel","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.manifold_dimension-Tuple{Stiefel{<:Any, ‚Ñù}}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::Stiefel) Return the dimension of the  Stiefel  manifold  M = $\\operatorname{St}(n,k,ùîΩ)$ . The dimension is given by \\[\\begin{aligned}\n\\dim \\mathrm{St}(n, k, ‚Ñù) &= nk - \\frac{1}{2}k(k+1)\\\\\n\\dim \\mathrm{St}(n, k, ‚ÑÇ) &= 2nk - k^2\\\\\n\\dim \\mathrm{St}(n, k, ‚Ñç) &= 4nk - k(2k-1)\n\\end{aligned}\\] source"},{"id":1646,"pagetitle":"Stiefel","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.representation_size-Tuple{Stiefel}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::Stiefel) Returns the representation size of the  Stiefel M = $\\operatorname{St}(n,k)$ , i.e.  (n,k) , which is the matrix dimensions. source"},{"id":1647,"pagetitle":"Stiefel","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.retract-Tuple{Stiefel, Any, Any, CayleyRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(::Stiefel, p, X, ::CayleyRetraction) Compute the retraction on the  Stiefel  that is based on the Cayley transform[ Zhu16 ]. Using \\[  W_{p,X} = \\operatorname{P}_pXp^{\\mathrm{H}} - pX^{\\mathrm{H}}\\operatorname{P_p}\n  \\quad\\text{where}\n  \\operatorname{P}_p = I - \\frac{1}{2}pp^{\\mathrm{H}}\\] the formula reads \\[    \\operatorname{retr}_pX = \\Bigl(I - \\frac{1}{2}W_{p,X}\\Bigr)^{-1}\\Bigl(I + \\frac{1}{2}W_{p,X}\\Bigr)p.\\] It is implemented as the case  $m=1$  of the  PadeRetraction . source"},{"id":1648,"pagetitle":"Stiefel","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.retract-Tuple{Stiefel, Any, Any, PadeRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::Stiefel, p, X, ::PadeRetraction{m}) Compute the retraction on the  Stiefel  manifold  M  based on the Pad√© approximation of order  $m$  [ ZD18 ]. Let  $p_m$  and  $q_m$  be defined for any matrix  $A ‚àà ‚Ñù^{n√óx}$  as \\[  p_m(A) = \\sum_{k=0}^m \\frac{(2m-k)!m!}{(2m)!(m-k)!}\\frac{A^k}{k!}\\] and \\[  q_m(A) = \\sum_{k=0}^m \\frac{(2m-k)!m!}{(2m)!(m-k)!}\\frac{(-A)^k}{k!}\\] respectively. Then the Pad√© approximation (of the matrix exponential  $\\exp(A)$ ) reads \\[  r_m(A) = q_m(A)^{-1}p_m(A)\\] Defining further \\[  W_{p,X} = \\operatorname{P}_pXp^{\\mathrm{H}} - pX^{\\mathrm{H}}\\operatorname{P_p}\n  \\quad\\text{where }\n  \\operatorname{P}_p = I - \\frac{1}{2}pp^{\\mathrm{H}}\\] the retraction reads \\[  \\operatorname{retr}_pX = r_m(W_{p,X})p\\] source"},{"id":1649,"pagetitle":"Stiefel","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.retract-Tuple{Stiefel, Any, Any, PolarRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::Stiefel, p, X, ::PolarRetraction) Compute the SVD-based retraction  PolarRetraction  on the  Stiefel  manifold  M . With  $USV = p + X$  the retraction reads \\[\\operatorname{retr}_p X = U\\bar{V}^\\mathrm{H}.\\] source"},{"id":1650,"pagetitle":"Stiefel","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.retract-Tuple{Stiefel, Any, Any, QRRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::Stiefel, p, X, ::QRRetraction) Compute the QR-based retraction  QRRetraction  on the  Stiefel  manifold  M . With  $QR = p + X$  the retraction reads \\[\\operatorname{retr}_p X = QD,\\] where  $D$  is a  $n√ók$  matrix with \\[D = \\operatorname{diag}\\bigl(\\operatorname{sgn}(R_{ii}+0,5)_{i=1}^k \\bigr),\\] where ``\\operatorname{sgn}(p) = \\begin{cases} 1 & \\text{ for } p > 0,\\\n0 & \\text{ for } p = 0,\\\n-1& \\text{ for } p < 0. \\end{cases}`` source"},{"id":1651,"pagetitle":"Stiefel","title":"ManifoldsBase.vector_transport_direction","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.vector_transport_direction-Tuple{Stiefel, Any, Any, Any, DifferentiatedRetractionVectorTransport{CayleyRetraction}}","content":" ManifoldsBase.vector_transport_direction  ‚Äî  Method vector_transport_direction(::Stiefel, p, X, d, ::DifferentiatedRetractionVectorTransport{CayleyRetraction}) Compute the vector transport given by the differentiated retraction of the  CayleyRetraction , cf. [ Zhu16 ] Equation (17). The formula reads \\[\\operatorname{T}_{p,d}(X) =\n\\Bigl(I - \\frac{1}{2}W_{p,d}\\Bigr)^{-1}W_{p,X}\\Bigl(I - \\frac{1}{2}W_{p,d}\\Bigr)^{-1}p,\\] with \\[  W_{p,X} = \\operatorname{P}_pXp^{\\mathrm{H}} - pX^{\\mathrm{H}}\\operatorname{P_p}\n  \\quad\\text{where }\n  \\operatorname{P}_p = I - \\frac{1}{2}pp^{\\mathrm{H}}\\] Since this is the differentiated retraction as a vector transport, the result will be in the tangent space at  $q=\\operatorname{retr}_p(d)$  using the  CayleyRetraction . source"},{"id":1652,"pagetitle":"Stiefel","title":"ManifoldsBase.vector_transport_direction","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.vector_transport_direction-Tuple{Stiefel, Any, Any, Any, DifferentiatedRetractionVectorTransport{PolarRetraction}}","content":" ManifoldsBase.vector_transport_direction  ‚Äî  Method vector_transport_direction(M::Stiefel, p, X, d, DifferentiatedRetractionVectorTransport{PolarRetraction}) Compute the vector transport by computing the push forward of  retract(::Stiefel, ::Any, ::Any, ::PolarRetraction)  Section 3.5 of [ Zhu16 ]: \\[T_{p,d}^{\\text{Pol}}(X) = q*Œõ + (I-qq^{\\mathrm{T}})X(1+d^\\mathrm{T}d)^{-\\frac{1}{2}},\\] where  $q = \\operatorname{retr}^{\\mathrm{Pol}}_p(d)$ , and  $Œõ$  is the unique solution of the Sylvester equation \\[    Œõ(I+d^\\mathrm{T}d)^{\\frac{1}{2}} + (I + d^\\mathrm{T}d)^{\\frac{1}{2}} = q^\\mathrm{T}X - X^\\mathrm{T}q\\] source"},{"id":1653,"pagetitle":"Stiefel","title":"ManifoldsBase.vector_transport_direction","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.vector_transport_direction-Tuple{Stiefel, Any, Any, Any, DifferentiatedRetractionVectorTransport{QRRetraction}}","content":" ManifoldsBase.vector_transport_direction  ‚Äî  Method vector_transport_direction(M::Stiefel, p, X, d, DifferentiatedRetractionVectorTransport{QRRetraction}) Compute the vector transport by computing the push forward of the  retract(::Stiefel, ::Any, ::Any, ::QRRetraction) , See  [ AMS08 ], p. 173, or Section 3.5 of [ Zhu16 ]. \\[T_{p,d}^{\\text{QR}}(X) = q*\\rho_{\\mathrm{s}}(q^\\mathrm{T}XR^{-1}) + (I-qq^{\\mathrm{T}})XR^{-1},\\] where  $q = \\operatorname{retr}^{\\mathrm{QR}}_p(d)$ ,  $R$  is the  $R$  factor of the QR decomposition of  $p + d$ , and \\[\\bigl( \\rho_{\\mathrm{s}}(A) \\bigr)_{ij}\n= \\begin{cases}\nA_{ij}&\\text{ if } i > j\\\\\n0 \\text{ if } i = j\\\\\n-A_{ji} \\text{ if } i < j.\\\\\n\\end{cases}\\] source"},{"id":1654,"pagetitle":"Stiefel","title":"ManifoldsBase.vector_transport_to","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.vector_transport_to-Tuple{Stiefel, Any, Any, Any, DifferentiatedRetractionVectorTransport{PolarRetraction}}","content":" ManifoldsBase.vector_transport_to  ‚Äî  Method vector_transport_to(M::Stiefel, p, X, q, DifferentiatedRetractionVectorTransport{PolarRetraction}) Compute the vector transport by computing the push forward of the  retract(M::Stiefel, ::Any, ::Any, ::PolarRetraction) , see Section 4 of [ HGA15 ] or  Section 3.5 of [ Zhu16 ]: \\[T_{q\\gets p}^{\\text{Pol}}(X) = q*Œõ + (I-qq^{\\mathrm{T}})X(1+d^\\mathrm{T}d)^{-\\frac{1}{2}},\\] where  $d = \\bigl( \\operatorname{retr}^{\\mathrm{Pol}}_p\\bigr)^{-1}(q)$ , and  $Œõ$  is the unique solution of the Sylvester equation \\[    Œõ(I+d^\\mathrm{T}d)^{\\frac{1}{2}} + (I + d^\\mathrm{T}d)^{\\frac{1}{2}} = q^\\mathrm{T}X - X^\\mathrm{T}q\\] source"},{"id":1655,"pagetitle":"Stiefel","title":"ManifoldsBase.vector_transport_to","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.vector_transport_to-Tuple{Stiefel, Any, Any, Any, DifferentiatedRetractionVectorTransport{QRRetraction}}","content":" ManifoldsBase.vector_transport_to  ‚Äî  Method vector_transport_to(M::Stiefel, p, X, q, DifferentiatedRetractionVectorTransport{QRRetraction}) Compute the vector transport by computing the push forward of the  retract(M::Stiefel, ::Any, ::Any, ::QRRetraction) , see  [ AMS08 ], p. 173, or Section 3.5 of [ Zhu16 ]. \\[T_{q \\gets p}^{\\text{QR}}(X) = q*\\rho_{\\mathrm{s}}(q^\\mathrm{T}XR^{-1}) + (I-qq^{\\mathrm{T}})XR^{-1},\\] where  $d = \\bigl(\\operatorname{retr}^{\\mathrm{QR}}\\bigr)^{-1}_p(q)$ ,  $R$  is the  $R$  factor of the QR decomposition of  $p+X$ , and \\[\\bigl( \\rho_{\\mathrm{s}}(A) \\bigr)_{ij}\n= \\begin{cases}\nA_{ij}&\\text{ if } i > j\\\\\n0 \\text{ if } i = j\\\\\n-A_{ji} \\text{ if } i < j.\\\\\n\\end{cases}\\] source"},{"id":1656,"pagetitle":"Stiefel","title":"ManifoldsBase.vector_transport_to","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.vector_transport_to-Tuple{Stiefel, Any, Any, Any, ProjectionTransport}","content":" ManifoldsBase.vector_transport_to  ‚Äî  Method vector_transport_to(M::Stiefel, p, X, q, ::ProjectionTransport) Compute a vector transport by projection, i.e. project  X  from the tangent space at  p  by projection it onto the tangent space at  q . source"},{"id":1657,"pagetitle":"Stiefel","title":"Default metric: the Euclidean metric","ref":"/manifolds/stable/manifolds/stiefel/#Default-metric:-the-Euclidean-metric","content":" Default metric: the Euclidean metric The  EuclideanMetric  is obtained from the embedding of the Stiefel manifold in  $‚Ñù^{n,k}$ ."},{"id":1658,"pagetitle":"Stiefel","title":"Base.exp","ref":"/manifolds/stable/manifolds/stiefel/#Base.exp-Tuple{Stiefel, Vararg{Any}}","content":" Base.exp  ‚Äî  Method exp(M::Stiefel, p, X) Compute the exponential map on the  Stiefel {n,k,ùîΩ} () manifold  M  emanating from  p  in tangent direction  X . \\[\\exp_p X = \\begin{pmatrix}\n   p\\\\X\n \\end{pmatrix}\n \\operatorname{Exp}\n \\left(\n \\begin{pmatrix} p^{\\mathrm{H}}X & - X^{\\mathrm{H}}X\\\\\n I_n & p^{\\mathrm{H}}X\\end{pmatrix}\n \\right)\n\\begin{pmatrix}  \\exp( -p^{\\mathrm{H}}X) \\\\ 0_n\\end{pmatrix},\\] where  $\\operatorname{Exp}$  denotes matrix exponential,  $‚ãÖ^{\\mathrm{H}}$  denotes the complex conjugate transpose or Hermitian, and  $I_k$  and  $0_k$  are the identity matrix and the zero matrix of dimension  $k√ók$ , respectively. source"},{"id":1659,"pagetitle":"Stiefel","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldDiff.riemannian_Hessian-Tuple{Stiefel, Vararg{Any, 4}}","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method Y = riemannian_Hessian(M::Stiefel, p, G, H, X)\nriemannian_Hessian!(M::Stiefel, Y, p, G, H, X) Compute the Riemannian Hessian  $\\operatorname{Hess} f(p)[X]$  given the Euclidean gradient  $‚àá f(\\tilde p)$  in  G  and the Euclidean Hessian  $‚àá^2 f(\\tilde p)[\\tilde X]$  in  H , where  $\\tilde p, \\tilde X$  are the representations of  $p,X$  in the embedding,. Here, we adopt Eq. (5.6) [ Ngu23 ], where we use for the  EuclideanMetric $Œ±_0=Œ±_1=1$  in their formula. Then the formula reads \\[    \\operatorname{Hess}f(p)[X]\n    =\n    \\operatorname{proj}_{T_p\\mathcal M}\\Bigl(\n        ‚àá^2f(p)[X] - \\frac{1}{2} X \\bigl((‚àáf(p))^{\\mathrm{H}}p + p^{\\mathrm{H}}‚àáf(p)\\bigr)\n    \\Bigr).\\] Compared to Eq. (5.6) also the metric conversion simplifies to the identity. source"},{"id":1660,"pagetitle":"Stiefel","title":"ManifoldsBase.Weingarten","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.Weingarten-Tuple{Stiefel, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Weingarten(M::Stiefel, p, X, V) Compute the Weingarten map  $\\mathcal W_p$  at  p  on the  Stiefel M  with respect to the tangent vector  $X \\in T_p\\mathcal M$  and the normal vector  $V \\in N_p\\mathcal M$ . The formula is due to [ AMT13 ] given by \\[\\mathcal W_p(X,V) = -Xp^{\\mathrm{H}}V - \\frac{1}{2}p\\bigl(X^\\mathrm{H}V + V^{\\mathrm{H}}X\\bigr)\\] source"},{"id":1661,"pagetitle":"Stiefel","title":"ManifoldsBase.get_basis","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.get_basis-Tuple{Stiefel{<:Any, ‚Ñù}, Any, DefaultOrthonormalBasis{‚Ñù, TangentSpaceType}}","content":" ManifoldsBase.get_basis  ‚Äî  Method get_basis(M::Stiefel{<:Any,‚Ñù}, p, B::DefaultOrthonormalBasis) Create the default basis using the parametrization for any  $X ‚àà T_p\\mathcal M$ . Set  $p_\\bot \\in ‚Ñù^{n√ó(n-k)}$  the matrix such that the  $n√ón$  matrix of the common columns  $[p\\ p_\\bot]$  is an ONB. For any skew symmetric matrix  $a ‚àà ‚Ñù^{k√ók}$  and any  $b ‚àà ‚Ñù^{(n-k)√ók}$  the matrix \\[X = pa + p_\\bot b ‚àà T_p\\mathcal M\\] and we can use the  $\\frac{1}{2}k(k-1) + (n-k)k = nk-\\frac{1}{2}k(k+1)$  entries of  $a$  and  $b$  to specify a basis for the tangent space. using unit vectors for constructing both the upper matrix of  $a$  to build a skew symmetric matrix and the matrix b, the default basis is constructed. Since  $[p\\ p_‚ä•]$  is an automorphism on  $‚Ñù^{n√óp}$  the elements of  $a$  and  $b$  are orthonormal coordinates for the tangent space. To be precise exactly one element in the upper triangular entries of  $a$  is set to  $1$  its symmetric entry to  $-1$  and we normalize with the factor  $\\frac{1}{\\sqrt{2}}$  and for  $b$  one can just use unit vectors reshaped to a matrix to obtain orthonormal set of parameters. source"},{"id":1662,"pagetitle":"Stiefel","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.injectivity_radius-Tuple{Stiefel}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::Stiefel) Return the injectivity radius for the  Stiefel  manifold  M , which is globally  $œÄ$  [ ZS24 ]. source"},{"id":1663,"pagetitle":"Stiefel","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.inverse_retract-Tuple{Stiefel, Any, Any, ProjectionInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::Stiefel, p, q, method::ProjectionInverseRetraction) Compute a projection-based inverse retraction. The inverse retraction is computed by projecting the logarithm map in the embedding to the tangent space at  $p$ . source"},{"id":1664,"pagetitle":"Stiefel","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.project-Tuple{Stiefel, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Stiefel,p) Projects  p  from the embedding onto the  Stiefel M , i.e. compute  q  as the polar decomposition of  $p$  such that  $q^{\\mathrm{H}}q$  is the identity, where  $‚ãÖ^{\\mathrm{H}}$  denotes the hermitian, i.e. complex conjugate transposed. source"},{"id":1665,"pagetitle":"Stiefel","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.project-Tuple{Stiefel, Vararg{Any}}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Stiefel, p, X) Project  X  onto the tangent space of  p  to the  Stiefel  manifold  M . The formula reads \\[\\operatorname{proj}_{T_p\\mathcal M}(X) = X - p \\operatorname{Sym}(p^{\\mathrm{H}}X),\\] where  $\\operatorname{Sym}(q)$  is the symmetrization of  $q$ , e.g. by  $\\operatorname{Sym}(q) = \\frac{q^{\\mathrm{H}}+q}{2}$ . source"},{"id":1666,"pagetitle":"Stiefel","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.retract-Tuple{Stiefel, Any, Any, ProjectionRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::Stiefel, p, X, method::ProjectionRetraction) Compute a projection-based retraction. The retraction is computed by projecting the exponential map in the embedding to  M . source"},{"id":1667,"pagetitle":"Stiefel","title":"The canonical metric","ref":"/manifolds/stable/manifolds/stiefel/#The-canonical-metric","content":" The canonical metric Any  $X‚ààT_p\\mathcal M$ ,  $p‚àà\\mathcal M$ , can be written as \\[X = pA + (I_n-pp^{\\mathrm{T}})B,\n\\quad\nA ‚àà ‚Ñù^{p√óp} \\text{ skew-symmetric},\n\\quad\nB ‚àà ‚Ñù^{n√óp} \\text{ arbitrary.}\\] In the  EuclideanMetric , the elements from  $A$  are counted twice (i.e. weighted with a factor of 2). The canonical metric avoids this."},{"id":1668,"pagetitle":"Stiefel","title":"Manifolds.ApproximateLogarithmicMap","ref":"/manifolds/stable/manifolds/stiefel/#Manifolds.ApproximateLogarithmicMap","content":" Manifolds.ApproximateLogarithmicMap  ‚Äî  Type ApproximateLogarithmicMap <: ApproximateInverseRetraction An approximate implementation of the logarithmic map, which is an  inverse_retract ion. See  inverse_retract(::MetricManifold{‚Ñù,<:Stiefel{<:Any,‚Ñù},CanonicalMetric}, ::Any, ::Any, ::ApproximateLogarithmicMap)  for a use case. Fields max_iterations  ‚Äì maximal number of iterations used in the approximation tolerance  ‚Äì¬†a tolerance used as a stopping criterion source"},{"id":1669,"pagetitle":"Stiefel","title":"Manifolds.CanonicalMetric","ref":"/manifolds/stable/manifolds/stiefel/#Manifolds.CanonicalMetric","content":" Manifolds.CanonicalMetric  ‚Äî  Type CanonicalMetric <: AbstractMetric The Canonical Metric refers to a metric for the  Stiefel  manifold, see[ EAS98 ]. source"},{"id":1670,"pagetitle":"Stiefel","title":"Base.exp","ref":"/manifolds/stable/manifolds/stiefel/#Base.exp-Tuple{MetricManifold{‚Ñù, <:Stiefel{<:Any, ‚Ñù}, CanonicalMetric}, Vararg{Any}}","content":" Base.exp  ‚Äî  Method q = exp(M::MetricManifold{‚Ñù,<:Stiefel{<:Any,‚Ñù},CanonicalMetric}, p, X)\nexp!(M::MetricManifold{‚Ñù,<:Stiefel{<:Any,‚Ñù},CanonicalMetric}, q, p, X) Compute the exponential map on the  Stiefel (n, k)  manifold with respect to the  CanonicalMetric . First, decompose The tangent vector  $X$  into its horizontal and vertical component with respect to  $p$ , i.e. \\[X = pp^{\\mathrm{T}}X + (I_n-pp^{\\mathrm{T}})X,\\] where  $I_n$  is the  $n√ón$  identity matrix. We introduce  $A=p^{\\mathrm{T}}X$  and  $QR = (I_n-pp^{\\mathrm{T}})X$  the  qr  decomposition of the vertical component. Then using the matrix exponential  $\\operatorname{Exp}$  we introduce  $B$  and  $C$  as \\[\\begin{pmatrix}\nB\\\\C\n\\end{pmatrix}\n\\coloneqq\n\\operatorname{Exp}\\left(\n\\begin{pmatrix}\nA & -R^{\\mathrm{T}}\\\\ R & 0\n\\end{pmatrix}\n\\right)\n\\begin{pmatrix}I_k\\\\0\\end{pmatrix}\\] the exponential map reads \\[q = \\exp_p X = pC + QB.\\] For more details, see [ EAS98 ][ Zim17 ]. source"},{"id":1671,"pagetitle":"Stiefel","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldDiff.riemannian_Hessian-Union{Tuple{ùîΩ}, Tuple{MetricManifold{ùîΩ, Stiefel, CanonicalMetric}, Vararg{Any, 4}}} where ùîΩ","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method Y = riemannian_Hessian(M::MetricManifold{‚Ñù, Stiefel, CanonicalMetric}, p, G, H, X)\nriemannian_Hessian!(M::MetricManifold{‚Ñù, Stiefel, CanonicalMetric}, Y, p, G, H, X) Compute the Riemannian Hessian  $\\operatorname{Hess} f(p)[X]$  given the Euclidean gradient  $‚àá f(\\tilde p)$  in  G  and the Euclidean Hessian  $‚àá^2 f(\\tilde p)[\\tilde X]$  in  H , where  $\\tilde p, \\tilde X$  are the representations of  $p,X$  in the embedding,. Here, we adopt Eq. (5.6) [ Ngu23 ], for the  CanonicalMetric $Œ±_0=1, Œ±_1=\\frac{1}{2}$  in their formula. The formula reads \\[    \\operatorname{Hess}f(p)[X]\n    =\n    \\operatorname{proj}_{T_p\\mathcal M}\\Bigl(\n        ‚àá^2f(p)[X] - \\frac{1}{2} X \\bigl( (‚àáf(p))^{\\mathrm{H}}p + p^{\\mathrm{H}}‚àáf(p)\\bigr)\n        - \\frac{1}{2} \\bigl( P ‚àáf(p) p^{\\mathrm{H}} + p ‚àáf(p))^{\\mathrm{H}} P)X\n    \\Bigr),\\] where  $P = I-pp^{\\mathrm{H}}$ . source"},{"id":1672,"pagetitle":"Stiefel","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.inner-Tuple{MetricManifold{‚Ñù, <:Stiefel{<:Any, ‚Ñù}, CanonicalMetric}, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::MetricManifold{‚Ñù, Stiefel{<:Any,‚Ñù}, X, CanonicalMetric}, p, X, Y) Compute the inner product on the  Stiefel  manifold with respect to the  CanonicalMetric . The formula reads \\[g_p(X,Y) = \\operatorname{tr}\\bigl( X^{\\mathrm{T}}(I_n - \\frac{1}{2}pp^{\\mathrm{T}})Y \\bigr).\\] source"},{"id":1673,"pagetitle":"Stiefel","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.inverse_retract-Tuple{MetricManifold{‚Ñù, <:Stiefel{<:Any, ‚Ñù}, CanonicalMetric}, Any, Any, ApproximateLogarithmicMap}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method X = inverse_retract(M::MetricManifold{‚Ñù, Stiefel{<:Any,‚Ñù}, CanonicalMetric}, p, q, a::ApproximateLogarithmicMap)\ninverse_retract!(M::MetricManifold{‚Ñù, Stiefel{<:Any,‚Ñù}, X, CanonicalMetric}, p, q, a::ApproximateLogarithmicMap) Compute an approximation to the logarithmic map on the  Stiefel (n, k)  manifold with respect to the  CanonicalMetric  using a matrix-algebraic based approach to an iterative inversion of the formula of the  exp . The algorithm is derived in [ Zim17 ] and it uses the  max_iterations  and the  tolerance  field from the  ApproximateLogarithmicMap . source"},{"id":1674,"pagetitle":"Stiefel","title":"The submersion or normal metric","ref":"/manifolds/stable/manifolds/stiefel/#The-submersion-or-normal-metric","content":" The submersion or normal metric"},{"id":1675,"pagetitle":"Stiefel","title":"Manifolds.StiefelFactorization","ref":"/manifolds/stable/manifolds/stiefel/#Manifolds.StiefelFactorization","content":" Manifolds.StiefelFactorization  ‚Äî  Type StiefelFactorization{UT,XT} <: AbstractManifoldPoint Represent points (and vectors) on  Stiefel(n, k)  with  $2k√ók$  factors [ ZH22 ]. Given a point  $p ‚àà \\mathrm{St}(n, k)$  and another matrix  $B ‚àà ‚Ñù^{n√ók}$  for  $k ‚â§ \\lfloor\\frac{n}{2}\\rfloor$  the factorization is \\[\\begin{aligned}\nB &= UZ\\\\\nU &= \\begin{bmatrix}p & Q\\end{bmatrix} ‚àà \\mathrm{St}(n, 2k)\\\\\nZ &= \\begin{bmatrix}Z_1 \\\\ Z_2\\end{bmatrix}, \\quad Z_1,Z_2 ‚àà ‚Ñù^{k√ók}.\n\\end{aligned}\\] If  $B ‚àà \\mathrm{St}(n, k)$ , then  $Z ‚àà \\mathrm{St}(2k, k)$ . Note that not every matrix  $B$  can be factorized in this way. For a fixed  $U$ , if  $r ‚àà \\mathrm{St}(n, k)$  has the factor  $Z_r ‚àà \\mathrm{St}(2k, k)$ , then  $X_r ‚àà T_r \\mathrm{St}(n, k)$  has the factor  $Z_{X_r} ‚àà T_{Z_r} \\mathrm{St}(2k, k)$ . $Q$  is determined by choice of a second matrix  $A ‚àà ‚Ñù^{n√ók}$  with the decomposition \\[\\begin{aligned}\nA &= UZ\\\\\nZ_1 &= p^\\mathrm{T} A \\\\\nQ Z_2 &= (I - p p^\\mathrm{T}) A,\n\\end{aligned}\\] where here  $Q Z_2$  is the any decomposition that produces  $Q ‚àà \\mathrm{St}(n, k)$ , for which we choose the QR decomposition. This factorization is useful because it is closed under addition, subtraction, scaling, projection, and the Riemannian exponential and logarithm under the  StiefelSubmersionMetric . That is, if all matrices involved are factorized to have the same  $U$ , then all of these operations and any algorithm that depends only on them can be performed in terms of the  $2k√ók$  matrices  $Z$ . For  $n ‚â´ k$ , this can be much more efficient than working with the full matrices. Warning This type is intended strictly for internal use and should not be directly used. source"},{"id":1676,"pagetitle":"Stiefel","title":"Manifolds.StiefelSubmersionMetric","ref":"/manifolds/stable/manifolds/stiefel/#Manifolds.StiefelSubmersionMetric","content":" Manifolds.StiefelSubmersionMetric  ‚Äî  Type StiefelSubmersionMetric{T<:Real} <: RiemannianMetric The submersion (or normal) metric family on the  Stiefel  manifold. The family, with a single real parameter  $Œ±>-1$ , has two special cases: $Œ± = -\\frac{1}{2}$ :  EuclideanMetric $Œ± = 0$ :  CanonicalMetric The family was described in [ HML21 ]. This implementation follows the description in [ ZH22 ]. Constructor StiefelSubmersionMetric(Œ±) Construct the submersion metric on the Stiefel manifold with the parameter  $Œ±$ . source"},{"id":1677,"pagetitle":"Stiefel","title":"Base.exp","ref":"/manifolds/stable/manifolds/stiefel/#Base.exp-Tuple{MetricManifold{‚Ñù, <:Stiefel{<:Any, ‚Ñù}, <:StiefelSubmersionMetric}, Vararg{Any}}","content":" Base.exp  ‚Äî  Method q = exp(M::MetricManifold{‚Ñù,<:Stiefel{<:Any,‚Ñù},<:StiefelSubmersionMetric}, p, X)\nexp!(M::MetricManifold{‚Ñù,<:Stiefel{<:Any,‚Ñù},<:StiefelSubmersionMetric}, q, p, X) Compute the exponential map on the  Stiefel(n,k)  manifold with respect to the  StiefelSubmersionMetric . The exponential map is given by \\[\\exp_p X = \\operatorname{Exp}\\bigl(\n    -\\frac{2Œ±+1}{Œ±+1} p p^\\mathrm{T} X p^\\mathrm{T} +\n    X p^\\mathrm{T} - p X^\\mathrm{T}\n\\bigr) p \\operatorname{Exp}\\bigl(\\frac{\\alpha}{\\alpha+1} p^\\mathrm{T} X\\bigr)\\] This implementation is based on [ ZH22 ]. For  $k < \\frac{n}{2}$  the exponential is computed more efficiently using  StiefelFactorization . source"},{"id":1678,"pagetitle":"Stiefel","title":"Base.log","ref":"/manifolds/stable/manifolds/stiefel/#Base.log-Tuple{MetricManifold{‚Ñù, <:Stiefel{<:Any, ‚Ñù}, <:StiefelSubmersionMetric}, Any, Any}","content":" Base.log  ‚Äî  Method log(M::MetricManifold{‚Ñù,<:Stiefel{<:Any,‚Ñù},<:StiefelSubmersionMetric}, p, q; kwargs...) Compute the logarithmic map on the  Stiefel(n,k)  manifold with respect to the  StiefelSubmersionMetric . The logarithmic map is computed using  ShootingInverseRetraction . For  $k ‚â§ \\lfloor\\frac{n}{2}\\rfloor$ , this is sped up using the  $k$ -shooting method of [ ZH22 ]. Keyword arguments are forwarded to  ShootingInverseRetraction ; see that documentation for details. Their defaults are: num_transport_points=4 tolerance=sqrt(eps()) max_iterations=1_000 source"},{"id":1679,"pagetitle":"Stiefel","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldDiff.riemannian_Hessian-Tuple{MetricManifold{‚Ñù, <:Stiefel{<:Any, ‚Ñù}, <:StiefelSubmersionMetric}, Vararg{Any, 4}}","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method Y = riemannian_Hessian(M::MetricManifold{‚Ñù,<:Stiefel{<:Any,‚Ñù},StiefelSubmersionMetric}, p, G, H, X)\nriemannian_Hessian!(MetricManifold{‚Ñù,<:Stiefel{<:Any,‚Ñù},StiefelSubmersionMetric}, Y, p, G, H, X) Compute the Riemannian Hessian  $\\operatorname{Hess} f(p)[X]$  given the Euclidean gradient  $‚àá f(\\tilde p)$  in  G  and the Euclidean Hessian  $‚àá^2 f(\\tilde p)[\\tilde X]$  in  H , where  $\\tilde p, \\tilde X$  are the representations of  $p,X$  in the embedding,. Here, we adopt Eq. (5.6) [ Ngu23 ], for the  CanonicalMetric $Œ±_0=1, Œ±_1=\\frac{1}{2}$  in their formula. The formula reads \\[    \\operatorname{Hess}f(p)[X]\n    =\n    \\operatorname{proj}_{T_p\\mathcal M}\\Bigl(\n        ‚àá^2f(p)[X] - \\frac{1}{2} X \\bigl( (‚àáf(p))^{\\mathrm{H}}p + p^{\\mathrm{H}}‚àáf(p)\\bigr)\n        - \\frac{2Œ±+1}{2(Œ±+1)} \\bigl( P ‚àáf(p) p^{\\mathrm{H}} + p ‚àáf(p))^{\\mathrm{H}} P)X\n    \\Bigr),\\] where  $P = I-pp^{\\mathrm{H}}$ . Compared to Eq. (5.6) we have that their  $Œ±_0 = 1$ and  $\\alpha_1 =  \\frac{2Œ±+1}{2(Œ±+1)} + 1$ . source"},{"id":1680,"pagetitle":"Stiefel","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.inner-Tuple{MetricManifold{‚Ñù, <:Stiefel{<:Any, ‚Ñù}, <:StiefelSubmersionMetric}, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::MetricManifold{‚Ñù,<:Stiefel{<:Any,‚Ñù},<:StiefelSubmersionMetric}, p, X, Y) Compute the inner product on the  Stiefel  manifold with respect to the  StiefelSubmersionMetric . The formula reads \\[g_p(X,Y) = \\operatorname{tr}\\bigl( X^{\\mathrm{T}}(I_n - \\frac{2Œ±+1}{2(Œ±+1)}pp^{\\mathrm{T}})Y \\bigr),\\] where  $Œ±$  is the parameter of the metric. source"},{"id":1681,"pagetitle":"Stiefel","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/stiefel/#ManifoldsBase.inverse_retract-Tuple{MetricManifold{‚Ñù, <:Stiefel, <:StiefelSubmersionMetric}, Any, Any, ShootingInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(\n    M::MetricManifold{‚Ñù,<:Stiefel{<:Any,‚Ñù},<:StiefelSubmersionMetric},\n    p,\n    q,\n    method::ShootingInverseRetraction,\n) Compute the inverse retraction using  ShootingInverseRetraction . In general the retraction is computed using the generic shooting method. inverse_retract(\n    M::MetricManifold{‚Ñù,<:Stiefel{<:Any,‚Ñù},<:StiefelSubmersionMetric},\n    p,\n    q,\n    method::ShootingInverseRetraction{\n        ExponentialRetraction,\n        ProjectionInverseRetraction,\n        <:Union{ProjectionTransport,ScaledVectorTransport{ProjectionTransport}},\n    },\n) Compute the inverse retraction using  ShootingInverseRetraction  more efficiently. For  $k < \\frac{n}{2}$  the retraction is computed more efficiently using  StiefelFactorization . source"},{"id":1682,"pagetitle":"Stiefel","title":"Internal types and functions","ref":"/manifolds/stable/manifolds/stiefel/#Internal-types-and-functions","content":" Internal types and functions"},{"id":1683,"pagetitle":"Stiefel","title":"Manifolds.stiefel_factorization","ref":"/manifolds/stable/manifolds/stiefel/#Manifolds.stiefel_factorization-Tuple{Any, Any}","content":" Manifolds.stiefel_factorization  ‚Äî  Method stiefel_factorization(p, x) -> StiefelFactorization Compute the  StiefelFactorization  of  $x$  relative to the point  $p$ . source"},{"id":1684,"pagetitle":"Stiefel","title":"Literature","ref":"/manifolds/stable/manifolds/stiefel/#Literature","content":" Literature [AMT13] P.¬†-.-A.¬†Absil, R.¬†Mahony and J.¬†Trumpf.  An Extrinsic Look at the Riemannian Hessian . In:  Geometric Science of Information , edited by F.¬†Nielsen and F.¬†Barbaresco (Springer Berlin Heidelberg, 2013); pp.¬†361‚Äì368. [AMS08] P.-A.¬†Absil, R.¬†Mahony and R.¬†Sepulchre.  Optimization Algorithms on Matrix Manifolds  (Princeton University Press, 2008), available online at  press.princeton.edu/chapters/absil/ . [EAS98] A.¬†Edelman, T.¬†A.¬†Arias and S.¬†T.¬†Smith.  The Geometry of Algorithms with Orthogonality Constraints .  SIAM¬†Journal¬†on¬†Matrix¬†Analysis¬†and¬†Applications  20 , 303‚Äì353  (1998),  arXiv:806030 . [HGA15] W.¬†Huang, K.¬†A.¬†Gallivan and P.-A.¬†Absil.  A Broyden Class of Quasi-Newton Methods for Riemannian Optimization .  SIAM¬†Journal¬†on¬†Optimization  25 , 1660‚Äì1685  (2015). [HML21] K.¬†H√ºper, I.¬†Markina and F.¬†S.¬†Leite.  A Lagrangian approach to extremal curves on Stiefel manifolds .  Journal¬†of¬†Geometric¬†Mechanics  13 , 55  (2021). [KFT13] T.¬†Kaneko, S.¬†Fiori and T.¬†Tanaka.  Empirical Arithmetic Averaging Over the Compact Stiefel Manifold .  IEEE¬†Transactions¬†on¬†Signal¬†Processing  61 , 883‚Äì894  (2013). [Ngu23] D.¬†Nguyen.  Operator-Valued Formulas for Riemannian Gradient and Hessian and Families of Tractable Metrics in Riemannian Optimization .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  198 , 135‚Äì164  (2023),  arXiv:2009.10159 . [Zhu16] X.¬†Zhu.  A Riemannian conjugate gradient method for optimization on the Stiefel manifold .  Computational¬†Optimization¬†and¬†Applications  67 , 73‚Äì110  (2016). [ZD18] X.¬†Zhu and C.¬†Duan.  On matrix exponentials and their approximations related to optimization on the Stiefel manifold .  Optimization¬†Letters  13 , 1069‚Äì1083  (2018). [Zim17] R.¬†Zimmermann.  A Matrix-Algebraic Algorithm for the Riemannian Logarithm on the Stiefel Manifold under the Canonical Metric .  SIAM¬†J.¬†Matrix¬†Anal.¬†Appl.  38 , 322‚Äì342  (2017),  arXiv:1604.05054 . [ZH22] R.¬†Zimmermann and K.¬†H√ºper.  Computing the Riemannian Logarithm on the Stiefel Manifold: Metrics, Methods, and Performance .  SIAM¬†Journal¬†on¬†Matrix¬†Analysis¬†and¬†Applications  43 , 953‚Äì980  (2022),  arXiv:2103.12046 . [ZS24] R.¬†Zimmermann and J.¬†Stoye.  The injectivity radius of the compact Stiefel manifold under the Euclidean metric , arXiv¬†Preprint (2024),  arXiv:2405.02268 ."},{"id":1687,"pagetitle":"Symmetric matrices","title":"Symmetric matrices","ref":"/manifolds/stable/manifolds/symmetric/#Symmetric-matrices","content":" Symmetric matrices"},{"id":1688,"pagetitle":"Symmetric matrices","title":"Manifolds.SymmetricMatrices","ref":"/manifolds/stable/manifolds/symmetric/#Manifolds.SymmetricMatrices","content":" Manifolds.SymmetricMatrices  ‚Äî  Type SymmetricMatrices{n,ùîΩ} <: AbstractDecoratorManifold{ùîΩ} The  AbstractManifold $\\operatorname{Sym}(n)$  consisting of the real- or complex-valued symmetric matrices of size  $n√ón$ , i.e. the set \\[\\operatorname{Sym}(n) = \\bigl\\{p  ‚àà ùîΩ^{n√ón}\\ \\big|\\ p^{\\mathrm{H}} = p \\bigr\\},\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the Hermitian, i.e. complex conjugate transpose, and the field  $ùîΩ ‚àà \\{ ‚Ñù, ‚ÑÇ\\}$ . Though it is slightly redundant, usually the matrices are stored as  $n√ón$  arrays. Note that in this representation, the complex valued case has to have a real-valued diagonal, which is also reflected in the  manifold_dimension . Constructor SymmetricMatrices(n::Int, field::AbstractNumbers=‚Ñù) Generate the manifold of  $n√ón$  symmetric matrices. source"},{"id":1689,"pagetitle":"Symmetric matrices","title":"ManifoldsBase.Weingarten","ref":"/manifolds/stable/manifolds/symmetric/#ManifoldsBase.Weingarten-Tuple{SymmetricMatrices, Any, Any, Any}","content":" ManifoldsBase.Weingarten  ‚Äî  Method Y = Weingarten(M::SymmetricMatrices, p, X, V)\nWeingarten!(M::SymmetricMatrices, Y, p, X, V) Compute the Weingarten map  $\\mathcal W_p$  at  p  on the  SymmetricMatrices M  with respect to the tangent vector  $X \\in T_p\\mathcal M$  and the normal vector  $V \\in N_p\\mathcal M$ . Since this a flat space by itself, the result is always the zero tangent vector. source"},{"id":1690,"pagetitle":"Symmetric matrices","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/symmetric/#ManifoldsBase.check_point-Tuple{SymmetricMatrices, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::SymmetricMatrices{n,ùîΩ}, p; kwargs...) Check whether  p  is a valid manifold point on the  SymmetricMatrices M , i.e. whether  p  is a symmetric matrix of size  (n,n)  with values from the corresponding  AbstractNumbers ùîΩ . The tolerance for the symmetry of  p  can be set using  kwargs... . source"},{"id":1691,"pagetitle":"Symmetric matrices","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/symmetric/#ManifoldsBase.check_vector-Tuple{SymmetricMatrices, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::SymmetricMatrices{n,ùîΩ}, p, X; kwargs... ) Check whether  X  is a tangent vector to manifold point  p  on the  SymmetricMatrices M , i.e.  X  has to be a symmetric matrix of size  (n,n)  and its values have to be from the correct  AbstractNumbers . The tolerance for the symmetry of  X  can be set using  kwargs... . source"},{"id":1692,"pagetitle":"Symmetric matrices","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/symmetric/#ManifoldsBase.is_flat-Tuple{SymmetricMatrices}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::SymmetricMatrices) Return true.  SymmetricMatrices  is a flat manifold. source"},{"id":1693,"pagetitle":"Symmetric matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/symmetric/#ManifoldsBase.manifold_dimension-Union{Tuple{SymmetricMatrices{<:Any, ùîΩ}}, Tuple{ùîΩ}} where ùîΩ","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::SymmetricMatrices{n,ùîΩ}) Return the dimension of the  SymmetricMatrices  matrix  M  over the number system  ùîΩ , i.e. \\[\\begin{aligned}\n\\dim \\mathrm{Sym}(n,‚Ñù) &= \\frac{n(n+1)}{2},\\\\\n\\dim \\mathrm{Sym}(n,‚ÑÇ) &= 2\\frac{n(n+1)}{2} - n = n^2,\n\\end{aligned}\\] where the last  $-n$  is due to the zero imaginary part for Hermitian matrices source"},{"id":1694,"pagetitle":"Symmetric matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/symmetric/#ManifoldsBase.project-Tuple{SymmetricMatrices, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::SymmetricMatrices, p, X) Project the matrix  X  onto the tangent space at  p  on the  SymmetricMatrices M , \\[\\operatorname{proj}_p(X) = \\frac{1}{2} \\bigl( X + X^{\\mathrm{H}} \\bigr),\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the Hermitian, i.e. complex conjugate transposed. source"},{"id":1695,"pagetitle":"Symmetric matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/symmetric/#ManifoldsBase.project-Tuple{SymmetricMatrices, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::SymmetricMatrices, p) Projects  p  from the embedding onto the  SymmetricMatrices M , i.e. \\[\\operatorname{proj}_{\\operatorname{Sym}(n)}(p) = \\frac{1}{2} \\bigl( p + p^{\\mathrm{H}} \\bigr),\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the Hermitian, i.e. complex conjugate transposed. source"},{"id":1698,"pagetitle":"Symmetric positive definite","title":"Symmetric positive definite matrices","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#SymmetricPositiveDefiniteSection","content":" Symmetric positive definite matrices"},{"id":1699,"pagetitle":"Symmetric positive definite","title":"Manifolds.SymmetricPositiveDefinite","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Manifolds.SymmetricPositiveDefinite","content":" Manifolds.SymmetricPositiveDefinite  ‚Äî  Type SymmetricPositiveDefinite{T} <: AbstractDecoratorManifold{‚Ñù} The manifold of symmetric positive definite matrices, i.e. \\[\\mathcal P(n) =\n\\bigl\\{\np ‚àà ‚Ñù^{n√ón}\\ \\big|\\ a^\\mathrm{T}pa > 0 \\text{ for all } a ‚àà ‚Ñù^{n}\\backslash\\{0\\}\n\\bigr\\}\\] The tangent space at  $T_p\\mathcal P(n)$  reads \\[    T_p\\mathcal P(n) =\n    \\bigl\\{\n        X \\in \\mathbb R^{n√ón} \\big|\\ X=X^\\mathrm{T}\n    \\bigr\\},\\] i.e. the set of symmetric matrices, Constructor SymmetricPositiveDefinite(n; parameter::Symbol=:type) generates the manifold  $\\mathcal P(n) \\subset ‚Ñù^{n√ón}$ source This manifold can ‚Äì for example ‚Äì be illustrated as ellipsoids:  since the eigenvalues are all positive they can be taken as lengths of the axes of an ellipsoids while the directions are given by the eigenvectors. The manifold can be equipped with different metrics"},{"id":1700,"pagetitle":"Symmetric positive definite","title":"Common and metric independent functions","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Common-and-metric-independent-functions","content":" Common and metric independent functions"},{"id":1701,"pagetitle":"Symmetric positive definite","title":"Base.convert","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Base.convert-Tuple{Type{AbstractMatrix}, SPDPoint}","content":" Base.convert  ‚Äî  Method convert(::Type{AbstractMatrix}, p::SPDPoint) return the point  p  as a matrix. The matrix is either stored within the  SPDPoint  or reconstructed from  p.eigen . source"},{"id":1702,"pagetitle":"Symmetric positive definite","title":"Base.rand","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Base.rand-Tuple{SymmetricPositiveDefinite}","content":" Base.rand  ‚Äî  Method rand(M::SymmetricPositiveDefinite; œÉ::Real=1) Generate a random symmetric positive definite matrix on the  SymmetricPositiveDefinite  manifold  M . source"},{"id":1703,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.check_point-Tuple{SymmetricPositiveDefinite, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::SymmetricPositiveDefinite, p; kwargs...) checks, whether  p  is a valid point on the  SymmetricPositiveDefinite M , i.e. is a matrix of size  (N,N) , symmetric and positive definite. The tolerance for the second to last test can be set using the  kwargs... . source"},{"id":1704,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.check_vector-Tuple{SymmetricPositiveDefinite, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::SymmetricPositiveDefinite, p, X; kwargs... ) Check whether  X  is a tangent vector to  p  on the  SymmetricPositiveDefinite M , i.e. atfer  check_point (M,p) ,  X  has to be of same dimension as  p  and a symmetric matrix, i.e. this stores tangent vectors as elements of the corresponding Lie group. The tolerance for the last test can be set using the  kwargs... . source"},{"id":1705,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.injectivity_radius-Tuple{SymmetricPositiveDefinite}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::SymmetricPositiveDefinite[, p])\ninjectivity_radius(M::MetricManifold{SymmetricPositiveDefinite,AffineInvariantMetric}[, p])\ninjectivity_radius(M::MetricManifold{SymmetricPositiveDefinite,LogCholeskyMetric}[, p]) Return the injectivity radius of the  SymmetricPositiveDefinite . Since  M  is a Hadamard manifold with respect to the  AffineInvariantMetric  and the  LogCholeskyMetric , the injectivity radius is globally  $‚àû$ . source"},{"id":1706,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.is_flat-Tuple{SymmetricPositiveDefinite}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::SymmetricPositiveDefinite) Return false.  SymmetricPositiveDefinite  is not a flat manifold. source"},{"id":1707,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.manifold_dimension-Tuple{SymmetricPositiveDefinite}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::SymmetricPositiveDefinite) returns the dimension of  SymmetricPositiveDefinite M $=\\mathcal P(n), n ‚àà ‚Ñï$ , i.e. \\[\\dim \\mathcal P(n) = \\frac{n(n+1)}{2}.\\] source"},{"id":1708,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.project-Tuple{SymmetricPositiveDefinite, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::SymmetricPositiveDefinite, p, X) project a matrix from the embedding onto the tangent space  $T_p\\mathcal P(n)$  of the  SymmetricPositiveDefinite  matrices, i.e. the set of symmetric matrices. source"},{"id":1709,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.representation_size","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.representation_size-Tuple{SymmetricPositiveDefinite}","content":" ManifoldsBase.representation_size  ‚Äî  Method representation_size(M::SymmetricPositiveDefinite) Return the size of an array representing an element on the  SymmetricPositiveDefinite  manifold  M , i.e.  $n√ón$ , the size of such a symmetric positive definite matrix on  $\\mathcal M = \\mathcal P(n)$ . source"},{"id":1710,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.zero_vector-Tuple{SymmetricPositiveDefinite, Any}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(M::SymmetricPositiveDefinite, p) returns the zero tangent vector in the tangent space of the symmetric positive definite matrix  p  on the  SymmetricPositiveDefinite  manifold  M . source"},{"id":1711,"pagetitle":"Symmetric positive definite","title":"Default metric: the affine invariant metric","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Default-metric:-the-affine-invariant-metric","content":" Default metric: the affine invariant metric"},{"id":1712,"pagetitle":"Symmetric positive definite","title":"Manifolds.AffineInvariantMetric","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Manifolds.AffineInvariantMetric","content":" Manifolds.AffineInvariantMetric  ‚Äî  Type AffineInvariantMetric <: AbstractMetric The linear affine metric is the metric for symmetric positive definite matrices, that employs matrix logarithms and exponentials, which yields a linear and affine metric. source This metric is also the default metric, i.e. any call of the following functions with  P=SymmetricPositiveDefinite(3)  will result in  MetricManifold(P,AffineInvariantMetric()) and hence yield the formulae described in this section."},{"id":1713,"pagetitle":"Symmetric positive definite","title":"Base.exp","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Base.exp-Tuple{SymmetricPositiveDefinite, Vararg{Any}}","content":" Base.exp  ‚Äî  Method exp(M::SymmetricPositiveDefinite, p, X)\nexp(M::MetricManifold{<:SymmetricPositiveDefinite,AffineInvariantMetric}, p, X) Compute the exponential map from  p  with tangent vector  X  on the  SymmetricPositiveDefinite M  with its default  MetricManifold  having the  AffineInvariantMetric . The formula reads \\[\\exp_p X = p^{\\frac{1}{2}}\\operatorname{Exp}(p^{-\\frac{1}{2}} X p^{-\\frac{1}{2}})p^{\\frac{1}{2}},\\] where  $\\operatorname{Exp}$  denotes to the matrix exponential. source"},{"id":1714,"pagetitle":"Symmetric positive definite","title":"Base.log","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Base.log-Tuple{SymmetricPositiveDefinite, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::SymmetricPositiveDefinite, p, q)\nlog(M::MetricManifold{SymmetricPositiveDefinite,AffineInvariantMetric}, p, q) Compute the logarithmic map from  p  to  q  on the  SymmetricPositiveDefinite  as a  MetricManifold  with  AffineInvariantMetric . The formula reads \\[\\log_p q =\np^{\\frac{1}{2}}\\operatorname{Log}(p^{-\\frac{1}{2}}qp^{-\\frac{1}{2}})p^{\\frac{1}{2}},\\] where  $\\operatorname{Log}$  denotes to the matrix logarithm. source"},{"id":1715,"pagetitle":"Symmetric positive definite","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldDiff.riemannian_Hessian-Tuple{SymmetricPositiveDefinite, Vararg{Any, 4}}","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method riemannian_Hessian(M::SymmetricPositiveDefinite, p, G, H, X) The Riemannian Hessian can be computed as stated in Eq. (7.3) [ Ngu23 ]. Let  $\\nabla f(p)$  denote the Euclidean gradient  G ,  $\\nabla^2 f(p)[X]$  the Euclidean Hessian  H , and  $\\operatorname{sym}(X) = \\frac{1}{2}\\bigl(X^{\\mathrm{T}}+X\\bigr)$  the symmetrization operator. Then the formula reads \\[    \\operatorname{Hess}f(p)[X]\n    =\n    p\\operatorname{sym}(‚àá^2 f(p)[X])p\n    + \\operatorname{sym}\\bigl( X\\operatorname{sym}\\bigl(‚àá f(p)\\bigr)p)\\] source"},{"id":1716,"pagetitle":"Symmetric positive definite","title":"Manifolds.manifold_volume","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Manifolds.manifold_volume-Tuple{SymmetricPositiveDefinite}","content":" Manifolds.manifold_volume  ‚Äî  Method manifold_volume(::SymmetricPositiveDefinite) Return volume of the  SymmetricPositiveDefinite  manifold, i.e. infinity. source"},{"id":1717,"pagetitle":"Symmetric positive definite","title":"Manifolds.volume_density","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Manifolds.volume_density-Tuple{SymmetricPositiveDefinite, Any, Any}","content":" Manifolds.volume_density  ‚Äî  Method volume_density(::SymmetricPositiveDefinite, p, X) Compute the volume density of the  SymmetricPositiveDefinite  manifold at  p  in direction  X . See [ CKA17 ], Section 6.2 for details. Note that metric in Manifolds.jl has a different scaling factor than the reference. source"},{"id":1718,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.change_metric","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.change_metric-Tuple{SymmetricPositiveDefinite, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_metric  ‚Äî  Method change_metric(M::SymmetricPositiveDefinite, E::EuclideanMetric, p, X) Given a tangent vector  $X ‚àà T_p\\mathcal P(n)$  with respect to the  EuclideanMetric g_E , this function changes into the  AffineInvariantMetric  (default) metric on the  SymmetricPositiveDefinite M . To be precise we are looking for  $c\\colon T_p\\mathcal P(n) ‚Üí T_p\\mathcal P(n)$  such that for all  $Y,Z ‚àà T_p\\mathcal P(n)$ ` it holds \\[‚ü®Y,Z‚ü© = \\operatorname{tr}(YZ) = \\operatorname{tr}(p^{-1}c(Y)p^{-1}c(Z)) = g_p(c(Z),c(Y))\\] and hence  $c(X) = pX$  is computed. source"},{"id":1719,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.change_representer","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.change_representer-Tuple{SymmetricPositiveDefinite, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(M::SymmetricPositiveDefinite, E::EuclideanMetric, p, X) Given a tangent vector  $X ‚àà T_p\\mathcal M$  representing a linear function on the tangent space at  p  with respect to the  EuclideanMetric g_E , this is turned into the representer with respect to the (default) metric, the  AffineInvariantMetric  on the  SymmetricPositiveDefinite M . To be precise we are looking for  $Z‚ààT_p\\mathcal P(n)$  such that for all  $Y‚ààT_p\\mathcal P(n)$ ` it holds \\[‚ü®X,Y‚ü© = \\operatorname{tr}(XY) = \\operatorname{tr}(p^{-1}Zp^{-1}Y) = g_p(Z,Y)\\] and hence  $Z = pXp$ . source"},{"id":1720,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.distance-Tuple{SymmetricPositiveDefinite, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::SymmetricPositiveDefinite, p, q)\ndistance(M::MetricManifold{SymmetricPositiveDefinite,AffineInvariantMetric}, p, q) Compute the distance on the  SymmetricPositiveDefinite  manifold between  p  and  q , as a  MetricManifold  with  AffineInvariantMetric . The formula reads \\[d_{\\mathcal P(n)}(p,q)\n= \\lVert \\operatorname{Log}(p^{-\\frac{1}{2}}qp^{-\\frac{1}{2}})\\rVert_{\\mathrm{F}}.,\\] where  $\\operatorname{Log}$  denotes the matrix logarithm and  $\\lVert‚ãÖ\\rVert_{\\mathrm{F}}$  denotes the matrix Frobenius norm. source"},{"id":1721,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.get_basis","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.get_basis-Tuple{SymmetricPositiveDefinite, Any, DefaultOrthonormalBasis}","content":" ManifoldsBase.get_basis  ‚Äî  Method [Œû,Œ∫] = get_basis(M::SymmetricPositiveDefinite, p, B::DefaultOrthonormalBasis)\n[Œû,Œ∫] = get_basis(M::MetricManifold{<:SymmetricPositiveDefinite,AffineInvariantMetric}, p, B::DefaultOrthonormalBasis) Return a default ONB for the tangent space  $T_p\\mathcal P(n)$  of the  SymmetricPositiveDefinite  with respect to the  AffineInvariantMetric . \\[    g_p(X,Y) = \\operatorname{tr}(p^{-1} X p^{-1} Y),\\] The basis constructed here is based on the ONB for symmetric matrices constructed as follows. Let \\[\\Delta_{i,j} = (a_{k,l})_{k,l=1}^n \\quad \\text{ with }\na_{k,l} =\n\\begin{cases}\n  1 & \\mbox{ for } k=l \\text{ if } i=j\\\\\n  \\frac{1}{\\sqrt{2}} & \\mbox{ for } k=i, l=j \\text{ or } k=j, l=i\\\\\n  0 & \\text{ else.}\n\\end{cases}\\] which forms an ONB for the space of symmetric matrices. We then form the ONB by \\[   \\Xi_{i,j} = p^{\\frac{1}{2}}\\Delta_{i,j}p^{\\frac{1}{2}},\\qquad i=1,\\ldots,n, j=i,\\ldots,n.\\] source"},{"id":1722,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.get_basis_diagonalizing","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.get_basis_diagonalizing-Tuple{SymmetricPositiveDefinite, Any, DiagonalizingOrthonormalBasis}","content":" ManifoldsBase.get_basis_diagonalizing  ‚Äî  Method [Œû,Œ∫] = get_basis_diagonalizing(M::SymmetricPositiveDefinite, p, B::DiagonalizingOrthonormalBasis)\n[Œû,Œ∫] = get_basis_diagonalizing(M::MetricManifold{<:SymmetricPositiveDefinite,AffineInvariantMetric}, p, B::DiagonalizingOrthonormalBasis) Return a orthonormal basis  Œû  as a vector of tangent vectors (of length  manifold_dimension  of  M ) in the tangent space of  p  on the  MetricManifold  of  SymmetricPositiveDefinite  manifold  M  with  AffineInvariantMetric  that diagonalizes the curvature tensor  $R(u,v)w$  with eigenvalues  Œ∫  and where the direction  B.frame_direction $V$  has curvature  0 . The construction is based on an ONB for the symmetric matrices similar to  get_basis(::SymmetricPositiveDefinite, p, ::DefaultOrthonormalBasis  just that the ONB here is build from the eigen vectors of  $p^{\\frac{1}{2}}Vp^{\\frac{1}{2}}$ . source"},{"id":1723,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.get_coordinates","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.get_coordinates-Tuple{SymmetricPositiveDefinite, Any, Any, Any, DefaultOrthonormalBasis}","content":" ManifoldsBase.get_coordinates  ‚Äî  Method get_coordinates(::SymmetricPositiveDefinite, p, X, ::DefaultOrthonormalBasis) Using the basis from  get_basis  the coordinates with respect to this ONB can be simplified to \\[   c_k = \\mathrm{tr}(p^{-\\frac{1}{2}}\\Delta_{i,j} X)\\] where  $k$  is trhe linearized index of the  $i=1,\\ldots,n, j=i,\\ldots,n$ . source"},{"id":1724,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.get_vector","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.get_vector-Tuple{SymmetricPositiveDefinite, Any, Any, Any, DefaultOrthonormalBasis}","content":" ManifoldsBase.get_vector  ‚Äî  Method get_vector(::SymmetricPositiveDefinite, p, c, ::DefaultOrthonormalBasis) Using the basis from  get_basis  the vector reconstruction with respect to this ONB can be simplified to \\[   X = p^{\\frac{1}{2}} \\Biggl( \\sum_{i=1,j=i}^n c_k \\Delta_{i,j} \\Biggr) p^{\\frac{1}{2}}\\] where  $k$  is the linearized index of the  $i=1,\\ldots,n, j=i,\\ldots,n$ . source"},{"id":1725,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.inner-Tuple{SymmetricPositiveDefinite, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::SymmetricPositiveDefinite, p, X, Y)\ninner(M::MetricManifold{SymmetricPositiveDefinite,AffineInvariantMetric}, p, X, Y) Compute the inner product of  X ,  Y  in the tangent space of  p  on the  SymmetricPositiveDefinite  manifold  M , as a  MetricManifold  with  AffineInvariantMetric . The formula reads \\[g_p(X,Y) = \\operatorname{tr}(p^{-1} X p^{-1} Y),\\] source"},{"id":1726,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.is_flat-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, AffineInvariantMetric}}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,AffineInvariantMetric}) Return false.  SymmetricPositiveDefinite  with  AffineInvariantMetric  is not a flat manifold. source"},{"id":1727,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.parallel_transport_to-Tuple{SymmetricPositiveDefinite, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method parallel_transport_to(M::SymmetricPositiveDefinite, p, X, q)\nparallel_transport_to(M::MetricManifold{SymmetricPositiveDefinite,AffineInvariantMetric}, p, X, y) Compute the parallel transport of  X  from the tangent space at  p  to the tangent space at  q  on the  SymmetricPositiveDefinite  as a  MetricManifold  with the  AffineInvariantMetric . The formula reads \\[\\mathcal P_{q‚Üêp}X = p^{\\frac{1}{2}}\n\\operatorname{Exp}\\bigl(\n\\frac{1}{2}p^{-\\frac{1}{2}}\\log_p(q)p^{-\\frac{1}{2}}\n\\bigr)\np^{-\\frac{1}{2}}X p^{-\\frac{1}{2}}\n\\operatorname{Exp}\\bigl(\n\\frac{1}{2}p^{-\\frac{1}{2}}\\log_p(q)p^{-\\frac{1}{2}}\n\\bigr)\np^{\\frac{1}{2}},\\] where  $\\operatorname{Exp}$  denotes the matrix exponential and  log  the logarithmic map on  SymmetricPositiveDefinite  (again with respect to the  AffineInvariantMetric ). source"},{"id":1728,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.riemann_tensor","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.riemann_tensor-Tuple{SymmetricPositiveDefinite, Vararg{Any, 4}}","content":" ManifoldsBase.riemann_tensor  ‚Äî  Method riemann_tensor(::SymmetricPositiveDefinite, p, X, Y, Z) Compute the value of Riemann tensor on the  SymmetricPositiveDefinite  manifold. The formula reads [ Ren11 ]  $R(X,Y)Z=p^{1/2}R(X_I, Y_I)Z_Ip^{1/2}$ , where  $R_I(X_I, Y_I)Z_I=\\frac{1}{4}[Z_I, [X_I, Y_I]]$ ,   $X_I=p^{-1/2}Xp^{-1/2}$ ,  $Y_I=p^{-1/2}Yp^{-1/2}$  and  $Z_I=p^{-1/2}Zp^{-1/2}$ . source"},{"id":1729,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.sectional_curvature_max","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.sectional_curvature_max-Tuple{SymmetricPositiveDefinite}","content":" ManifoldsBase.sectional_curvature_max  ‚Äî  Method sectional_curvature_max(M::SymmetricPositiveDefinite) Return minimum sectional curvature of  SymmetricPositiveDefinite  manifold, that is 0. source"},{"id":1730,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.sectional_curvature_min","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.sectional_curvature_min-Tuple{SymmetricPositiveDefinite}","content":" ManifoldsBase.sectional_curvature_min  ‚Äî  Method sectional_curvature_min(M::SymmetricPositiveDefinite) Return minimum sectional curvature of  SymmetricPositiveDefinite  manifold, that is 0 for SPD(1) and SPD(2) and -0.25 otherwise. source"},{"id":1731,"pagetitle":"Symmetric positive definite","title":"Bures-Wasserstein metric","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#BuresWassersteinMetricSection","content":" Bures-Wasserstein metric"},{"id":1732,"pagetitle":"Symmetric positive definite","title":"Manifolds.BuresWassersteinMetric","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Manifolds.BuresWassersteinMetric","content":" Manifolds.BuresWassersteinMetric  ‚Äî  Type BurresWassertseinMetric <: AbstractMetric The Bures Wasserstein metric for symmetric positive definite matrices [ MMP18 ] source"},{"id":1733,"pagetitle":"Symmetric positive definite","title":"Base.exp","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Base.exp-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, BuresWassersteinMetric}, Any, Any}","content":" Base.exp  ‚Äî  Method exp(::MetricManifold{‚Ñù,SymmetricPositiveDefinite,BuresWassersteinMetric}, p, X) Compute the exponential map on  SymmetricPositiveDefinite  with respect to the  BuresWassersteinMetric  given by \\[    \\exp_p(X) = p+X+L_p(X)pL_p(X)\\] where  $q=L_p(X)$  denotes the Lyapunov operator, i.e. it solves  $pq + qp = X$ . source"},{"id":1734,"pagetitle":"Symmetric positive definite","title":"Base.log","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Base.log-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, BuresWassersteinMetric}, Any, Any}","content":" Base.log  ‚Äî  Method log(::MetricManifold{SymmetricPositiveDefinite,BuresWassersteinMetric}, p, q) Compute the logarithmic map on  SymmetricPositiveDefinite  with respect to the  BuresWassersteinMetric  given by \\[    \\log_p(q) = (pq)^{\\frac{1}{2}} + (qp)^{\\frac{1}{2}} - 2 p\\] where  $q=L_p(X)$  denotes the Lyapunov operator, i.e. it solves  $pq + qp = X$ . source"},{"id":1735,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.change_representer","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.change_representer-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, BuresWassersteinMetric}, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(M::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,BuresWassersteinMetric}, E::EuclideanMetric, p, X) Given a tangent vector  $X ‚àà T_p\\mathcal M$  representing a linear function on the tangent space at  p  with respect to the  EuclideanMetric g_E , this is turned into the representer with respect to the (default) metric, the  BuresWassersteinMetric  on the  SymmetricPositiveDefinite M . To be precise we are looking for  $Z‚ààT_p\\mathcal P(n)$  such that for all  $Y‚ààT_p\\mathcal P(n)$ ` it holds \\[‚ü®X,Y‚ü© = \\operatorname{tr}(XY) = ‚ü®Z,Y‚ü©_{\\mathrm{BW}}\\] for all  $Y$  and hence we get  $Z$ = 2(A+A^{\\mathrm{T}}) $with$ A=Xp``. source"},{"id":1736,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.distance-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, BuresWassersteinMetric}, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(::MetricManifold{SymmetricPositiveDefinite,BuresWassersteinMetric}, p, q) Compute the distance with respect to the  BuresWassersteinMetric  on  SymmetricPositiveDefinite  matrices, i.e. \\[d(p,q) =\n    \\operatorname{tr}(p) + \\operatorname{tr}(q) - 2\\operatorname{tr}\\Bigl( (p^{\\frac{1}{2}}qp^{\\frac{1}{2}} \\bigr)^\\frac{1}{2} \\Bigr),\\] where the last trace can be simplified (by rotating the matrix products in the trace) to  $\\operatorname{tr}(pq)$ . source"},{"id":1737,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.inner-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, BuresWassersteinMetric}, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(::MetricManifold{‚Ñù,SymmetricPositiveDefinite,BuresWassersteinMetric}, p, X, Y) Compute the inner product  SymmetricPositiveDefinite  with respect to the  BuresWassersteinMetric  given by \\[    ‚ü®X,Y‚ü© = \\frac{1}{2}\\operatorname{tr}(L_p(X)Y)\\] where  $q=L_p(X)$  denotes the Lyapunov operator, i.e. it solves  $pq + qp = X$ . source"},{"id":1738,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.is_flat-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, BuresWassersteinMetric}}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,BuresWassersteinMetric}) Return false.  SymmetricPositiveDefinite  with  BuresWassersteinMetric  is not a flat manifold. source"},{"id":1739,"pagetitle":"Symmetric positive definite","title":"Generalized Bures-Wasserstein metric","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Generalized-Bures-Wasserstein-metric","content":" Generalized Bures-Wasserstein metric"},{"id":1740,"pagetitle":"Symmetric positive definite","title":"Manifolds.GeneralizedBuresWassersteinMetric","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Manifolds.GeneralizedBuresWassersteinMetric","content":" Manifolds.GeneralizedBuresWassersteinMetric  ‚Äî  Type GeneralizedBurresWassertseinMetric{T<:AbstractMatrix} <: AbstractMetric The generalized Bures Wasserstein metric for symmetric positive definite matrices, see [ HMJG21 ]. This metric internally stores the symmetric positive definite matrix  $M$  to generalise the metric, where the name also follows the mentioned preprint. source"},{"id":1741,"pagetitle":"Symmetric positive definite","title":"Base.exp","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Base.exp-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, <:GeneralizedBuresWassersteinMetric}, Any, Any}","content":" Base.exp  ‚Äî  Method exp(::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,<:GeneralizedBuresWassersteinMetric}, p, X) Compute the exponential map on  SymmetricPositiveDefinite  with respect to the  GeneralizedBuresWassersteinMetric  given by \\[    \\exp_p(X) = p+X+\\mathcal ML_{p,M}(X)pML_{p,M}(X)\\] where  $q=L_{M,p}(X)$  denotes the generalized Lyapunov operator, i.e. it solves  $pqM + Mqp = X$ . source"},{"id":1742,"pagetitle":"Symmetric positive definite","title":"Base.log","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Base.log-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, <:GeneralizedBuresWassersteinMetric}, Any, Any}","content":" Base.log  ‚Äî  Method log(::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,<:GeneralizedBuresWassersteinMetric}, p, q) Compute the logarithmic map on  SymmetricPositiveDefinite  with respect to the  BuresWassersteinMetric  given by \\[    \\log_p(q) = M(M^{-1}pM^{-1}q)^{\\frac{1}{2}} + (qM^{-1}pM^{-1})^{\\frac{1}{2}}M - 2 p.\\] source"},{"id":1743,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.change_representer","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.change_representer-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, <:GeneralizedBuresWassersteinMetric}, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(M::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,<:GeneralizedBuresWassersteinMetric}, E::EuclideanMetric, p, X) Given a tangent vector  $X ‚àà T_p\\mathcal M$  representing a linear function on the tangent space at  p  with respect to the  EuclideanMetric g_E , this is turned into the representer with respect to the (default) metric, the  GeneralizedBuresWassersteinMetric  on the  SymmetricPositiveDefinite M . To be precise we are looking for  $Z‚ààT_p\\mathcal P(n)$  such that for all  $Y‚ààT_p\\mathcal P(n)$  it holds \\[‚ü®X,Y‚ü© = \\operatorname{tr}(XY) = ‚ü®Z,Y‚ü©_{\\mathrm{BW}}\\] for all  $Y$  and hence we get  $Z = 2pXM + 2MXp$ . source"},{"id":1744,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.distance-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, <:GeneralizedBuresWassersteinMetric}, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(::MetricManifold{SymmetricPositiveDefinite,GeneralizedBuresWassersteinMetric}, p, q) Compute the distance with respect to the  BuresWassersteinMetric  on  SymmetricPositiveDefinite  matrices, i.e. \\[d(p,q) = \\operatorname{tr}(M^{-1}p) + \\operatorname{tr}(M^{-1}q)\n       - 2\\operatorname{tr}\\bigl( (p^{\\frac{1}{2}}M^{-1}qM^{-1}p^{\\frac{1}{2}} \\bigr)^{\\frac{1}{2}},\\] source"},{"id":1745,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.inner-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, <:GeneralizedBuresWassersteinMetric}, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,<:GeneralizedBuresWassersteinMetric}, p, X, Y) Compute the inner product  SymmetricPositiveDefinite  with respect to the  GeneralizedBuresWassersteinMetric  given by \\[    ‚ü®X,Y‚ü© = \\frac{1}{2}\\operatorname{tr}(L_{p,M}(X)Y)\\] where  $q=L_{M,p}(X)$  denotes the generalized Lyapunov operator, i.e. it solves  $pqM + Mqp = X$ . source"},{"id":1746,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.is_flat-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, <:GeneralizedBuresWassersteinMetric}}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,<:GeneralizedBuresWassersteinMetric}) Return false.  SymmetricPositiveDefinite  with  GeneralizedBuresWassersteinMetric  is not a flat manifold. source"},{"id":1747,"pagetitle":"Symmetric positive definite","title":"Log-Euclidean metric","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Log-Euclidean-metric","content":" Log-Euclidean metric"},{"id":1748,"pagetitle":"Symmetric positive definite","title":"Manifolds.LogEuclideanMetric","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Manifolds.LogEuclideanMetric","content":" Manifolds.LogEuclideanMetric  ‚Äî  Type LogEuclideanMetric <: RiemannianMetric The LogEuclidean Metric consists of the Euclidean metric applied to all elements after mapping them into the Lie Algebra, i.e. performing a matrix logarithm beforehand. source"},{"id":1749,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.distance-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, LogEuclideanMetric}, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,LogEuclideanMetric}, p, q) Compute the distance on the  SymmetricPositiveDefinite  manifold between  p  and  q  as a  MetricManifold  with  LogEuclideanMetric . The formula reads \\[    d_{\\mathcal P(n)}(p,q) = \\lVert \\operatorname{Log} p - \\operatorname{Log} q \\rVert_{\\mathrm{F}}\\] where  $\\operatorname{Log}$  denotes the matrix logarithm and  $\\lVert‚ãÖ\\rVert_{\\mathrm{F}}$  denotes the matrix Frobenius norm. source"},{"id":1750,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.is_flat-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, LogEuclideanMetric}}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,LogEuclideanMetric}) Return false.  SymmetricPositiveDefinite  with  LogEuclideanMetric  is not a flat manifold. source"},{"id":1751,"pagetitle":"Symmetric positive definite","title":"Log-Cholesky metric","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Log-Cholesky-metric","content":" Log-Cholesky metric"},{"id":1752,"pagetitle":"Symmetric positive definite","title":"Manifolds.LogCholeskyMetric","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Manifolds.LogCholeskyMetric","content":" Manifolds.LogCholeskyMetric  ‚Äî  Type LogCholeskyMetric <: RiemannianMetric The Log-Cholesky metric imposes a metric based on the Cholesky decomposition as introduced by [ Lin19 ]. source"},{"id":1753,"pagetitle":"Symmetric positive definite","title":"Base.exp","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Base.exp-Tuple{MetricManifold{‚Ñù, SymmetricPositiveDefinite, LogCholeskyMetric}, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::MetricManifold{SymmetricPositiveDefinite,LogCholeskyMetric}, p, X) Compute the exponential map on the  SymmetricPositiveDefinite M  with  LogCholeskyMetric  from  p  into direction  X . The formula reads \\[\\exp_p X = (\\exp_y W)(\\exp_y W)^\\mathrm{T}\\] where  $\\exp_xW$  is the exponential map on  CholeskySpace ,  $y$  is the Cholesky decomposition of  $p$ ,  $W = y(y^{-1}Xy^{-\\mathrm{T}})_\\frac{1}{2}$ , and  $(‚ãÖ)_\\frac{1}{2}$  denotes the lower triangular matrix with the diagonal multiplied by  $\\frac{1}{2}$ . source"},{"id":1754,"pagetitle":"Symmetric positive definite","title":"Base.log","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Base.log-Tuple{MetricManifold{‚Ñù, SymmetricPositiveDefinite, LogCholeskyMetric}, Vararg{Any}}","content":" Base.log  ‚Äî  Method log(M::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,LogCholeskyMetric}, p, q) Compute the logarithmic map on  SymmetricPositiveDefinite M  with respect to the  LogCholeskyMetric  emanating from  p  to  q . The formula can be adapted from the  CholeskySpace  as \\[\\log_p q = xW^{\\mathrm{T}} + Wx^{\\mathrm{T}},\\] where  $x$  is the Cholesky factor of  $p$  and  $W=\\log_x y$  for  $y$  the Cholesky factor of  $q$  and the just mentioned logarithmic map is the one on  CholeskySpace . source"},{"id":1755,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.distance-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, LogCholeskyMetric}, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::MetricManifold{SymmetricPositiveDefinite,LogCholeskyMetric}, p, q) Compute the distance on the manifold of  SymmetricPositiveDefinite  nmatrices, i.e. between two symmetric positive definite matrices  p  and  q  with respect to the  LogCholeskyMetric . The formula reads \\[d_{\\mathcal P(n)}(p,q) = \\sqrt{\n \\lVert ‚åä x ‚åã - ‚åä y ‚åã \\rVert_{\\mathrm{F}}^2\n + \\lVert \\log(\\operatorname{diag}(x)) - \\log(\\operatorname{diag}(y))\\rVert_{\\mathrm{F}}^2 }\\ \\ ,\\] where  $x$  and  $y$  are the Cholesky factors of  $p$  and  $q$ , respectively,  $‚åä‚ãÖ‚åã$  denbotes the strictly lower triangular matrix of its argument, and  $\\lVert‚ãÖ\\rVert_{\\mathrm{F}}$  the Frobenius norm. source"},{"id":1756,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.inner-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, LogCholeskyMetric}, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,LogCholeskyMetric}, p, X, Y) Compute the inner product of two matrices  X ,  Y  in the tangent space of  p  on the  SymmetricPositiveDefinite  manifold  M , as a  MetricManifold  with  LogCholeskyMetric . The formula reads \\[    g_p(X,Y) = ‚ü®a_z(X),a_z(Y)‚ü©_z,\\] where  $‚ü®‚ãÖ,‚ãÖ‚ü©_x$  denotes inner product on the  CholeskySpace ,  $z$  is the Cholesky factor of  $p$ ,  $a_z(W) = z (z^{-1}Wz^{-\\mathrm{T}})_{\\frac{1}{2}}$ , and  $(‚ãÖ)_\\frac{1}{2}$  denotes the lower triangular matrix with the diagonal multiplied by  $\\frac{1}{2}$ source"},{"id":1757,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.is_flat-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, LogCholeskyMetric}}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,LogCholeskyMetric}) Return true.  SymmetricPositiveDefinite  with  LogCholeskyMetric  is a flat manifold. See Proposition 8 of [ Lin19 ]. source"},{"id":1758,"pagetitle":"Symmetric positive definite","title":"ManifoldsBase.parallel_transport_to","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#ManifoldsBase.parallel_transport_to-Tuple{MetricManifold{‚Ñù, <:SymmetricPositiveDefinite, LogCholeskyMetric}, Any, Any, Any}","content":" ManifoldsBase.parallel_transport_to  ‚Äî  Method vector_transport_to(\n    M::MetricManifold{‚Ñù,<:SymmetricPositiveDefinite,LogCholeskyMetric},\n    p,\n    X,\n    q,\n    ::ParallelTransport,\n) Parallel transport the tangent vector  X  at  p  along the geodesic to  q  with respect to the  SymmetricPositiveDefinite  manifold  M  and  LogCholeskyMetric . The parallel transport is based on the parallel transport on  CholeskySpace : Let  $x$  and  $y$  denote the Cholesky factors of  p  and  q , respectively and  $W = x(x^{-1}Xx^{-\\mathrm{T}})_\\frac{1}{2}$ , where  $(‚ãÖ)_\\frac{1}{2}$  denotes the lower triangular matrix with the diagonal multiplied by  $\\frac{1}{2}$ . With  $V$  the parallel transport on  CholeskySpace  from  $x$  to  $y$ . The formula hear reads \\[\\mathcal P_{q‚Üêp}X = yV^{\\mathrm{T}} + Vy^{\\mathrm{T}}.\\] source"},{"id":1759,"pagetitle":"Symmetric positive definite","title":"Statistics","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Statistics","content":" Statistics"},{"id":1760,"pagetitle":"Symmetric positive definite","title":"Statistics.mean","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Statistics.mean-Tuple{SymmetricPositiveDefinite, Any}","content":" Statistics.mean  ‚Äî  Method mean(\n    M::SymmetricPositiveDefinite,\n    x::AbstractVector,\n    [w::AbstractWeights,]\n    method = GeodesicInterpolation();\n    kwargs...,\n) Compute the Riemannian  mean  of  x  using  GeodesicInterpolation . source"},{"id":1761,"pagetitle":"Symmetric positive definite","title":"Efficient representation","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Efficient-representation","content":" Efficient representation When a point  p  is used in several occasions, it might be beneficial to store the eigenvalues and vectors of  p  and optionally its square root and the inverse of the square root. The  SPDPoint  can be used for exactly that."},{"id":1762,"pagetitle":"Symmetric positive definite","title":"Manifolds.SPDPoint","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Manifolds.SPDPoint","content":" Manifolds.SPDPoint  ‚Äî  Type SPDPoint <: AbstractManifoldsPoint Store the result of  eigen(p)  of an SPD matrix and (optionally)  $p^{1/2}$  and  $p^{-1/2}$  to avoid their repeated computations. This result only has the result of  eigen  as a mandatory storage, the other three can be stored. If they are not stored they are computed and returned (but then still not stored) when required. Constructor SPDPoint(p::AbstractMatrix; store_p=true, store_sqrt=true, store_sqrt_inv=true) Create an SPD point using an symmetric positive defincite matrix  p , where you can optionally store  p ,  sqrt  and  sqrt_inv source and there are three internal functions to be able to use  SPDPoint  interchangeably with the default representation as a matrix."},{"id":1763,"pagetitle":"Symmetric positive definite","title":"Manifolds.spd_sqrt","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Manifolds.spd_sqrt","content":" Manifolds.spd_sqrt  ‚Äî  Function spd_sqrt(p::AbstractMatrix)\nspd_sqrt(p::SPDPoint) return  $p^{\\frac{1}{2}}$  by either computing it (if it is missing or for the  AbstractMatrix ) or returning the stored value from within the  SPDPoint . This method assumes that  p  represents an spd matrix. source"},{"id":1764,"pagetitle":"Symmetric positive definite","title":"Manifolds.spd_sqrt_inv","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Manifolds.spd_sqrt_inv","content":" Manifolds.spd_sqrt_inv  ‚Äî  Function spd_sqrt_inv(p::SPDPoint) return  $p^{-\\frac{1}{2}}$  by either computing it (if it is missing or for the  AbstractMatrix ) or returning the stored value from within the  SPDPoint . This method assumes that  p  represents an spd matrix. source"},{"id":1765,"pagetitle":"Symmetric positive definite","title":"Manifolds.spd_sqrt_and_sqrt_inv","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Manifolds.spd_sqrt_and_sqrt_inv","content":" Manifolds.spd_sqrt_and_sqrt_inv  ‚Äî  Function spd_sqrt_and_sqrt_inv(p::AbstractMatrix)\nspd_sqrt_and_sqrt_inv(p::SPDPoint) return  $p^{\\frac{1}{2}}$  and  $p^{-\\frac{1}{2}}$  by either computing them (if they are missing or for the  AbstractMatrix ) or returning their stored value from within the  SPDPoint . Compared to calling single methods  spd_sqrt  and  spd_sqrt_inv  this method only computes the eigenvectors once for the case of the  AbstractMatrix  or if both are missing. This method assumes that  p  represents an spd matrix. source"},{"id":1766,"pagetitle":"Symmetric positive definite","title":"Literature","ref":"/manifolds/stable/manifolds/symmetricpositivedefinite/#Literature","content":" Literature"},{"id":1769,"pagetitle":"Symmetric positive semidefinite fixed rank","title":"Symmetric Positive Semidefinite Matrices of Fixed Rank","ref":"/manifolds/stable/manifolds/symmetricpsdfixedrank/#Symmetric-Positive-Semidefinite-Matrices-of-Fixed-Rank","content":" Symmetric Positive Semidefinite Matrices of Fixed Rank"},{"id":1770,"pagetitle":"Symmetric positive semidefinite fixed rank","title":"Manifolds.SymmetricPositiveSemidefiniteFixedRank","ref":"/manifolds/stable/manifolds/symmetricpsdfixedrank/#Manifolds.SymmetricPositiveSemidefiniteFixedRank","content":" Manifolds.SymmetricPositiveSemidefiniteFixedRank  ‚Äî  Type SymmetricPositiveSemidefiniteFixedRank{T,ùîΩ} <: AbstractDecoratorManifold{ùîΩ} The  AbstractManifold $\\operatorname{SPS}_k(n)$  consisting of the real- or complex-valued symmetric positive semidefinite matrices of size  $n√ón$  and rank  $k$ , i.e. the set \\[\\operatorname{SPS}_k(n) = \\bigl\\{\np  ‚àà ùîΩ^{n√ón}\\ \\big|\\ p^{\\mathrm{H}} = p,\napa^{\\mathrm{H}} \\geq 0 \\text{ for all } a ‚àà ùîΩ\n\\text{ and } \\operatorname{rank}(p) = k\\bigr\\},\\] where  $‚ãÖ^{\\mathrm{H}}$  denotes the Hermitian, i.e. complex conjugate transpose, and the field  $ùîΩ ‚àà \\{ ‚Ñù, ‚ÑÇ\\}$ . We sometimes  $\\operatorname{SPS}_{k,ùîΩ}(n)$ , when distinguishing the real- and complex-valued manifold is important. An element is represented by  $q ‚àà ùîΩ^{n√ók}$  from the factorization  $p = qq^{\\mathrm{H}}$ . Note that since for any unitary (orthogonal)  $A ‚àà ùîΩ^{n√ón}$  we have  $(Aq)(Aq)^{\\mathrm{H}} = qq^{\\mathrm{H}} = p$ , the representation is not unique, or in other words, the manifold is a quotient manifold of  $ùîΩ^{n√ók}$ . The tangent space at  $p$ ,  $T_p\\operatorname{SPS}_k(n)$ , is also represented by matrices  $Y ‚àà ùîΩ^{n√ók}$  and reads as \\[T_p\\operatorname{SPS}_k(n) = \\bigl\\{\nX ‚àà ùîΩ^{n√ón}\\,|\\,X = qY^{\\mathrm{H}} + Yq^{\\mathrm{H}}\n\\text{ i.e. } X = X^{\\mathrm{H}}\n\\bigr\\}.\\] Note that the metric used yields a non-complete manifold. The metric was used in [ JBAS10 ][ MA20 ]. Constructor SymmetricPositiveSemidefiniteFixedRank(n::Int, k::Int, field::AbstractNumbers=‚Ñù; parameter::Symbol=:type) Generate the manifold of  $n√ón$  symmetric positive semidefinite matrices of rank  $k$  over the  field  of real numbers  ‚Ñù  or complex numbers  ‚ÑÇ . source"},{"id":1771,"pagetitle":"Symmetric positive semidefinite fixed rank","title":"Base.exp","ref":"/manifolds/stable/manifolds/symmetricpsdfixedrank/#Base.exp-Tuple{SymmetricPositiveSemidefiniteFixedRank, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::SymmetricPositiveSemidefiniteFixedRank, q, Y) Compute the exponential map on the  SymmetricPositiveSemidefiniteFixedRank , which just reads \\[    \\exp_q Y = q+Y.\\] Note Since the manifold is represented in the embedding and is a quotient manifold, the exponential and logarithmic map are a bijection only with respect to the equivalence classes. Computing \\[    q_2 = \\exp_p(\\log_pq)\\] might yield a matrix  $q_2\\neq q$ , but they represent the same point on the quotient manifold, i.e.  $d_{\\operatorname{SPS}_k(n)}(q_2,q) = 0$ . source"},{"id":1772,"pagetitle":"Symmetric positive semidefinite fixed rank","title":"Base.log","ref":"/manifolds/stable/manifolds/symmetricpsdfixedrank/#Base.log-Tuple{SymmetricPositiveSemidefiniteFixedRank, Any, Any}","content":" Base.log  ‚Äî  Method log(M::SymmetricPositiveSemidefiniteFixedRank, q, p) Compute the logarithmic map on the  SymmetricPositiveSemidefiniteFixedRank  manifold by minimizing  $\\lVert p - qY\\rVert$  with respect to  $Y$ . Note Since the manifold is represented in the embedding and is a quotient manifold, the exponential and logarithmic map are a bijection only with respect to the equivalence classes. Computing \\[    q_2 = \\exp_p(\\log_pq)\\] might yield a matrix  $q_2‚â†q$ , but they represent the same point on the quotient manifold, i.e.  $d_{\\operatorname{SPS}_k(n)}(q_2,q) = 0$ . source"},{"id":1773,"pagetitle":"Symmetric positive semidefinite fixed rank","title":"ManifoldsBase._isapprox","ref":"/manifolds/stable/manifolds/symmetricpsdfixedrank/#ManifoldsBase._isapprox-Union{Tuple{T}, Tuple{SymmetricPositiveSemidefiniteFixedRank, T, Any}} where T","content":" ManifoldsBase._isapprox  ‚Äî  Method isapprox(M::SymmetricPositiveSemidefiniteFixedRank, p, q; kwargs...) test, whether two points  p ,  q  are (approximately) nearly the same. Since this is a quotient manifold in the embedding, the test is performed by checking their distance, if they are not the same, i.e. that  $d_{\\mathcal M}(p,q) \\approx 0$ , where the comparison is performed with the classical  isapprox . The  kwargs...  are passed on to this accordingly. source"},{"id":1774,"pagetitle":"Symmetric positive semidefinite fixed rank","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/symmetricpsdfixedrank/#ManifoldsBase.check_point-Tuple{SymmetricPositiveSemidefiniteFixedRank, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::SymmetricPositiveSemidefiniteFixedRank, q; kwargs...) Check whether  q  is a valid manifold point on the  SymmetricPositiveSemidefiniteFixedRank M , i.e. whether  p=q*q'  is a symmetric matrix of size  (n,n)  with values from the corresponding  AbstractNumbers ùîΩ . The symmetry of  p  is not explicitly checked since by using  q  p is symmetric by construction. The tolerance for the symmetry of  p  can and the rank of  q*q'  be set using  kwargs... . source"},{"id":1775,"pagetitle":"Symmetric positive semidefinite fixed rank","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/symmetricpsdfixedrank/#ManifoldsBase.check_vector-Tuple{SymmetricPositiveSemidefiniteFixedRank, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::SymmetricPositiveSemidefiniteFixedRank, p, X; kwargs... ) Check whether  X  is a tangent vector to manifold point  p  on the  SymmetricPositiveSemidefiniteFixedRank M , i.e.  X  has to be a symmetric matrix of size  (n,n)  and its values have to be from the correct  AbstractNumbers . Due to the reduced representation this is fulfilled as soon as the matrix is of correct size. source"},{"id":1776,"pagetitle":"Symmetric positive semidefinite fixed rank","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/symmetricpsdfixedrank/#ManifoldsBase.distance-Tuple{SymmetricPositiveSemidefiniteFixedRank, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::SymmetricPositiveSemidefiniteFixedRank, p, q) Compute the distance between two points  p ,  q  on the  SymmetricPositiveSemidefiniteFixedRank , which is the Frobenius norm of  $Y$  which minimizes  $\\lVert p - qY\\rVert$  with respect to  $Y$ . source"},{"id":1777,"pagetitle":"Symmetric positive semidefinite fixed rank","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/symmetricpsdfixedrank/#ManifoldsBase.is_flat-Tuple{SymmetricPositiveSemidefiniteFixedRank}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::SymmetricPositiveSemidefiniteFixedRank) Return false.  SymmetricPositiveSemidefiniteFixedRank  is not a flat manifold. See Theorem A.18 in [ MA20 ]. source"},{"id":1778,"pagetitle":"Symmetric positive semidefinite fixed rank","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/symmetricpsdfixedrank/#ManifoldsBase.manifold_dimension-Tuple{SymmetricPositiveSemidefiniteFixedRank}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::SymmetricPositiveSemidefiniteFixedRank) Return the dimension of the  SymmetricPositiveSemidefiniteFixedRank  matrix  M  over the number system  ùîΩ , i.e. \\[\\begin{aligned}\n\\dim \\operatorname{SPS}_{k,‚Ñù}(n) &= kn - \\frac{k(k-1)}{2},\\\\\n\\dim \\operatorname{SPS}_{k,‚ÑÇ}(n) &= 2kn - k^2,\n\\end{aligned}\\] where the last  $k^2$  is due to the zero imaginary part for Hermitian matrices diagonal source"},{"id":1779,"pagetitle":"Symmetric positive semidefinite fixed rank","title":"ManifoldsBase.vector_transport_to","ref":"/manifolds/stable/manifolds/symmetricpsdfixedrank/#ManifoldsBase.vector_transport_to-Tuple{SymmetricPositiveSemidefiniteFixedRank, Any, Any, Any, ProjectionTransport}","content":" ManifoldsBase.vector_transport_to  ‚Äî  Method vector_transport_to(M::SymmetricPositiveSemidefiniteFixedRank, p, X, q) transport the tangent vector  X  at  p  to  q  by projecting it onto the tangent space at  q . source"},{"id":1780,"pagetitle":"Symmetric positive semidefinite fixed rank","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/symmetricpsdfixedrank/#ManifoldsBase.zero_vector-Tuple{SymmetricPositiveSemidefiniteFixedRank, Vararg{Any}}","content":" ManifoldsBase.zero_vector  ‚Äî  Method  zero_vector(M::SymmetricPositiveSemidefiniteFixedRank, p) returns the zero tangent vector in the tangent space of the symmetric positive definite matrix  p  on the  SymmetricPositiveSemidefiniteFixedRank  manifold  M . source"},{"id":1783,"pagetitle":"Symplectic matrices","title":"Symplectic matrices","ref":"/manifolds/stable/manifolds/symplectic/#Symplectic-matrices","content":" Symplectic matrices The  SymplecticMatrices  manifold, denoted  $\\operatorname{Sp}(2n, ùîΩ)$ , is a closed, embedded, submanifold of  $ùîΩ^{2n√ó2n}$  that represents transformations into symplectic subspaces which keep the canonical symplectic form over  $ùîΩ^{2n√ó2n}$  invariant under the standard embedding inner product. The canonical symplectic form is a non-degenerate bilinear and skew symmetric map  $\\omega\\colon ùîΩ ùîΩ^{2n}√óùîΩ^{2n} ‚Üí ùîΩ$ , given by  $\\omega(x, y) = x^T Q_{2n} y$  for elements  $x, y \\in ùîΩ^{2n}$ , with \\[    Q_{2n} =\n    \\begin{bmatrix}\n     0_n  &  I_n \\\\\n    -I_n  &  0_n\n    \\end{bmatrix}.\\] That means that an element  $p \\in \\operatorname{Sp}(2n)$  must fulfill the requirement that \\[    \\omega (p x, p y) = x^T(p^TQp)y = x^TQy = \\omega(x, y),\\] leading to the requirement on  $p$  that  $p^TQp = Q$ . The symplectic manifold also forms a group under matrix multiplication, called the  $\\textit{symplectic group}$ . Since all the symplectic matrices necessarily have determinant one, the  symplectic group $\\operatorname{Sp}(2n, ùîΩ)$  is a subgroup of the special linear group,  $\\operatorname{SL}(2n, ùîΩ)$ . When the underlying field is either  $‚Ñù$  or  $‚ÑÇ$  the symplectic group with a manifold structure constitutes a Lie group, with the Lie Algebra \\[    \\mathfrak{sp}(2n,F) = \\{H \\in ùîΩ^{2n√ó2n} \\;|\\; Q H + H^{T} Q = 0\\}.\\] This set is also known as the  Hamiltonian matrices , which have the property that  $(QH)^T = QH$  and are commonly used in physics."},{"id":1784,"pagetitle":"Symplectic matrices","title":"Manifolds.ExtendedSymplecticMetric","ref":"/manifolds/stable/manifolds/symplectic/#Manifolds.ExtendedSymplecticMetric","content":" Manifolds.ExtendedSymplecticMetric  ‚Äî  Type ExtendedSymplecticMetric <: AbstractMetric The extension of the  RealSymplecticMetric  at a point  $p \\in \\mathrm{Sp}(2n)$  as an inner product over the embedding space  $‚Ñù^{2n√ó2n}$ , i.e. \\[    ‚ü®x, y‚ü©_p = ‚ü®p^{-1}x, p^{-1}‚ü©_{\\mathrm{Fr}}\n    = \\operatorname{tr}(x^{\\mathrm{T}}(pp^{\\mathrm{T}})^{-1}y), \\text{ for all } x, y \\in ‚Ñù^{2n√ó2n}.\\] source"},{"id":1785,"pagetitle":"Symplectic matrices","title":"Manifolds.RealSymplecticMetric","ref":"/manifolds/stable/manifolds/symplectic/#Manifolds.RealSymplecticMetric","content":" Manifolds.RealSymplecticMetric  ‚Äî  Type RealSymplecticMetric <: RiemannianMetric The canonical Riemannian metric on the symplectic manifold, defined pointwise for  $p \\in \\mathrm{Sp}(2n)$  by [ Fio11 ]] \\[\\begin{align*}\n  & g_p \\colon T_p\\mathrm{Sp}(2n)√óT_p\\mathrm{Sp}(2n) ‚Üí ‚Ñù, \\\\\n  & g_p(Z_1, Z_2) = \\operatorname{tr}((p^{-1}Z_1)^{\\mathrm{T}} (p^{-1}Z_2)).\n\\end{align*}\\] This metric is also the default metric for the  SymplecticMatrices  manifold. source"},{"id":1786,"pagetitle":"Symplectic matrices","title":"Manifolds.SymplecticElement","ref":"/manifolds/stable/manifolds/symplectic/#Manifolds.SymplecticElement","content":" Manifolds.SymplecticElement  ‚Äî  Type SymplecticElement{T} A lightweight structure to represent the action of the matrix representation of the canonical symplectic form, \\[J_{2n}(Œª) = Œª\\begin{bmatrix}\n0_n & I_n \\\\\n -I_n & 0_n\n\\end{bmatrix} ‚àà ‚Ñù^{2n√ó2n},\\] where we write  $J_{2n} = J_{2n}(1)$  for short. The canonical symplectic form is represented by \\[\\omega_{2n}(x, y) = x^{\\mathrm{T}}J_{2n}y, \\quad x, y ‚àà ‚Ñù^{2n}.\\] The entire matrix is however not instantiated in memory, instead a scalar  $Œª$  of type  T  is stored, which is used to keep track of scaling and transpose operations applied  to each  SymplecticElement . This type acts similar to  I  from  LinearAlgeba . Constructor SymplecticElement(Œª=1) Generate the sumplectic matrix with scaling  $1$ . source"},{"id":1787,"pagetitle":"Symplectic matrices","title":"Manifolds.SymplecticMatrices","ref":"/manifolds/stable/manifolds/symplectic/#Manifolds.SymplecticMatrices","content":" Manifolds.SymplecticMatrices  ‚Äî  Type SymplecticMatricesMatrices{T, ùîΩ} <: AbstractEmbeddedManifold{ùîΩ, DefaultIsometricEmbeddingType} The symplectic manifold consists of all  $2n√ó2n$  matrices which preserve the canonical symplectic form over  $ùîΩ^{2n√ó2n}√óùîΩ^{2n√ó2n}$ , \\[  \\omega\\colon ùîΩ^{2n√ó2n}√óùîΩ^{2n√ó2n} ‚Üí ùîΩ,\n  \\quad \\omega(x, y) = p^{\\mathrm{T}} J_{2n} q, \\  x, y \\in ùîΩ^{2n√ó2n},\\] where  $J_{2n} = \\begin{bmatrix} 0_n & I_n \\\\ -I_n & 0_n \\end{bmatrix}$  denotes the  SymplecticElement . The symplectic manifold consists of \\[\\mathrm{Sp}(2n, ‚Ñù) = \\bigl\\{ p ‚àà ‚Ñù^{2n√ó2n} \\, \\big| \\, p^{\\mathrm{T}}J_{2n}p = J_{2n} \\bigr\\},\\] The tangent space at a point  $p$  is given by [ BZ21 ] \\[\\begin{align*}\n  T_p\\mathrm{Sp}(2n)\n    &= \\{X \\in ‚Ñù^{2n√ó2n} \\ |\\ p^{T}J_{2n}X + X^{T}J_{2n}p = 0 \\}, \\\\\n    &= \\{X = pJ_{2n}S \\ \\mid\\ S ‚àà R^{2n√ó2n}, S^{\\mathrm{T}} = S \\}.\n\\end{align*}\\] Constructor SymplecticMatrices(2n, field=‚Ñù; parameter::Symbol=:type) Generate the (real-valued) symplectic manifold of  $2n√ó2n$  symplectic matrices. The constructor for the  SymplecticMatrices  manifold accepts the even column/row embedding dimension  $2n$  for the real symplectic manifold,  $‚Ñù^{2n√ó2n}$ . source"},{"id":1788,"pagetitle":"Symplectic matrices","title":"Base.exp","ref":"/manifolds/stable/manifolds/symplectic/#Base.exp-Tuple{SymplecticMatrices, Any, Any}","content":" Base.exp  ‚Äî  Method exp(M::SymplecticMatrices, p, X)\nexp!(M::SymplecticMatrices, q, p, X) The Exponential mapping on the Symplectic manifold with the  RealSymplecticMetric  Riemannian metric. For the point  $p \\in \\mathrm{Sp}(2n)$  the exponential mapping along the tangent vector  $X \\in T_p\\mathrm{Sp}(2n)$  is computed as [ WSF18 ] \\[    \\operatorname{exp}_p(X) = p \\operatorname{Exp}((p^{-1}X)^{\\mathrm{T}})\n                                \\operatorname{Exp}(p^{-1}X - (p^{-1}X)^{\\mathrm{T}}),\\] where  $\\operatorname{Exp}(‚ãÖ)$  denotes the matrix exponential. source"},{"id":1789,"pagetitle":"Symplectic matrices","title":"Base.inv","ref":"/manifolds/stable/manifolds/symplectic/#Base.inv-Tuple{SymplecticMatrices{<:Any, ‚Ñù}, Any}","content":" Base.inv  ‚Äî  Method inv(::SymplecticMatrices, A)\ninv!(::SymplecticMatrices, A) Compute the symplectic inverse  $A^+$  of matrix  $A ‚àà ‚Ñù^{2n√ó2n}$ . See  symplectic_inverse  for details. source"},{"id":1790,"pagetitle":"Symplectic matrices","title":"Base.rand","ref":"/manifolds/stable/manifolds/symplectic/#Base.rand-Tuple{Any}","content":" Base.rand  ‚Äî  Method rand(::SymplecticStiefel; vector_at=nothing, œÉ::Real=1.0) Generate a random point on  $\\mathrm{Sp}(2n)$  or a random tangent vector  $X \\in T_p\\mathrm{Sp}(2n)$  if  vector_at  is set to a point  $p \\in \\mathrm{Sp}(2n)$ . A random point on  $\\mathrm{Sp}(2n)$  is constructed by generating a random Hamiltonian matrix  $Œ© \\in \\mathfrak{sp}(2n,F)$  with norm  œÉ , and then transforming it to a symplectic matrix by applying the Cayley transform \\[  \\operatorname{cay}: \\mathfrak{sp}(2n,F) ‚Üí \\mathrm{Sp}(2n),\n  \\ \\Omega \\mapsto (I - \\Omega)^{-1}(I + \\Omega).\\] To generate a random tangent vector in  $T_p\\mathrm{Sp}(2n)$ , this code employs the second tangent vector space parametrization of  SymplecticMatrices . It first generates a random symmetric matrix  $S$  by  S = randn(2n, 2n)  and then symmetrizes it as  S = S + S' . Then  $S$  is normalized to have Frobenius norm of  œÉ  and  X = pJS  is returned, where  J  is the  SymplecticElement . source"},{"id":1791,"pagetitle":"Symplectic matrices","title":"ManifoldDiff.gradient","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldDiff.gradient-Tuple{SymplecticMatrices, Any, Any, ManifoldDiff.RiemannianProjectionBackend}","content":" ManifoldDiff.gradient  ‚Äî  Method gradient(M::SymplecticMatrices, f, p, backend::RiemannianProjectionBackend;\n         extended_metric=true)\ngradient!(M::SymplecticMatrices, f, p, backend::RiemannianProjectionBackend;\n         extended_metric=true) Compute the manifold gradient  $\\text{grad}f(p)$  of a scalar function  $f \\colon \\mathrm{Sp}(2n) ‚Üí ‚Ñù$  at  $p \\in \\mathrm{Sp}(2n)$ . The element  $\\text{grad}f(p)$  is found as the Riesz representer of the differential  $\\text{D}f(p) \\colon T_p\\mathrm{Sp}(2n) ‚Üí ‚Ñù$  with respect to the Riemannian metric inner product at  $p$  [ Fio11 ]]. That is,  $\\text{grad}f(p) \\in T_p\\mathrm{Sp}(2n)$  solves the relation \\[    g_p(\\text{grad}f(p), X) = \\text{D}f(p) \\quad\\forall\\; X \\in T_p\\mathrm{Sp}(2n).\\] The default behaviour is to first change the representation of the Euclidean gradient from the Euclidean metric to the  RealSymplecticMetric  at  $p$ , and then we projecting the result onto the correct tangent tangent space  $T_p\\mathrm{Sp}(2n, ‚Ñù)$  w.r.t the Riemannian metric  $g_p$  extended to the entire embedding space. Arguments: extended_metric = true : If  true , compute the gradient  $\\text{grad}f(p)$  by   first changing the representer of the Euclidean gradient of a smooth extension   of  $f$ ,  $‚àáf(p)$ , with respect to the  RealSymplecticMetric  at  $p$    extended to the entire embedding space, before projecting onto the correct   tangent vector space with respect to the same extended metric  $g_p$ .   If  false , compute the gradient by first projecting  $‚àáf(p)$  onto the   tangent vector space, before changing the representer in the tangent   vector space to comply with the  RealSymplecticMetric . source"},{"id":1792,"pagetitle":"Symplectic matrices","title":"ManifoldDiff.riemannian_gradient","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldDiff.riemannian_gradient-Tuple{SymplecticMatrices, Any, Any}","content":" ManifoldDiff.riemannian_gradient  ‚Äî  Method riemannian_gradient(M::SymplecticMatrices, p, Y) Given a gradient  $Y = \\operatorname{grad} \\tilde f(p)$  in the embedding  $‚Ñù^{2n√ó2n}$  or at least around the  SymplecticMatrices M  where  p  (the embedding of) a point on  M , we restrict  $\\tilde f$  to the manifold and denote that by  $f$ . Then the Riemannian gradient  $X = \\operatorname{grad} f(p)$  is given by \\[  X = Yp^{\\mathrm{T}}p + J_{2n}pY^{\\mathrm{T}}J_{2n}p,\\] where  $J_{2n}$  denotes the  SymplecticElement . source"},{"id":1793,"pagetitle":"Symplectic matrices","title":"Manifolds.inv!","ref":"/manifolds/stable/manifolds/symplectic/#Manifolds.inv!-Tuple{SymplecticMatrices{<:Any, ‚Ñù}, Any}","content":" Manifolds.inv!  ‚Äî  Method inv!(M::SymplecticMatrices, A) Compute the  symplectic_inverse  of a suqare matrix A inplace of A source"},{"id":1794,"pagetitle":"Symplectic matrices","title":"Manifolds.project_normal!","ref":"/manifolds/stable/manifolds/symplectic/#Manifolds.project_normal!-Union{Tuple{ùîΩ}, Tuple{MetricManifold{ùîΩ, <:Euclidean, ExtendedSymplecticMetric}, Any, Any, Any}} where ùîΩ","content":" Manifolds.project_normal!  ‚Äî  Method project_normal!(::MetricManifold{ùîΩ,<:Euclidean,ExtendedSymplecticMetric}, Y, p, X) Project onto the normal of the tangent space  $(T_p\\mathrm{Sp}(2n))^{\\perp_g}$  at a point  $p ‚àà \\mathrm{Sp}(2n)$ , relative to the riemannian metric  $g$ RealSymplecticMetric . That is, \\[(T_p\\mathrm{Sp}(2n))^{\\perp_g}\n = \\{Y ‚àà ‚Ñù^{2n√ó2n} : g_p(Y, X) = 0 \\test{ for all } X \\in T_p\\mathrm{Sp}(2n)\\}.\\] The closed form projection operator onto the normal space is given by [ GSAS21 ] \\[\\operatorname{P}^{(T_p\\mathrm{Sp}(2n))\\perp}_{g_p}(X) = pJ_{2n}\\operatorname{skew}(p^{\\mathrm{T}}J_{2n}^{\\mathrm{T}}X),\\] where  $\\operatorname{skew}(A) = \\frac{1}{2}(A - A^{\\mathrm{T}})$  and  $J_{2n} = \\begin{bmatrix} 0_n & I_n \\\\ -I_n & 0_n \\end{bmatrix}$  denotes the  SymplecticElement . This function is not exported. source"},{"id":1795,"pagetitle":"Symplectic matrices","title":"Manifolds.symplectic_inverse","ref":"/manifolds/stable/manifolds/symplectic/#Manifolds.symplectic_inverse-Tuple{AbstractMatrix}","content":" Manifolds.symplectic_inverse  ‚Äî  Method symplectic_inverse(A) Given a matrix \\[  A ‚àà ‚Ñù^{2n√ó2k},\\quad\n  A =\n  \\begin{bmatrix}\n  A_{1,1} & A_{1,2} \\\\\n  A_{2,1} & A_{2, 2}\n  \\end{bmatrix}\\] the symplectic inverse is defined as: \\[A^{+} := J_{2k}^{\\mathrm{T}} A^{\\mathrm{T}} J_{2n},\\] where  $J_{2n} = \\begin{bmatrix} 0_n & I_n \\\\ -I_n & 0_n \\end{bmatrix}$  denotes the  SymplecticElement . The symplectic inverse of A can be expressed explicitly as: \\[A^{+} =\n  \\begin{bmatrix}\n    A_{2, 2}^{\\mathrm{T}} & -A_{1, 2}^{\\mathrm{T}} \\\\[1.2mm]\n   -A_{2, 1}^{\\mathrm{T}} &  A_{1, 1}^{\\mathrm{T}}\n  \\end{bmatrix}.\\] source"},{"id":1796,"pagetitle":"Symplectic matrices","title":"Manifolds.symplectic_inverse_times","ref":"/manifolds/stable/manifolds/symplectic/#Manifolds.symplectic_inverse_times-Tuple{SymplecticMatrices, Any, Any}","content":" Manifolds.symplectic_inverse_times  ‚Äî  Method symplectic_inverse_times(::SymplecticMatrices, p, q)\nsymplectic_inverse_times!(::SymplecticMatrices, A, p, q) Directly compute the symplectic inverse of  $p \\in \\mathrm{Sp}(2n)$ , multiplied with  $q \\in \\mathrm{Sp}(2n)$ . That is, this function efficiently computes  $p^+q = (J_{2n}p^{\\mathrm{T}}J_{2n})q ‚àà ‚Ñù^{2n√ó2n}$ , where  $J_{2n} = \\begin{bmatrix} 0_n & I_n \\\\ -I_n & 0_n \\end{bmatrix}$  denotes the  SymplecticElement . source"},{"id":1797,"pagetitle":"Symplectic matrices","title":"ManifoldsBase.change_representer","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldsBase.change_representer-Tuple{MetricManifold{<:Any, <:Euclidean, ExtendedSymplecticMetric}, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(MetMan::MetricManifold{<:Any, <:Euclidean, ExtendedSymplecticMetric},\n                   EucMet::EuclideanMetric, p, X)\nchange_representer!(MetMan::MetricManifold{<:Any, <:Euclidean, ExtendedSymplecticMetric},\n                    Y, EucMet::EuclideanMetric, p, X) Change the representation of a matrix  $Œæ ‚àà ‚Ñù^{2n√ó2n}$  into the inner product space  $(‚Ñù^{2n√ó2n}, g_p)$  where the inner product is given by  $g_p(Œæ, Œ∑) = \\langle p^{-1}Œæ, p^{-1}Œ∑ \\rangle = \\operatorname{tr}(Œæ^{\\mathrm{T}}(pp^{\\mathrm{T}})^{-1}Œ∑)$ , as the extension of the  RealSymplecticMetric  onto the entire embedding space. By changing the representation we mean to apply a mapping \\[    c_p : ‚Ñù^{2n√ó2n} ‚Üí ‚Ñù^{2n√ó2n},\\] defined by requiring that it satisfy the metric compatibility condition \\[    g_p(c_p(Œæ), Œ∑) = ‚ü®p^{-1}c_p(Œæ), p^{-1}Œ∑‚ü© = ‚ü®Œæ, Œ∑‚ü©^{\\text{Euc}}\n        \\;‚àÄ\\; Œ∑ ‚àà T_p\\mathrm{Sp}(2n, ‚Ñù).\\] In this case, we compute the mapping \\[    c_p(Œæ) = pp^{\\mathrm{T}} Œæ.\\] source"},{"id":1798,"pagetitle":"Symplectic matrices","title":"ManifoldsBase.change_representer","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldsBase.change_representer-Tuple{SymplecticMatrices, EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method change_representer(::SymplecticMatrices, ::EuclideanMetric, p, X)\nchange_representer!(::SymplecticMatrices, Y, ::EuclideanMetric, p, X) Compute the representation of a tangent vector  $Œæ ‚àà T_p\\mathrm{Sp}(2n, ‚Ñù)$  s.t. \\[  g_p(c_p(Œæ), Œ∑) = ‚ü®Œæ, Œ∑‚ü©^{\\text{Euc}} \\text{for all } Œ∑ ‚àà T_p\\mathrm{Sp}(2n, ‚Ñù).\\] with the conversion function \\[  c_p : T_p\\mathrm{Sp}(2n, ‚Ñù) ‚Üí T_p\\mathrm{Sp}(2n, ‚Ñù), \\quad\n  c_p(Œæ) = \\frac{1}{2} pp^{\\mathrm{T}} Œæ + \\frac{1}{2} pJ_{2n} Œæ^{\\mathrm{T}} pJ_{2n},\\] where  $J_{2n} = \\begin{bmatrix} 0_n & I_n \\\\ -I_n & 0_n \\end{bmatrix}$  denotes the  SymplecticElement . Each of the terms  $c_p^1(Œæ) = p p^{\\mathrm{T}} Œæ$  and  $c_p^2(Œæ) = pJ_{2n} Œæ^{\\mathrm{T}} pJ_{2n}$  from the above definition of  $c_p(Œ∑)$  are themselves metric compatible in the sense that \\[    c_p^i : T_p\\mathrm{Sp}(2n, ‚Ñù) ‚Üí ‚Ñù^{2n√ó2n}\\quad\n    g_p^i(c_p(Œæ), Œ∑) = ‚ü®Œæ, Œ∑‚ü©^{\\text{Euc}} \\;‚àÄ\\; Œ∑ ‚àà T_p\\mathrm{Sp}(2n, ‚Ñù),\\] for  $i \\in {1, 2}$ . However the range of each function alone is not confined to    $T_p\\mathrm{Sp}(2n, ‚Ñù)$ , but the convex combination \\[    c_p(Œæ) = \\frac{1}{2}c_p^1(Œæ) + \\frac{1}{2}c_p^2(Œæ)\\] does have the correct range  $T_p\\mathrm{Sp}(2n, ‚Ñù)$ . source"},{"id":1799,"pagetitle":"Symplectic matrices","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldsBase.check_point-Union{Tuple{T}, Tuple{SymplecticMatrices, T}} where T","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::SymplecticMatrices, p; kwargs...) Check whether  p  is a valid point on the  SymplecticMatrices M = $\\mathrm{Sp}(2n)$ , i.e. that it has the right  AbstractNumbers  type and  $p^{+}p$  is (approximately) the identity, where  $A^+$  denotes the  symplectic_inverse . The tolerance can be set with  kwargs... . source"},{"id":1800,"pagetitle":"Symplectic matrices","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldsBase.check_vector-Tuple{SymplecticMatrices, Vararg{Any}}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::SymplecticMatrices, p, X; kwargs...) Checks whether  X  is a valid tangent vector at  p  on the  SymplecticMatrices M = $\\mathrm{Sp}(2n)$ , which requires that \\[p^{T}J_{2n}X + X^{T}J_{2n}p = 0\\] holds (approximately), where  $J_{2n} = \\begin{bmatrix} 0_n & I_n \\\\ -I_n & 0_n \\end{bmatrix}$  denotes the  SymplecticElement . The tolerance can be set with  kwargs... source"},{"id":1801,"pagetitle":"Symplectic matrices","title":"ManifoldsBase.distance","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldsBase.distance-Tuple{SymplecticMatrices, Any, Any}","content":" ManifoldsBase.distance  ‚Äî  Method distance(M::SymplecticMatrices, p, q) Compute an approximate geodesic distance between two Symplectic matrices  $p, q \\in \\mathrm{Sp}(2n)$ , as done in [ WSF18 ]. \\[  \\operatorname{dist}(p, q)\n    ‚âà \\lVert\\operatorname{Log}(p^+q)\\rVert_{\\mathrm{Fr}},\\] where the  $\\operatorname{Log}(‚ãÖ)$  operator is the matrix logarithm. This approximation is justified by first recalling the Baker-Campbell-Hausdorf formula, \\[\\operatorname{Log}(\\operatorname{Exp}(A)\\operatorname{Exp}(B))\n = A + B + \\frac{1}{2}[A, B] + \\frac{1}{12}[A, [A, B]] + \\frac{1}{12}[B, [B, A]]\n    + \\ldots \\;.\\] Then we write the expression for the exponential map from  $p$  to  $q$  as \\[    q =\n    \\operatorname{exp}_p(X)\n    =\n    p \\operatorname{Exp}((p^{+}X)^{\\mathrm{T}})\n    \\operatorname{Exp}([p^{+}X - (p^{+}X)^{\\mathrm{T}}]),\n    X \\in T_p\\mathrm{Sp},\\] and with the geodesic distance between  $p$  and  $q$  given by \\[\\operatorname{dist}(p, q) = \\lVert X \\rVert_p = \\lVert p^+ X \\rVert_{\\mathrm{Fr}}\\] we see that \\[  \\begin{align*}\n   \\lVert\\operatorname{Log}(p^+q)\\rVert_{\\mathrm{Fr}}\n    &=\\Bigl\\lVert\n        \\operatorname{Log}\\bigl(\n            \\operatorname{Exp}((p^{+}X)^{\\mathrm{T}})\n            \\operatorname{Exp}(p^{+}X - (p^{+}X)^{\\mathrm{T}})\n        \\bigr)\n    \\Bigr\\rVert_{\\mathrm{Fr}} \\\\\n    &=\\lVert p^{+}X + \\frac{1}{2}[(p^{+}X)^{\\mathrm{T}}, p^{+}X - (p^{+}X)^{\\mathrm{T}}]\n        + \\ldots\\lVert_{\\mathrm{Fr}} \\\\\n    &‚âà\\lVert p^{+}X\\rVert_{\\mathrm{Fr}} = \\operatorname{dist}(p, q).\n  \\end{align*}\\] source"},{"id":1802,"pagetitle":"Symplectic matrices","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldsBase.inner-Tuple{SymplecticMatrices{<:Any, ‚Ñù}, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(::SymplecticMatrices{<:Any,‚Ñù}, p, X, Y) Compute the canonical Riemannian inner product  RealSymplecticMetric \\[    g_p(X, Y) = \\operatorname{tr}((p^{-1}X)^{\\mathrm{T}} (p^{-1}Y))\\] between the two tangent vectors  $X, Y \\in T_p\\mathrm{Sp}(2n)$ . source"},{"id":1803,"pagetitle":"Symplectic matrices","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldsBase.inverse_retract-Tuple{SymplecticMatrices, Any, Any, CayleyInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::SymplecticMatrices, p, q, ::CayleyInverseRetraction) Compute the Cayley Inverse Retraction  $X = \\mathcal{L}_p^{\\mathrm{Sp}}(q)$  such that the Cayley Retraction from  $p$  along  $X$  lands at  $q$ , i.e.  $\\mathcal{R}_p(X) = q$  [ BZ21 ]. For  $p, q ‚àà \\mathrm{Sp}(2n, ‚Ñù)$  then, we can define the inverse cayley retraction as long as the following matrices exist. \\[    U = (I + p^+ q)^{-1}, \\quad V = (I + q^+ p)^{-1},\\] where  $(‚ãÖ)^+$  denotes the  symplectic_inverse . Then inverse cayley retraction at  $p$  applied to  $q$  is \\[\\mathcal{L}_p^{\\mathrm{Sp}}(q)\n  = 2p\\bigl(V - U\\bigr) + 2\\bigl((p + q)U - p\\bigr) ‚àà T_p\\mathrm{Sp}(2n).\\] source"},{"id":1804,"pagetitle":"Symplectic matrices","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldsBase.is_flat-Tuple{SymplecticMatrices}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::SymplecticMatrices) Return false.  SymplecticMatrices  is not a flat manifold. source"},{"id":1805,"pagetitle":"Symplectic matrices","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldsBase.manifold_dimension-Tuple{SymplecticMatrices}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(::SymplecticMatrices) Returns the dimension of the symplectic manifold embedded in  $‚Ñù^{2n√ó2n}$ , i.e. \\[  \\operatorname{dim}(\\mathrm{Sp}(2n)) = (2n + 1)n.\\] source"},{"id":1806,"pagetitle":"Symplectic matrices","title":"ManifoldsBase.project!","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldsBase.project!-Tuple{MetricManifold{<:Any, <:Euclidean, ExtendedSymplecticMetric}, Any, Any, Any}","content":" ManifoldsBase.project!  ‚Äî  Method project!(::MetricManifold{ùîΩ,<:Euclidean,ExtendedSymplecticMetric}, Y, p, X) where {ùîΩ} Compute the projection of  $X ‚àà R^{2n√ó2n}$  onto  $T_p\\mathrm{Sp}(2n, ‚Ñù)$  with respect to the  RealSymplecticMetric $g$ . The closed form projection mapping is given by [ GSAS21 ] \\[  \\operatorname{P}^{T_p\\mathrm{Sp}(2n)}_{g_p}(X) = pJ_{2n}\\operatorname{sym}(p^{\\mathrm{T}}J_{2n}^{\\mathrm{T}}X),\\] where  $\\operatorname{sym}(A) = \\frac{1}{2}(A + A^{\\mathrm{T}})$  and and  $J_{2n} = \\begin{bmatrix} 0_n & I_n \\\\ -I_n & 0_n \\end{bmatrix}$  denotes the  SymplecticElement . source"},{"id":1807,"pagetitle":"Symplectic matrices","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldsBase.project-Tuple{SymplecticMatrices, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(::SymplecticMatrices, p, A)\nproject!(::SymplecticMatrices, Y, p, A) Given a point  $p \\in \\mathrm{Sp}(2n)$ , project an element  $A \\in ‚Ñù^{2n√ó2n}$  onto the tangent space  $T_p\\mathrm{Sp}(2n)$  relative to the euclidean metric of the embedding  $‚Ñù^{2n√ó2n}$ . That is, we find the element  $X \\in T_p\\operatorname{Sp}(2n)$  which solves the constrained optimization problem \\[    \\operatorname{min}_{X \\in ‚Ñù^{2n√ó2n}} \\frac{1}{2}\\lVert X - A\\rVert^2, \\quad\n    \\text{such that}\\;\n    h(X) := X^{\\mathrm{T}} J_{2n} p + p^{\\mathrm{T}} J_{2n} X = 0,\\] where  $h: ‚Ñù^{2n√ó2n} ‚Üí \\operatorname{skew}(2n)$  denotes the restriction of  $X$  onto the tangent space  $T_p\\operatorname{SpSt}(2n, 2k)$  and  $J_{2n} = \\begin{bmatrix} 0_n & I_n \\\\ -I_n & 0_n \\end{bmatrix}$  denotes the  SymplecticElement . source"},{"id":1808,"pagetitle":"Symplectic matrices","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/symplectic/#ManifoldsBase.retract-Tuple{SymplecticMatrices, Any, Any}","content":" ManifoldsBase.retract  ‚Äî  Method retract(::SymplecticMatrices, p, X, ::CayleyRetraction)\nretract!(::SymplecticMatrices, q, p, X, ::CayleyRetraction) Compute the Cayley retraction on  $p ‚àà \\mathrm{Sp}(2n, ‚Ñù)$  in the direction of tangent vector  $X ‚àà T_p\\mathrm{Sp}(2n, ‚Ñù)$ , as defined in by Birtea et al in proposition 2 [ BCC20 ]. Using the  symplectic_inverse $A^+$  of a matrix  $A \\in ‚Ñù^{2n√ó2n}$  the retraction  $\\mathcal{R}: T\\mathrm{Sp}(2n) ‚Üí \\mathrm{Sp}(2n)$  is defined pointwise as \\[\\begin{align*}\n\\mathcal{R}_p(X) &= p \\operatorname{cay}\\left(\\frac{1}{2}p^{+}X\\right), \\\\\n                 &= p \\operatorname{exp}_{1/1}(p^{+}X), \\\\\n                 &= p (2I - p^{+}X)^{-1}(2I + p^{+}X).\n\\end{align*}\\] Here  $\\operatorname{exp}_{1/1}(z) = (2 - z)^{-1}(2 + z)$  denotes the Pad√© (1, 1) approximation to  $\\operatorname{exp}(z)$ . source"},{"id":1809,"pagetitle":"Symplectic matrices","title":"Literature","ref":"/manifolds/stable/manifolds/symplectic/#Literature","content":" Literature [BZ21] T.¬†Bendokat and R.¬†Zimmermann.  The real symplectic Stiefel and Grassmann manifolds: metrics, geodesics and applications , arXiv¬†Preprint,¬†2108.12447 (2021),  arXiv:2108.12447 . [BCC20] P.¬†Birtea, I.¬†Ca√ßu and D.¬†ComƒÉnescu.  Optimization on the real symplectic group .  Monatshefte¬†f√ºr¬†Mathematik  191 , 465‚Äì485  (2020). [Fio11] S.¬†Fiori.  Solving Minimal-Distance Problems over the Manifold of Real-Symplectic Matrices .  SIAM¬†Journal¬†on¬†Matrix¬†Analysis¬†and¬†Applications  32 , 938‚Äì968  (2011). [GSAS21] B.¬†Gao, N.¬†T.¬†Son, P.-A.¬†Absil and T.¬†Stykel.  Riemannian Optimization on the Symplectic Stiefel Manifold .  SIAM¬†Journal¬†on¬†Optimization  31 , 1546‚Äì1575  (2021). [WSF18] J.¬†Wang, H.¬†Sun and S.¬†Fiori.  A Riemannian-steepest-descent approach for optimization on the real symplectic group .  Mathematical¬†Methods¬†in¬†the¬†Applied¬†Science  41 , 4273‚Äì4286  (2018)."},{"id":1812,"pagetitle":"Symplectic Grassmann","title":"(Real) Symplectic Grassmann","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#(Real)-Symplectic-Grassmann","content":" (Real) Symplectic Grassmann"},{"id":1813,"pagetitle":"Symplectic Grassmann","title":"Manifolds.SymplecticGrassmann","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#Manifolds.SymplecticGrassmann","content":" Manifolds.SymplecticGrassmann  ‚Äî  Type SymplecticGrassmann{T,ùîΩ} <: AbstractEmbeddedManifold{ùîΩ, DefaultIsometricEmbeddingType} The symplectic Grassmann manifold consists of all symplectic subspaces of  $‚Ñù^{2n}$  of dimension  $2k$ ,  $n ‚â• k$ . Points on this manifold can be represented as corresponding representers on the  SymplecticStiefel \\[\\operatorname{SpGr}(2n,2k) = \\bigl\\{ \\operatorname{span}(p)\\ \\big| \\ p ‚àà \\operatorname{SpSt}(2n, 2k, ‚Ñù)\\},\\] or as projectors \\[\\operatorname{SpGr}(2n, 2k, ‚Ñù) = \\bigl\\{ p ‚àà ‚Ñù^{2n√ó2n} \\ \\big| \\ p^2 = p, \\operatorname{rank}(p) = 2k, p^+=p \\bigr\\},\\] where  $‚ãÖ^+$  is the  symplectic_inverse . See also  ProjectorPoint  and  StiefelPoint  for these two representations, where arrays are interpreted as those on the Stiefel manifold. With respect to the quotient structure, the canonical projection  $œÄ = œÄ_{\\mathrm{SpSt},\\mathrm{SpGr}}$  is given by \\[œÄ: \\mathrm{SpSt}(2n2k) ‚Üí \\mathrm{SpGr}(2n,2k), p ‚Ü¶ œÄ(p) = pp^+.\\] The tangent space is either the tangent space from the symplectic Stiefel manifold, where tangent vectors are representers of their corresponding congruence classes, or for the representation as projectors, using a  ProjectorTangentVector  as \\[  T_p\\operatorname{SpGr}(2n, 2k, ‚Ñù) =\n  \\bigl\\{ [X,p] \\ \\mid\\ X ‚àà \\mathfrak{sp}(2n,‚Ñù), Xp+pX = X \\bigr\\},\\] where  $[X,p] = Xp-pX$  denotes the matrix commutator and  $\\mathfrak{sp}(2n,‚Ñù)$  is the Lie algebra of the symplectic group consisting of  HamiltonianMatrices . The first representation is in  StiefelPoint s and  StiefelTangentVector s, which both represent their symplectic Grassmann equivalence class. Arrays are interpreted in this representation as well For the representation in  ProjectorPoint  and  ProjectorTangentVector s, we use the representation from the surjective submersion \\[œÅ: \\mathrm{SpSt}(2n,2k) ‚Üí \\mathrm{SpGr}(2n,2k),\n\\qquad\nœÅ(p) = pp^+\\] and its differential \\[\\mathrm{d}œÅ(p,X) = Xp^+ + pX^+,\\] respectively. The manifold was first introduced in [ BZ21 ] Constructor SymplecticGrassmann(2n::Int, 2k::Int, field::AbstractNumbers=‚Ñù; parameter::Symbol=:type) Generate the (real-valued) symplectic Grassmann manifold. of   $2k$  dimensional symplectic subspace of  $‚Ñù^{2n}$ . Note that both dimensions passed to this constructor have to be even. source"},{"id":1814,"pagetitle":"Symplectic Grassmann","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#ManifoldsBase.manifold_dimension-Tuple{SymplecticGrassmann{<:Any, ‚Ñù}}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(::SymplecticGrassmann) Return the dimension of the  SymplecticGrassmann (2n,2k) , which is \\[\\operatorname{dim}\\operatorname{SpGr}(2n, 2k) = 4(n-k)k,\\] see [ BZ21 ], Section 4. source"},{"id":1815,"pagetitle":"Symplectic Grassmann","title":"The (default) symplectic Stiefel representation","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#The-(default)-symplectic-Stiefel-representation","content":" The (default) symplectic Stiefel representation"},{"id":1816,"pagetitle":"Symplectic Grassmann","title":"Base.exp","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#Base.exp-Tuple{SymplecticGrassmann, Any, Any}","content":" Base.exp  ‚Äî  Method exp(::SymplecticGrassmann, p, X)\nexp!(M::SymplecticGrassmann, q, p, X) Compute the exponential mapping \\[  \\exp\\colon T\\mathrm{SpGr}(2n, 2k) ‚Üí \\mathrm{SpGr}(2n, 2k)\\] when representing points and tangent vectors as symplectic bases and their tangents, i.e. on the  SymplecticStiefel  manifold. Then we can just pass this on to  exp(::SymplecticStiefel, p, X) . source"},{"id":1817,"pagetitle":"Symplectic Grassmann","title":"ManifoldDiff.riemannian_gradient","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#ManifoldDiff.riemannian_gradient-Tuple{SymplecticGrassmann, Any, Any}","content":" ManifoldDiff.riemannian_gradient  ‚Äî  Method riemannian_gradient(M::SymplecticGrassmann, p, Y) Given a gradient  $Y = \\operatorname{grad} \\tilde f(p)$  in the embedding  $‚Ñù^{2n√ó2k}$  or at least around the  SymplecticGrassmann M  where  p  (the embedding of) a point on  M , and the restriction  $\\tilde f$  to the  SymplecticStiefel  be invariant for the equivalence classes. In other words  $f(p) = f(qp)$  for  $q \\in \\mathrm{Sp}(2k, ‚Ñù)$ , where  $\\mathrm{Sp}(2k, ‚Ñù)$  denotes the  SymplecticMatrices  manifold. Then the Riemannian gradient  $X = \\operatorname{grad} f(p)$  is given by \\[  X = J_{2n}^THJ_{2k}p^{\\mathrm{T}}p - J_{2n}^TpJ_{2k}H^{\\mathrm{T}}p,\\] where  $J_{2n}$  denotes the  SymplecticElement , and  $H = (I_{2n} - pp^+)J_{2n}^{\\mathrm{T}}YJ$ . source"},{"id":1818,"pagetitle":"Symplectic Grassmann","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#ManifoldsBase.check_point-Tuple{SymplecticGrassmann, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::SymplecticGrassmann, p; kwargs...) Check whether  p  is a valid point on the  SymplecticGrassmann ,  $\\operatorname{SpGr}(2n, 2k)$  manifold by verifying that it is a valid representer of an equivalence class of the corersponding  SymplecticStiefel  manifold. source"},{"id":1819,"pagetitle":"Symplectic Grassmann","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#ManifoldsBase.check_vector-Tuple{SymplecticGrassmann, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::SymplecticGrassmann, p, X; kwargs...) Check whether  X  is a valid tangent vector at  p  on the  SymplecticGrassmann ,  $\\operatorname{SpGr}(2n, 2k)$  manifold by verifying that it is a valid representer of an equivalence class of the corersponding  SymplecticStiefel  manifolds tangent space at  p . source"},{"id":1820,"pagetitle":"Symplectic Grassmann","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#ManifoldsBase.inner-Tuple{SymplecticGrassmann, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(::SymplecticGrassmann, p, X, Y) Compute the Riemannian inner product  $g^{\\mathrm{SpGr}}_p(X,Y)$  on the  SymplecticGrassmann  manifold  \\mathrm{SpGr} `. For the case where  $p$  is represented by a point on the  SymplecticStiefel  manifold acting as a representant of its equivalence class  $[p] \\in \\mathrm{SpGr}$  and the tangent vectors  $X,Y \\in \\mathrm{Hor}_p^œÄ\\operatorname{SpSt}(2n,2k)$  are horizontal tangent vectors. Then the inner product reads according to Proposition Lemma 4.8 [ BZ21 ]. \\[g^{\\mathrm{SpGr}}_p(X,Y) = \\operatorname{tr}\\bigl(\n        (p^{\\mathrm{T}}p)^{-1}X^{\\mathrm{T}}(I_{2n} - pp^+)Y\n    \\bigr),\\] where  $I_{2n}$  denotes the identity matrix and  $(‚ãÖ)^+$  the  symplectic_inverse . source"},{"id":1821,"pagetitle":"Symplectic Grassmann","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#ManifoldsBase.inverse_retract-Tuple{SymplecticGrassmann, Any, Any, CayleyInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(::SymplecticGrassmann, p, q, ::CayleyInverseRetraction)\ninverse_retract!(::SymplecticGrassmann, X, p, q, ::CayleyInverseRetraction) Compute the Cayley Inverse Retraction on the Symplectic Grassmann manifold, when the points are represented as symplectic bases, i.e. on the  SymplecticStiefel . Here we can directly employ the  CaleyInverseRetraction  on the symplectic Stiefel manifold. source"},{"id":1822,"pagetitle":"Symplectic Grassmann","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#ManifoldsBase.retract-Tuple{SymplecticGrassmann, Any, Any, CayleyRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(::SymplecticGrassmann, p, X, ::CayleyRetraction)\nretract!(::SymplecticGrassmann, q, p, X, ::CayleyRetraction) Compute the Cayley retraction on the Symplectic Grassmann manifold, when the points are represented as symplectic bases, i.e. on the  SymplecticStiefel . Here we can directly employ the  CaleyRetraction  on the symplectic Stiefel manifold. source"},{"id":1823,"pagetitle":"Symplectic Grassmann","title":"The symplectic projector representation","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#The-symplectic-projector-representation","content":" The symplectic projector representation"},{"id":1824,"pagetitle":"Symplectic Grassmann","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#ManifoldsBase.check_point-Tuple{SymplecticGrassmann, ProjectorPoint}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::SymplecticGrassmann, p::ProjectorPoint; kwargs...) Check whether  p  is a valid point on the  SymplecticGrassmann ,  $\\operatorname{SpGr}(2n, 2k)$ , that is a proper symplectic projection: $p^2 = p$ , that is  $p$  is a projection $\\operatorname{rank}(p) = 2k$ , that is, the supspace projected onto is of right dimension $p^+ = p$  the projection is symplectic. source"},{"id":1825,"pagetitle":"Symplectic Grassmann","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#ManifoldsBase.check_vector-Tuple{SymplecticGrassmann, ProjectorPoint, ProjectorTangentVector}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::SymplecticGrassmann, p::ProjectorPoint, X::ProjectorTangentVector; kwargs...) Check whether  X  is a valid tangent vector at  p  on the  SymplecticGrassmann ,  $\\operatorname{SpGr}(2n, 2k)$  manifold by verifying that it $X^+ = X$ $X = Xp + pX$ For details see Proposition 4.2 in [ BZ21 ] and the definition of  $\\mathfrak{sp}_P(2n)$  before, especially the  $\\bar{Œ©}$ , which is the representation for  $X$  used here. source"},{"id":1826,"pagetitle":"Symplectic Grassmann","title":"Literature","ref":"/manifolds/stable/manifolds/symplecticgrassmann/#Literature","content":" Literature [BZ21] T.¬†Bendokat and R.¬†Zimmermann.  The real symplectic Stiefel and Grassmann manifolds: metrics, geodesics and applications , arXiv¬†Preprint,¬†2108.12447 (2021),  arXiv:2108.12447 ."},{"id":1829,"pagetitle":"Symplectic Stiefel","title":"(Real) Symplectic Stiefel","ref":"/manifolds/stable/manifolds/symplecticstiefel/#(Real)-Symplectic-Stiefel","content":" (Real) Symplectic Stiefel The  SymplecticStiefel  manifold, denoted  $\\operatorname{SpSt}(2n, 2k)$ , represents canonical symplectic bases of  $2k$  dimensonal symplectic subspaces of  $‚Ñù^{2n√ó2n}$ . This means that the columns of each element  $p \\in \\operatorname{SpSt}(2n, 2k) \\subset ‚Ñù^{2n√ó2k}$  constitute a canonical symplectic basis of  $\\operatorname{span}(p)$ . The canonical symplectic form is a non-degenerate, bilinear, and skew symmetric map  $\\omega_{2k}\\colon ùîΩ^{2k}√óùîΩ^{2k} ‚Üí ùîΩ$ , given by  $\\omega_{2k}(x, y) = x^T Q_{2k} y$  for elements  $x, y \\in ùîΩ^{2k}$ , with \\[    Q_{2k} =\n    \\begin{bmatrix}\n     0_k  &  I_k \\\\\n    -I_k  &  0_k\n    \\end{bmatrix}.\\] Specifically given an element  $p \\in \\operatorname{SpSt}(2n, 2k)$  we require that \\[    \\omega_{2n} (p x, p y) = x^T(p^TQ_{2n}p)y = x^TQ_{2k}y = \\omega_{2k}(x, y) \\;\\forall\\; x, y \\in ùîΩ^{2k},\\] leading to the requirement on  $p$  that  $p^TQ_{2n}p = Q_{2k}$ . In the case that  $k = n$ , this manifold reduces to the  SymplecticMatrices  manifold, which is also known as the symplectic group."},{"id":1830,"pagetitle":"Symplectic Stiefel","title":"Manifolds.SymplecticStiefel","ref":"/manifolds/stable/manifolds/symplecticstiefel/#Manifolds.SymplecticStiefel","content":" Manifolds.SymplecticStiefel  ‚Äî  Type SymplecticStiefel{T,ùîΩ} <: AbstractEmbeddedManifold{ùîΩ, DefaultIsometricEmbeddingType} The symplectic Stiefel manifold consists of all  $2n√ó2k, n ‚â• k$  matrices satisfying the requirement \\[\\mathrm{SpSt}(2n, 2k, ‚Ñù)\n    := \\bigl\\{ p ‚àà ‚Ñù^{2n√ó2k} \\ \\big| \\ p^{\\mathrm{T}}J_{2n}p = J_{2k} \\bigr\\},\\] where  $J_{2n}$  denotes the  SymplecticElement \\[J_{2n} = \\begin{bmatrix} 0_n & I_n \\\\ -I_n & 0_n \\end{bmatrix}.\\] The symplectic Stiefel tangent space at  $p$  can be parametrized as [ BZ21 ] \\[\\begin{align*}\n    T_p\\mathrm{SpSt}(2n, 2k)\n    &= \\{X ‚àà ‚Ñù^{2n√ó2k} ‚à£ p^{T}J_{2n}X + X^{T}J_{2n}p = 0 \\}, \\\\\n    &= \\{X = pŒ© + p^sB \\mid\n        Œ© ‚àà ‚Ñù^{2k√ó2k}, Œ©^+ = -Œ©, \\\\\n        &\\quad\\qquad p^s ‚àà \\mathrm{SpSt}(2n, 2(n- k)), B ‚àà ‚Ñù^{2(n-k)√ó2k}, \\},\n\\end{align*}\\] where  $Œ© ‚àà \\mathfrak{sp}(2n,F)$  is  Hamiltonian  and  $p^s$  means the symplectic complement of  $p$  s.t.  $p^{+}p^{s} = 0$ . Here  $p^+$  denotes the  symplectic_inverse . You can also use  StiefelPoint  and  StiefelTangentVector  with this manifold, they are equivalent to using arrays. Constructor SymplecticStiefel(2n::Int, 2k::Int, field::AbstractNumbers=‚Ñù; parameter::Symbol=:type) Generate the (real-valued) symplectic Stiefel manifold of  $2n√ó2k$  matrices which span a  $2k$  dimensional symplectic subspace of  $‚Ñù^{2n√ó2k}$ . The constructor for the  SymplecticStiefel  manifold accepts the even column dimension  $2n$  and an even number of columns  $2k$  for the real symplectic Stiefel manifold with elements  $p ‚àà ‚Ñù^{2n√ó2k}$ . source"},{"id":1831,"pagetitle":"Symplectic Stiefel","title":"Base.exp","ref":"/manifolds/stable/manifolds/symplecticstiefel/#Base.exp-Tuple{SymplecticStiefel, Any, Any}","content":" Base.exp  ‚Äî  Method exp(::SymplecticStiefel, p, X)\nexp!(M::SymplecticStiefel, q, p, X) Compute the exponential mapping \\[  \\exp\\colon T\\mathrm{SpSt}(2n, 2k) ‚Üí \\mathrm{SpSt}(2n, 2k)\\] at a point  $p ‚àà \\mathrm{SpSt}(2n, 2k)$  in the direction of  $X ‚àà T_p\\mathrm{SpSt}(2n, 2k)$ . The tangent vector  $X$  can be written in the form  $X = \\bar{\\Omega}p$  [ BZ21 ], with \\[  \\bar{\\Omega} = X (p^{\\mathrm{T}}p)^{-1}p^{\\mathrm{T}}\n    + J_{2n}p(p^{\\mathrm{T}}p)^{-1}X^{\\mathrm{T}}(I_{2n}\n    - J_{2n}^{\\mathrm{T}}p(p^{\\mathrm{T}}p)^{-1}p^{\\mathrm{T}}J_{2n})J_{2n}\n    ‚àà ‚Ñù^{2n√ó2n},\\] where  $J_{2n} = \\begin{bmatrix} 0_n & I_n \\\\ -I_n & 0_n \\end{bmatrix}$  denotes the  SymplecticElement . Using this expression for  $X$ , the exponential mapping can be computed as \\[  \\exp_p(X) = \\operatorname{Exp}([\\bar{\\Omega} - \\bar{\\Omega}^{\\mathrm{T}}])\n                             \\operatorname{Exp}(\\bar{\\Omega}^{\\mathrm{T}})p,\\] where  $\\operatorname{Exp}(‚ãÖ)$  denotes the matrix exponential. Computing the above mapping directly however, requires taking matrix exponentials of two  $2n√ó2n$  matrices, which is computationally expensive when  $n$  increases. Therefore we instead follow [ BZ21 ] who express the above exponential mapping in a way which only requires taking matrix exponentials of an  $8k√ó8k$  matrix and a  $4k√ó4k$  matrix. To this end, first define \\[\\bar{A} = J_{2k}p^{\\mathrm{T}}X(p^{\\mathrm{T}}p)^{-1}J_{2k} +\n            (p^{\\mathrm{T}}p)^{-1}X^{\\mathrm{T}}(p - J_{2n}^{\\mathrm{T}}p(p^{\\mathrm{T}}p)^{-1}J_{2k}) ‚àà ‚Ñù^{2k√ó2k},\\] and \\[\\bar{H} = (I_{2n} - pp^+)J_{2n}X(p^{\\mathrm{T}}p)^{-1}J_{2k} ‚àà ‚Ñù^{2n√ó2k}.\\] We then let  $\\bar{\\Delta} = p\\bar{A} + \\bar{H}$ , and define the matrices \\[    Œ≥ = \\left[\\left(I_{2n} - \\frac{1}{2}pp^+\\right)\\bar{\\Delta} \\quad\n              -p \\right] ‚àà ‚Ñù^{2n√ó4k},\\] and \\[    Œª = \\left[J_{2n}^{\\mathrm{T}}pJ_{2k} \\quad\n        \\left(\\bar{\\Delta}^+\\left(I_{2n}\n              - \\frac{1}{2}pp^+\\right)\\right)^{\\mathrm{T}}\\right] ‚àà ‚Ñù^{2n√ó4k}.\\] With the above defined matrices it holds that  $\\bar{\\Omega} = ŒªŒ≥^{\\mathrm{T}}$ .  As a last preliminary step, concatenate  $Œ≥$  and  $Œª$  to define the matrices  $Œì = [Œª \\quad -Œ≥] ‚àà ‚Ñù^{2n√ó8k}$  and  $Œõ = [Œ≥ \\quad Œª] ‚àà ‚Ñù^{2n√ó8k}$ . With these matrix constructions done, we can compute the exponential mapping as \\[  \\exp_p(X) = Œì \\operatorname{Exp}(ŒõŒì^{\\mathrm{T}})\n    \\begin{bmatrix} 0_{4k} \\\\ I_{4k} \\end{bmatrix}\n    \\operatorname{Exp}(ŒªŒ≥^{\\mathrm{T}})\n    \\begin{bmatrix} 0_{2k} \\\\ I_{2k} \\end{bmatrix}.\\] which only requires computing the matrix exponentials of  $ŒõŒì^{\\mathrm{T}} ‚àà ‚Ñù^{8k√ó8k}$  and  $ŒªŒ≥^{\\mathrm{T}} ‚àà ‚Ñù^{4k√ó4k}$ . source"},{"id":1832,"pagetitle":"Symplectic Stiefel","title":"Base.inv","ref":"/manifolds/stable/manifolds/symplecticstiefel/#Base.inv-Tuple{SymplecticStiefel, Any}","content":" Base.inv  ‚Äî  Method inv(::SymplecticStiefel, A)\ninv!(::SymplecticStiefel, q, p) Compute the symplectic inverse  $A^+$  of matrix  $A ‚àà ‚Ñù^{2n√ó2k}$ . Given a matrix \\[A ‚àà ‚Ñù^{2n√ó2k},\\quad\nA =\n\\begin{bmatrix}\nA_{1, 1} & A_{1, 2} \\\\\nA_{2, 1} & A_{2, 2}\n\\end{bmatrix}, \\quad A_{i, j} ‚àà ‚Ñù^{2n√ó2k}\\] the symplectic inverse is defined as: \\[A^{+} := J_{2k}^{\\mathrm{T}} A^{\\mathrm{T}} J_{2n},\\] where  $J_{2n} = \\begin{bmatrix} 0_n & I_n \\\\ -I_n & 0_n \\end{bmatrix}$  denotes the  SymplecticElement . The symplectic inverse of a matrix A can be expressed explicitly as: \\[A^{+} =\n  \\begin{bmatrix}\n    A_{2, 2}^{\\mathrm{T}} & -A_{1, 2}^{\\mathrm{T}} \\\\[1.2mm]\n   -A_{2, 1}^{\\mathrm{T}} &  A_{1, 1}^{\\mathrm{T}}\n  \\end{bmatrix}.\\] source"},{"id":1833,"pagetitle":"Symplectic Stiefel","title":"Base.rand","ref":"/manifolds/stable/manifolds/symplecticstiefel/#Base.rand-Tuple{SymplecticStiefel}","content":" Base.rand  ‚Äî  Method rand(M::SymplecticStiefel; vector_at=nothing, œÉ = 1.0) Generate a random point  $p ‚àà \\mathrm{SpSt}(2n, 2k)$  or a random tangent vector  $X ‚àà T_p\\mathrm{SpSt}(2n, 2k)$  if  vector_at  is set to a point  $p ‚àà \\mathrm{Sp}(2n)$ . A random point on  $\\mathrm{SpSt}(2n, 2k)$  is found by first generating a random point on the symplectic manifold  $\\mathrm{Sp}(2n)$ , and then projecting onto the Symplectic Stiefel manifold using the  canonical_project $œÄ_{\\mathrm{SpSt}(2n, 2k)}$ . That is,  $p = œÄ_{\\mathrm{SpSt}(2n, 2k)}(p_{\\mathrm{Sp}})$ . To generate a random tangent vector in  $T_p\\mathrm{SpSt}(2n, 2k)$  this code exploits the second tangent vector space parametrization of  SymplecticStiefel , that any  $X ‚àà T_p\\mathrm{SpSt}(2n, 2k)$  can be written as  $X = pŒ©_X + p^sB_X$ . To generate random tangent vectors at  $p$  then, this function sets  $B_X = 0$  and generates a random Hamiltonian matrix  $Œ©_X ‚àà \\mathfrak{sp}(2n,F)$  with Frobenius norm of  œÉ  before returning  $X = pŒ©_X$ . source"},{"id":1834,"pagetitle":"Symplectic Stiefel","title":"ManifoldDiff.riemannian_gradient","ref":"/manifolds/stable/manifolds/symplecticstiefel/#ManifoldDiff.riemannian_gradient-Tuple{SymplecticStiefel, Any, Any}","content":" ManifoldDiff.riemannian_gradient  ‚Äî  Method X = riemannian_gradient(::SymplecticStiefel, f, p, Y; embedding_metric::EuclideanMetric=EuclideanMetric())\nriemannian_gradient!(::SymplecticStiefel, f, X, p, Y; embedding_metric::EuclideanMetric=EuclideanMetric()) Compute the riemannian gradient  X  of  f  on  SymplecticStiefel   at a point  p , provided that the gradient of the function  $\\tilde f$ , which is  f  continued into the embedding is given by  Y . The metric in the embedding is the Euclidean metric. The manifold gradient  X  is computed from  Y  as \\[    X = Yp^{\\mathrm{T}}p + J_{2n}pY^{\\mathrm{T}}J_{2n}p,\\] where  $J_{2n} = \\begin{bmatrix} 0_n & I_n \\\\ -I_n & 0_n \\end{bmatrix}$  denotes the  SymplecticElement . source"},{"id":1835,"pagetitle":"Symplectic Stiefel","title":"Manifolds.canonical_project","ref":"/manifolds/stable/manifolds/symplecticstiefel/#Manifolds.canonical_project-Tuple{SymplecticStiefel, Any}","content":" Manifolds.canonical_project  ‚Äî  Method canonical_project(::SymplecticStiefel, p_Sp)\ncanonical_project!(::SymplecticStiefel, p, p_Sp) Define the canonical projection from  $\\mathrm{Sp}(2n, 2n)$  onto  $\\mathrm{SpSt}(2n, 2k)$ , by projecting onto the first  $k$  columns and the  $n + 1$ 'th onto the  $n + k$ 'th columns [ BZ21 ]. It is assumed that the point  $p$  is on  $\\mathrm{Sp}(2n, 2n)$ . source"},{"id":1836,"pagetitle":"Symplectic Stiefel","title":"Manifolds.get_total_space","ref":"/manifolds/stable/manifolds/symplecticstiefel/#Manifolds.get_total_space-Union{Tuple{SymplecticStiefel{ManifoldsBase.TypeParameter{Tuple{n, k}}, ‚Ñù}}, Tuple{k}, Tuple{n}} where {n, k}","content":" Manifolds.get_total_space  ‚Äî  Method get_total_space(::SymplecticStiefel) Return the total space of the  SymplecticStiefel  manifold, which is the corresponding  SymplecticMatrices  manifold. source"},{"id":1837,"pagetitle":"Symplectic Stiefel","title":"Manifolds.symplectic_inverse_times","ref":"/manifolds/stable/manifolds/symplecticstiefel/#Manifolds.symplectic_inverse_times-Tuple{SymplecticStiefel, Any, Any}","content":" Manifolds.symplectic_inverse_times  ‚Äî  Method symplectic_inverse_times(::SymplecticStiefel, p, q)\nsymplectic_inverse_times!(::SymplecticStiefel, A, p, q) Directly compute the symplectic inverse of  $p ‚àà \\mathrm{SpSt}(2n, 2k)$ , multiplied with  $q ‚àà \\mathrm{SpSt}(2n, 2k)$ . That is, this function efficiently computes  $p^+q = (J_{2k}p^{\\mathrm{T}}J_{2n})q ‚àà ‚Ñù^{2k√ó2k}$ , where  $J_{2n}, J_{2k}$  are the  SymplecticElement  of sizes  $2n√ó2n$  and  $2k√ó2k$  respectively. This function performs this common operation without allocating more than a  $2k√ó2k$  matrix to store the result in, or in the case of the in-place function, without allocating memory at all. source"},{"id":1838,"pagetitle":"Symplectic Stiefel","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/symplecticstiefel/#ManifoldsBase.check_point-Tuple{SymplecticStiefel{<:Any, ‚Ñù}, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::SymplecticStiefel, p; kwargs...) Check whether  p  is a valid point on the  SymplecticStiefel ,  $\\mathrm{SpSt}(2n, 2k)$  manifold, that is  $p^{+}p$  is the identity,  $(‚ãÖ)^+$  denotes the  symplectic_inverse . source"},{"id":1839,"pagetitle":"Symplectic Stiefel","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/symplecticstiefel/#ManifoldsBase.check_vector-Tuple{SymplecticStiefel, Vararg{Any}}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::SymplecticMatrices, p, X; kwargs...) Checks whether  X  is a valid tangent vector at  p  on the  SymplecticStiefel ,  $\\mathrm{SpSt}(2n, 2k)$  manifold. The check consists of verifying that  $H = p^{+}X ‚àà ùî§_{2k}$ , where  $ùî§$  is the Lie Algebra of the symplectic group  $\\mathrm{Sp}(2k)$ , that is the set of [ HamiltonianMatrices ])(@ref), where  $(‚ãÖ)^+$  denotes the  symplectic_inverse . source"},{"id":1840,"pagetitle":"Symplectic Stiefel","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/symplecticstiefel/#ManifoldsBase.inner-Tuple{SymplecticStiefel, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::SymplecticStiefel, p, X. Y) Compute the Riemannian inner product  $g^{\\mathrm{SpSt}}$  at  $p ‚àà \\mathrm{SpSt}$  of tangent vectors  $Y, X ‚àà T_p\\mathrm{SpSt}$ . Given by Proposition 3.10 in [ BZ21 ]. \\[g^{\\mathrm{SpSt}}_p(X, Y)\n  = \\operatorname{tr}\\Bigl(\n    X^{\\mathrm{T}}\\bigl(\n      I_{2n} - \\frac{1}{2}J_{2n}^{\\mathrm{T}} p(p^{\\mathrm{T}}p)^{-1}p^{\\mathrm{T}}J_{2n}\n    \\bigr) Y (p^{\\mathrm{T}}p)^{-1}\\Bigr).\\] source"},{"id":1841,"pagetitle":"Symplectic Stiefel","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/symplecticstiefel/#ManifoldsBase.inverse_retract-Tuple{SymplecticStiefel, Any, Any, CayleyInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(::SymplecticStiefel, p, q, ::CayleyInverseRetraction)\ninverse_retract!(::SymplecticStiefel, X, p, q, ::CayleyInverseRetraction) Compute the Cayley Inverse Retraction  $X = \\mathcal{L}_p^{\\mathrm{SpSt}}(q)$  such that the Cayley Retraction from  $p$  along  $X$  lands at  $q$ , i.e.  $\\mathcal{R}_p(X) = q$  [ BZ21 ]. For  $p, q ‚àà \\mathrm{SpSt}(2n, 2k, ‚Ñù)$  we can define the inverse cayley retraction as long as the following matrices exist. \\[    U = (I + p^+ q)^{-1} ‚àà ‚Ñù^{2k√ó2k},\n    \\quad\n    V = (I + q^+ p)^{-1} ‚àà ‚Ñù^{2k√ó2k},\\] where  $(‚ãÖ)^+$  denotes the  symplectic_inverse . THen the inverse retraction reads \\[\\mathcal{L}_p^{\\mathrm{Sp}}(q) = 2p\\bigl(V - U\\bigr) + 2\\bigl((p + q)U - p\\bigr) ‚àà T_p\\mathrm{Sp}(2n).\\] source"},{"id":1842,"pagetitle":"Symplectic Stiefel","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/symplecticstiefel/#ManifoldsBase.is_flat-Tuple{SymplecticStiefel}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::SymplecticStiefel) Return false.  SymplecticStiefel  is not a flat manifold. source"},{"id":1843,"pagetitle":"Symplectic Stiefel","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/symplecticstiefel/#ManifoldsBase.manifold_dimension-Tuple{SymplecticStiefel}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(::SymplecticStiefel) Returns the dimension of the symplectic Stiefel manifold embedded in  $‚Ñù^{2n√ó2k}$ , i.e. [ BZ21 ] \\[    \\operatorname{dim}(\\mathrm{SpSt}(2n, 2k)) = (4n - 2k + 1)k.\\] source"},{"id":1844,"pagetitle":"Symplectic Stiefel","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/symplecticstiefel/#ManifoldsBase.project-Tuple{SymplecticStiefel, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(::SymplecticStiefel, p, A)\nproject!(::SymplecticStiefel, Y, p, A) Given a point  $p ‚àà \\mathrm{SpSt}(2n, 2k)$ , project an element  $A ‚àà ‚Ñù^{2n√ó2k}$  onto the tangent space  $T_p\\mathrm{SpSt}(2n, 2k)$  relative to the euclidean metric of the embedding  $‚Ñù^{2n√ó2k}$ . That is, we find the element  $X ‚àà T_p\\mathrm{SpSt}(2n, 2k)$  which solves the constrained optimization problem \\[    \\displaystyle\\operatorname{min}_{X ‚àà ‚Ñù^{2n√ó2k}} \\frac{1}{2}||X - A||^2, \\quad\n    \\text{s.t.}\\;\n    h(X) := X^{\\mathrm{T}} J p + p^{\\mathrm{T}} J X = 0,\\] where  $h : ‚Ñù^{2n√ó2k} ‚Üí \\operatorname{skew}(2k)$  defines the restriction of  $X$  onto the tangent space  $T_p\\mathrm{SpSt}(2n, 2k)$ . source"},{"id":1845,"pagetitle":"Symplectic Stiefel","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/symplecticstiefel/#ManifoldsBase.retract-Tuple{SymplecticStiefel, Any, Any, CayleyRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(::SymplecticStiefel, p, X, ::CayleyRetraction)\nretract!(::SymplecticStiefel, q, p, X, ::CayleyRetraction) Compute the Cayley retraction on the Symplectic Stiefel manifold, from  p  along  X  (computed inplace of  q ). Given a point  $p ‚àà \\mathrm{SpSt}(2n, 2k)$ , every tangent vector  $X ‚àà T_p\\mathrm{SpSt}(2n, 2k)$  is of the form  $X = \\tilde{\\Omega}p$ , with \\[    \\tilde{\\Omega} = \\left(I_{2n} - \\frac{1}{2}pp^+\\right)Xp^+ -\n                     pX^+\\left(I_{2n} - \\frac{1}{2}pp^+\\right) ‚àà ‚Ñù^{2n√ó2n},\\] as shown in Proposition 3.5 of [ BZ21 ]. Using this representation of  $X$ , the Cayley retraction on  $\\mathrm{SpSt}(2n, 2k)$  is defined pointwise as \\[    \\mathcal{R}_p(X) = \\operatorname{cay}\\left(\\frac{1}{2}\\tilde{\\Omega}\\right)p.\\] The operator  $\\operatorname{cay}(A) = (I - A)^{-1}(I + A)$  is the Cayley transform. However, the computation of an  $2n√ó2n$  matrix inverse in the expression above can be reduced down to inverting a  $2k√ó2k$  matrix due to Proposition 5.2 of [ BZ21 ]. Let  $A = p^+X$  and  $H = X - pA$ . Then an equivalent expression for the Cayley retraction defined pointwise above is \\[  \\mathcal{R}_p(X) = -p + (H + 2p)(H^+H/4 - A/2 + I_{2k})^{-1}.\\] This expression is computed inplace of  q . source"},{"id":1846,"pagetitle":"Symplectic Stiefel","title":"Literature","ref":"/manifolds/stable/manifolds/symplecticstiefel/#Literature","content":" Literature [BZ21] T.¬†Bendokat and R.¬†Zimmermann.  The real symplectic Stiefel and Grassmann manifolds: metrics, geodesics and applications , arXiv¬†Preprint,¬†2108.12447 (2021),  arXiv:2108.12447 ."},{"id":1849,"pagetitle":"Torus","title":"Torus","ref":"/manifolds/stable/manifolds/torus/#Torus","content":" Torus The torus  $ùïã^d ‚âÖ [-œÄ,œÄ)^d$  is modeled as an  AbstractPowerManifold   of the (real-valued)  Circle  and uses  ArrayPowerRepresentation . Points on the torus are hence row vectors,  $x ‚àà ‚Ñù^{d}$ ."},{"id":1850,"pagetitle":"Torus","title":"Example","ref":"/manifolds/stable/manifolds/torus/#Example","content":" Example The following code can be used to make a three-dimensional torus  $ùïã^3$  and compute a tangent vector: using Manifolds\nM = Torus(3)\np = [0.5, 0.0, 0.0]\nq = [0.0, 0.5, 1.0]\nX = log(M, p, q) 3-element Vector{Float64}:\n -0.5\n  0.5\n  1.0"},{"id":1851,"pagetitle":"Torus","title":"Types and functions","ref":"/manifolds/stable/manifolds/torus/#Types-and-functions","content":" Types and functions Most functions are directly implemented for an  AbstractPowerManifold   with  ArrayPowerRepresentation  except the following special cases:"},{"id":1852,"pagetitle":"Torus","title":"Manifolds.Torus","ref":"/manifolds/stable/manifolds/torus/#Manifolds.Torus","content":" Manifolds.Torus  ‚Äî  Type Torus{N} <: AbstractPowerManifold The n-dimensional torus is the  $n$ -dimensional product of the  Circle . The  Circle  is stored internally within  M.manifold , such that all functions of  AbstractPowerManifold   can be used directly. source"},{"id":1853,"pagetitle":"Torus","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/torus/#ManifoldsBase.check_point-Tuple{Torus, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::Torus{n},p) Checks whether  p  is a valid point on the  Torus M , i.e. each of its entries is a valid point on the  Circle  and the length of  x  is  n . source"},{"id":1854,"pagetitle":"Torus","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/torus/#ManifoldsBase.check_vector-Union{Tuple{N}, Tuple{Torus{N}, Any, Any}} where N","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::Torus{n}, p, X; kwargs...) Checks whether  X  is a valid tangent vector to  p  on the  Torus M . This means, that  p  is valid, that  X  is of correct dimension and elementwise a tangent vector to the elements of  p  on the  Circle . source"},{"id":1855,"pagetitle":"Torus","title":"Embedded Torus","ref":"/manifolds/stable/manifolds/torus/#Embedded-Torus","content":" Embedded Torus Two-dimensional torus embedded in  $‚Ñù^3$ ."},{"id":1856,"pagetitle":"Torus","title":"Manifolds.DefaultTorusAtlas","ref":"/manifolds/stable/manifolds/torus/#Manifolds.DefaultTorusAtlas","content":" Manifolds.DefaultTorusAtlas  ‚Äî  Type DefaultTorusAtlas() Atlas for torus with charts indexed by two angles numbers  $Œ∏‚ÇÄ, œÜ‚ÇÄ ‚àà [-œÄ, œÄ)$ . Inverse of a chart  $(Œ∏‚ÇÄ, œÜ‚ÇÄ)$  is given by \\[x(Œ∏, œÜ) = (R + r\\cos(Œ∏ + Œ∏‚ÇÄ))\\cos(œÜ + œÜ‚ÇÄ) \\\\\ny(Œ∏, œÜ) = (R + r\\cos(Œ∏ + Œ∏‚ÇÄ))\\sin(œÜ + œÜ‚ÇÄ) \\\\\nz(Œ∏, œÜ) = r\\sin(Œ∏ + Œ∏‚ÇÄ)\\] source"},{"id":1857,"pagetitle":"Torus","title":"Manifolds.EmbeddedTorus","ref":"/manifolds/stable/manifolds/torus/#Manifolds.EmbeddedTorus","content":" Manifolds.EmbeddedTorus  ‚Äî  Type EmbeddedTorus{TR<:Real} <: AbstractDecoratorManifold{‚Ñù} Surface in ‚Ñù¬≥ described by parametric equations: \\[x(Œ∏, œÜ) = (R + r\\cos Œ∏)\\cos œÜ \\\\\ny(Œ∏, œÜ) = (R + r\\cos Œ∏)\\sin œÜ \\\\\nz(Œ∏, œÜ) = r\\sin Œ∏\\] for Œ∏, œÜ in  $[-œÄ, œÄ)$ . It is assumed that  $R > r > 0$ . Alternative names include anchor ring, donut and doughnut. Constructor EmbeddedTorus(R, r) source"},{"id":1858,"pagetitle":"Torus","title":"Manifolds.affine_connection","ref":"/manifolds/stable/manifolds/torus/#Manifolds.affine_connection-Tuple{Manifolds.EmbeddedTorus, Manifolds.DefaultTorusAtlas, Vararg{Any, 4}}","content":" Manifolds.affine_connection  ‚Äî  Method affine_connection(M::EmbeddedTorus, A::DefaultTorusAtlas, i, a, Xc, Yc) Affine connection on  EmbeddedTorus M . source"},{"id":1859,"pagetitle":"Torus","title":"Manifolds.check_chart_switch","ref":"/manifolds/stable/manifolds/torus/#Manifolds.check_chart_switch-Tuple{Manifolds.EmbeddedTorus, Manifolds.DefaultTorusAtlas, Any, Any}","content":" Manifolds.check_chart_switch  ‚Äî  Method check_chart_switch(::EmbeddedTorus, A::DefaultTorusAtlas, i, a; œµ = pi/3) Return true if parameters  a  lie closer than  œµ  to chart boundary. source"},{"id":1860,"pagetitle":"Torus","title":"Manifolds.gaussian_curvature","ref":"/manifolds/stable/manifolds/torus/#Manifolds.gaussian_curvature-Tuple{Manifolds.EmbeddedTorus, Any}","content":" Manifolds.gaussian_curvature  ‚Äî  Method gaussian_curvature(M::EmbeddedTorus, p) Gaussian curvature at point  p  from  EmbeddedTorus M . source"},{"id":1861,"pagetitle":"Torus","title":"Manifolds.inverse_chart_injectivity_radius","ref":"/manifolds/stable/manifolds/torus/#Manifolds.inverse_chart_injectivity_radius-Tuple{Manifolds.EmbeddedTorus, Manifolds.DefaultTorusAtlas, Any}","content":" Manifolds.inverse_chart_injectivity_radius  ‚Äî  Method inverse_chart_injectivity_radius(M::AbstractManifold, A::AbstractAtlas, i) Injectivity radius of  get_point  for chart  i  from the  DefaultTorusAtlas A  of the  EmbeddedTorus . source"},{"id":1862,"pagetitle":"Torus","title":"Manifolds.normal_vector","ref":"/manifolds/stable/manifolds/torus/#Manifolds.normal_vector-Tuple{Manifolds.EmbeddedTorus, Any}","content":" Manifolds.normal_vector  ‚Äî  Method normal_vector(M::EmbeddedTorus, p) Outward-pointing normal vector on the  EmbeddedTorus  at the point  p . source"},{"id":1863,"pagetitle":"Torus","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/torus/#ManifoldsBase.check_point-Tuple{Manifolds.EmbeddedTorus, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::EmbeddedTorus, p; kwargs...) Check whether  p  is a valid point on the  EmbeddedTorus M . The tolerance for the last test can be set using the  kwargs... . The method checks if  $(p_1^2 + p_2^2 + p_3^2 + R^2 - r^2)^2$  is approximately equal to  $4R^2(p_1^2 + p_2^2)$ . source"},{"id":1864,"pagetitle":"Torus","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/torus/#ManifoldsBase.check_vector-Tuple{Manifolds.EmbeddedTorus, Any, Any}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::EmbeddedTorus, p, X; atol=eps(eltype(p)), kwargs...) Check whether  X  is a valid vector tangent to  p  on the  EmbeddedTorus M . The method checks if the vector  X  is orthogonal to the vector normal to the torus, see  normal_vector . Absolute tolerance can be set using  atol . source"},{"id":1865,"pagetitle":"Torus","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/torus/#ManifoldsBase.inner-Tuple{Manifolds.EmbeddedTorus, Manifolds.DefaultTorusAtlas, Vararg{Any, 4}}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::EmbeddedTorus, ::DefaultTorusAtlas, i, a, Xc, Yc) Inner product on  EmbeddedTorus  in chart  i  in the  DefaultTorusAtlas . between vectors with coordinates  Xc  and  Yc  tangent at point with parameters  a . Vector coordinates must be given in the induced basis. source"},{"id":1866,"pagetitle":"Torus","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/torus/#ManifoldsBase.is_flat-Tuple{Manifolds.EmbeddedTorus}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::EmbeddedTorus) Return false.  EmbeddedTorus  is not a flat manifold. source"},{"id":1867,"pagetitle":"Torus","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/torus/#ManifoldsBase.manifold_dimension-Tuple{Manifolds.EmbeddedTorus}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(M::EmbeddedTorus) Return the dimension of the  EmbeddedTorus M  that is 2. source"},{"id":1870,"pagetitle":"Tucker","title":"Tucker manifold","ref":"/manifolds/stable/manifolds/tucker/#Tucker","content":" Tucker manifold"},{"id":1871,"pagetitle":"Tucker","title":"Manifolds.Tucker","ref":"/manifolds/stable/manifolds/tucker/#Manifolds.Tucker","content":" Manifolds.Tucker  ‚Äî  Type Tucker{T, D, ùîΩ} <: AbstractManifold{ùîΩ} The manifold of  $N_1√ó\\dots√óN_D$  real-valued or complex-valued tensors of fixed multilinear rank  $(R_1, \\dots, R_D)$  . If  $R_1 = \\dots = R_D = 1$ , this is the Segre manifold, i.e., the set of rank-1 tensors. Representation in HOSVD format Let  $ùîΩ$  be the real or complex numbers. Any tensor  $p$  on the Tucker manifold can be represented as a multilinear product in HOSVD [ LMV00 ] form \\[p = (U_1,\\dots,U_D) ‚ãÖ \\mathcal{C}\\] where  $\\mathcal C \\in ùîΩ^{R_1√ó\\dots√óR_D}$  and, for  $d=1,\\dots,D$ , the matrix  $U_d \\in ùîΩ^{N_d√óR_d}$  contains the singular vectors of the  $d$ th unfolding of  $\\mathcal{A}$ Tangent space The tangent space to the Tucker manifold at  $p = (U_1,\\dots,U_D) ‚ãÖ \\mathcal{C}$  is [ KL10 ] \\[T_p \\mathcal{M} =\n\\bigl\\{\n(U_1,\\dots,U_D) ‚ãÖ \\mathcal{C}^\\prime\n+ \\sum_{d=1}^D \\bigl(\n    (U_1, \\dots, U_{d-1}, U_d^\\prime, U_{d+1}, \\dots, U_D)\n    ‚ãÖ \\mathcal{C}\n\\bigr)\n\\bigr\\}\\] where  $\\mathcal{C}^\\prime$  is arbitrary,  $U_d^{\\mathrm{H}}$  is the Hermitian adjoint of  $U_d$ , and  $U_d^{\\mathrm{H}} U_d^\\prime = 0$  for all  $d$ . Constructor Tucker(N::NTuple{D, Int}, R::NTuple{D, Int}[, field=‚Ñù]; parameter::Symbol=:type) Generate the manifold of  field -valued tensors of dimensions   N[1] √ó ‚Ä¶ √ó N[D]  and multilinear rank  R = (R[1], ‚Ä¶, R[D]) . source"},{"id":1872,"pagetitle":"Tucker","title":"Manifolds.TuckerPoint","ref":"/manifolds/stable/manifolds/tucker/#Manifolds.TuckerPoint","content":" Manifolds.TuckerPoint  ‚Äî  Type TuckerPoint{T,D} An order  D  tensor of fixed multilinear rank and entries of type  T , which makes it a point on the  Tucker  manifold. The tensor is represented in HOSVD form. Constructors: TuckerPoint(core::AbstractArray{T,D}, factors::Vararg{<:AbstractMatrix{T},D}) where {T,D} Construct an order  D  tensor of element type  T  that can be represented as the multilinear product  (factors[1], ‚Ä¶, factors[D]) ‚ãÖ core . It is assumed that the dimensions of the core are the multilinear rank of the tensor and that the matrices  factors  each have full rank. No further assumptions are made. TuckerPoint(p::AbstractArray{T,D}, mlrank::NTuple{D,Int}) where {T,D} The low-multilinear rank tensor arising from the sequentially truncated the higher-order singular value decomposition of the  D -dimensional array  p  of type  T . The singular values are truncated to get a multilinear rank  mlrank  [ VVM12 ]. source"},{"id":1873,"pagetitle":"Tucker","title":"Manifolds.TuckerTangentVector","ref":"/manifolds/stable/manifolds/tucker/#Manifolds.TuckerTangentVector","content":" Manifolds.TuckerTangentVector  ‚Äî  Type TuckerTangentVector{T, D} <: AbstractTangentVector Tangent vector to the  D -th order  Tucker  manifold at  $p = (U_1,\\dots,U_D) ‚ãÖ \\mathcal{C}$ . The numbers are of type  T  and the vector is represented as \\[X =\n(U_1,\\dots,U_D) ‚ãÖ \\mathcal{C}^\\prime +\n\\sum_{d=1}^D (U_1,\\dots,U_{d-1},U_d^\\prime,U_{d+1},\\dots,U_D) ‚ãÖ \\mathcal{C}\\] where  $U_d^\\mathrm{H} U_d^\\prime = 0$ . Constructor TuckerTangentVector(C‚Ä≤::Array{T,D}, U‚Ä≤::NTuple{D,Matrix{T}}) where {T,D} Constructs a  D th order  TuckerTangentVector  of number type  T  with  $C^\\prime$  and  $U^\\prime$ , so that, together with a  TuckerPoint $p$  as above, the tangent vector can be represented as  $X$  in the above expression. source"},{"id":1874,"pagetitle":"Tucker","title":"Base.convert","ref":"/manifolds/stable/manifolds/tucker/#Base.convert-Union{Tuple{D}, Tuple{T}, Tuple{ùîΩ}, Tuple{Type{Matrix{T}}, CachedBasis{ùîΩ, DefaultOrthonormalBasis{ùîΩ, TangentSpaceType}, Manifolds.HOSVDBasis{T, D}}}} where {ùîΩ, T, D}","content":" Base.convert  ‚Äî  Method Base.convert(::Type{Matrix{T}}, basis::CachedBasis{ùîΩ,DefaultOrthonormalBasis{ùîΩ, TangentSpaceType},HOSVDBasis{T, D}}) where {ùîΩ, T, D}\nBase.convert(::Type{Matrix}, basis::CachedBasis{ùîΩ,DefaultOrthonormalBasis{ùîΩ, TangentSpaceType},HOSVDBasis{T, D}}) where {ùîΩ, T, D} Convert a HOSVD-derived cached basis from [ DBV21 ] of the  D th order  Tucker  manifold with number type  T  to a matrix. The columns of this matrix are the vectorisations of the  embed dings of the basis vectors. source"},{"id":1875,"pagetitle":"Tucker","title":"Base.foreach","ref":"/manifolds/stable/manifolds/tucker/#Base.foreach","content":" Base.foreach  ‚Äî  Function Base.foreach(f, M::Tucker, p::TuckerPoint, basis::AbstractBasis, indices=1:manifold_dimension(M)) Let  basis  be and  AbstractBasis  at a point  p  on  M . Suppose  f  is a function that takes an index and a vector as an argument. This function applies  f  to  i  and the  i th basis vector sequentially for each  i  in  indices . Using a  CachedBasis  may speed up the computation. NOTE : The i'th basis vector is overwritten in each iteration. If any information about the vector is to be stored,  f  must make a copy. source"},{"id":1876,"pagetitle":"Tucker","title":"Base.ndims","ref":"/manifolds/stable/manifolds/tucker/#Base.ndims-Union{Tuple{TuckerPoint{T, D}}, Tuple{D}, Tuple{T}} where {T, D}","content":" Base.ndims  ‚Äî  Method Base.ndims(p::TuckerPoint{T,D}) where {T,D} The order of the tensor corresponding to the  TuckerPoint p , i.e.,  D . source"},{"id":1877,"pagetitle":"Tucker","title":"Base.size","ref":"/manifolds/stable/manifolds/tucker/#Base.size-Tuple{TuckerPoint}","content":" Base.size  ‚Äî  Method Base.size(p::TuckerPoint) The dimensions of a  TuckerPoint p , when regarded as a full tensor (see  embed ). source"},{"id":1878,"pagetitle":"Tucker","title":"ManifoldsBase.check_point","ref":"/manifolds/stable/manifolds/tucker/#ManifoldsBase.check_point-Tuple{Tucker, Any}","content":" ManifoldsBase.check_point  ‚Äî  Method check_point(M::Tucker, p; kwargs...) Check whether the multidimensional array or  TuckerPoint p  is a point on the  Tucker  manifold, i.e. it is a  D th order  N[1] √ó ‚Ä¶ √ó N[D]  tensor of multilinear rank  (R[1], ‚Ä¶, R[D]) . The keyword arguments are passed to the matrix rank function applied to the unfoldings. For a  TuckerPoint  it is checked that the point is in correct HOSVD form. source"},{"id":1879,"pagetitle":"Tucker","title":"ManifoldsBase.check_vector","ref":"/manifolds/stable/manifolds/tucker/#ManifoldsBase.check_vector-Union{Tuple{D}, Tuple{T}, Tuple{Tucker{<:Any, D}, TuckerPoint{T, D}, TuckerTangentVector}} where {T, D}","content":" ManifoldsBase.check_vector  ‚Äî  Method check_vector(M::Tucker{<:Any,D}, p::TuckerPoint{T,D}, X::TuckerTangentVector) where {T,D} Check whether a  TuckerTangentVector X  is is in the tangent space to the  D th order  Tucker  manifold  M  at the  D th order  TuckerPoint p . This is the case when the dimensions of the factors in  X  agree with those of  p  and the factor matrices of  X  are in the orthogonal complement of the HOSVD factors of  p . source"},{"id":1880,"pagetitle":"Tucker","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/tucker/#ManifoldsBase.embed-Tuple{Tucker, TuckerPoint, TuckerTangentVector}","content":" ManifoldsBase.embed  ‚Äî  Method embed(::Tucker, p::TuckerPoint, X::TuckerTangentVector) Convert a tangent vector  X  with base point  p  on the rank  R Tucker  manifold to a full tensor, represented as an  N[1] √ó ‚Ä¶ √ó N[D] -array. source"},{"id":1881,"pagetitle":"Tucker","title":"ManifoldsBase.embed","ref":"/manifolds/stable/manifolds/tucker/#ManifoldsBase.embed-Tuple{Tucker, TuckerPoint}","content":" ManifoldsBase.embed  ‚Äî  Method embed(::Tucker, p::TuckerPoint) Convert a  TuckerPoint p  on the rank  R Tucker  manifold to a full  N[1] √ó ‚Ä¶ √ó N[D] -array by evaluating the Tucker decomposition. source"},{"id":1882,"pagetitle":"Tucker","title":"ManifoldsBase.get_basis","ref":"/manifolds/stable/manifolds/tucker/#ManifoldsBase.get_basis-Union{Tuple{ùîΩ}, Tuple{Tucker, TuckerPoint}, Tuple{Tucker, TuckerPoint, DefaultOrthonormalBasis{ùîΩ, TangentSpaceType}}} where ùîΩ","content":" ManifoldsBase.get_basis  ‚Äî  Method get_basis(:: Tucker, p::TuckerPoint, basisType::DefaultOrthonormalBasis{ùîΩ, TangentSpaceType}) where ùîΩ An implicitly stored basis of the tangent space to the Tucker manifold. Assume  $p = (U_1,\\dots,U_D) ‚ãÖ \\mathcal{C}$  is in HOSVD format and that, for  $d=1,\\dots,D$ , the singular values of the  $d$ 'th unfolding are  $\\sigma_{dj}$ , with  $j = 1,\\dots,R_d$ . The basis of the tangent space is as follows: [ DBV21 ] \\[\\bigl\\{\n(U_1,\\dots,U_D) e_i\n\\bigr\\} \\cup \\bigl\\{\n(U_1,\\dots, \\sigma_{dj}^{-1} U_d^{\\perp} e_i e_j^T,\\dots,U_D) ‚ãÖ \\mathcal{C}\n\\bigr\\}\\] for all  $d = 1,\\dots,D$  and all canonical basis vectors  $e_i$  and  $e_j$ . Every  $U_d^\\perp$  is such that  $[U_d \\quad U_d^{\\perp}]$  forms an orthonormal basis of  $‚Ñù^{N_d}$ . source"},{"id":1883,"pagetitle":"Tucker","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/tucker/#ManifoldsBase.inner-Tuple{Tucker, TuckerPoint, TuckerTangentVector, TuckerTangentVector}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::Tucker, p::TuckerPoint, X::TuckerTangentVector, Y::TuckerTangentVector) The Euclidean inner product between tangent vectors  X  and  X  at the point  p  on the Tucker manifold. This is equal to  embed(M, p, X) ‚ãÖ embed(M, p, Y) . inner(::Tucker, A::TuckerPoint, X::TuckerTangentVector, Y)\ninner(::Tucker, A::TuckerPoint, X, Y::TuckerTangentVector) The Euclidean inner product between  X  and  Y  where  X  is a vector tangent to the Tucker manifold at  p  and  Y  is a vector in the ambient space or vice versa. The vector in the ambient space is represented as a full tensor, i.e., a multidimensional array. source"},{"id":1884,"pagetitle":"Tucker","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/tucker/#ManifoldsBase.inverse_retract-Tuple{Tucker, Any, TuckerPoint, TuckerPoint, ProjectionInverseRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::Tucker, p::TuckerPoint, q::TuckerPoint, ::ProjectionInverseRetraction) The projection inverse retraction on the Tucker manifold interprets  q  as a point in the ambient Euclidean space (see  embed ) and projects it onto the tangent space at to  M  at  p . source"},{"id":1885,"pagetitle":"Tucker","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/tucker/#ManifoldsBase.is_flat-Tuple{Tucker}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::Tucker) Return false.  Tucker  is not a flat manifold. source"},{"id":1886,"pagetitle":"Tucker","title":"ManifoldsBase.manifold_dimension","ref":"/manifolds/stable/manifolds/tucker/#ManifoldsBase.manifold_dimension-Tuple{Tucker}","content":" ManifoldsBase.manifold_dimension  ‚Äî  Method manifold_dimension(::Tucker) The dimension of the manifold of  $N_1√ó\\dots√óN_D$  tensors of multilinear rank  $(R_1, \\dots, R_D)$ , i.e. \\[\\mathrm{dim}(\\mathcal{M}) = \\prod_{d=1}^D R_d + \\sum_{d=1}^D R_d (N_d - R_d).\\] source"},{"id":1887,"pagetitle":"Tucker","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/tucker/#ManifoldsBase.project-Tuple{Tucker, Any, TuckerPoint, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(M::Tucker, p::TuckerPoint, X) The least-squares projection of a dense tensor  X  onto the tangent space to  M  at  p . source"},{"id":1888,"pagetitle":"Tucker","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/tucker/#ManifoldsBase.retract-Tuple{Tucker, Any, Any, PolarRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(::Tucker, p::TuckerPoint, X::TuckerTangentVector, ::PolarRetraction) The truncated HOSVD-based retraction [ KSV13 ] to the Tucker manifold, i.e. the result is the sequentially truncated HOSVD approximation of  $p + X$ . In the exceptional case that the multilinear rank of  $p + X$  is lower than that of  $p$ , this retraction produces a boundary point, which is outside the manifold. source"},{"id":1889,"pagetitle":"Tucker","title":"ManifoldsBase.zero_vector","ref":"/manifolds/stable/manifolds/tucker/#ManifoldsBase.zero_vector-Tuple{Tucker, TuckerPoint}","content":" ManifoldsBase.zero_vector  ‚Äî  Method zero_vector(::Tucker, p::TuckerPoint) The zero element in the tangent space to  p  on the  Tucker  manifold, represented as a  TuckerTangentVector . source"},{"id":1890,"pagetitle":"Tucker","title":"Literature","ref":"/manifolds/stable/manifolds/tucker/#Literature","content":" Literature [DBV21] N.¬†Dewaele, P.¬†Breiding and N.¬†Vannieuwenhoven.  The condition number of many tensor decompositions is invariant under Tucker compression , arXiv¬†Preprint (2021),  arXiv:2106.13034 . [KL10] O.¬†Koch and C.¬†Lubich.  Dynamical Tensor Approximation .  SIAM¬†Journal¬†on¬†Matrix¬†Analysis¬†and¬†Applications  31 , 2360‚Äì2375  (2010). [KSV13] D.¬†Kressner, M.¬†Steinlechner and B.¬†Vandereycken.  Low-rank tensor completion by Riemannian optimization .  BIT¬†Numerical¬†Mathematics  54 , 447‚Äì468  (2013). [LMV00] L.¬†D.¬†Lathauwer, B.¬†D.¬†Moor and J.¬†Vandewalle.  A Multilinear Singular Value Decomposition .  SIAM¬†Journal¬†on¬†Matrix¬†Analysis¬†and¬†Applications  21 , 1253‚Äì1278  (2000). [VVM12] N.¬†Vannieuwenhoven, R.¬†Vandebril and K.¬†Meerbergen.  A New Truncation Strategy for the Higher-Order Singular Value Decomposition .  SIAM¬†Journal¬†on¬†Scientific¬†Computing  34 , A1027‚ÄìA1052  (2012)."},{"id":1893,"pagetitle":"Vector bundle","title":"Vector bundles","ref":"/manifolds/stable/manifolds/vector_bundle/#VectorBundleSection","content":" Vector bundles Vector bundle  $E$  is a special case of a  fiber bundle  where each fiber is a vector space. Tangent bundle is a simple example of a vector bundle, where each fiber is the tangent space at the specified point  $p$ . An object representing a tangent bundle can be obtained using the constructor called  TangentBundle . There is also another type,  VectorSpaceFiber , that represents a specific fiber at a given point. This is also considered a manifold."},{"id":1894,"pagetitle":"Vector bundle","title":"FVector","ref":"/manifolds/stable/manifolds/vector_bundle/#FVector","content":" FVector For cases where confusion between different types of vectors is possible, the type  FVector  can be used to express which type of vector space the vector belongs to. It is used for example in musical isomorphisms (the  flat  and  sharp  functions) that are used to go from a tangent space to cotangent space and vice versa."},{"id":1895,"pagetitle":"Vector bundle","title":"Documentation","ref":"/manifolds/stable/manifolds/vector_bundle/#Documentation","content":" Documentation"},{"id":1896,"pagetitle":"Vector bundle","title":"Manifolds.TensorProductType","ref":"/manifolds/stable/manifolds/vector_bundle/#Manifolds.TensorProductType","content":" Manifolds.TensorProductType  ‚Äî  Type TensorProductType(spaces::VectorSpaceType...) Vector space type corresponding to the tensor product of given vector space types. source"},{"id":1897,"pagetitle":"Vector bundle","title":"Manifolds.TangentBundle","ref":"/manifolds/stable/manifolds/vector_bundle/#Manifolds.TangentBundle","content":" Manifolds.TangentBundle  ‚Äî  Type TangentBundle{ùîΩ,M} = VectorBundle{ùîΩ,TangentSpaceType,M} where {ùîΩ,M<:AbstractManifold{ùîΩ}} Tangent bundle for manifold of type  M , as a manifold with the Sasaki metric [ Sas58 ]. Exact retraction and inverse retraction can be approximated using  FiberBundleProductRetraction ,  FiberBundleInverseProductRetraction  and  SasakiRetraction .  FiberBundleProductVectorTransport  can be used as a vector transport. Constructors TangentBundle(M::AbstractManifold)\nTangentBundle(M::AbstractManifold, vtm::FiberBundleProductVectorTransport) source"},{"id":1898,"pagetitle":"Vector bundle","title":"Manifolds.VectorBundle","ref":"/manifolds/stable/manifolds/vector_bundle/#Manifolds.VectorBundle","content":" Manifolds.VectorBundle  ‚Äî  Type VectorBundle{ùîΩ,TVS,TM,VTV} = FiberBundle{ùîΩ,TVS,TM,TVT} where {TVS<:VectorSpaceType} Alias for  FiberBundle  when fiber type is a  TVS  of type  VectorSpaceType . VectorSpaceFiberType  is used to encode vector spaces as fiber types. source"},{"id":1899,"pagetitle":"Vector bundle","title":"Manifolds.fiber_bundle_transport","ref":"/manifolds/stable/manifolds/vector_bundle/#Manifolds.fiber_bundle_transport-Tuple{AbstractManifold, ManifoldsBase.FiberType}","content":" Manifolds.fiber_bundle_transport  ‚Äî  Method fiber_bundle_transport(M::AbstractManifold, fiber::FiberType) Determine the vector transport used for  exp  and  log  maps on a vector bundle with fiber type  fiber  and manifold  M . source"},{"id":1900,"pagetitle":"Vector bundle","title":"ManifoldsBase.injectivity_radius","ref":"/manifolds/stable/manifolds/vector_bundle/#ManifoldsBase.injectivity_radius-Tuple{FiberBundle{ùîΩ, TangentSpaceType, M} where {ùîΩ, M<:AbstractManifold{ùîΩ}}}","content":" ManifoldsBase.injectivity_radius  ‚Äî  Method injectivity_radius(M::TangentBundle) Injectivity radius of  TangentBundle  manifold is infinite if the base manifold is flat and 0 otherwise. See  https://mathoverflow.net/questions/94322/injectivity-radius-of-the-sasaki-metric . source"},{"id":1901,"pagetitle":"Vector bundle","title":"ManifoldsBase.inner","ref":"/manifolds/stable/manifolds/vector_bundle/#ManifoldsBase.inner-Tuple{FiberBundle, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(B::VectorBundle, p, X, Y) Inner product of tangent vectors  X  and  Y  at point  p  from the vector bundle  B  over manifold  B.fiber  (denoted  $\\mathcal M$ ). Notation: The point  $p = (x_p, V_p)$  where  $x_p ‚àà \\mathcal M$  and  $V_p$  belongs to the fiber  $F=œÄ^{-1}(\\{x_p\\})$  of the vector bundle  $B$  where  $œÄ$  is the canonical projection of that vector bundle  $B$ . The tangent vector  $v = (V_{X,M}, V_{X,F}) ‚àà T_{x}B$  where  $V_{X,M}$  is a tangent vector from the tangent space  $T_{x_p}\\mathcal M$  and  $V_{X,F}$  is a tangent vector from the tangent space  $T_{V_p}F$  (isomorphic to  $F$ ). Similarly for the other tangent vector  $w = (V_{Y,M}, V_{Y,F}) ‚àà T_{x}B$ . The inner product is calculated as $‚ü®X, Y‚ü©_p = ‚ü®V_{X,M}, V_{Y,M}‚ü©_{x_p} + ‚ü®V_{X,F}, V_{Y,F}‚ü©_{V_p}.$ source"},{"id":1902,"pagetitle":"Vector bundle","title":"ManifoldsBase.inverse_retract","ref":"/manifolds/stable/manifolds/vector_bundle/#ManifoldsBase.inverse_retract-Tuple{VectorBundle{ùîΩ} where ùîΩ, Any, Any, Manifolds.FiberBundleInverseProductRetraction}","content":" ManifoldsBase.inverse_retract  ‚Äî  Method inverse_retract(M::VectorBundle, p, q, ::FiberBundleInverseProductRetraction) Compute the allocating variant of the  FiberBundleInverseProductRetraction , which by default allocates and calls  inverse_retract_product! . source"},{"id":1903,"pagetitle":"Vector bundle","title":"ManifoldsBase.is_flat","ref":"/manifolds/stable/manifolds/vector_bundle/#ManifoldsBase.is_flat-Tuple{VectorBundle{ùîΩ} where ùîΩ}","content":" ManifoldsBase.is_flat  ‚Äî  Method is_flat(::VectorBundle) Return true if the underlying manifold of  VectorBundle M  is flat. source"},{"id":1904,"pagetitle":"Vector bundle","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/vector_bundle/#ManifoldsBase.project-Tuple{VectorBundle{ùîΩ} where ùîΩ, Any, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(B::VectorBundle, p, X) Project the element  X  of the ambient space of the tangent space  $T_p B$  to the tangent space  $T_p B$ . Notation: The point  $p = (x_p, V_p)$  where  $x_p ‚àà \\mathcal M$  and  $V_p$  belongs to the fiber  $F=œÄ^{-1}(\\{x_p\\})$  of the vector bundle  $B$  where  $œÄ$  is the canonical projection of that vector bundle  $B$ . The vector  $x = (V_{X,M}, V_{X,F})$  where  $x_p$  belongs to the ambient space of  $T_{x_p}\\mathcal M$  and  $V_{X,F}$  belongs to the ambient space of the fiber  $F=œÄ^{-1}(\\{x_p\\})$  of the vector bundle  $B$  where  $œÄ$  is the canonical projection of that vector bundle  $B$ . The projection is calculated by projecting  $V_{X,M}$  to tangent space  $T_{x_p}\\mathcal M$  and then projecting the vector  $V_{X,F}$  to the fiber  $F$ . source"},{"id":1905,"pagetitle":"Vector bundle","title":"ManifoldsBase.project","ref":"/manifolds/stable/manifolds/vector_bundle/#ManifoldsBase.project-Tuple{VectorBundle{ùîΩ} where ùîΩ, Any}","content":" ManifoldsBase.project  ‚Äî  Method project(B::VectorBundle, p) Project the point  p  from the ambient space of the vector bundle  B  over manifold  B.fiber  (denoted  $\\mathcal M$ ) to the vector bundle. Notation: The point  $p = (x_p, V_p)$  where  $x_p$  belongs to the ambient space of  $\\mathcal M$  and  $V_p$  belongs to the ambient space of the fiber  $F=œÄ^{-1}(\\{x_p\\})$  of the vector bundle  $B$  where  $œÄ$  is the canonical projection of that vector bundle  $B$ . The projection is calculated by projecting the point  $x_p$  to the manifold  $\\mathcal M$  and then projecting the vector  $V_p$  to the tangent space  $T_{x_p}\\mathcal M$ . source"},{"id":1906,"pagetitle":"Vector bundle","title":"ManifoldsBase.retract","ref":"/manifolds/stable/manifolds/vector_bundle/#ManifoldsBase.retract-Tuple{VectorBundle{ùîΩ} where ùîΩ, Any, Any, Manifolds.FiberBundleProductRetraction}","content":" ManifoldsBase.retract  ‚Äî  Method retract(M::VectorBundle, p, q, ::FiberBundleProductRetraction) Compute the allocating variant of the  FiberBundleProductRetraction , which by default allocates and calls  retract_product! . source"},{"id":1907,"pagetitle":"Vector bundle","title":"ManifoldsBase.vector_transport_to","ref":"/manifolds/stable/manifolds/vector_bundle/#ManifoldsBase.vector_transport_to-Tuple{VectorBundle{ùîΩ} where ùîΩ, Any, Any, Any, Manifolds.FiberBundleProductVectorTransport}","content":" ManifoldsBase.vector_transport_to  ‚Äî  Method vector_transport_to(M::VectorBundle, p, X, q, m::FiberBundleProductVectorTransport) Compute the vector transport the tangent vector  X at  p  to  q  on the  VectorBundle M  using the  FiberBundleProductVectorTransport m . source"},{"id":1908,"pagetitle":"Vector bundle","title":"Example","ref":"/manifolds/stable/manifolds/vector_bundle/#Example","content":" Example The following code defines a point on the tangent bundle of the sphere  $S^2$  and a tangent vector to that point. using Manifolds, RecursiveArrayTools\nM = Sphere(2)\nTB = TangentBundle(M)\np = ArrayPartition([1.0, 0.0, 0.0], [0.0, 1.0, 3.0])\nX = ArrayPartition([0.0, 1.0, 0.0], [0.0, 0.0, -2.0]) ([0.0, 1.0, 0.0], [0.0, 0.0, -2.0]) An approximation of the exponential in the Sasaki metric using 1000 steps can be calculated as follows. q = retract(TB, p, X, SasakiRetraction(1000))\nprintln(\"Approximation of the exponential map: \", q) Approximation of the exponential map: RecursiveArrayTools.ArrayPartition{Float64, Tuple{Vector{Float64}, Vector{Float64}}}(([0.6759570857309888, 0.35241486404386485, 0.6472138609849252], [-1.0318269583261073, 0.6273324630574116, 0.7360618920075961]))"},{"id":1911,"pagetitle":"Contributing","title":"Contributing to Manifolds.jl","ref":"/manifolds/stable/misc/CONTRIBUTING/#Contributing-to-Manifolds.jl","content":" Contributing to  Manifolds.jl First, thanks for taking the time to contribute. Any contribution is appreciated and welcome. The following is a set of guidelines to  Manifolds.jl ."},{"id":1912,"pagetitle":"Contributing","title":"Table of contents","ref":"/manifolds/stable/misc/CONTRIBUTING/#Table-of-contents","content":" Table of contents Contributing to  Manifolds.jl Table of Contents How to ask a question How to file an issue How to contribute Add a missing method Provide a new manifold Code style"},{"id":1913,"pagetitle":"Contributing","title":"How to ask a question","ref":"/manifolds/stable/misc/CONTRIBUTING/#How-to-ask-a-question","content":" How to ask a question You can most easily reach the developers in the Julia Slack channel  #manifolds . You can apply for the Julia Slack workspace  here  if you haven't joined yet. You can also ask your question on  discourse.julialang.org ."},{"id":1914,"pagetitle":"Contributing","title":"How to file an issue","ref":"/manifolds/stable/misc/CONTRIBUTING/#How-to-file-an-issue","content":" How to file an issue If you found a bug or want to propose a feature, issues are tracked within the  GitHub repository ."},{"id":1915,"pagetitle":"Contributing","title":"How to contribute","ref":"/manifolds/stable/misc/CONTRIBUTING/#How-to-contribute","content":" How to contribute"},{"id":1916,"pagetitle":"Contributing","title":"Overview of resources","ref":"/manifolds/stable/misc/CONTRIBUTING/#Overview-of-resources","content":" Overview of resources ManifoldsBase.jl  documents the  main design principles  for Riemannian manifolds in the  JuliaManifolds  ecosystem The  main set of functions  serves as a guide, showing which functions the Library of manifolds in  Manifolds.jl  provides. A  tutorial on how to define a manifold  serves as a starting point on how to introduce a new manifold The  changelog  documents all additions and changes. The corresponding file to edit is the  NEWS.md This file  CONTRIBUTING.md   provides a technical introduction to contributing to  Manifolds.jl"},{"id":1917,"pagetitle":"Contributing","title":"Add a missing method","ref":"/manifolds/stable/misc/CONTRIBUTING/#Add-a-missing-method","content":" Add a missing method Within  Manifolds.jl , there might be manifolds, that are only partially define the list of methods from the interface given in  ManifoldsBase.jl . If you notice a missing method but are aware of an algorithm or theory about it, contributing the method is welcome. Even just the smallest function is a good contribution."},{"id":1918,"pagetitle":"Contributing","title":"Provide a new manifold","ref":"/manifolds/stable/misc/CONTRIBUTING/#Provide-a-new-manifold","content":" Provide a new manifold A main contribution you can provide is another manifold that is not yet included in the package. A manifold is a concrete subtype of  AbstractManifold  from  ManifoldsBase.jl . A  tutorial on how to define a manifold  helps to get started on a new manifold. Every new manifold is welcome, even if you only add a few functions, for example when your use case for now does not require more features. One important detail is that the interface provides an in-place as well as a non-mutating variant See for example  exp!  and  exp . The non-mutating one,  exp , always falls back to allocating the according memory, here a point on the manifold, to then call the in-place variant. This way it suffices to provide the in-place variant,  exp! . The allocating variant only needs to defined if a more efficient version than the default is available. Note that since the first argument is  always  the  AbstractManifold , the mutated argument is always the second one in the signature. In the example there are  exp(M, p, X)  for the exponential map that allocates its result  q , and  exp!(M, q, p, X)  for the in-place one, which computes and returns the  q . Since a user probably looks for the documentation on the allocating variant, we recommend to attach the documentation string to this variant, mentioning all possible function signatures including the mutating one. You can best achieve this by adding a documentation string to the method with a general signature with the first argument being your manifold: struct MyManifold <: AbstractManifold end\n\n@doc \"\"\"\n    exp(M::MyManifold, p, X)\n    exp!(M::MyManifold, q, p, X)\n\nDescribe the function, its input and output as well as a mathematical formula.\n\"\"\"\nexp(::MyManifold, ::Any, ::Any) You can also save the string to a variable, for example  _doc_myM_exp  and attach it to both functions"},{"id":1919,"pagetitle":"Contributing","title":"Code style","ref":"/manifolds/stable/misc/CONTRIBUTING/#Code-style","content":" Code style Please follow the  documentation guidelines  from the Julia documentation as well as  Blue Style . Run  JuliaFormatter.jl  on the repository running  using JuliaFormatter; format(\".\")  on the main folder of the project. Please follow a few internal conventions: Please include a description of the manifold and a reference to the general theory in the  struct  of your manifold that inherits from  AbstractManifold '. Include the mathematical formulae for any implemented function if a closed form exists. Within the source code of one manifold, the  struct  the manifold should be the first element of the file. an alphabetical order of functions in every file is preferable. The preceding implies that the mutating variant of a function follows the non-mutating variant. There should be no dangling  =  signs. Always add a newline between things of different types ( struct /method/const). Always add a newline between methods for different functions (including allocating and in-place variants). Prefer to have no newline between methods for the same function; when reasonable, merge the documentation string. Always document all input variables and keyword arguments if possible provide both mathematical formulae and literature references using  DocumenterCitations.jl  and BibTeX where possible All  import / using / include  should be in the main module file."},{"id":1922,"pagetitle":"Changelog","title":"Changelog","ref":"/manifolds/stable/misc/NEWS/#Changelog","content":" Changelog All notable changes to this project will be documented in this file. The format is based on  Keep a Changelog , and this project adheres to  Semantic Versioning ."},{"id":1923,"pagetitle":"Changelog","title":"[0.10.23] 2025-07-19","ref":"/manifolds/stable/misc/NEWS/#[0.10.23]-2025-07-19","content":" [0.10.23] 2025-07-19"},{"id":1924,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed","content":" Fixed fix a small bug in the point checks of general unitary matrices."},{"id":1925,"pagetitle":"Changelog","title":"[0.10.22] 2025-06-25","ref":"/manifolds/stable/misc/NEWS/#[0.10.22]-2025-06-25","content":" [0.10.22] 2025-06-25"},{"id":1926,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-2","content":" Fixed Support for  ForwardDiff.jl  v1"},{"id":1927,"pagetitle":"Changelog","title":"[0.10.21] 2025-06-24","ref":"/manifolds/stable/misc/NEWS/#[0.10.21]-2025-06-24","content":" [0.10.21] 2025-06-24"},{"id":1928,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added","content":" Added Support for  ForwardDiff.jl  v1"},{"id":1929,"pagetitle":"Changelog","title":"[0.10.20] 2025-06-16","ref":"/manifolds/stable/misc/NEWS/#[0.10.20]-2025-06-16","content":" [0.10.20] 2025-06-16"},{"id":1930,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-3","content":" Fixed Fix in-place computation of  exp!  for  GeneralLinear ."},{"id":1931,"pagetitle":"Changelog","title":"[0.10.19] 2025-06-14","ref":"/manifolds/stable/misc/NEWS/#[0.10.19]-2025-06-14","content":" [0.10.19] 2025-06-14"},{"id":1932,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-2","content":" Added riemannian_gradient  for the  GeneralLinear  manifold riemannian_gradient  method for  ProductManifold ."},{"id":1933,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-4","content":" Fixed Fix the projection and embedding of tangent vectors on  GeneralLinear ."},{"id":1934,"pagetitle":"Changelog","title":"[0.10.18] 2025-05-29","ref":"/manifolds/stable/misc/NEWS/#[0.10.18]-2025-05-29","content":" [0.10.18] 2025-05-29"},{"id":1935,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-5","content":" Fixed Fix the supertype of  PoincareBallTangentVector  to be  AbstractTangentVector Fix the supertype of  StiefelTangentVector  to be  AbstractTangentVector Fix  riemannian_gradient  for fixed rank matrices, which did not work due to a small bug in the default fallback and a missing metric specification."},{"id":1936,"pagetitle":"Changelog","title":"[0.10.17] 2025-04-21","ref":"/manifolds/stable/misc/NEWS/#[0.10.17]-2025-04-21","content":" [0.10.17] 2025-04-21"},{"id":1937,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed","content":" Changed deprecate  GroupManifold s and its concrete subtypes as well as all functions related to Lie groups. They can now be found in the new package  LieGroups.jl , see their  How to transition from  GroupManifold s tutorial  for all details. Note that while it is currently not so easy to use  Manifolds.jl  and  LieGroups.jl  together due to the common definitions that are here now deprecated, it might still take a reasonable time to do a breaking release here, since we do not have a very good reason to yet."},{"id":1938,"pagetitle":"Changelog","title":"[0.10.16] 2025-04-08","ref":"/manifolds/stable/misc/NEWS/#[0.10.16]-2025-04-08","content":" [0.10.16] 2025-04-08"},{"id":1939,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-2","content":" Changed Added all  [compat]  entries also for the  docs/  and  tutorials/  environments get_vector  on  Circle  no longer returns  SArray  when no static arrays are passed as arguments."},{"id":1940,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-6","content":" Fixed a copy pase error in the new determinant one manifold from the last release."},{"id":1941,"pagetitle":"Changelog","title":"[0.10.15] 2025-03-28","ref":"/manifolds/stable/misc/NEWS/#[0.10.15]-2025-03-28","content":" [0.10.15] 2025-03-28"},{"id":1942,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-3","content":" Added a  DeterminantOneMatrices  manifold of matrices of determinant one."},{"id":1943,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-3","content":" Changed the following internal types were renamed since their super type also uses the singular AbsoluteDeterminantOneMatrices  to  AbsoluteDeterminantOneMatrixType DeterminantOneMatrices  to  DeterminantOneMatrixType"},{"id":1944,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-7","content":" Fixed fix  rand!  to also work on the  Circle(‚ÑÇ)"},{"id":1945,"pagetitle":"Changelog","title":"[0.10.14] - 2025-02-18","ref":"/manifolds/stable/misc/NEWS/#[0.10.14]-2025-02-18","content":" [0.10.14] - 2025-02-18"},{"id":1946,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-4","content":" Changed Introduced new implementation of parallel transport on  Rotations(3)  based on Rodrigues' rotation formula."},{"id":1947,"pagetitle":"Changelog","title":"[0.10.13] - 2025-02-10","ref":"/manifolds/stable/misc/NEWS/#[0.10.13]-2025-02-10","content":" [0.10.13] - 2025-02-10"},{"id":1948,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-5","content":" Changed Bumped dependency of ManifoldsBase.jl to 1.0, split  exp  into  exp  (without optional argument  t ) and  exp_fused  (with argument  t ) and similarly  retract  to  retract  and  retract_fused . ManifoldsBase.jl 1.0 also moved from  TVector  to  TangentVector s in type names. The following names are adapted Renamed  HyperboloidTVector  (now deprecated) to  HyperboloidTangentVector Renamed  OrthogonalTVector  (now deprecated) to  OrthogonalTangentVector Renamed  PoincareBallTVector  (now deprecated) to  PoincareBallTangentVector Renamed  PoincareHalfSpaceTVector  (now deprecated) to  PoincareHalfSpaceTangentVector Renamed  ProjectorTVector  (now deprecated) to  ProjectorTangentVector Renamed  StiefelTVector  (now deprecated) to  StiefelTangentVector Renamed  TuckerTVector  (now deprecated) to  TuckerTangentVector Renamed  UMVTVector  (now deprecated) to  UMVTangentVector The internal access  array_value  is now called  internal_value , compare to its renaming in  ManifoldsBase"},{"id":1949,"pagetitle":"Changelog","title":"[0.10.12] - 2025-01-10","ref":"/manifolds/stable/misc/NEWS/#[0.10.12]-2025-01-10","content":" [0.10.12] - 2025-01-10"},{"id":1950,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-4","content":" Added Orthonormal bases for  CholeskySpace  and  LogCholesky  metric for  SymmetricPositiveDefinite . rand  for  CholeskySpace ."},{"id":1951,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-6","content":" Changed Improved performance of selected  get_vector  and  get_coordinates  methods for complex  Euclidean  manifold."},{"id":1952,"pagetitle":"Changelog","title":"[0.10.11] - 2025-01-02","ref":"/manifolds/stable/misc/NEWS/#[0.10.11]-2025-01-02","content":" [0.10.11] - 2025-01-02"},{"id":1953,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-5","content":" Added Bases and rand for  HeisenbergMatrices  and  InvertibleMatrices ."},{"id":1954,"pagetitle":"Changelog","title":"[0.10.10] - 2024-12-20","ref":"/manifolds/stable/misc/NEWS/#[0.10.10]-2024-12-20","content":" [0.10.10] - 2024-12-20"},{"id":1955,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-6","content":" Added the  Segre  manifold the  WarpedMetric  for the  Segre manifold"},{"id":1956,"pagetitle":"Changelog","title":"[0.10.9] - 2024-12-16","ref":"/manifolds/stable/misc/NEWS/#[0.10.9]-2024-12-16","content":" [0.10.9] - 2024-12-16"},{"id":1957,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-7","content":" Added the  Segre  manifold the  WarpedMetric  for the  Segre manifold The manifold  HeisenbergMatrices  as the underlying manifold of  HeisenbergGroup ."},{"id":1958,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-7","content":" Changed about.md  now also lists contributors of manifolds and a very short history of the package."},{"id":1959,"pagetitle":"Changelog","title":"[0.10.8] ‚Äì 2024-11-27","ref":"/manifolds/stable/misc/NEWS/#[0.10.8]-‚Äì-2024-11-27","content":" [0.10.8] ‚Äì 2024-11-27"},{"id":1960,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-8","content":" Changed Some methods related to  get_vector  for  GeneralUnitaryMatrices  now have  AbstractVector  upper bound for coefficients. Minimum Julia version is now 1.10 (the LTS which replaced 1.6) The dependency ManifoldDiff.jl has been upgraded from v0.3 to v0.4, to bring compatibility with DifferentiationInterface.jl."},{"id":1961,"pagetitle":"Changelog","title":"[0.10.7] ‚Äì 2024-11-16","ref":"/manifolds/stable/misc/NEWS/#[0.10.7]-‚Äì-2024-11-16","content":" [0.10.7] ‚Äì 2024-11-16"},{"id":1962,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-8","content":" Added adjoint_matrix  for Lie groups, with optimized implementations for SO(2), SO(3), SE(2) and SE(3)."},{"id":1963,"pagetitle":"Changelog","title":"[0.10.6] ‚Äì 2024-11-06","ref":"/manifolds/stable/misc/NEWS/#[0.10.6]-‚Äì-2024-11-06","content":" [0.10.6] ‚Äì 2024-11-06"},{"id":1964,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-9","content":" Added Two new actions:  ComplexPlanarRotation ,  QuaternionRotation . New function  quaternion_rotation_matrix  for converting quaternions to rotation matrices. make.jl  script now has more command line arguments, for example  --exclude-tutorials  when you do not want to build the tutorials but still look at the docs. See  make.jl --help  for more information."},{"id":1965,"pagetitle":"Changelog","title":"[0.10.5] ‚Äì 2024-10-24","ref":"/manifolds/stable/misc/NEWS/#[0.10.5]-‚Äì-2024-10-24","content":" [0.10.5] ‚Äì 2024-10-24"},{"id":1966,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-10","content":" Added the manifold  InvertibleMatrices  of invertible matrices"},{"id":1967,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-9","content":" Changed rewrote the  CONTRIBUTING.md  and adapt it to today's links and references."},{"id":1968,"pagetitle":"Changelog","title":"[0.10.4] - 2024-10-20","ref":"/manifolds/stable/misc/NEWS/#[0.10.4]-2024-10-20","content":" [0.10.4] - 2024-10-20"},{"id":1969,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-11","content":" Added uniform_distribution  now has an error hint explaining what has to be done to make it work. Euclidean  now follows the new  has_components  function from  ManifoldsBase.jl  (0.15.18) and can handle also the  r -norms now. Union type  MatrixGroup Columnwise group action with arbitrary matrix groups uniform_distribution  now has an error hint explaining what has to be done to make it work. lie_bracket  is exactly zero on orthogonal Lie algebra in 2D"},{"id":1970,"pagetitle":"Changelog","title":"[0.10.3] - 2024-10-04","ref":"/manifolds/stable/misc/NEWS/#[0.10.3]-2024-10-04","content":" [0.10.3] - 2024-10-04"},{"id":1971,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-10","content":" Changed Mildly breaking : the number system parameter now corresponds to the coefficients standing in front of basis vectors in a linear combination instead of components of a vector. For example,  DefaultOrthonormalBasis() == DefaultOrthonormalBasis(‚Ñù)  of  Euclidean(3, field=‚ÑÇ)  now has 6 vectors, and  DefaultOrthonormalBasis(‚ÑÇ)  of the same manifold has 3 basis vectors."},{"id":1972,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-8","content":" Fixed Fixed  solve_exp_ode  only returning the starting position ( #744 ) Fixed documentation of  solve_exp_ode  function signature ( #740 )"},{"id":1973,"pagetitle":"Changelog","title":"[0.10.2] - 2024-09-24","ref":"/manifolds/stable/misc/NEWS/#[0.10.2]-2024-09-24","content":" [0.10.2] - 2024-09-24"},{"id":1974,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-12","content":" Added GroupManifold  can now be called with two arguments, the third one defaulting to  LeftInvariantRepresentation ."},{"id":1975,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-11","content":" Changed fixes a few typographical errors."},{"id":1976,"pagetitle":"Changelog","title":"[0.10.1] ‚Äì 2024-08-29","ref":"/manifolds/stable/misc/NEWS/#[0.10.1]-‚Äì-2024-08-29","content":" [0.10.1] ‚Äì 2024-08-29"},{"id":1977,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-12","content":" Changed identity_element  on  ProductManifold  without  RecursiveArrayTools.jl  now prints a useful error message."},{"id":1978,"pagetitle":"Changelog","title":"[0.10.0] ‚Äì 2024-08-24","ref":"/manifolds/stable/misc/NEWS/#[0.10.0]-‚Äì-2024-08-24","content":" [0.10.0] ‚Äì 2024-08-24"},{"id":1979,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-13","content":" Changed Distributions.jl ,  RecursiveArrayTools.jl  and  HybridArrays.jl  were moved to weak dependencies to reduce load time and improve extensibility. translate_diff ,  inv_diff  and thus  apply_diff_group , are available for all the groups with invariant tangent vector storage. SpecialEuclidean  group now has a different default tangent vector representation, the left-invariant one; to get the old representation pass  vectors=HybridTangentRepresentation()  to the constructor of  SpecialEuclidean . adjoint_action  takes a direction argument; by default it is  LeftAction . adjoint_action!  is the necessary method to implement in groups with left-invariant tangent vector representation. Fixed a few typos in the doc string of the SPD fixed determinant description. Random point on the  MultinomialSymmetricPositiveDefinite  manifold was improved to make it more robust."},{"id":1980,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-13","content":" Added Introduced  exp_inv  and  log_inv  based on  exp_lie  and  log_lie . They are invariant to the group operation. A tutorial about usage of group-related functionality."},{"id":1981,"pagetitle":"Changelog","title":"Removed","ref":"/manifolds/stable/misc/NEWS/#Removed","content":" Removed Deprecated bindings: ExtrinsicEstimation()  (should be replaced with  ExtrinsicEstimation(EfficientEstimator()) ), Symplectic  (renamed to  SymplecticMatrices ), SymplecticMatrix  (renamed to  SymplecticElement ). AbstractEstimationMethod  (renamed to  AbstractApproximationMethod ). VectorBundleVectorTransport  (renamed to  FiberBundleProductVectorTransport ). rand  on  SymplecticMatrices  and  SymplecticStiefel  no longer accepts  hamiltonian_norm  as an alias for  œÉ . mean!  and  median!  no longer accept  extrinsic_method  (should be replaced with  e = ExtrinsicEstimation(extrinsic_method) ). As a result of making  Distributions.jl  and  RecursiveArrayTools.jl  weak dependencies the following symbols are no longer exported from  Manifolds.jl . Essential functionality is still available but distribution-related features may change in the future without a breaking release. ArrayPartition  ( RecursiveArrayTools.jl  needs to be explicitly imported), ProjectedPointDistribution  (not exported), normal_tvector_distribution  (not exported), projected_distribution  (not exported), uniform_distribution  (not exported). Ability to create non-real  SymplecticStiefel  and  SymplecticGrassmann  manifolds; essential functionality was missing so it was removed until a more developed version is developed."},{"id":1982,"pagetitle":"Changelog","title":"[0.9.20] ‚Äì 2024-06-17","ref":"/manifolds/stable/misc/NEWS/#[0.9.20]-‚Äì-2024-06-17","content":" [0.9.20] ‚Äì 2024-06-17"},{"id":1983,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-14","content":" Added implemented parallel transport on the Grassmann manifold with respect to Stiefel representation"},{"id":1984,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-14","content":" Changed since now all exp/log/parallel transport are available for all representations of  Grassmann , these are now also set as defaults, since they are more exact."},{"id":1985,"pagetitle":"Changelog","title":"[0.9.19] ‚Äì 2024-06-12","ref":"/manifolds/stable/misc/NEWS/#[0.9.19]-‚Äì-2024-06-12","content":" [0.9.19] ‚Äì 2024-06-12"},{"id":1986,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-15","content":" Changed Updated  Project.toml  compatibility entries. Updated CI for Julia 1.11-beta"},{"id":1987,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-9","content":" Fixed a few typos in the doc string of the SPD fixed determinant description several other typographical errors throughout the documentation"},{"id":1988,"pagetitle":"Changelog","title":"[0.9.18] ‚Äì 2024-05-07","ref":"/manifolds/stable/misc/NEWS/#[0.9.18]-‚Äì-2024-05-07","content":" [0.9.18] ‚Äì 2024-05-07"},{"id":1989,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-15","content":" Added added the injectivity radius for the Stiefel manifold with Euclidean metric"},{"id":1990,"pagetitle":"Changelog","title":"[0.9.17] ‚Äì 2024-04-23","ref":"/manifolds/stable/misc/NEWS/#[0.9.17]-‚Äì-2024-04-23","content":" [0.9.17] ‚Äì 2024-04-23"},{"id":1991,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-16","content":" Added Hyperrectangle  manifold with boundary."},{"id":1992,"pagetitle":"Changelog","title":"[0.9.16] ‚Äì 2024-04-01","ref":"/manifolds/stable/misc/NEWS/#[0.9.16]-‚Äì-2024-04-01","content":" [0.9.16] ‚Äì 2024-04-01"},{"id":1993,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-16","content":" Changed NonlinearSolve.jl  and  PythonCall.jl  are no longer an upper bounded dependency (bugs were fixed)."},{"id":1994,"pagetitle":"Changelog","title":"[0.9.15] ‚Äì 2024-03-24","ref":"/manifolds/stable/misc/NEWS/#[0.9.15]-‚Äì-2024-03-24","content":" [0.9.15] ‚Äì 2024-03-24"},{"id":1995,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-17","content":" Added using  DocumenterInterLinks  for links to other Julia packages documentation. Implementation of  sectional_curvature ,  sectional_curvature_min  and  sectional_curvature_max  for several manifolds. sectional_curvature_matrix  function and a tutorial on coordinate-free curvature."},{"id":1996,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-17","content":" Changed default_vector_transport_method  for  GeneralUnitaryMatrices  other than  Rotations  was changed to  ProjectionTransport ."},{"id":1997,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-10","content":" Fixed typographical errors in tutorials/working-in-charts.jl. several typographical errors in the docs unifies to use two backticks  ``  for math instead of  $  further in the docs"},{"id":1998,"pagetitle":"Changelog","title":"[0.9.14] ‚Äì 2024-01-31","ref":"/manifolds/stable/misc/NEWS/#[0.9.14]-‚Äì-2024-01-31","content":" [0.9.14] ‚Äì 2024-01-31"},{"id":1999,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-18","content":" Added rand  on  UnitaryMatrices rand  on arbitrary  GroupManifold s and manifolds with  IsGroupManifold  trait generating points and elements from the Lie algebra, respectively"},{"id":2000,"pagetitle":"Changelog","title":"[0.9.13] ‚Äì 2024-01-24","ref":"/manifolds/stable/misc/NEWS/#[0.9.13]-‚Äì-2024-01-24","content":" [0.9.13] ‚Äì 2024-01-24"},{"id":2001,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-19","content":" Added added the real symplectic Grassmann manifold  SymplecticGrassmann Introduce the manifold of  HamiltonianMatrices  and a wrapper for  Hamiltonian  matrices introduce  rand(:HamiltonianMatrices) extend  rand  to also  rand!  for  HamiltonianMatrices ,  SymplecticMatrices  and  SymplecticStiefel implement  riemannian_gradient  conversion for  SymplecticMatrices  and  SymplecticGrassmann the new manifold of  MultinomialSymmetricPositiveDefinite  matrices rand!  for  MultinomialDoublyStochastic  and  MultinomialSymmetric"},{"id":2002,"pagetitle":"Changelog","title":"Deprecated","ref":"/manifolds/stable/misc/NEWS/#Deprecated","content":" Deprecated Rename  Symplectic  to  SimplecticMatrices  in order to have a  Symplectic  wrapper for such matrices as well in the future for the next breaking change. Rename  SymplecticMatrix  to  SymplecticElement  to clarify that it is the special matrix  $J_{2n}$  and not an arbitrary symplectic matrix."},{"id":2003,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-11","content":" Fixed a bug that cause  project  for tangent vectors to return wrong results on  MultinomialDoublyStochastic"},{"id":2004,"pagetitle":"Changelog","title":"[0.9.12] ‚Äì 2024-01-21","ref":"/manifolds/stable/misc/NEWS/#[0.9.12]-‚Äì-2024-01-21","content":" [0.9.12] ‚Äì 2024-01-21"},{"id":2005,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-12","content":" Fixed Fixed  var  on  TranslationGroup ."},{"id":2006,"pagetitle":"Changelog","title":"[0.9.11] ‚Äì 2023-12-27","ref":"/manifolds/stable/misc/NEWS/#[0.9.11]-‚Äì-2023-12-27","content":" [0.9.11] ‚Äì 2023-12-27"},{"id":2007,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-13","content":" Fixed Fixed mixed array index number in-place  parallel_transport_to!  on zero-index  Euclidean , on Julia 1.6."},{"id":2008,"pagetitle":"Changelog","title":"[0.9.10] ‚Äì 2023-12-27","ref":"/manifolds/stable/misc/NEWS/#[0.9.10]-‚Äì-2023-12-27","content":" [0.9.10] ‚Äì 2023-12-27"},{"id":2009,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-20","content":" Added Compatibility with  RecursiveArrayTools  v3."},{"id":2010,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-14","content":" Fixed Fixed mixed array index number in-place  parallel_transport_to!  on real  Circle , on Julia 1.6. Violations of MD004 lint rule in this file."},{"id":2011,"pagetitle":"Changelog","title":"[0.9.9] ‚Äì 2023-12-25","ref":"/manifolds/stable/misc/NEWS/#[0.9.9]-‚Äì-2023-12-25","content":" [0.9.9] ‚Äì 2023-12-25"},{"id":2012,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-15","content":" Fixed introduced a nonzero  atol  for all point and vector checks that compre to zero. This makes those checks a bit more relaxed by default and resolves  #630 . default_estimation_method(M, f)  is deprecated, use  default_approximation_method(M, f)  for your specific method  f  on the manifold  M . AbstractEstimationMethod  is deprecated, use  AbstractApproximationMethod  instead."},{"id":2013,"pagetitle":"Changelog","title":"[0.9.8] - 2023-11-17","ref":"/manifolds/stable/misc/NEWS/#[0.9.8]-2023-11-17","content":" [0.9.8] - 2023-11-17"},{"id":2014,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-16","content":" Fixed Improved distribution of random vector generation for rotation matrices and complex circle."},{"id":2015,"pagetitle":"Changelog","title":"[0.9.7] ‚Äì 2023-11-14","ref":"/manifolds/stable/misc/NEWS/#[0.9.7]-‚Äì-2023-11-14","content":" [0.9.7] ‚Äì 2023-11-14"},{"id":2016,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-17","content":" Fixed Fixed  is_flat  for  CholeskySpace  and  SymmetricPositiveDefinite  with  LogCholeskyMetric https://github.com/JuliaManifolds/Manifolds.jl/issues/684 ."},{"id":2017,"pagetitle":"Changelog","title":"[0.9.6] - 2023-11-09","ref":"/manifolds/stable/misc/NEWS/#[0.9.6]-2023-11-09","content":" [0.9.6] - 2023-11-09"},{"id":2018,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-18","content":" Fixed Fixed real coefficient basis for complex circle (an issue exposed by  https://github.com/JuliaManifolds/ManifoldsBase.jl/pull/173 ). Fixed  VeeOrthogonalBasis  test for non-real manifolds."},{"id":2019,"pagetitle":"Changelog","title":"[0.9.5] - 2023-11-08","ref":"/manifolds/stable/misc/NEWS/#[0.9.5]-2023-11-08","content":" [0.9.5] - 2023-11-08"},{"id":2020,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-18","content":" Changed identity_element  now returns a complex matrix for unitary group. number_of_coordinates  is now exported."},{"id":2021,"pagetitle":"Changelog","title":"[0.9.4] - 2023-11-06","ref":"/manifolds/stable/misc/NEWS/#[0.9.4]-2023-11-06","content":" [0.9.4] - 2023-11-06"},{"id":2022,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-21","content":" Added Functions  inv_diff ,  inv_diff! ,  adjoint_inv_diff  and  adjoint_inv_diff!  that correspond to differentials and pullbacks of group inversion. Julia 1.10-rc CI workflow."},{"id":2023,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-19","content":" Changed Documentation project files are marked as compatible with  BoundaryValueDiffEq  v5."},{"id":2024,"pagetitle":"Changelog","title":"Fixed","ref":"/manifolds/stable/misc/NEWS/#Fixed-19","content":" Fixed Fixed issue with incorrect implementation of  apply_diff_group  in  GroupOperationAction  with left backward and right forward action  #669 ."},{"id":2025,"pagetitle":"Changelog","title":"[0.9.3] - 2023-10-28","ref":"/manifolds/stable/misc/NEWS/#[0.9.3]-2023-10-28","content":" [0.9.3] - 2023-10-28"},{"id":2026,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-22","content":" Added Support for  BoundaryValueDiffEq  v5."},{"id":2027,"pagetitle":"Changelog","title":"[0.9.2] - 2023-10-27","ref":"/manifolds/stable/misc/NEWS/#[0.9.2]-2023-10-27","content":" [0.9.2] - 2023-10-27"},{"id":2028,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-23","content":" Added rand(G; vector_at=Identity(G))  now works for translation, special orthogonal and special Euclidean groups  G  (issue  #665 ). get_embedding  now works for  GeneralUnitaryMultiplicationGroup . Github action that checks for NEWS.md changes."},{"id":2029,"pagetitle":"Changelog","title":"[0.9.1] - 2023-10-25","ref":"/manifolds/stable/misc/NEWS/#[0.9.1]-2023-10-25","content":" [0.9.1] - 2023-10-25"},{"id":2030,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-24","content":" Added a new retraction and its inverse for the fixed Rank Manifolds, the orthographic retraction."},{"id":2031,"pagetitle":"Changelog","title":"[0.9.0] - 2023-10-24","ref":"/manifolds/stable/misc/NEWS/#[0.9.0]-2023-10-24","content":" [0.9.0] - 2023-10-24"},{"id":2032,"pagetitle":"Changelog","title":"Added","ref":"/manifolds/stable/misc/NEWS/#Added-25","content":" Added Vector bundles are generalized to fiber bundles. Old  BundleFibers  functionality was reworked to better match mathematical abstractions. Fiber bundle functionality is experimental and minor changes may happen without a breaking release, with the exception of  TangentBundle  which is considered to be stable. RotationTranslationAction  is introduced."},{"id":2033,"pagetitle":"Changelog","title":"Changed","ref":"/manifolds/stable/misc/NEWS/#Changed-20","content":" Changed Sizes of all manifolds can now be either encoded in type or stored in a field to avoid over-specialization. The default is set to store the size in type parameter (except for  PowerManifold  and its variants), replicating the previous behavior. For field storage, pass the  parameter=:field  keyword argument to manifold constructor. For example statically sized  CenteredMatrices{m,n}  is now  CenteredMatrices{TypeParameter{Tuple{m,n}}} , whereas the type of special Euclidean group with field-stored size is  CenteredMatrices{Tuple{Int,Int}} . Similar change applies to: CenteredMatrices{m,n} , CholeskySpace{N} , Elliptope{N,K} , Euclidean , FixedRankMatrices{m,n,k} , KendallsPreShapeSpace{n,k} , KendallsShapeSpace{n,k} , GeneralLinear{n} , GeneralUnitaryMultiplicationGroup{n} , GeneralizedGrassmann{n,k} , GeneralizedStiefel{n,k} , Grassmann{n,k} , Heisenberg{n} , Hyperbolic{n} , MultinomialMatrices{N,M} , MultinomialDoublyStochastic{n} , MultinomialSymmetric{n} , Orthogonal{n} , PowerManifold , PositiveArrays , PositiveMatrices , PositiveNumbers , ProbabilitySimplex{n} , SPDFixedDeterminant{n} , SpecialLinear{n} , SpecialOrthogonal{n} , SpecialUnitary{n} , SpecialEuclidean{n} , SpecialEuclideanManifold{n} , Spectrahedron{n,k} , SphereSymmetricMatrices{N} , Stiefel{n,k} , SymmetricMatrices{N} , SymmetricPositiveDefinite{n} , SymmetricPositiveSemidefiniteFixedRank{n,k} , Symplectic{n} , SymplecticStiefel{n,k} , TranslationGroup , Tucker . For example function Base.show(io::IO, ::CenteredMatrices{m,n}) where {m,n}\n    return print(io, \"CenteredMatrices($m, $n)\")\nend needs to be replaced with function Base.show(io::IO, ::CenteredMatrices{TypeParameter{Tuple{m,n}}}) where {m,n}\n    return print(io, \"CenteredMatrices($m, $n)\")\nend for statically-sized groups and function Base.show(io::IO, M::CenteredMatrices{Tuple{Int,Int}})\n    m, n = get_parameter(M.size)\n    return print(io, \"CenteredMatrices($m, $n; parameter=:field)\")\nend for groups with size stored in field. Alternatively, you can use a single generic method like this: function Base.show(io::IO, M::CenteredMatrices{T}) where {T}\n    m, n = get_parameter(M)\n    if T <: TypeParameter\n        return print(io, \"CenteredMatrices($m, $n)\")\n    else\n        return print(io, \"CenteredMatrices($m, $n; parameter=:field)\")\n    end\nend Argument order for type aliases  RotationActionOnVector  and  RotationTranslationActionOnVector : most often dispatched on argument is now first. A more consistent handling of action direction was introduced. 4-valued  ActionDirection  was split into 2-valued  ActionDirection  (either left or right action) and  GroupActionSide  (action acting from the left or right side). See  https://github.com/JuliaManifolds/Manifolds.jl/issues/637  for a design discussion."},{"id":2034,"pagetitle":"Changelog","title":"Removed","ref":"/manifolds/stable/misc/NEWS/#Removed-2","content":" Removed ProductRepr  is removed; please use  ArrayPartition  instead. Default methods throwing \"not implemented\"  ErrorException  for some group-related operations. Standard  MethodError  is now thrown instead. LinearAffineMetric  was deprecated in a previous release and the symbol is now removed. Please use  AffineInvariantMetric  instead."},{"id":2037,"pagetitle":"About","title":"About Manifolds.jl","ref":"/manifolds/stable/misc/about/#About-Manifolds.jl","content":" About  Manifolds.jl Manifolds.jl  was started by  Seth Axen ,  Mateusz Baran ,  Ronny Bergmann , and  Antoine Levitt  in 2019, after a very fruitful discussion following the release of the first version of  Manopt.jl . The goal of  Manifolds.jl  is to provide a library of manifolds in Julia. The manifolds are implemented using the  ManifoldsBase.jl  interface."},{"id":2038,"pagetitle":"About","title":"Main developers","ref":"/manifolds/stable/misc/about/#Main-developers","content":" Main developers Mateusz Baran Ronny Bergmann"},{"id":2039,"pagetitle":"About","title":"Former Main Developers","ref":"/manifolds/stable/misc/about/#Former-Main-Developers","content":" Former Main Developers Seth Axen"},{"id":2040,"pagetitle":"About","title":"Contributors","ref":"/manifolds/stable/misc/about/#Contributors","content":" Contributors (in alphabetical order) Nick Dewaele  contributed the  Tucker manifold Ren√©e Dornig  contributed the  centered  matrices  and the  essential manifold David Hong  contributed uniform distributions on the Stiefel and Grassmann manifolds. Simon Jacobsson  contributed the  Segre manifold  including its  warped metric  thereon. Johannes Voll Kolst√∏  contributed the  symplectic manifold , the  symplectic Stiefel manifold Manuel Wei√ü  contributed  symmetric matrices as well as everyone else reporting, investigating, and fixing bugs or fixing typographical errors in the documentation, see the  GitHub contributors page . Of course all further  contributions  are always welcome!"},{"id":2041,"pagetitle":"About","title":"Projects using Manifolds.jl","ref":"/manifolds/stable/misc/about/#Projects-using-Manifolds.jl","content":" Projects using  Manifolds.jl Caesar.jl  ‚Äì Robust robotic localization and mapping Flowfusion.jl  ‚Äì training and sampling from diffusion and flow matching models ManoptExamples.jl  ‚Äì collecting examples of optimization problems on manifolds implemented using  Manifolds.jl  and  Manopt.jl . Do you use Manifolds.jl in you package? Let us know and open an  issue  or  pull request  to add it to the list!"},{"id":2042,"pagetitle":"About","title":"License","ref":"/manifolds/stable/misc/about/#License","content":" License MIT License"},{"id":2045,"pagetitle":"Internals","title":"Internal documentation","ref":"/manifolds/stable/misc/internals/#Internal-documentation","content":" Internal documentation This page documents the internal types and methods of  Manifolds.jl 's that might be of use for writing your own manifold."},{"id":2046,"pagetitle":"Internals","title":"Functions","ref":"/manifolds/stable/misc/internals/#Functions","content":" Functions"},{"id":2047,"pagetitle":"Internals","title":"Manifolds.eigen_safe","ref":"/manifolds/stable/misc/internals/#Manifolds.eigen_safe","content":" Manifolds.eigen_safe  ‚Äî  Function eigen_safe(x) Compute the eigendecomposition of  x . If  x  is a  StaticMatrix , it is converted to a  Matrix  before the decomposition. source"},{"id":2048,"pagetitle":"Internals","title":"Manifolds.get_parameter_type","ref":"/manifolds/stable/misc/internals/#Manifolds.get_parameter_type","content":" Manifolds.get_parameter_type  ‚Äî  Function get_parameter_type(M::AbstractManifold) Get  parameter  argument of the constructor of manifold  M . Returns either  :field  or  :type . See also get_parameter ,  TypeParameter source"},{"id":2049,"pagetitle":"Internals","title":"Manifolds.isnormal","ref":"/manifolds/stable/misc/internals/#Manifolds.isnormal","content":" Manifolds.isnormal  ‚Äî  Function isnormal(x; kwargs...) -> Bool Check if the matrix or number  x  is normal, that is, if it commutes with its adjoint: \\[x x^\\mathrm{H} = x^\\mathrm{H} x.\\] By default, this is an equality check. Provide  kwargs  for  isapprox  to perform an approximate check. source"},{"id":2050,"pagetitle":"Internals","title":"Manifolds.log_safe","ref":"/manifolds/stable/misc/internals/#Manifolds.log_safe","content":" Manifolds.log_safe  ‚Äî  Function log_safe(x) Compute the matrix logarithm of  x . If  x  is a  StaticMatrix , it is converted to a  Matrix  before computing the log. source"},{"id":2051,"pagetitle":"Internals","title":"Manifolds.log_safe!","ref":"/manifolds/stable/misc/internals/#Manifolds.log_safe!","content":" Manifolds.log_safe!  ‚Äî  Function log_safe!(y, x) Compute the matrix logarithm of  x . If the eltype of  y  is real, then the imaginary part of  x  is ignored, and a  DomainError  is raised if  real(x)  has no real logarithm. source"},{"id":2052,"pagetitle":"Internals","title":"Manifolds.mul!_safe","ref":"/manifolds/stable/misc/internals/#Manifolds.mul!_safe","content":" Manifolds.mul!_safe  ‚Äî  Function mul!_safe(Y, A, B) -> Y Call  mul!  safely, that is,  A  and/or  B  are permitted to alias with  Y . source"},{"id":2053,"pagetitle":"Internals","title":"Manifolds.nzsign","ref":"/manifolds/stable/misc/internals/#Manifolds.nzsign","content":" Manifolds.nzsign  ‚Äî  Function nzsign(z[, absz]) Compute a modified  sign(z)  that is always nonzero, i.e. where \\[\\operatorname(nzsign)(z) = \\begin{cases}\n    1 & \\text{if } z = 0\\\\\n    \\frac{z}{|z|} & \\text{otherwise}\n\\end{cases}\\] Note that the condition  absz == 0  would be incorrectly handled by ForwardDiff.jl. source"},{"id":2054,"pagetitle":"Internals","title":"Manifolds.realify","ref":"/manifolds/stable/misc/internals/#Manifolds.realify","content":" Manifolds.realify  ‚Äî  Function realify(X::AbstractMatrix{TùîΩ}, ùîΩ::AbstractNumbers) -> Y::AbstractMatrix{<:Real} Given a matrix  $X ‚àà ùîΩ^{n√ón}$ , compute  $Y ‚àà ‚Ñù^{m√óm}$ , where  $m = n \\operatorname{dim}_ùîΩ$ , and  $\\operatorname{dim}_ùîΩ$  is the  real_dimension  of the number field  $ùîΩ$ , using the map  $œï \\colon X ‚Ü¶ Y$ , that preserves the matrix product, so that for all  $C,D ‚àà ùîΩ^{n√ón}$ , \\[œï(C) œï(D) = œï(CD).\\] See  realify!  for an in-place version, and  unrealify!  to compute the inverse of  $œï$ . source"},{"id":2055,"pagetitle":"Internals","title":"Manifolds.realify!","ref":"/manifolds/stable/misc/internals/#Manifolds.realify!","content":" Manifolds.realify!  ‚Äî  Function realify!(Y::AbstractMatrix{<:Real}, X::AbstractMatrix{TùîΩ}, ùîΩ::AbstractNumbers) In-place version of  realify . source realify!(Y::AbstractMatrix{<:Real}, X::AbstractMatrix{<:Complex}, ::typeof(‚ÑÇ)) Given a complex matrix  $X = A + iB ‚àà ‚ÑÇ^{n√ón}$ , compute its realified matrix  $Y ‚àà ‚Ñù^{2n√ó2n}$ , written where \\[Y = \\begin{pmatrix}A & -B \\\\ B & A \\end{pmatrix}.\\] source"},{"id":2056,"pagetitle":"Internals","title":"Manifolds.symmetrize","ref":"/manifolds/stable/misc/internals/#Manifolds.symmetrize","content":" Manifolds.symmetrize  ‚Äî  Function symmetrize(X) Given a square matrix  X  compute  1/2 .* (X' + X) . source"},{"id":2057,"pagetitle":"Internals","title":"Manifolds.symmetrize!","ref":"/manifolds/stable/misc/internals/#Manifolds.symmetrize!","content":" Manifolds.symmetrize!  ‚Äî  Function symmetrize!(Y, X) Given a square matrix  X  compute  1/2 .* (X' + X)  in place of  Y . source"},{"id":2058,"pagetitle":"Internals","title":"Manifolds.unrealify!","ref":"/manifolds/stable/misc/internals/#Manifolds.unrealify!","content":" Manifolds.unrealify!  ‚Äî  Function unrealify!(X::AbstractMatrix{TùîΩ}, Y::AbstractMatrix{<:Real}, ùîΩ::AbstractNumbers[, n]) Given a real matrix  $Y ‚àà ‚Ñù^{m√óm}$ , where  $m = n \\operatorname{dim}_ùîΩ$ , and  $\\operatorname{dim}_ùîΩ$  is the  real_dimension  of the number field  $ùîΩ$ , compute in-place its equivalent matrix  $X ‚àà ùîΩ^{n√ón}$ . Note that this function does not check that  $Y$  has a valid structure to be un-realified. See  realify!  for the inverse of this function. source"},{"id":2059,"pagetitle":"Internals","title":"Manifolds.usinc","ref":"/manifolds/stable/misc/internals/#Manifolds.usinc","content":" Manifolds.usinc  ‚Äî  Function usinc(Œ∏::Real) Unnormalized version of  sinc  function, i.e.  $\\operatorname{usinc}(Œ∏) = \\frac{\\sin(Œ∏)}{Œ∏}$ . This is equivalent to  sinc(Œ∏/œÄ) . Note that ForwardDiff.jl would return wrong answer at Œ∏=0 if a simple equality was used. source"},{"id":2060,"pagetitle":"Internals","title":"Manifolds.usinc_from_cos","ref":"/manifolds/stable/misc/internals/#Manifolds.usinc_from_cos","content":" Manifolds.usinc_from_cos  ‚Äî  Function usinc_from_cos(x::Real) Unnormalized version of  sinc  function, i.e.  $\\operatorname{usinc}(Œ∏) = \\frac{\\sin(Œ∏)}{Œ∏}$ , computed from  $x = cos(Œ∏)$ . source"},{"id":2061,"pagetitle":"Internals","title":"Manifolds.vec2skew!","ref":"/manifolds/stable/misc/internals/#Manifolds.vec2skew!","content":" Manifolds.vec2skew!  ‚Äî  Function vec2skew!(X, v, k) Create a skew symmetric matrix in-place in  X  of size  $k√ók$  from a vector  v , for example for  v=[1,2,3]  and  k=3  this yields [  0  1  2;\n  -1  0  3;\n  -2 -3  0\n] source"},{"id":2062,"pagetitle":"Internals","title":"Types in Extensions","ref":"/manifolds/stable/misc/internals/#Types-in-Extensions","content":" Types in Extensions"},{"id":2065,"pagetitle":"Notation","title":"Notation overview","ref":"/manifolds/stable/misc/notation/#Notation-overview","content":" Notation overview Since manifolds include a reasonable amount of elements and functions, the following list tries to keep an overview of used notation throughout  Manifolds.jl . The order is alphabetical by name. They might be used in a plain form within the code or when referring to that code. This is for example the case with the calligraphic symbols. Within the documented functions, the utf8 symbols are used whenever possible, as long as that renders correctly in  $\\TeX$  within this documentation. Symbol Description Also used Comment $\\tau_p$ action map by group element  $p$ $\\mathrm{L}_p$ ,  $\\mathrm{R}_p$ either left or right $\\operatorname{Ad}_p(X)$ adjoint action of element  $p$  of a Lie group on the element  $X$  of the corresponding Lie algebra $√ó$ Cartesian product of two manifolds see  ProductManifold $^{\\wedge}$ (n-ary) Cartesian power of a manifold see  PowerManifold $‚ãÖ^\\mathrm{H}$ conjugate/Hermitian transpose $a$ coordinates of a point in a chart see  get_parameters $\\frac{\\mathrm{D}}{\\mathrm{d}t}$ covariant derivative of a vector field  $X(t)$ $T^*_p \\mathcal M$ the cotangent space at  $p$ $Œæ$ a cotangent vector from  $T^*_p \\mathcal M$ $Œæ_1, Œæ_2,‚Ä¶ ,Œ∑,\\zeta$ sometimes written with base point  $Œæ_p$ . $\\mathrm{d}\\phi_p(q)$ Differential of a map  $\\phi: \\mathcal M ‚Üí \\mathcal N$  with respect to  $p$  at a point  $q$ . For functions of multiple variables, for example  $\\phi(p, p_1)$  where  $p \\in \\mathcal M$  and  $p_1 \\in \\mathcal M_1$ , variable  $p$  is explicitly stated to specify with respect to which argument the differential is calculated. $\\mathrm{d}\\phi_q$ ,  $(\\mathrm{d}\\phi)_q$ ,  $(\\phi_*)_q$ ,  $D_p\\phi(q)$ pushes tangent vectors  $X \\in T_q \\mathcal M$  forward to  $\\mathrm{d}\\phi_p(q)[X] \\in T_{\\phi(q)} \\mathcal N$ $n$ dimension (of a manifold) $n_1,n_2,\\ldots,m, \\dim(\\mathcal M)$ for the real dimension sometimes also  $\\dim_{\\mathbb R}(\\mathcal M)$ $d(‚ãÖ,‚ãÖ)$ (Riemannian) distance $d_{\\mathcal M}(‚ãÖ,‚ãÖ)$ $\\exp_p X$ exponential map at  $p \\in \\mathcal M$  of a vector  $X \\in T_p \\mathcal M$ $\\exp_p(X)$ $F$ a fiber see  Fiber $\\mathbb F$ a field, usually  $\\mathbb F \\in \\{\\mathbb R,\\mathbb C, \\mathbb H\\}$ , i.e. the real, complex, and quaternion numbers, respectively. field a manifold or a basis is based on $\\gamma$ a geodesic $\\gamma_{p;q}$ ,  $\\gamma_{p,X}$ connecting two points  $p,q$  or starting in  $p$  with velocity  $X$ . $\\operatorname{grad} f(p)$ (Riemannian) gradient of function  $f \\colon \\mathcal{M} ‚Üí ‚Ñù$  at  $p \\in \\mathcal{M}$ $\\nabla f(p)$ (Euclidean) gradient of function  $f \\colon \\mathcal{M} ‚Üí ‚Ñù$  at  $p \\in \\mathcal{M}$  but thought of as evaluated in the embedding G $\\circ$ a group operation $‚ãÖ^\\mathrm{H}$ Hermitian or conjugate transposed for both complex or quaternion matrices $\\operatorname{Hess} f(p)$ (Riemannian) Hessian of function  $f \\colon T_p\\mathcal{M} ‚Üí T_p\\mathcal M$  (i.e. the 1-1-tensor form) at  $p \\in \\mathcal{M}$ $\\nabla^2 f(p)$ (Euclidean) Hessian of function  $f$  in the embedding H $e$ identity element of a group $I_k$ identity matrix of size  $k√ók$ $k$ indices $i,j$ $\\langle‚ãÖ,‚ãÖ\\rangle$ inner product (in  $T_p \\mathcal M$ ) $\\langle‚ãÖ,‚ãÖ\\rangle_p, g_p(‚ãÖ,‚ãÖ)$ $\\operatorname{retr}^{-1}_pq$ an inverse retraction $\\mathfrak g$ a Lie algebra $\\mathcal{G}$ a (Lie) group $\\log_p q$ logarithmic map at  $p \\in \\mathcal M$  of a point  $q \\in \\mathcal M$ $\\log_p(q)$ $\\mathcal M$ a manifold $\\mathcal M_1, \\mathcal M_2,\\ldots,\\mathcal N$ $N_p \\mathcal M$ the normal space of the tangent space  $T_p \\mathcal M$  in some embedding  $\\mathcal E$  that should be clear from context $V$ a normal vector from  $N_p \\mathcal M$ $W$ $\\operatorname{Exp}$ the matrix exponential $\\operatorname{Log}$ the matrix logarithm $\\mathcal P_{q\\gets p}X$ parallel transport of the vector  $X$  from  $T_p\\mathcal M$  to  $T_q\\mathcal M$ $\\mathcal P_{p,Y}X$ parallel transport in direction  $Y$ of the vector  $X$  from  $T_p\\mathcal M$  to  $T_q\\mathcal M$ ,  $q = \\exp_pY$ $\\mathcal P_{t_1\\gets t_0}^cX$ parallel transport along the curve  $c$ $\\mathcal P^cX=\\mathcal P_{1\\gets 0}^cX$ of the vector  $X$  from  $p=c(0)$  to  $c(1)$ $p$ a point on  $\\mathcal M$ $p_1, p_2, \\ldots,q$ for 3 points one might use  $x,y,z$ $\\operatorname{retr}_pX$ a retraction $\\kappa_p(X, Y)$ sectional curvature $Œæ$ a set of tangent vectors $\\{X_1,\\ldots,X_n\\}$ $J_{2n} \\in ‚Ñù^{2n√ó2n}$ the  SymplecticElement $T_p \\mathcal M$ the tangent space at  $p$ $X$ a tangent vector from  $T_p \\mathcal M$ $X_1,X_2,\\ldots,Y,Z$ sometimes written with base point  $X_p$ $\\operatorname{tr}$ trace (of a matrix) $‚ãÖ^\\mathrm{T}$ transposed $e_i \\in \\mathbb R^n$ the  $i$ th unit vector $e_i^n$ the space dimension ( $n$ ) is omitted, when clear from context $B$ a vector bundle $\\mathcal T_{q\\gets p}X$ vector transport of the vector  $X$  from  $T_p\\mathcal M$  to  $T_q\\mathcal M$ $\\mathcal T_{p,Y}X$ vector transport in direction  $Y$ of the vector  $X$  from  $T_p\\mathcal M$  to  $T_q\\mathcal M$ , where  $q$  is determined by  $Y$ , for example using the exponential map or some retraction. $\\operatorname{Vol}(\\mathcal M)$ volume of manifold  $\\mathcal M$ $\\theta_p(X)$ volume density for vector  $X$  tangent at point  $p$ $\\mathcal W$ the Weingarten map  $\\mathcal W: T_p\\mathcal M √ó N_p\\mathcal M ‚Üí T_p\\mathcal M$ $\\mathcal W_p$ the second notation to emphasize the dependency of the point  $p\\in\\mathcal M$ $0_k$ the  $k√ók$  zero matrix."},{"id":2066,"pagetitle":"Notation","title":"Comparison with notation commonly used in robotics","ref":"/manifolds/stable/misc/notation/#Comparison-with-notation-commonly-used-in-robotics","content":" Comparison with notation commonly used in robotics In robotics, a different notation is commonly used. The table below shows a quick guide how to translate between them for people coming from robotics background. We use [ SDA21 ] as the primary robotics source. Robotics concept Manifolds.jl notation $p \\circ q$ compose(G, p, q) $p^{-1}$ inv(G, p) $\\mathcal{E}$ Identity(G)  or  identity_element(G) group action  $p\\cdot p_m$ apply(A, p, p_m) Lie group exponential  $\\exp\\colon \\mathfrak{g} \\to \\mathcal{G}$ ,  $\\exp(X)=p$ exp_lie(G, p) Lie group logarithm  $\\log\\colon \\mathcal{G} \\to \\mathfrak{g}$ ,  $\\log(p)=X$ log_lie(G, X) $n$ -D vector TranslationGroup(n) ;  its action is  TranslationAction(Euclidean(n), TranslationGroup(n)) circle  $S^1$ CircleGroup() ; its action is  ComplexPlanarRotation rotation  $\\mathrm{SO}(n)$ SpecialOrthogonal(n) ; its action is  RotationAction(Euclidean(n), SpecialOrthogonal(n)) rigid motion  $\\mathrm{SE}(n)$ SpecialEuclidean(n) ; its action is  RotationTranslationAction(Euclidean(n), SpecialEuclidean(n)) unit quaternions  $S^3$ UnitaryMatrices(1, H) ; note that 3-sphere and the group of rotations (with its bi-invariant metric) are homeomorphic but not isomorphic size (as in Table I) related to  representation_size(G) dim (as in Table I) manifold_dimension(G) Lie algebra element with coordinates  $\\tau^{\\wedge}$ hat(G, Identity(G), tau) coordinates of an element of Lie algebra  $X^{\\vee}$ vee(G, Identity(G), X) capital exponential map  $\\operatorname{Exp}$ exp_lie(G, hat(G, Identity(G), tau)) capital logarithmic map  $\\operatorname{Log}$ vee(G, Identity(G), log_lie(G, p)) right- $\\oplus$ ,  $p \\oplus \\tau$ compose(G, exp_lie(G, hat(G, Identity(G), tau))) right- $\\ominus$ ,  $p \\ominus q$ vee(G, Identity(G), log_lie(G, compose(G, inv(G, q), p))) left- $\\oplus$ ,  $\\tau \\oplus p$ compose(G, exp_lie(G, hat(G, Identity(G), tau)), p) left- $\\ominus$ ,  $p \\ominus q$ vee(G, Identity(G), log_lie(G, compose(G, p, inv(G, q)))) adjoint  $\\mathrm{Ad}_{p}(\\tau^{\\wedge})$ adjoint_action(G, p, hat(G, Identity(G), tau)) adjoint matrix  $\\mathrm{Ad}_{p}$ adjoint_matrix(G, p) Jacobian of group inversion and composition these can be easily constructed from the adjoint matrix left and right Jacobians of a function In JuliaManifolds there is always one preferred way to store tangent vectors specified by each manifold, and so we follow the standard mathematical convention of having one Jacobian which follows the selected tangent vector storage convention. See for example  jacobian_exp_argument ,  jacobian_exp_basepoint ,  jacobian_log_argument ,  jacobian_log_basepoint  from  ManifoldDiff.jl . left and right Jacobians (of a group)  $\\mathbf{J}_l, \\mathbf{J}_r$ jacobian_exp_argument  for exponential coordinates. For other coordinate systems no replacement is available yet. Jacobians of group actions not available yet Be also careful that the meaning of  $\\mathbf{x}$  is inconsistent in Table I from [ SDA21 ]. It's a complex number for circle, quaternion for quaternion rotation and column vectors for other rows."},{"id":2069,"pagetitle":"References","title":"Literature","ref":"/manifolds/stable/misc/references/#Literature","content":" Literature We are slowly moving to using  DocumenterCitations.jl . The goal is to have all references used / mentioned in the documentation of Manifolds.jl also listed here. If you notice a reference still defined in a footnote, please change it into a BibTeX reference and  open a PR Usually you will find a small reference section at the end of every documentation page that contains references for just that page. [AMT13] P.¬†-.-A.¬†Absil, R.¬†Mahony and J.¬†Trumpf.  An Extrinsic Look at the Riemannian Hessian . In:  Geometric Science of Information , edited by F.¬†Nielsen and F.¬†Barbaresco (Springer Berlin Heidelberg, 2013); pp.¬†361‚Äì368. [AMS08] P.-A.¬†Absil, R.¬†Mahony and R.¬†Sepulchre.  Optimization Algorithms on Matrix Manifolds  (Princeton University Press, 2008), available online at  press.princeton.edu/chapters/absil/ . [AM12] P.-A.¬†Absil and J.¬†Malick.  Projection-like Retractions on Matrix Manifolds .  SIAM¬†Journal¬†on¬†Optimization  22 , 135‚Äì158  (2012). [AO14] P.-A.¬†Absil and I.¬†V.¬†Oseledets.  Low-rank retractions: a survey and new results .  Computational¬†Optimization¬†and¬†Applications  62 , 5‚Äì29  (2014). [ATV13] B.¬†Afsari, R.¬†Tron and R.¬†Vidal.  On the Convergence of Gradient Descent for Finding the Riemannian Center of Mass .  SIAM¬†Journal¬†on¬†Control¬†and¬†Optimization  51 , 2230‚Äì2260  (2013),  arXiv:1201.0925 . [AR13] D.¬†Andrica and R.-A.¬†Rohan.  Computing the Rodrigues coefficients of the exponential map of the Lie groups of matrices . Balkan¬†Journal¬†of¬†Geometry¬†and¬†Its¬†Applications  18 , 1‚Äì10 (2013). [ALRV14] E.¬†Andruchow, G.¬†Larotonda, L.¬†Recht and A.¬†Varela.  The left invariant metric in the general linear group .  Journal¬†of¬†Geometry¬†and¬†Physics  86 , 241‚Äì257  (2014),  arXiv:1109.0520 . [ABBR23] S.¬†D.¬†Axen, M.¬†Baran, R.¬†Bergmann and K.¬†Rzecki.  Manifolds.Jl: An Extensible Julia Framework for Data Analysis on Manifolds .  ACM¬†Transactions¬†on¬†Mathematical¬†Software  49  (2023). [AJLS17] N.¬†Ay, J.¬†Jost, H.¬†V.¬†L√™ and L.¬†Schwachh√∂fer.  Information Geometry  (Springer Cham, 2017). [BF14] T.¬†D.¬†Barfoot and P.¬†T.¬†Furgale.  Associating Uncertainty With Three-Dimensional Poses for Use in Estimation Problems .  IEEE¬†Transactions¬†on¬†Robotics  30 , 679‚Äì693  (2014). [Bac14] M.¬†Baƒç√°k.  Computing medians and means in Hadamard spaces .  SIAM¬†Journal¬†on¬†Optimization  24 , 1542‚Äì1566  (2014),  arXiv:1210.2145 . [BZ21] T.¬†Bendokat and R.¬†Zimmermann.  The real symplectic Stiefel and Grassmann manifolds: metrics, geodesics and applications , arXiv¬†Preprint,¬†2108.12447 (2021),  arXiv:2108.12447 . [BZA20] T.¬†Bendokat, R.¬†Zimmermann and P.-A.¬†Absil.  A Grassmann Manifold Handbook: Basic Geometry and Computational Aspects , arXiv¬†Preprint (2020),  arXiv:2011.13699 . [BG18] R.¬†Bergmann and P.-Y.¬†Gousenbourger.  A variational model for data fitting on manifolds by minimizing the acceleration of a B√©zier curve .  Frontiers¬†in¬†Applied¬†Mathematics¬†and¬†Statistics  4  (2018),  arXiv:1807.10090 . [BPS15] R.¬†Bergmann, J.¬†Persch and G.¬†Steidl.  A parallel Douglas Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds , arXiv¬†Preprint (2015),  arXiv:1512.02814 . [BPS16] R.¬†Bergmann, J.¬†Persch and G.¬†Steidl.  A parallel Douglas Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  9 , 901‚Äì937  (2016). [BP08] E.¬†Biny and S.¬†Pods.  The Geometry of Heisenberg Groups: With Applications in Signal Theory, Optics, Quantization, and Field Quantization  (American Mathematical Society, 2008). [BCC20] P.¬†Birtea, I.¬†Ca√ßu and D.¬†ComƒÉnescu.  Optimization on the real symplectic group .  Monatshefte¬†f√ºr¬†Mathematik  191 , 465‚Äì485  (2020). [BST03] L.¬†J.¬†Boya, E.¬†Sudarshan and T.¬†Tilma.  Volumes of compact manifolds .  Reports¬†on¬†Mathematical¬†Physics  52 , 401‚Äì422  (2003). [BP19] A.¬†L.¬†Brigant and S.¬†Puechmorel.  Approximation of Densities on Riemannian Manifolds .  Entropy  21 , 43  (2019). [CE08] J.¬†Cheeger and D.¬†G.¬†Ebin.  Comparison Theorems in Riemannian Geometry  (American Mathematical Society, Providence, R.I, 2008). [CLLD22] E.¬†Chevallier, D.¬†Li, Y.¬†Lu and D.¬†B.¬†Dunson.  Exponential-wrapped distributions on symmetric spaces . ArXiv¬†Preprint (2022). [CKA17] E.¬†Chevallier, E.¬†Kalunga and J.¬†Angulo.  Kernel Density Estimation on Spaces of Gaussian Distributions and Symmetric Positive Definite Matrices .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  10 , 191‚Äì215  (2017). [Chi12] G.¬†S.¬†Chirikjian.  Stochastic Models, Information Theory, and Lie Groups, Volume 2 . 1¬†Edition, Vol.¬†2 of  Applied and Numerical Harmonic Analysis  (Birkh√§user Boston, MA, 2012). [Dev86] L.¬†Devroye.  Non-Uniform Random Variate Generation  (Springer New York, NY, 1986). [DBV21] N.¬†Dewaele, P.¬†Breiding and N.¬†Vannieuwenhoven.  The condition number of many tensor decompositions is invariant under Tucker compression , arXiv¬†Preprint (2021),  arXiv:2106.13034 . [DH19] A.¬†Douik and B.¬†Hassibi.  Manifold Optimization Over the Set of Doubly Stochastic Matrices: A Second-Order Geometry .  IEEE¬†Transactions¬†on¬†Signal¬†Processing  67 , 5761‚Äì5774  (2019),  arXiv:1802.02628 . [EAS98] A.¬†Edelman, T.¬†A.¬†Arias and S.¬†T.¬†Smith.  The Geometry of Algorithms with Orthogonality Constraints .  SIAM¬†Journal¬†on¬†Matrix¬†Analysis¬†and¬†Applications  20 , 303‚Äì353  (1998),  arXiv:806030 . [FdHDF19] L.¬†Falorsi, P.¬†de¬†Haan, T.¬†R.¬†Davidson and P.¬†Forr√©.  Reparameterizing Distributions on Lie Groups , arXiv¬†Preprint (2019). [Fio11] S.¬†Fiori.  Solving Minimal-Distance Problems over the Manifold of Real-Symplectic Matrices .  SIAM¬†Journal¬†on¬†Matrix¬†Analysis¬†and¬†Applications  32 , 938‚Äì968  (2011). [FVJ08] P.¬†T.¬†Fletcher, S.¬†Venkatasubramanian and S.¬†Joshi.  Robust statistics on Riemannian manifolds via the geometric median . In:  2008 IEEE Conference on Computer Vision and Pattern Recognition  (2008). [GX02] J.¬†Gallier and D.¬†Xu.  Computing exponentials of skew-symmetric matrices and logarithms of orthogonal matrices . International¬†Journal¬†of¬†Robotics¬†and¬†Automation  17 , 1‚Äì11 (2002). [GQ20] J.¬†Gallier and J.¬†Quaintance.  Differential Geometry and Lie Groups: A Computational Perspective . Vol.¬†12 of  Geometry and Computing  ( Springer International Publishing, Cham, 2020 ). [GSAS21] B.¬†Gao, N.¬†T.¬†Son, P.-A.¬†Absil and T.¬†Stykel.  Riemannian Optimization on the Symplectic Stiefel Manifold .  SIAM¬†Journal¬†on¬†Optimization  31 , 1546‚Äì1575  (2021). [Ge14] J.¬†Ge.  DDVV-type inequality for skew-symmetric matrices and Simons-type inequality for Riemannian submersions .  Advances¬†in¬†Mathematics  251 , 62‚Äì86  (2014). [Gil08] M.¬†B.¬†Giles.  Collected Matrix Derivative Results for Forward and Reverse Mode Algorithmic Differentiation . In:  Advances in Automatic Differentiation ,  Lecture Notes in Computational Science and Engineering , edited by C.¬†H.¬†Bischof, H.¬†M.¬†B√ºcker, P.¬†Hovland, U.¬†Naumann and J.¬†Utke (Springer, Berlin, Heidelberg, 2008); pp.¬†35‚Äì44. [GMTP21] N.¬†Guigui, E.¬†Maignant, A.¬†Trouv√© and X.¬†Pennec.  Parallel Transport on Kendall Shape Spaces . In:  Geometric Science of Information  (SPringer Cham, 2021); pp.¬†103‚Äì110. [HMJG21] A.¬†Han, B.¬†Mushra, P.¬†Jawapanpuria and J.¬†Gao.  Learning with symmetric positive definite matrices via generalized Bures-Wasserstein geometry , arXive¬†preprint (2021),  arXiv:2110.10464 . [HU17] S.¬†Hosseini and A.¬†Uschmajew.  A Riemannian Gradient Sampling Algorithm for Nonsmooth Optimization on Manifolds .  SIAM¬†J.¬†Optim.  27 , 173‚Äì189  (2017). [HGA15] W.¬†Huang, K.¬†A.¬†Gallivan and P.-A.¬†Absil.  A Broyden Class of Quasi-Newton Methods for Riemannian Optimization .  SIAM¬†Journal¬†on¬†Optimization  25 , 1660‚Äì1685  (2015). [HML21] K.¬†H√ºper, I.¬†Markina and F.¬†S.¬†Leite.  A Lagrangian approach to extremal curves on Stiefel manifolds .  Journal¬†of¬†Geometric¬†Mechanics  13 , 55  (2021). [JSVV24] S.¬†Jacobsson, L.¬†Swijsen, J.¬†V.¬†Veken and N.¬†Vannieuwenhoven.  Warped geometries of Segre-Veronese manifolds  (2024),  arXiv:2410.00664 [math.NA] . [JBAS10] M.¬†Journ√©e, F.¬†Bach, P.-A.¬†Absil and R.¬†Sepulchre.  Low-Rank Optimization on the Cone of Positive Semidefinite Matrices .  SIAM¬†Journal¬†on¬†Optimization  20 , 2327‚Äì2351  (2010),  arXiv:0807.4423 . [Joy10] D.¬†Joyce.  On manifolds with corners  (2010),  arXiv:0910.3518 . [KFT13] T.¬†Kaneko, S.¬†Fiori and T.¬†Tanaka.  Empirical Arithmetic Averaging Over the Compact Stiefel Manifold .  IEEE¬†Transactions¬†on¬†Signal¬†Processing  61 , 883‚Äì894  (2013). [Kar77] H.¬†Karcher.  Riemannian center of mass and mollifier smoothing .  Communications¬†on¬†Pure¬†and¬†Applied¬†Mathematics  30 , 509‚Äì541  (1977). [Ken84] D.¬†G.¬†Kendall.  Shape Manifolds, Procrustean Metrics, and Complex Projective Spaces .  Bulletin¬†of¬†the¬†London¬†Mathematical¬†Society  16 , 81‚Äì121  (1984). [Ken89] D.¬†G.¬†Kendall.  A Survey of the Statistical Theory of Shape .  Statistical¬†Sciences  4 , 87‚Äì99  (1989). [KL10] O.¬†Koch and C.¬†Lubich.  Dynamical Tensor Approximation .  SIAM¬†Journal¬†on¬†Matrix¬†Analysis¬†and¬†Applications  31 , 2360‚Äì2375  (2010). [KSV13] D.¬†Kressner, M.¬†Steinlechner and B.¬†Vandereycken.  Low-rank tensor completion by Riemannian optimization .  BIT¬†Numerical¬†Mathematics  54 , 447‚Äì468  (2013). [LW19] N.¬†Langren√© and X.¬†Warin.  Fast and Stable Multivariate Kernel Density Estimation by Fast Sum Updating .  Journal¬†of¬†Computational¬†and¬†Graphical¬†Statistics  28 , 596‚Äì608  (2019). [LMV00] L.¬†D.¬†Lathauwer, B.¬†D.¬†Moor and J.¬†Vandewalle.  A Multilinear Singular Value Decomposition .  SIAM¬†Journal¬†on¬†Matrix¬†Analysis¬†and¬†Applications  21 , 1253‚Äì1278  (2000). [Lee19] J.¬†M.¬†Lee.  Introduction to Riemannian Manifolds  (Springer Cham, 2019). [Lin19] Z.¬†Lin.  Riemannian Geometry of Symmetric Positive Definite Matrices via Cholesky Decomposition .  SIAM¬†Journal¬†on¬†Matrix¬†Analysis¬†and¬†Applications  40 , 1353‚Äì1370  (2019),  arXiv:1908.09326 . [MMP18] L.¬†Malag√≥, L.¬†Montrucchio and G.¬†Pistone.  Wasserstein Riemannian geometry of Gaussian densities .  Information¬†Geometry  1 , 137‚Äì179  (2018). [Mar72] G.¬†Marsaglia.  Choosing a Point from the Surface of a Sphere .  Annals¬†of¬†Mathematical¬†Statistics  43 , 645‚Äì646  (1972). [MA20] E.¬†Massart and P.-A.¬†Absil.  Quotient Geometry with Simple Geodesics for the Manifold of Fixed-Rank Positive-Semidefinite Matrices .  SIAM¬†Journal¬†on¬†Matrix¬†Analysis¬†and¬†Applications  41 , 171‚Äì198  (2020). Preprint:  sites.uclouvain.be/absil/2018.06 . [MF12] P.¬†Muralidharan and P.¬†T.¬†Fletcher.  Sasaki metrics for analysis of longitudinal data on manifolds . In:  2012 IEEE Conference on Computer Vision and Pattern Recognition  (2012). [NM16] P.¬†Neff and R.¬†J.¬†Martin.  Minimal geodesics on GL(n) for left-invariant, right-O(n)-invariant Riemannian metrics .  J.¬†Geom.¬†Mech.  8 , 323‚Äì357  (2016),  arXiv:1409.7849 . [Ngu23] D.¬†Nguyen.  Operator-Valued Formulas for Riemannian Gradient and Hessian and Families of Tractable Metrics in Riemannian Optimization .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  198 , 135‚Äì164  (2023),  arXiv:2009.10159 . [Pen06] X.¬†Pennec.  Intrinsic Statistics on Riemannian Manifolds: Basic Tools for Geometric Measurements .  Journal¬†of¬†Mathematical¬†Imaging¬†and¬†Vision  25 , 127‚Äì154  (2006). [PA12] X.¬†Pennec and V.¬†Arsigny.  Exponential Barycenters of the Canonical Cartan Connection and Invariant Means on Lie Groups . In:  Matrix Information Geometry  (Springer, Berlin, Heidelberg, 2012); pp.¬†123‚Äì166,  arXiv:00699361 . [PL20] X.¬†Pennec and M.¬†Lorenzi.  Beyond Riemannian geometry: The affine connection setting for transformation groups . In:  Riemannian Geometric Statistics in Medical Image Analysis  (Elsevier, 2020); pp.¬†169‚Äì229. [Ren11] Q.¬†Rentmeesters.  A gradient method for geodesic data fitting on some symmetric Riemannian manifolds . In:  IEEE Conference on Decision and Control and European Control Conference  (2011); pp.¬†7141‚Äì7146. [Ric88] J.¬†M.¬†Rico Martinez.  Representations of the Euclidean group and its applications to the kinematics of spatial chains . Ph.D. Thesis, University of FLorida (1988). [Sas58] S.¬†Sasaki.  On the differential geometry of tangent bundles of Riemannian manifolds .  Tohoku¬†Math.¬†J.  10  (1958). [SDA21] J.¬†Sol√†, J.¬†Deray and D.¬†Atchuthan.  A micro Lie theory for state estimation in robotics  (Dec 2021),  arXiv:1812.01537 [cs.RO] , arXiv: 1812.01537. [SK16] A.¬†Srivastava and E.¬†P.¬†Klassen.  Functional and Shape Data Analysis  (Springer New York, 2016). [Suh13] E.¬†Suhubi.  Exterior Analysis: Using Applications of Differential Forms  (Academic Press, 2013). [Tor20] S.¬†Tornier.  Haar Measures  (2020). [TD17] R.¬†Tron and K.¬†Daniilidis.  The Space of Essential Matrices as a Riemannian Quotient Manifold .  SIAM¬†J.¬†Imaging¬†Sci.  10 , 1416‚Äì1445  (2017). [Van13] B.¬†Vandereycken.  Low-rank matrix completion by Riemannian optimization .  SIAM¬†Journal¬†on¬†Optimization  23 , 1214‚Äì1236  (2013). [VVM12] N.¬†Vannieuwenhoven, R.¬†Vandebril and K.¬†Meerbergen.  A New Truncation Strategy for the Higher-Order Singular Value Decomposition .  SIAM¬†Journal¬†on¬†Scientific¬†Computing  34 , A1027‚ÄìA1052  (2012). [WSF18] J.¬†Wang, H.¬†Sun and S.¬†Fiori.  A Riemannian-steepest-descent approach for optimization on the real symplectic group .  Mathematical¬†Methods¬†in¬†the¬†Applied¬†Science  41 , 4273‚Äì4286  (2018). [YWL21] K.¬†Ye, K.¬†S.-W.¬†Wong and L.-H.¬†Lim.  Optimization on flag manifolds .  Mathematical¬†Programming  194 , 621‚Äì660  (2021). [Zhu16] X.¬†Zhu.  A Riemannian conjugate gradient method for optimization on the Stiefel manifold .  Computational¬†Optimization¬†and¬†Applications  67 , 73‚Äì110  (2016). [ZD18] X.¬†Zhu and C.¬†Duan.  On matrix exponentials and their approximations related to optimization on the Stiefel manifold .  Optimization¬†Letters  13 , 1069‚Äì1083  (2018). [Zim17] R.¬†Zimmermann.  A Matrix-Algebraic Algorithm for the Riemannian Logarithm on the Stiefel Manifold under the Canonical Metric .  SIAM¬†J.¬†Matrix¬†Anal.¬†Appl.  38 , 322‚Äì342  (2017),  arXiv:1604.05054 . [ZH22] R.¬†Zimmermann and K.¬†H√ºper.  Computing the Riemannian Logarithm on the Stiefel Manifold: Metrics, Methods, and Performance .  SIAM¬†Journal¬†on¬†Matrix¬†Analysis¬†and¬†Applications  43 , 953‚Äì980  (2022),  arXiv:2103.12046 . [ZS24] R.¬†Zimmermann and J.¬†Stoye.  The injectivity radius of the compact Stiefel manifold under the Euclidean metric , arXiv¬†Preprint (2024),  arXiv:2405.02268 . [APSS17] F.¬†√Östr√∂m, S.¬†Petra, B.¬†Schmitzer and C.¬†Schn√∂rr.  Image Labeling by Assignment .  Journal¬†of¬†Mathematical¬†Imaging¬†and¬†Vision  58 , 211‚Äì238  (2017),  arXiv:1603.05285 ."},{"id":2072,"pagetitle":"explore curvature without coordinates","title":"Exploring curvature without coordinates","ref":"/manifolds/stable/tutorials/exploring-curvature/#Exploring-curvature-without-coordinates","content":" Exploring curvature without coordinates This part of documentation covers exploration of curvature of manifolds  $\\mathcal{M}$ . There are multiple ways to describe curvature: Christoffel symbols, Riemann tensor, Ricci tensor, sectional curvature, and many other. They are usually considered only in coordinates but there is a way to demonstrate curvature in coordinate-free way."},{"id":2073,"pagetitle":"explore curvature without coordinates","title":"Sectional curvature matrix","ref":"/manifolds/stable/tutorials/exploring-curvature/#Sectional-curvature-matrix","content":" Sectional curvature matrix Curvature of a manifold can be explored using the  sectional_curvature_matrix  function. Note that Riemann tensor and sectional curvature are equivalently full specifications of curvature in a manifold, see [ CE08 ], Eq. (1.12). Let‚Äôs take the  SymmetricPositiveDefinite  manifold as our first example. It has nonpositive sectional curvature: using Manifolds\nusing LinearAlgebra\nM = SymmetricPositiveDefinite(3)\np = rand(M)\ncm = sectional_curvature_matrix(M, p, DefaultOrthonormalBasis()) 6√ó6 Matrix{Float64}:\n  0.0          -0.25         -0.25         ‚Ä¶  -2.01765e-19  -2.52038e-19\n -0.25          0.0          -0.125           -0.125        -1.09534e-19\n -0.25         -0.125         0.0             -0.125        -0.25\n  1.99947e-19  -0.25         -7.67752e-22     -0.25          2.15264e-19\n -2.01765e-19  -0.125        -0.125            0.0          -0.25\n -2.52038e-19  -1.09534e-19  -0.25         ‚Ä¶  -0.25          0.0 We can verify that the curvature is consistent with an approximation based on the Bertrand‚ÄìDiguet‚ÄìPuiseux theorem, which relies only on an ONB, exponential map and distance calculation: cm_bdp = Manifolds.estimated_sectional_curvature_matrix(M, p, DefaultOrthonormalBasis(); r=1e-3, N_pts=100000)\nprintln(norm(cm - cm_bdp)) 0.005037564233546601 This approximation converges quite slowly with  N_pts  and is prone to numerical errors at low values of  r  and large values of  N_pts . You can also take the vectors from the basis and see what kind of planes they correspond to. It may be easier to see for the identity matrix as the base point. p = [1.0 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 1.0]\nV = get_vectors(M, p, get_basis(M, p, DefaultOrthonormalBasis()))\ncm = sectional_curvature_matrix(M, p, DefaultOrthonormalBasis())\nfor X in V\n    println(exp(M, p, X))\nend [2.718281828459045 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 1.0]\n[1.260591836521356 0.7675231451261162 0.0; 0.7675231451261162 1.2605918365213566 0.0; 0.0 0.0 1.0]\n[1.260591836521356 0.0 0.7675231451261162; 0.0 1.0 0.0; 0.7675231451261162 0.0 1.2605918365213566]\n[1.0 0.0 0.0; 0.0 2.718281828459045 0.0; 0.0 0.0 1.0]\n[1.0 0.0 0.0; 0.0 1.260591836521356 0.7675231451261162; 0.0 0.7675231451261162 1.2605918365213566]\n[1.0 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 2.718281828459045] The flat planes correspond to directions where the matrix changes independently. In other cases sectional curvature indicates hyperbolic characteristic of a submanifold. Sectional curvature can be either larger or smaller than entries in the matrix on other planes. Consider for example the manifold of rotation matrices in four dimensions, and a function that computes plane of maximum curvature using random search. function max_curvature(M::AbstractManifold, p)\n    mc = -Inf\n    X = zero_vector(M, p)\n    Y = zero_vector(M, p)\n    for _ in 1:10000\n        X_c = rand(M; vector_at=p)\n        Y_c = rand(M; vector_at=p)\n        sc = sectional_curvature(M, p, X_c, Y_c)\n        if sc > mc\n            mc = sc\n            X .= X_c\n            Y .= Y_c\n        end\n    end\n    return mc, X, Y\nend\n\nM = Rotations(4)\np = Matrix(I(4) * 1.0)\nprintln(sectional_curvature_matrix(M, p, DefaultOrthonormalBasis()))\nmc, X, Y = max_curvature(M, p)\nprintln(mc)\nprintln(X)\nprintln(Y) [0.0 0.12500000000000003 0.12500000000000003 0.0 0.12500000000000003 0.12500000000000003; 0.12500000000000003 0.0 0.12500000000000003 0.12500000000000003 0.0 0.12500000000000003; 0.12500000000000003 0.12500000000000003 0.0 0.12500000000000003 0.12500000000000003 0.0; 0.0 0.12500000000000003 0.12500000000000003 0.0 0.12500000000000003 0.12500000000000003; 0.12500000000000003 0.0 0.12500000000000003 0.12500000000000003 0.0 0.12500000000000003; 0.12500000000000003 0.12500000000000003 0.0 0.12500000000000003 0.12500000000000003 0.0]\n0.2399859917754552\n[0.0 0.1903652581458205 2.1681166411782615 -0.3106633546019589; -0.1903652581458205 0.0 -0.1031321188456162 -2.3027349669801755; -2.1681166411782615 0.1031321188456162 0.0 0.48568864247562477; 0.3106633546019589 2.3027349669801755 -0.48568864247562477 0.0]\n[0.0 -0.8784028220119777 -0.41645636044252726 -0.6463639002610797; 0.8784028220119777 0.0 -0.4447570761504328 0.737966812180493; 0.41645636044252726 0.4447570761504328 0.0 -1.0199448435140714; 0.6463639002610797 -0.737966812180493 1.0199448435140714 0.0] In the planes corresponding to orthonormal basis, the maximum sectional curvature is 0.125 but the true upper bound is 0.25."},{"id":2074,"pagetitle":"explore curvature without coordinates","title":"Literature","ref":"/manifolds/stable/tutorials/exploring-curvature/#Literature","content":" Literature [CE08] J.¬†Cheeger and D.¬†G.¬†Ebin.  Comparison Theorems in Riemannian Geometry  (American Mathematical Society, Providence, R.I, 2008)."},{"id":2077,"pagetitle":"üöÄ Get Started with Manifolds.jl","title":"üöÄ Get Started with Manifolds.jl","ref":"/manifolds/stable/tutorials/getstarted/#Get-Started-with-Manifolds.jl","content":" üöÄ Get Started with  Manifolds.jl This is a short overview of  Manifolds.jl  and how to get started working with your first Manifold. we first need to install the package, using for example using Pkg; Pkg.add(\"Manifolds\") Then you can load the package with using Manifolds"},{"id":2078,"pagetitle":"üöÄ Get Started with Manifolds.jl","title":"Using the Library of Manifolds","ref":"/manifolds/stable/tutorials/getstarted/#Using-the-Library-of-Manifolds","content":" Using the Library of Manifolds Manifolds.jl  is first of all a library of manifolds, see the list in the menu  here  under ‚Äúbasic manifolds‚Äù. Let‚Äôs look at three examples together with the first few functions on manifolds."},{"id":2079,"pagetitle":"üöÄ Get Started with Manifolds.jl","title":"1. The Euclidean space","ref":"/manifolds/stable/tutorials/getstarted/#1.-[The-Euclidean-space](https://juliamanifolds.github.io/Manifolds.jl/latest/manifolds/euclidean.html)","content":" 1.  The Euclidean space The Euclidean Space  Euclidean  brings us (back) into linear case of vectors, so in terms of manifolds, this is a very simple one. It is often useful to compare to classical algorithms, or implementations. M‚ÇÅ = Euclidean(3) Euclidean(3; field=‚Ñù) Since a manifold is a type in Julia, we write it in CamelCase. Its parameters are first a dimension or size parameter of the manifold, sometimes optional is a field the manifold is defined over. For example the above definition is the same as the real-valued case M‚ÇÅ === Euclidean(3, field=‚Ñù) true But we even introduced a short hand notation, since ‚Ñù is also just a symbol/variable to use‚Äù M‚ÇÅ === ‚Ñù^3 true And similarly here are two ways to create the manifold of vectors of length two with complex entries ‚Äì or mathematically the space  $\\mathbb C^2$ Euclidean(2, field=‚ÑÇ) === ‚ÑÇ^2 true The easiest to check is the dimension of a manifold. Here we have three ‚Äúdirections to walk into‚Äù at every point  $p\\in \\mathbb R ^3$  so  manifold_dimension  is manifold_dimension(M‚ÇÅ) 3"},{"id":2080,"pagetitle":"üöÄ Get Started with Manifolds.jl","title":"2. The hyperbolic space","ref":"/manifolds/stable/tutorials/getstarted/#2.-[The-hyperbolic-space](@ref-HyperbolicSpace)","content":" 2.  The hyperbolic space The  $d$ -dimensional  hyperbolic space  is usually represented in  $\\mathbb R^{d+1}$  as the set of points  $p\\in\\mathbb R^3$  fulfilling \\[p_1^2+p_2^2+‚ãÖs+p_d^2-p_{d+1}^2 = -1.\\] We define the manifold using M‚ÇÇ = Hyperbolic(2) Hyperbolic(2) And we can again just start with looking at the manifold dimension of  M‚ÇÇ manifold_dimension(M‚ÇÇ) 2 A next useful function is to check, whether some  $p‚àà\\mathbb R^3$  is a point on the manifold  M‚ÇÇ . We can check is_point(M‚ÇÇ, [0, 0, 1]) true or is_point(M‚ÇÇ, [1, 0, 1]) false Keyword arguments are passed on to any numerical checks, for example an absolute tolerance when checking the above equality. But in an interactive session an error message might be helpful. A positional (third) argument is present to activate this. Setting this parameter to true, we obtain an error message that gives insight into why the point is not a point on  M‚ÇÇ . Note that the  LoadError:  is due to quarto, on  REPL  you would just get the  DomainError . is_point(M‚ÇÇ, [0, 0, 1.001]; error=:error) LoadError: DomainError with -1.0020009999999997:\nThe point [0.0, 0.0, 1.001] does not lie on Hyperbolic(2) since its Minkowski inner product is not -1.\nDomainError with -1.0020009999999997:\nThe point [0.0, 0.0, 1.001] does not lie on Hyperbolic(2) since its Minkowski inner product is not -1.\n\nStacktrace:\n [1] ÔøΩ[0mÔøΩ[1mis_pointÔøΩ[22mÔøΩ[0mÔøΩ[1m(ÔøΩ[22m::ÔøΩ[0mManifoldsBase.TraitListÔøΩ[90m{IsEmbeddedManifold, ManifoldsBase.TraitList{IsDefaultMetric{MinkowskiMetric}, ManifoldsBase.TraitList{IsMetricManifold, ManifoldsBase.EmptyTrait}}}ÔøΩ[39m, ÔøΩ[90mMÔøΩ[39m::ÔøΩ[0mHyperbolicÔøΩ[90m{ManifoldsBase.TypeParameter{Tuple{2}}}ÔøΩ[39m, ÔøΩ[90mpÔøΩ[39m::ÔøΩ[0mVectorÔøΩ[90m{Float64}ÔøΩ[39m; ÔøΩ[90merrorÔøΩ[39m::ÔøΩ[0mSymbol, ÔøΩ[90mkwargsÔøΩ[39m::ÔøΩ[0m@KwargsÔøΩ[90m{}ÔøΩ[39mÔøΩ[0mÔøΩ[1m)ÔøΩ[22m\nÔøΩ[90m   @ÔøΩ[39m ÔøΩ[35mManifoldsBaseÔøΩ[39m ÔøΩ[90m~/.julia/packages/ManifoldsBase/nqeOA/src/ÔøΩ[39mÔøΩ[90mÔøΩ[4mdecorator_trait.jl:422ÔøΩ[24mÔøΩ[39m\n [2] ÔøΩ[0mÔøΩ[1mis_pointÔøΩ[22m\nÔøΩ[90m   @ÔøΩ[39m ÔøΩ[90m~/.julia/packages/ManifoldsBase/nqeOA/src/ÔøΩ[39mÔøΩ[90mÔøΩ[4mdecorator_trait.jl:392ÔøΩ[24mÔøΩ[39mÔøΩ[90m [inlined]ÔøΩ[39m\n [3] ÔøΩ[0mÔøΩ[1m#is_point#153ÔøΩ[22m\nÔøΩ[90m   @ÔøΩ[39m ÔøΩ[90m~/.julia/packages/ManifoldsBase/nqeOA/src/ÔøΩ[39mÔøΩ[90mÔøΩ[4mnested_trait.jl:313ÔøΩ[24mÔøΩ[39mÔøΩ[90m [inlined]ÔøΩ[39m\n [4] ÔøΩ[0mÔøΩ[1m#is_point#152ÔøΩ[22m\nÔøΩ[90m   @ÔøΩ[39m ÔøΩ[90m~/.julia/packages/ManifoldsBase/nqeOA/src/ÔøΩ[39mÔøΩ[90mÔøΩ[4mnested_trait.jl:306ÔøΩ[24mÔøΩ[39mÔøΩ[90m [inlined]ÔøΩ[39m\n [5] top-level scope\nÔøΩ[90m   @ÔøΩ[39m ÔøΩ[90mÔøΩ[4mIn[13]:1ÔøΩ[24mÔøΩ[39m"},{"id":2081,"pagetitle":"üöÄ Get Started with Manifolds.jl","title":"3. The sphere","ref":"/manifolds/stable/tutorials/getstarted/#3.-[The-sphere](@ref-SphereSection)","content":" 3.  The sphere The sphere $\\mathbb S^d$  is the  $d$ -dimensional sphere represented in its embedded form, that is unit vectors  $p \\in \\mathbb R^{d+1}$  with unit norm  $\\lVert p \\rVert_2 = 1$ . M‚ÇÉ = Sphere(2) Sphere(2, ‚Ñù) If we only have a point that is approximately on the manifold, we can allow for a tolerance. Usually these are the same values of  atol  and  rtol  allowed in  isapprox , i.e.¬†we get is_point(M‚ÇÉ, [0, 0, 1.001]; atol=1e-3) true Here we can show a last nice check:  is_vector  to check whether a tangent vector  X  is a representation of a tangent vector  $X‚ààT_p\\mathcal M$  to a point  p  on the manifold. This function has two positional arguments, the first to again indicate whether to throw an error, the second to disable the check that  p  is a valid point on the manifold. Usually this validity is essential for the tangent check, but if it was for example performed before, it can be turned off to spare time. For example in our first example the point is not of unit norm is_vector(M‚ÇÉ, [2, 0, 0], [0, 1, 1]) false But the orthogonality of  p  and  X  is still valid, we can disable the point check, but even setting the error to true we get here is_vector(M‚ÇÉ, [2, 0, 0], [0, 1, 1], true, false) false But of course it is better to use a valid point in the first place is_vector(M‚ÇÉ, [1, 0, 0], [0, 1, 1]) true and for these we again get informative error messages @expect_error is_vector(M‚ÇÉ, [1, 0, 0], [0.1, 1, 1]; error=:error) DomainError LoadError: LoadError: UndefVarError: `@expect_error` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nin expression starting at In[19]:1\nLoadError: UndefVarError: `@expect_error` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nin expression starting at In[19]:1 To learn about how to define a manifold yourself check out the  How to define your own manifold  tutorial of  ManifoldsBase.jl .‚Äù"},{"id":2082,"pagetitle":"üöÄ Get Started with Manifolds.jl","title":"Building more advanced manifolds","ref":"/manifolds/stable/tutorials/getstarted/#Building-more-advanced-manifolds","content":" Building more advanced manifolds Based on these basic manifolds we can directly build more advanced manifolds. The first one concerns vectors or matrices of data on a manifold, the  PowerManifold . M‚ÇÑ = M‚ÇÇ^2 PowerManifold(Hyperbolic(2), 2) Then points are represented by arrays, where the power manifold dimension is added in the end. In other words ‚Äì for the hyperbolic manifold here, we have a matrix with 2 columns, where each column is a valid point on hyperbolic space. p = [0 0; 0 1; 1 sqrt(2)] 3√ó2 Matrix{Float64}:\n 0.0  0.0\n 0.0  1.0\n 1.0  1.41421 [is_point(M‚ÇÇ, p[:, 1]), is_point(M‚ÇÇ, p[:, 2])] 2-element Vector{Bool}:\n 1\n 1 But of course the method we used previously also works for power manifolds: is_point(M‚ÇÑ, p) true Note that nested power manifolds are combined into one as in M‚ÇÑ‚ÇÇ = M‚ÇÑ^4 PowerManifold(Hyperbolic(2), 2, 4) which represents  $2√ó4$  ‚Äì matrices of hyperbolic points represented in  $3√ó2√ó4$  arrays. We can ‚Äì alternatively ‚Äì use a power manifold with nested arrays M‚ÇÖ = PowerManifold(M‚ÇÉ, NestedPowerRepresentation(), 2) PowerManifold(Sphere(2, ‚Ñù), NestedPowerRepresentation(), 2) which emphasizes that we have vectors of length 2 that contain points, so we store them that way. p‚ÇÇ = [[0.0, 0.0, 1.0], [0.0, 1.0, 0.0]] 2-element Vector{Vector{Float64}}:\n [0.0, 0.0, 1.0]\n [0.0, 1.0, 0.0] To unify both representations, elements of the power manifold can also be accessed in the classical indexing fashion, if we start with the corresponding manifold first. This way one can implement algorithms also independent of which representation is used.‚Äù p[M‚ÇÑ, 1] 3-element Vector{Float64}:\n 0.0\n 0.0\n 1.0 p‚ÇÇ[M‚ÇÖ, 2] 3-element Vector{Float64}:\n 0.0\n 1.0\n 0.0 Another constructor is the  ProductManifold  to combine different manifolds. Here of course the order matters. First we construct these using  $√ó$ M‚ÇÜ = M‚ÇÇ √ó M‚ÇÉ ProductManifold with 2 submanifolds:\n Hyperbolic(2)\n Sphere(2, ‚Ñù) Since now the representations might differ from element to element, we have to encapsulate these in their own type. using RecursiveArrayTools: ArrayPartition\np‚ÇÉ = ArrayPartition([0, 0, 1], [0, 1, 0]) ([0, 0, 1], [0, 1, 0]) Here  ArrayPartition  taken from  RecursiveArrayTools.jl  to store the point on the product manifold efficiently in one array, still allowing efficient access to the product elements. is_point(M‚ÇÜ, p‚ÇÉ; error=:error) true But accessing single components still works the same.‚Äù p‚ÇÉ[M‚ÇÜ, 1] 3-element Vector{Int64}:\n 0\n 0\n 1 Finally, also the  TangentBundle , the manifold collecting all tangent spaces on a manifold is available as‚Äù M‚Çá = TangentBundle(M‚ÇÉ) TangentBundle(Sphere(2, ‚Ñù))"},{"id":2083,"pagetitle":"üöÄ Get Started with Manifolds.jl","title":"Implementing generic Functions","ref":"/manifolds/stable/tutorials/getstarted/#Implementing-generic-Functions","content":" Implementing generic Functions In this section we take a look how to implement generic functions on manifolds. For our example here, we want to implement the so-called  B√©zier curve  using the so-called  de-Casteljau algorithm . The linked algorithm can easily be generalised to manifolds by replacing lines with geodesics. This was for example used in [ BG18 ] and the following example is an extended version of an example from [ ABBR23 ]. The algorithm works recursively. For the case that we have a B√©zier curve with just two points, the algorithm just evaluates the geodesic connecting both at some time point  $t‚àà[0,1]$ . The function to evaluate a shortest geodesic (it might not be unique, but then a deterministic choice is taken) between two points  p  and  q  on a manifold  M shortest_geodesic(M, p, q, t) . function de_Casteljau(M::AbstractManifold, t, pts::NTuple{2})\n    return shortest_geodesic(M, pts[1], pts[2], t)\nend de_Casteljau (generic function with 1 method) function de_Casteljau(M::AbstractManifold, t, pts::NTuple)\n    p = de_Casteljau(M, t, pts[1:(end - 1)])\n    q = de_Casteljau(M, t, pts[2:end])\n    return shortest_geodesic(M, p, q, t)\nend de_Casteljau (generic function with 2 methods) Which can now be used on any manifold where the shortest geodesic is implemented Now on several manifolds the  exponential map  and its (locally defined) inverse, the logarithmic map might not be available in an implementation. So one way to generalise this, is the use of a retraction (see [ AMS08 ], Def. 4.1.1 for details) and its (local) inverse. The function itself is quite similar to the exponential map, just that  retract(M, p, X, m)  has one further parameter, the type of retraction to take, so  m  is a subtype of  m , the same for the  inverse_retract(M, p, q, n)  with an  AbstractInverseRetractionMethod n . Thinking of a generic implementation, we would like to have a way to specify one, that is available. This can be done by using  default_retraction_method  and  default_inverse_retraction_method , respectively. We implement function generic_de_Casteljau(\n    M::AbstractManifold,\n    t,\n    pts::NTuple{2};\n    m::AbstractRetractionMethod=default_retraction_method(M),\n    n::AbstractInverseRetractionMethod=default_inverse_retraction_method(M),\n)\n    X = inverse_retract(M, pts[1], pts[2], n)\n    return retract(M, pts[1], X, t, m)\nend generic_de_Casteljau (generic function with 1 method) and for the recursion function generic_de_Casteljau(\n    M::AbstractManifold,\n    t,\n    pts::NTuple;\n    m::AbstractRetractionMethod=default_retraction_method(M),\n    n::AbstractInverseRetractionMethod=default_inverse_retraction_method(M),\n)\n    p = generic_de_Casteljau(M, t, pts[1:(end - 1)]; m=m, n=n)\n    q = generic_de_Casteljau(M, t, pts[2:end]; m=m, n=n)\n    X = inverse_retract(M, p, q, n)\n    return retract(M, p, X, t, m)\nend generic_de_Casteljau (generic function with 2 methods) Note that on a manifold  M  where the exponential map is implemented, the  default_retraction_method(M)  returns  ExponentialRetraction , which yields that the  retract  function falls back to calling  exp . The same mechanism exists for  parallel_transport_to(M, p, X, q)  and the more general  vector_transport_to(M, p, X, q, m)  whose  AbstractVectorTransportMethod m  has a default defined by  default_vector_transport_method(M) ."},{"id":2084,"pagetitle":"üöÄ Get Started with Manifolds.jl","title":"Allocating and in-place computations","ref":"/manifolds/stable/tutorials/getstarted/#Allocating-and-in-place-computations","content":" Allocating and in-place computations Memory allocation is a  critical performance issue  when programming in Julia. To take this into account,  Manifolds.jl  provides special functions to reduce the amount of allocations. We again look at the  exponential map . On a manifold  M  the exponential map needs a point  p  (to start from) and a tangent vector  X , which can be seen as direction to ‚Äúwalk into‚Äù as well as the length to walk into this direction. In  Manifolds.jl  the function can then be called with  q = exp(M, p, X)  (see  exp(M, p, X) ). This function returns the resulting point  q , which requires to allocate new memory. To avoid this allocation, the function  exp!(M, q, p, X)  can be called. Here  q  is allocated beforehand and is passed as the memory, where the result is returned in. It might be used even for interims computations, as long as it does not introduce side effects. Thas means that even with  exp!(M, p, p, X)  the result is correct. Let‚Äôs look at an example. We take another look at the  Sphere , but now a high-dimensional one. We can also illustrate how to generate radnom points and tangent vectors. M = Sphere(10000)\np‚ÇÑ = rand(M)\nX = rand(M; vector_at=p‚ÇÑ) Looking at the allocations required we get @allocated exp(M, p‚ÇÑ, X) 10934136 While if we have already allocated memory for the resulting point on the manifold, for example q‚ÇÇ = zero(p‚ÇÑ); There are no new memory allocations necessary if we use the in-place function.‚Äù @allocated exp!(M, q‚ÇÇ, p‚ÇÑ, X) 0 This methodology is used for all functions that compute a new point or tangent vector. By default all allocating functions allocate memory and call the in-place function. This also means that if you implement a new manifold, you just have to implement the in-place version."},{"id":2085,"pagetitle":"üöÄ Get Started with Manifolds.jl","title":"Decorating a manifold","ref":"/manifolds/stable/tutorials/getstarted/#Decorating-a-manifold","content":" Decorating a manifold As you saw until now, an [ AbstractManifold ]@extref  ManifoldsBase.AbstractManifold ) describes a Riemannian manifold. For completeness, this also includes the chosen  Riemannian metric tensor  or inner product on the tangent spaces. In  Manifolds.jl  these are assumed to be a ‚Äúreasonable default‚Äù. For example on the  Sphere (n)  we used above, the default metric is the one inherited from restricting the inner product from the embedding space onto each tangent space. Consider a manifold like M‚Çà = SymmetricPositiveDefinite(3) SymmetricPositiveDefinite(3) which is the manifold of  $3√ó3$  matrices that are  symmetric and positive definite . which has a default as well, the affine invariant  AffineInvariantMetric , but also has several different metrics. To switch the metric, we use the idea of a  decorator pattern  approach. Defining M‚Çà‚ÇÇ = MetricManifold(M‚Çà, BuresWassersteinMetric()) MetricManifold(SymmetricPositiveDefinite(3), BuresWassersteinMetric()) changes the manifold to use the  BuresWassersteinMetric . This changes all functions that depend on the metric, most prominently the Riemannian matric, but also the exponential and logarithmic map and hence also geodesics. All functions that are not dependent on a metric ‚Äì for example the manifold dimension, the tests of points and vectors we already looked at, but also all retractions ‚Äì stay unchanged. This means that for example [manifold_dimension(M‚Çà‚ÇÇ), manifold_dimension(M‚Çà)] 2-element Vector{Int64}:\n 6\n 6 both calls the same underlying function. On the other hand with p‚ÇÖ, X‚ÇÖ = one(zeros(3, 3)), [1.0 0.0 1.0; 0.0 1.0 0.0; 1.0 0.0 1.0] ([1.0 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 1.0], [1.0 0.0 1.0; 0.0 1.0 0.0; 1.0 0.0 1.0]) but for example the exponential map and the norm yield different results [exp(M‚Çà, p‚ÇÖ, X‚ÇÖ), exp(M‚Çà‚ÇÇ, p‚ÇÖ, X‚ÇÖ)] 2-element Vector{Matrix{Float64}}:\n [4.194528049465325 0.0 3.194528049465325; 0.0 2.718281828459045 0.0; 3.194528049465325 0.0 4.194528049465328]\n [2.5 0.0 1.5; 0.0 2.25 0.0; 1.5 0.0 2.5] [norm(M‚Çà, p‚ÇÖ, X‚ÇÖ), norm(M‚Çà‚ÇÇ, p‚ÇÖ, X‚ÇÖ)] 2-element Vector{Float64}:\n 2.23606797749979\n 1.118033988749895 Technically this done using Traits ‚Äì the trait here is the  IsMetricManifold  trait. Our trait system allows to combine traits but also to inherit properties in a hierarchical way, see  here  for the technical details. The same approach is used for specifying a different  connection specifying a manifold as a certain  quotient manifold specifying a certain  embedding s specify a certain  group action Again, for all of these, the concrete types only have to be used if you want to do a second, different from the details, property, for example a second way to embed a manifold. If a manifold is (in its usual representation) an embedded manifold, this works with the default manifold type already, since then it is again set as the reasonable default."},{"id":2086,"pagetitle":"üöÄ Get Started with Manifolds.jl","title":"Literature","ref":"/manifolds/stable/tutorials/getstarted/#Literature","content":" Literature [AMS08] P.-A.¬†Absil, R.¬†Mahony and R.¬†Sepulchre.  Optimization Algorithms on Matrix Manifolds  (Princeton University Press, 2008), available online at  press.princeton.edu/chapters/absil/ . [ABBR23] S.¬†D.¬†Axen, M.¬†Baran, R.¬†Bergmann and K.¬†Rzecki.  Manifolds.Jl: An Extensible Julia Framework for Data Analysis on Manifolds .  ACM¬†Transactions¬†on¬†Mathematical¬†Software  49  (2023). [BG18] R.¬†Bergmann and P.-Y.¬†Gousenbourger.  A variational model for data fitting on manifolds by minimizing the acceleration of a B√©zier curve .  Frontiers¬†in¬†Applied¬†Mathematics¬†and¬†Statistics  4  (2018),  arXiv:1807.10090 ."},{"id":2089,"pagetitle":"work with groups","title":"work with groups","ref":"/manifolds/stable/tutorials/groups/#work-with-groups","content":" work with groups Introduction: group of rotations on a plane From Coordinates To Coordinates Actions and Operations Relationship between groups, metrics and connections Literature This is a short overview of group support in  Manifolds.jl  and how to get started working with them. Groups currently available in  Manifolds.jl  are listed in  group section . You can read more about the theory of Lie groups for example in [ Chi12 ]. An example application of Lie groups in robotics is described in [ SDA21 ]. First, let‚Äôs load libraries we will use.  RecursiveArrayTools.jl  is necessary because its  ArrayPartition  is used as one of the possible representations of elements of product and semidirect product groups.  StaticArrays.jl  can be used to speed up some operations. using Manifolds, RecursiveArrayTools, StaticArrays"},{"id":2090,"pagetitle":"work with groups","title":"Introduction: group of rotations on a plane","ref":"/manifolds/stable/tutorials/groups/#Introduction:-group-of-rotations-on-a-plane","content":" Introduction: group of rotations on a plane Let‚Äôs first consider an example of the group of rotations of a plane,  $\\operatorname{SO}(2)$ . They can be represented in several ways, for example as angles of rotation (which corresponds to  RealCircleGroup ), unit complex numbers ( CircleGroup ), or rotation matrices ( SpecialOrthogonal ). Let‚Äôs consider the last representation since it is the most nontrivial one and can be more easily generalized to other groups. The associated manifolds and groups are defined by: G = SpecialOrthogonal(2)\nM = base_manifold(G)\n@assert M === Rotations(2) ‚îå Warning: SpecialOrthogonal will move to LieGroups.jl and be renamed to SpecialOrthogonalGroup.\n‚îÇ   caller = ip:0x0\n‚îî @ Core :-1 This duality (Lie group and the underlying manifold being separate) is a common pattern in  Manifolds.jl . The group  G  can be used for both Lie group-specific operations and metric-specific operation, while the manifold  M  only allows using manifold and metric operations. This way groups can be specialized in ways not relevant to plain manifolds, and if someone doesn‚Äôt use the groups structure, they don‚Äôt have to consider it by just using the manifold. Some basic definitions # default basis\nB = DefaultOrthogonalBasis()\n# Identity rotation\np0 = @SMatrix [1.0 0; 0 1]\n\n# Group identity element of a special type\nIG = Identity(G) Identity(MultiplicationOperation) Let‚Äôs say we want to define a manifold point  p_i  some rotation Œ∏ from the  identity_element  reference rotation  p0  (another point on the manifold that we will use as reference) # + radians rotation from x-axis on plane to point i\nxŒ∏i = œÄ/6 0.5235987755982988"},{"id":2091,"pagetitle":"work with groups","title":"From Coordinates","ref":"/manifolds/stable/tutorials/groups/#From-Coordinates","content":" From Coordinates To get our first Lie algebra element we can use the  hat  function which is commonly used in robotics, or equivalently a more generalized  get_vector , function: X_ = hat(G, IG, xŒ∏i)              # specific definition to Lie algebras\nxXi = get_vector(G, p0, xŒ∏i, B)   # generalized definition beyond Lie algebras\nprintln(xXi)\n@assert isapprox(X_, xXi) [0.0 -0.5235987755982988; 0.5235987755982988 0.0] Note that  hat  here assumes a default (orthogonal) basis for the more general  get_vector . Note In this case, the same would work given the base manifold  Rotations(2) : _X_ = hat(M, p0, xŒ∏i)             # Lie groups definition\n_X = get_vector(M, p0, xŒ∏i, B)   # generalized definition\n@assert _X_ == xXi; @assert _X == xXi One more caveat here is that for the Rotation matrices, the tangent vectors are always stored as elements from the Lie algebra. Now, we can transform this algebra element to a point on the manifold using the exponential map  exp : xRi = exp(G, p0, xXi)\n# similarly for known underlying manifold\nxRi_ = exp(M, p0, xXi)\n\n@assert isapprox(xRi, xRi_)"},{"id":2092,"pagetitle":"work with groups","title":"To Coordinates","ref":"/manifolds/stable/tutorials/groups/#To-Coordinates","content":" To Coordinates The logarithmic map transforms elements of the group back to its Lie algebra: xXi_ = log(G, p0, xRi)\nxXi__ = log(M, p0, xRi)\n@assert xXi ‚âà xXi__ Similarly, the coordinate values can be extracted from the algebra elements using  vee , or using the more generalized  get_coordinates : # extracting coordinates using vee\nxŒ∏i__ = vee(G, p0, xXi_)[1]\n_xŒ∏i__ = vee(M, p0, xXi_)[1]\n\n# OR, the preferred generalized get_coordinate function\nxŒ∏i_ = get_coordinates(G, p0, xXi_, B)[1]\n_xŒ∏i_ = get_coordinates(M, p0, xXi_, B)[1]\n\n# confirm all versions are correct\n@assert isapprox(xŒ∏i, xŒ∏i_); @assert isapprox(xŒ∏i, _xŒ∏i_)\n@assert isapprox(xŒ∏i, xŒ∏i__); @assert isapprox(xŒ∏i, _xŒ∏i__)"},{"id":2093,"pagetitle":"work with groups","title":"Actions and Operations","ref":"/manifolds/stable/tutorials/groups/#Actions-and-Operations","content":" Actions and Operations With the basics in hand on how to move between the coordinate, algebra, and group representations, let‚Äôs briefly look at composition and application of points on the manifold. For example, a  Rotations  manifold is the mathematical representation, but the points have an application purpose in retaining information regarding a specific rotation. Points from a Lie group may have an associated action (i.e.¬†a rotation) which we  apply . Consider rotating through  Œ∏ = œÄ/6  three vectors  V  from their native domain  Euclidean(2) , from the reference point  a  to a new point  b . Engineering disciplines sometimes refer to the action of a manifold point  a  or  b  as reference frames. More generally, by taking the tangent space at point  p , we are defining a local coordinate frame with basis  B , and should not be confused with ‚Äúreference frame‚Äù  a  or  b . Keeping with our two-dimensional example above: aV1 = [1; 0]\naV2 = [0; 1]\naV3 = [10; 10]\n\nA_left = RotationAction(Euclidean(2), G)\n\nbŒ∏a = œÄ/6\nbXa = get_vector(base_manifold(G), p0, bŒ∏a, B)\n\nbRa = exp(G, p0, bXa)\n\nfor aV in [aV1; aV2; aV3]\n    bV = apply(A_left, bRa, aV)\n    # test we are getting the rotated vectors in Euclidean(2) as expected\n    @assert isapprox(bV[1], norm(aV) * cos(bŒ∏a))\n    @assert isapprox(bV[2], norm(aV) * sin(bŒ∏a))\nend Note In general, actions are usually non-commutative and the user must therefore be aware whether they want to use  LeftAction  or  RightAction . In this case, the default  LeftAction()  is used. Finally, the actions (i.e.¬†points from a manifold) can be  compose d together. Consider putting together two rotations  aRb  and  bRc  such that a single composite rotation  aRc  is found. The next bit of code composes five rotations of  œÄ/4  increments: A_left = RotationAction(M, G)\naRi = copy(p0)\n\niŒ∏i_ = œÄ/4\nx_Œ∏ = get_vector(M, p0, iŒ∏i_, B) #hat(Rn, R0, Œ∏)\niRi_ = exp(M, p0, x_Œ∏)\n\n# do 5 times over:\n# Ri_ = Ri*iRi_\nfor i in 1:5\n    aRi = compose(A_left, aRi, iRi_)\nend\n\n# drop back to a algebra, then coordinate\naXi = log(G, p0, aRi)\naŒ∏i = get_coordinates(G, p0, aXi, B)\n\n# should wrap around to 3rd quadrant of xy-plane\n@assert isapprox(-3œÄ/4, aŒ∏i[1]) Warning compose  or  apply  must be done with group (not algebra) elements. This example shows how these two element types can easily be confused, since both the manifold group and algebra elements can have exactly the same data storage type ‚Äì i.e.¬†a 2x2 matrix. As a last note, other rotation representations, including quaternions, Pauli matrices, etc., have similar features. A contrasting example in rotations, however, are Euler angles which can also store rotation information but quickly becomes problematic with familiar problems such as  ‚Äúgimbal-lock‚Äù ."},{"id":2094,"pagetitle":"work with groups","title":"Relationship between groups, metrics and connections","ref":"/manifolds/stable/tutorials/groups/#Relationship-between-groups,-metrics-and-connections","content":" Relationship between groups, metrics and connections Group structure provides a canonical way to define  exponential  and logarithmic maps from the Lie algebra. They can be calculated in  Manifolds.jl  using the  exp_lie  and  log_lie  functions. Such exponential and logarithmic maps can be extended invariantly to tangent spaces at any point of the Lie group. This extension is implemented using functions  exp_inv  and  log_inv . Finally, there are  log  and  exp  functions which are metric (or connection)-related functions in  Manifolds.jl . For groups which can be equipped with a bi-invariant metric,  log  and  log_inv  return the same result, similarly  exp  and  exp_inv . However, only compact groups and their products with Euclidean spaces can have a bi-invariant metric (see for example Theorem 21.9 in [ GQ20 ]). A prominent example of a Lie group without a bi-invariant metric is the special Euclidean group (in two or more dimensions). Then we have a choice between a metric but non-invariant exponential map (which is generally the default choice for  exp ) or a non-metric, invariant exponential map ( exp_inv ). Which one should be used depends on whether being metric or being invariant is more important in a particular application. G = SpecialEuclidean(2)\np = ArrayPartition([1.0, -1.0], xRi)\nX = ArrayPartition([2.0, -3.0], aXi)\nq_m = exp(G, p, X)\nprintln(q_m)\nq_i = exp_inv(G, p, X)\nprintln(q_i) ‚îå Warning: SpecialEuclidean will move to LieGroups.jl and be renamed to SpecialEuclideanGroup.\n‚îÇ   caller = ip:0x0\n‚îî @ Core :-1\n‚îå Warning: TranslationGroup will move to LieGroups.jl.\n‚îÇ   caller = ip:0x0\n‚îî @ Core :-1\n\nArrayPartition{\n\n‚îå Warning: SemidirectProductGroup will move to LieGroups.jl and be renamed to LeftSemidirectProductLieGroup.\n‚îÇ   caller = ip:0x0\n‚îî @ Core :-1\n\nFloat64, Tuple{Vector{Float64}, SMatrix{2, 2, Float64, 4}}}(([3.0, -4.0], [-0.25881904510252124 0.9659258262890682; -0.9659258262890682 -0.25881904510252124]))\nArrayPartition{Float64, Tuple{Vector{Float64}, MMatrix{2, 2, Float64, 4}}}(([0.8121200537878321, -3.8212723543456155], [-0.25881904510252124 0.9659258262890682; -0.9659258262890682 -0.25881904510252124])) As we can see, the results differ. We can observe the invariance as follows: p2 = ArrayPartition([2.0, -1.0], xRi)\nq1_m = exp(G, translate(G, p2, p), translate_diff(G, p2, p, X))\nq2_m = translate(G, p2, exp(G, p, X))\nprintln(isapprox(q1_m, q2_m))\n\nq1_i = exp_inv(G, translate(G, p2, p), translate_diff(G, p2, p, X))\nq2_i = translate(G, p2, exp_inv(G, p, X))\nprintln(isapprox(q1_i, q2_i)) false\ntrue Now,  q1_m  and  q2_m  are different due to non-invariance of the metric connection but  q1_i  and  q2_i  are equal due to invarianced of  exp_inv . The following table outlines invariance of  exp  and  log  of various groups. Group Zero torsion connection Invariant ProductGroup Product of connections in each submanifold üü° [1] SemidirectProductGroup Same as underlying product ‚ùå TranslationGroup CartanSchoutenZero ‚úÖ CircleGroup CartanSchoutenZero ‚úÖ GeneralLinearGroup Metric connection from the left invariant metric induced from the standard basis on the Lie algebra ‚ùå GeneralUnitaryMultiplicationGroup CartanSchoutenZero  (explicitly) ‚úÖ HeisenbergGroup CartanSchoutenZero ‚úÖ SpecialLinearGroup Same as  GeneralLinear ‚ùå"},{"id":2095,"pagetitle":"work with groups","title":"Literature","ref":"/manifolds/stable/tutorials/groups/#Literature","content":" Literature [Chi12] G.¬†S.¬†Chirikjian.  Stochastic Models, Information Theory, and Lie Groups, Volume 2 . 1¬†Edition, Vol.¬†2 of  Applied and Numerical Harmonic Analysis  (Birkh√§user Boston, MA, 2012). [GQ20] J.¬†Gallier and J.¬†Quaintance.  Differential Geometry and Lie Groups: A Computational Perspective . Vol.¬†12 of  Geometry and Computing  ( Springer International Publishing, Cham, 2020 ). [SDA21] J.¬†Sol√†, J.¬†Deray and D.¬†Atchuthan.  A micro Lie theory for state estimation in robotics  (Dec 2021),  arXiv:1812.01537 [cs.RO] , arXiv: 1812.01537. 1 Yes if all component connections are invariant separately, otherwise no"},{"id":2098,"pagetitle":"perform Hand gesture analysis","title":"Hand gesture analysis","ref":"/manifolds/stable/tutorials/hand-gestures/#Hand-gesture-analysis","content":" Hand gesture analysis In this tutorial we will learn how to use Kendall‚Äôs shape space to analyze hand gesture data. Let‚Äôs start by loading libraries required for our work. using Manifolds, CSV, DataFrames, Plots, MultivariateStats Our first function loads dataset of hand gestures, described  here . function load_hands()\n    hands_url = \"https://raw.githubusercontent.com/geomstats/geomstats/master/geomstats/datasets/data/hands/hands.txt\"\n    hand_labels_url = \"https://raw.githubusercontent.com/geomstats/geomstats/master/geomstats/datasets/data/hands/labels.txt\"\n\n    hands = Matrix(CSV.read(download(hands_url), DataFrame, header=false))\n    hands = reshape(hands, size(hands, 1), 3, 22)\n    hand_labels = CSV.read(download(hand_labels_url), DataFrame, header=false).Column1\n    return hands, hand_labels\nend load_hands (generic function with 1 method) The following code plots a sample gesture as a 3D scatter plot of points. hands, hand_labels = load_hands()\nscatter3d(hands[1, 1, :], hands[1, 2, :], hands[1, 3, :]) Each gesture is represented by 22 landmarks in  $‚Ñù¬≥$ , so we use the appropriate Kendall‚Äôs shape space Mshape = KendallsShapeSpace(3, 22) KendallsShapeSpace(3, 22) Hands read from the dataset are projected to the shape space to remove translation and scaling variability. Rotational variability is then handled using the quotient structure of  KendallsShapeSpace hands_projected = [project(Mshape, hands[i, :, :]) for i in axes(hands, 1)] In the next part let‚Äôs do tangent space PCA. This starts with computing a mean point and computing logithmic maps at mean to each point in the dataset. mean_hand = mean(Mshape, hands_projected)\nhand_logs = [log(Mshape, mean_hand, p) for p in hands_projected] For a tangent PCA, we need coordinates in a basis. Some libraries skip this step because the representation of tangent vectors forms a linear subspace of an Euclidean space so PCA automatically detects which directions have no variance but this is a more generic way to solve this issue. B = get_basis(Mshape, mean_hand, ProjectedOrthonormalBasis(:svd))\nhand_log_coordinates = [get_coordinates(Mshape, mean_hand, X, B) for X in hand_logs] This code prepares data for MultivariateStats ‚Äì  mean=0  is set because we‚Äôve centered the data geometrically to  mean_hand  in the code above. red_coords = reduce(hcat, hand_log_coordinates)\nfp = fit(PCA, red_coords; mean=0) PCA(indim = 59, outdim = 19, principalratio = 0.991345477862632)\n\nPattern matrix (unstandardized loadings):\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n             PC1           PC2           PC3           PC4           PC5           PC6           PC7           PC8           PC9          PC10          PC11          PC12          PC13          PC14          PC15          PC16          PC17          PC18          PC19\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n1    0.0225725    -0.025823      0.00247119   -0.00194504    0.011187      0.00865122   -0.00432       0.00232697   -0.00133113   -0.00603743    0.00279173    0.00459607    0.0026104    -0.00194346   -0.0010809     0.00371279   -0.0026941    -0.00336716   -4.78115e-6\n2   -0.0298018     0.012855     -0.0253788    -0.0107526    -0.000642301   0.00243343   -0.0129186     0.0117729     0.0159723     0.00107763   -0.00131555    0.00285026    0.00463989   -0.00216125    0.00161956    0.00105094    0.00029935   -0.000818863   0.000226847\n3   -0.00496432   -0.0149723    -0.00377204    0.0160204    -0.0052388     0.00212934   -0.00568703   -0.00338036    0.000889778  -0.00214546   -0.000896225  -0.00137907   -0.00247179    0.00239153   -0.000377108  -0.00215105    0.0027909     0.000117751  -0.00127213\n4    0.0056335    -0.0129395     0.00752466    0.00555847   -0.00280322    0.00782194    0.00111858    0.0085054    -0.00273334   -0.00581609    0.00450121   -0.00038663    0.00167242    0.00231527    0.000307966  -0.00131181    0.000720744   0.00111114    0.00361935\n5   -0.0113838     0.0232298    -0.00547126   -0.0179074    -0.00225183   -0.0123001    -0.00379069    0.00375553   -0.0110292     0.00304722   -0.00459228   -0.00477702   -0.000442113   0.00166295    0.00238507    0.00489784   -0.00261756    0.00101093   -0.00178457\n6   -0.0345859    -0.0233477     0.0296546     0.00689836    0.003326     -0.00135883   -0.0119614     0.00329535   -0.00698649   -0.00493814    0.00376636    0.000613432  -0.00149234   -0.000250127   0.00623361    0.00254364   -0.000999917   9.66511e-6   -0.000225613\n7   -0.0374342    -0.0209963     0.00235653    0.000683237   0.00758501   -0.00999398    0.00657121    0.00782893   -0.00406903   -0.00581627   -0.00079722    0.0041202     0.00319899    0.00585783   -0.00167581    0.0030249     0.00322473   -0.00108314   -0.00116042\n8    0.02496       0.00943876    0.011668     -0.00772333    0.00960598   -0.00142074    0.00211964   -0.000840499  -0.000234754  -0.000749657   0.000307986   0.0066857    -0.000133384   0.00300887    0.0022704     0.00120301    0.00203841    0.000966472   0.00120561\n9    0.0296003     0.0472131     0.00210213   -0.00393545   -0.00142636   -0.00236192   -0.00533011   -0.00345383   -0.00439341   -0.00852948   -0.000396924  -0.000150678   0.00330386    0.00089967    0.00388721   -0.00199243   -0.00161544    0.00329972   -0.00160867\n10   0.0187554     0.00337061    0.0254247     0.00182716   -0.00726025    0.00345359    0.0118753     0.00392614   -0.00737372   -0.000261283  -0.0044591     0.000623762  -0.00123177   -0.00187434    0.00347221    0.00122414    0.00162647    0.000179093  -0.000639519\n11  -0.0276189    -0.000868697   0.00124921   -0.0271846    -0.0182645    -0.00654734   -0.00377689   -0.00309552    0.00263585    0.0083263     0.0117277    -0.000277824  -0.0013572    -0.00171603   -0.00248708    0.0002054    -0.00129968    0.000538015   0.000635364\n12   0.0742252    -0.0111949    -0.00393113   -0.00375025   -0.00466852    0.00718482   -0.00421926   -0.000133097  -0.00285935    0.00220137   -0.0063239     0.00591546   -0.00391862   -0.00220676   -0.000179801   0.00446115   -0.000848733  -0.00206532    0.00226783\n13   0.0426582     0.00910716    0.0199886     0.00477317    0.000139353   0.00719425   -0.0105414    -0.00511285   -0.00217195    0.0106759    -0.000717452   0.00304458   -0.00227829    0.00118983    0.00302885    0.00259039    0.000590182  -0.00113679   -0.0014884\n14   0.0473028     0.00651493    0.00140138    0.00225381    0.00439764   -0.00750531    0.0193882    -0.00406276    0.0131133    -0.00935677    0.00405362   -0.00109819   -0.000160323   0.00193398    0.00109956    0.000814692   0.000914761   0.00198064    0.00125932\n15   0.0332915     0.0221761     0.0200686    -0.000418236  -0.00238077    0.0136492    -0.00572773   -0.00525892    0.00565712   -0.00186374    0.00193716   -0.00251317    0.00147158   -0.00144082   -0.0036298     0.00233316   -0.000552749  -0.00117945   -0.00143965\n16  -0.019766     -0.0126687    -0.0327939    -0.00786485    0.00374512    0.00537198   -0.0105409    -0.0120939    -4.87631e-5   -0.000200912   0.00649485    0.00341607    0.00856351   -0.000627434   0.00512775    0.00157364    0.0025359    -0.000340485   0.00302362\n17  -0.0163038     0.00392205    0.00368172   -0.00108868   -0.00403097   -0.00250602   -0.00154047   -0.0091664     0.012208     -0.00212529   -0.00249079    0.00425261   -0.0009888     0.000699026   0.00159838    0.00182519   -0.00268461    0.0024567    -0.00287173\n18   0.0315265     0.014339     -0.0281659     0.0188983     0.00116313    0.00517276   -0.012306      0.00964101    0.00378792    0.003776     -0.00471276   -0.00276278    0.00073812   -0.0018049    -0.00110501   -0.00142815   -0.00324852    0.00125496    0.00036833\n19  -0.01342      -0.0266179     0.00692541   -0.00394567    0.00696648    0.00546101    0.0107503     0.00161672    0.00243154    0.0111542    -0.00859616   -0.0022637    -0.00125188   -0.00218111    0.00418941   -0.00245702    0.00109089    0.00262105    0.000894704\n20  -0.000739408   0.00263129    0.00124919    0.000518372  -0.00405283   -0.00980204   -0.00494474   -0.00280603   -0.00543177   -0.00590718    0.00579551    0.00110744   -0.00812888    0.00441085   -0.00391783    0.00174295   -0.000252033  -0.00209523    0.00204457\n21   0.0504932    -0.0110797    -0.0026189    -0.00962761   -0.0153567    -0.00273023    0.00361822    0.00201177   -0.00445139   -0.000379839   0.00688118   -0.000391294  -0.000934333  -0.00118075   -0.00270666   -0.000350291   0.000905076   0.00117337   -0.000755983\n22  -0.046736      0.00232599   -0.0170992    -0.00282647   -0.014517      0.00487345    0.00150701   -0.00101275   -0.00475473    0.00292961    0.00419816    0.0025307     0.00131901    0.00431956   -0.00200533   -0.00153549   -0.000850689  -0.000199973  -0.000797035\n23  -0.0173763    -0.0135675    -0.0235186    -0.00469928   -0.010083      0.00622843    0.00612574    0.000719204  -0.00473516   -0.00170497   -0.0088133    -0.00339875    0.0030805     0.00313125   -0.00334306    0.00617814    0.00103001    0.00544743    0.000548909\n24  -0.00826661    0.0167308     0.00865901   -0.00214922    0.0150335     0.00823526   -0.000297993  -0.00237168   -0.000986393   0.00220198   -0.000681713  -0.000197612   0.000283165   0.00363122    0.000841859  -0.000161184  -0.00456144    0.000377615   0.000890576\n25  -0.00644559    0.010877      0.00278186   -0.00911345   -0.0107489     0.0243946     0.00709473    0.0105892    -0.00627612   -0.00400938    0.00573931   -0.000879342  -0.00421579   -0.00280443    0.00183861   -0.00101372    0.00165889   -0.00303974    0.0016519\n26   0.0441278     0.0158065    -0.00400205   -0.000325081   0.00203837    0.0080049     0.00759341    0.00350391   -0.00106124    0.000470616   0.000231166   0.000781562   0.000640405   0.00106251   -0.00165851    0.0036525     0.00222514   -0.00223304   -0.00322942\n27  -0.0185767     0.0125668     0.0141537     0.0144925     0.00211121   -0.0084411    -0.00693538    0.00588689    0.00509395   -0.000498211  -8.28981e-5   -0.00196856    0.000165433   0.00150428    0.00217742    0.00406355    0.00266332    0.000538881   0.000787166\n28  -0.0288949     0.0285503    -0.00938241    0.00314241    0.00451199   -0.000322886   0.0102419    -0.0057291     0.00452652   -0.00434567    0.00333208    0.000768434   0.000697567  -0.000947317   0.00197396    0.00231527   -0.00262045   -0.0052142    -0.0022789\n29   0.054652      0.016176      0.0233463    -0.00263117    0.0140462    -0.00251733    0.00410277    0.00636276    0.00827887    0.00558304    0.00735049    0.00273341    0.00209918   -0.00136746   -0.00493028    0.0020373     0.00130772    0.00142239   -0.00115467\n30  -0.0234619     0.0175726     0.0109159     0.0120296    -0.00457707   -0.0145693    -7.70199e-5   -0.00427574   -0.00308155    0.0048407    -0.000529341  -0.00208163   -0.00219104   -0.00363319   -0.00608362    0.00641312    7.09267e-5    0.00118223    0.00326152\n31  -0.00743661   -0.0210373    -0.0126597    -0.00541652    0.00933005    0.00815691    0.00468841    0.00631952    0.0111953     0.00090758    0.000217408  -0.0062275    -0.00622832    0.000955856   0.00072591    0.00169066   -0.00115975   -0.00147563   -0.00250162\n32   0.00942946    0.0348065    -0.00344763    0.00870651   -0.0074523    -0.00813785    0.00974278    0.00503949   -0.0115295     0.00778616    0.00187608   -6.06739e-5    0.0101328    -0.00379749    0.00144605   -0.000102658  -0.00132698   -0.00276075    0.00106114\n33   0.0632112    -0.0391171    -0.00200189   -0.0220903    -0.0144734    -0.0113132     0.0077086    -0.000969037   0.00503636    0.00706564    0.00437077    0.00130097    0.00340908    0.0051326     0.00253346    0.000507903  -0.000707207  -0.00056295   -0.00194977\n34   0.0207818    -0.0135932     0.000771381   0.0036835     0.0213596    -0.00486239    0.00157098   -0.00771602   -0.00359822   -0.00378203   -0.0016533    -0.0110685    -0.000753667  -0.00229644   -0.00150167   -0.000303523   0.00158846   -0.000607838   0.000615078\n35  -0.000379201  -0.0120639     0.0297885    -0.00270901    0.00650958   -0.000380978  -0.00121355    0.0101664    -0.00517949   -0.000558258   0.00197674    0.000277632   0.00797185   -0.000760715  -0.00264005   -0.00348748   -0.00231271    0.00137201    0.0015573\n36   0.00115252    0.000667601  -0.000336516   0.0137512    -0.00182112    0.000312716   0.0107091    -0.0131901    -0.000686853   0.00429926   -0.00405865   -0.00152666    0.00779814    0.00289293    0.000712759  -0.00290687    0.000187923  -0.00341138   -0.00143035\n37  -0.0710484    -0.0120054    -0.00742916    0.0315282     0.000899437   0.00447448    0.00873192   -0.00168979    0.00408006    0.0081241     0.0111211    -0.00380385   -0.0027588     0.000382961   0.000901206   0.00209142   -0.00297548    0.000369737   0.000194095\n38   0.0173624    -0.00401998    0.00476487   -0.000229019   0.00277068    0.0121718    -0.00471613   -0.00762391    0.00490114    6.67001e-5   -0.00184396    0.00294105    0.00142187    0.00170863    0.00048986   -0.000522409   0.00089876    0.00154977   -0.00243131\n39  -0.0132061     0.0340923    -0.00256571   -0.00165163   -0.0056104    -0.00854407   -0.00256856    0.00640797    0.00421851    0.00576642   -0.00326446    0.00247372   -0.00086705   -0.003962      0.00141905   -0.000994299   0.00542489    0.000229981  -0.00130346\n40   0.0201995     0.00167282   -0.015568      0.000124091   0.00826228    0.00495587    0.0025755     0.0135085     0.0068398    -0.00135169   -0.000719346  -0.000414134   0.00126898   -0.00109902   -0.00309125    0.00299976   -0.00179919    0.000209681   0.00138728\n41   0.0311951    -0.00349583   -0.000650444   0.00225274   -0.0202231     0.00133542    0.000267878  -0.00500205    0.00441379    4.99341e-6   -0.00226762   -0.00423897   -0.00153824   -0.00378025   -0.00351691   -0.000162936  -0.0024964     0.000454088   0.000183778\n42   0.00295307   -0.00780823    0.00723101    0.0102236    -0.00344839    0.0073579    -0.00253667   -0.00139851   -0.00143191    0.00254803    0.00531096    0.00456018   -0.00235568   -0.00376306   -0.00340022   -0.00172833    0.00121904    0.00142326   -0.000685875\n43   0.0194641    -0.0113322    -0.00277425    0.00886678    0.010623     -0.0172909    -0.00366068    0.00715197    0.00562361    0.00432699    0.00736201    0.00317588   -0.00247544   -0.00147035    0.00388555   -0.00220526    0.00103274    0.00163166    0.000317339\n44  -0.0404226    -0.0150374     0.0122555    -0.00944404    0.0105142     0.000430679  -0.00492254   -0.0094228     0.0056596     0.000305474  -0.00401212    0.00405241    0.00754805   -0.0016228    -0.00720144    0.000628635   0.00159903   -0.00156151    0.00249425\n45   0.019093      0.0161417    -0.0106445     0.00289789   -0.00179217   -0.00983076    0.00218895    0.00481717    0.0085834    -0.00607766    0.000501381  -0.00255306   -0.000130565   0.00217908    0.00346562   -0.000368367   0.00076262   -0.000891547   0.00282931\n46   0.0260759     0.00663921   -0.0162865    -0.000305536   0.011383     -0.00196883   -0.00363319   -0.0036993    -0.00455917   -0.00290846    0.00888095   -0.00789423    0.00304331   -0.00740744    0.00050636   -0.000302411   0.0030843     0.00049445   -0.00284163\n47  -0.0221746    -0.0199404     0.000534764   0.00477627    0.0141621     0.000433482   0.000167613   0.00463961   -0.00399233    0.00393828   -0.000758662  -0.00181117    0.00373518   -0.00279076   -0.000500963   0.00307397    0.0025922    -0.00118273   -0.00313347\n48  -0.0550543     0.0363964    -0.00742737    0.00509457    0.00478327    0.00596863    0.0123141     0.00319395    0.00032336    0.00227062    0.00182639    0.0089813    -0.00131728    0.00282792   -0.00285173    0.000575806   0.000630176   0.00142055   -0.000739086\n49   0.00913939   -0.0112506     0.00275407   -0.000108444   0.0175965    -0.00720219    0.0028736    -0.00852751   -0.00367575    0.00224067    0.00306352    0.00371249   -0.0013583    -0.00438014    0.00156735    0.00259369   -0.00342623    0.00252017   -0.000817948\n50  -0.00233578   -0.0202756     0.0265616     0.0127293    -0.0118266     0.00893602    0.00197975    0.00795907    0.00501867    0.00222461    0.00222965   -0.00193209    0.00671279    0.000512806  -0.000123589   0.00127378   -0.00139443    0.001997     -0.0018316\n51  -0.0153559    -0.0100082     0.0139632     0.00493122   -0.0222626     0.000493596   0.00833022   -0.0038709     0.00743823   -0.00699599   -0.0016895     0.00127742    0.00430974   -0.00836289    0.00714681    0.00356698   -0.000744286   0.000825278   0.00269039\n52   0.0263224    -0.017408     -0.0229181    -0.00281498    0.00866802   -0.0019128     0.00195427    0.00439744   -0.00584011    0.00819845    0.000279562   0.000429365  -0.000174549   0.00240462    0.00358596    0.00083813   -0.00110688   -0.000486164   0.00155618\n53   0.0340365     0.0249426    -0.000528411   0.00975803    0.0137844     0.0129551     0.00420257   -0.00700206   -0.00177581    0.00589381    0.00212311    0.00317137   -0.00186415    0.00236349    0.00271734   -0.000199331   0.000225671   0.00280694    0.00416741\n54  -0.0117798     0.00903529    0.0387701     0.00454727   -0.00907697   -0.00488634   -0.00773384   -6.19644e-6    0.0089247     0.00618783   -0.00347535   -0.00495008   -0.000644665   0.00498122    0.00158708   -0.000299986   0.00137401   -0.00415685    0.00151692\n55   0.040961     -0.0047418    -0.00467653    0.0299573    -0.010076     -0.00532661   -0.00871529    0.0040513    -0.00358455   -0.00462993    0.00250156    0.00223835    0.0054579     0.00519887   -0.000305873   0.000448611  -0.00211423    0.00125517   -0.00247527\n56   0.0102925     0.013984      0.000812639  -0.00373706   -0.00349584    0.0124504    -0.00441857   -0.0044716     0.00140511    0.00328495    0.00684282   -0.00884618    0.0042292     0.00577289   -0.00200728    0.00248224    0.00354552    0.00261187    0.00147238\n57   0.0477407     1.14678e-5   -0.00662024    0.00219729    0.00430952   -0.00449388    0.00119093    0.000191644   0.00325373    0.00379379   -0.000464058  -0.00390622    0.00283012    0.00361447   -0.000321092   0.000398221  -0.00180963   -0.00322221    0.00339463\n58   0.0399973    -0.0189741    -0.0247896     0.0297376    -0.013145     -0.00537322    0.000798352  -0.00376624   -0.00048942    0.0013669    -0.00261489    0.00622029   -0.00186308   -0.00227817   -0.00329716    6.72505e-6    0.00269157   -0.00133317    0.000372921\n59  -0.00330724    0.000903053   0.00809613   -0.00496936    0.00629712   -0.00809498    0.00396068    0.0024341     0.00266899   -0.00182288   -0.00357433    0.00138473    0.00143361   -0.00171682   -0.00717317   -0.00423229   -0.00252162    0.00019209    0.000617265\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nImportance of components:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n                                 PC1        PC2        PC3         PC4         PC5         PC6         PC7         PC8         PC9        PC10        PC11         PC12         PC13         PC14         PC15         PC16         PC17         PC18        PC19\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSS Loadings (Eigenvalues)  0.0571665  0.0189413  0.0127985  0.00692343  0.00559569  0.00374975  0.00277789  0.00221675  0.00204783  0.00136863  0.00118197  0.000853869  0.000786129  0.000575358  0.000561562  0.000342726  0.000253164  0.000229252  0.00020354\nVariance explained         0.477945   0.15836    0.107003   0.0578838   0.0467831   0.0313501   0.0232248   0.0185333   0.017121    0.0114425   0.00988197  0.00713883   0.00657249   0.00481032   0.00469498   0.00286538   0.00211659   0.00191668   0.00170171\nCumulative variance        0.477945   0.636305   0.743308   0.801192    0.847975    0.879325    0.90255     0.921083    0.938204    0.949647    0.959528    0.966667     0.97324      0.97805      0.982745     0.98561      0.987727     0.989644     0.991345\nProportion explained       0.482117   0.159743   0.107937   0.0583891   0.0471916   0.0316238   0.0234275   0.0186951   0.0172705   0.0115424   0.00996824  0.00720116   0.00662987   0.00485231   0.00473597   0.0028904    0.00213507   0.00193341   0.00171657\nCumulative proportion      0.482117   0.64186    0.749797   0.808186    0.855378    0.887002    0.910429    0.929124    0.946395    0.957937    0.967905    0.975106     0.981736     0.986589     0.991325     0.994215     0.99635      0.998283     1.0\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Now let‚Äôs show explained variance of each principal component. plot(principalvars(fp), title=\"explained variance\", label=\"Tangent PCA\") The next plot shows how projections on the first two principal components look like. fig = plot(; title=\"coordinates per gesture of the first two principal components\")\nfor label_num in [0, 1]\n    mask = hand_labels .== label_num\n    cur_hand_logs = red_coords[:, mask]\n    cur_t = MultivariateStats.transform(fp, cur_hand_logs)\n    scatter!(fig, cur_t[1, :], cur_t[2, :], label=\"gesture \" * string(label_num))\nend\nxlabel!(fig, \"principal component 1\")\nylabel!(fig, \"principal component 2\")\nfig The following heatmap displays pairwise distances between gestures. We can use them for clustering, classification, etc. hand_distances = [\n    distance(Mshape, hands_projected[i], hands_projected[j]) for\n    i in eachindex(hands_projected), j in eachindex(hands_projected)\n]\nheatmap(hand_distances, aspect_ratio=:equal)"},{"id":2101,"pagetitle":"integrate on manifolds and handle probability densities","title":"Integration","ref":"/manifolds/stable/tutorials/integration/#Integration","content":" Integration This part of documentation covers integration of scalar functions defined on manifolds  $f \\colon \\mathcal{M} \\to ‚Ñù$ : \\[\\int_{\\mathcal M} f(p) \\mathrm{d}p\\] The basic concepts are derived from geometric measure theory. In principle, there are many ways in which a manifold can be equipped with a measure that can be later used to define an integral. One of the most popular ways is based on pushing the Lebesgue measure on a tangent space through the exponential map. Any other suitable atlas could be used, not just the one defined by normal coordinates, though each one requires different volume density corrections due to the Jacobian determinant of the pushforward.  Manifolds.jl  provides the function  volume_density  that calculates that quantity, denoted  $\\theta_p(X)$ . See for example [ BP19 ], Definition 11, for a precise description using Jacobi fields. While many sources define volume density as a function of two points,  Manifolds.jl  decided to use the more general point-tangent vector formulation. The two-points variant can be implemented as using Manifolds\nvolume_density_two_points(M::AbstractManifold, p, q) = volume_density(M, p, log(M, p, q)) volume_density_two_points (generic function with 1 method) The simplest way to of integrating a function on a compact manifold is through a  Monte Carlo integrator . A simple variant can be implemented as follows (assuming uniform distribution of  rand ): using LinearAlgebra, Distributions, SpecialFunctions\nfunction simple_mc_integrate(M::AbstractManifold, f; N::Int = 1000)\n    V = manifold_volume(M)\n    sum = 0.0\n    q = rand(M)\n    for i in 1:N\n        sum += f(M, q)\n        rand!(M, q)\n    end\n    return V * sum/N\nend simple_mc_integrate (generic function with 1 method) We used the function  manifold_volume  to get the volume of the set over which the integration is performed, as described in the linked Wikipedia article."},{"id":2102,"pagetitle":"integrate on manifolds and handle probability densities","title":"Distributions","ref":"/manifolds/stable/tutorials/integration/#Distributions","content":" Distributions We will now try to verify that volume density correction correctly changes probability density of an exponential-wrapped normal distribution.  pdf_tangent_space  (defined in the next code block) represents probability density of a normally distributed random variable  $X_T$  in the tangent space  $T_p \\mathcal{M}$ . Its probability density (with respect to the Lebesgue measure of the tangent space) is  $f_{X_T}\\colon T_p \\mathcal{M} \\to ‚Ñù$ . pdf_manifold  (defined below) refers to the probability density of the distribution  $X_M$  from the tangent space  $T_p \\mathcal{M}$  wrapped using exponential map on the manifold. The formula for probability density with respect to pushforward measure of the Lebesgue measure in the tangent space reads \\[f_{X_M}(q) = \\sum_{X \\in T_p\\mathcal{M}, \\exp_p(X)=q} \\frac{f_{X_T}(X)}{\\theta_p(X)}\\] volume_density  function calculates the correction  $\\theta_p(X)$ . function pdf_tangent_space(M::AbstractManifold, p)\n    return pdf(MvNormal(zeros(manifold_dimension(M)), 0.2*I), p)\nend\n\nfunction pdf_manifold(M::AbstractManifold, q)\n    p = [1.0, 0.0, 0.0]\n    X = log(M, p, q)\n    Xc = get_coordinates(M, p, X, DefaultOrthonormalBasis())\n    vd = abs(volume_density(M, p, X))\n    if vd > eps()\n        return pdf_tangent_space(M, Xc) / vd\n    else\n        return 0.0\n    end\nend\n\nprintln(simple_mc_integrate(Sphere(2), pdf_manifold; N=1000000)) 1.0021829276055931 The function  simple_mc_integrate , defined in the previous section, is used to verify that the density integrates to 1 over the manifold. Note that our  pdf_manifold  implements a simplified version of  $f_{X_M}$  which assumes that the probability mass of  pdf_tangent_space  outside of (local) injectivity radius at  $p$  is negligible. In such case there is only one non-zero summand in the formula for  $f_{X_M}(q)$ , namely  $X=\\log_p(q)$ . Otherwise we would have to consider other vectors  $Y\\in T_p \\mathcal{M}$  such that  $\\exp_p(Y) = q$  in that sum. Remarkably, exponential-wrapped distributions possess three important qualities [ CLLD22 ]: Densities of  $X_M$  are explicit. There is no normalization constant that needs to be computed like in truncated distributions. Sampling from  $X_M$  is easy. It suffices to get a sample from  $X_T$  and pass it to the exponential map. If mean of  $X_T$  is 0, then there is a simple correspondence between moments of  $X_M$  and  $X_T$ , for example  $p$  is the mean of  $X_M$ ."},{"id":2103,"pagetitle":"integrate on manifolds and handle probability densities","title":"Kernel density estimation","ref":"/manifolds/stable/tutorials/integration/#Kernel-density-estimation","content":" Kernel density estimation We can also make a Pelletier‚Äôs isotropic kernel density estimator. Given points  $p_1, p_2, \\dots, p_n$  on  $d$ -dimensional manifold  $\\mathcal M$  the density at point  $q$  is defined as \\[f(q) = \\frac{1}{n h^d} \\sum_{i=1}^n \\frac{1}{\\theta_q(\\log_q(p_i))}K\\left( \\frac{d(q, p_i)}{h} \\right),\\] where  $h$  is the bandwidth, a small positive number less than the injectivity radius of  $\\mathcal M$  and  $K\\colon‚Ñù\\to‚Ñù$  is a kernel function. Note that Pelletier‚Äôs estimator can only use radially-symmetric kernels. The radially symmetric multivariate Epanechnikov kernel used in the example below is described in [ LW19 ]. struct PelletierKDE{TM<:AbstractManifold,TPts<:AbstractVector}\n    M::TM\n    bandwidth::Float64\n    pts::TPts\nend\n\n(kde::PelletierKDE)(::AbstractManifold, p) = kde(p)\nfunction (kde::PelletierKDE)(p)\n    n = length(kde.pts)\n    d = manifold_dimension(kde.M)\n    sum_kde = 0.0\n    function epanechnikov_kernel(x)\n        if x < 1\n            return gamma(2+d/2) * (1-x^2)/(œÄ^(d/2))\n        else\n            return 0.0\n        end\n    end\n    for i in 1:n\n        X = log(kde.M, p, kde.pts[i])\n        Xn = norm(kde.M, p, X)\n        sum_kde += epanechnikov_kernel(Xn / kde.bandwidth) / volume_density(kde.M, p, X)\n    end\n    sum_kde /= n * kde.bandwidth^d\n    return sum_kde\nend\n\nM = Sphere(2)\npts = rand(M, 8)\nkde = PelletierKDE(M, 0.7, pts)\nprintln(simple_mc_integrate(Sphere(2), kde; N=1000000))\nprintln(kde(rand(M))) 0.9978204829454278\n0.0708163510201507"},{"id":2104,"pagetitle":"integrate on manifolds and handle probability densities","title":"Technical notes","ref":"/manifolds/stable/tutorials/integration/#Technical-notes","content":" Technical notes This section contains a few technical notes that are relevant to the problem of integration on manifolds but can be freely skipped on the first read of the tutorial."},{"id":2105,"pagetitle":"integrate on manifolds and handle probability densities","title":"Conflicting statements about volume of a manifold","ref":"/manifolds/stable/tutorials/integration/#Conflicting-statements-about-volume-of-a-manifold","content":" Conflicting statements about volume of a manifold manifold_volume  and  volume_density  are closely related to each other, though very few sources explore this connection, and some even claiming a certain level of arbitrariness in defining  manifold_volume . Volume is sometimes considered arbitrary because Riemannian metrics on some spaces like the manifold of rotations are defined with arbitrary constants. However, once a constant is picked (and it must be picked before any useful computation can be performed), all geometric operations must follow in a consistent way: inner products, exponential and logarithmic maps, volume densities, etc.  Manifolds.jl  consistently picks such constants and provides a unified framework, though it sometimes results in picking a different constant than what is the most popular in some sub-communities."},{"id":2106,"pagetitle":"integrate on manifolds and handle probability densities","title":"Haar measures","ref":"/manifolds/stable/tutorials/integration/#Haar-measures","content":" Haar measures On Lie groups the situation regarding integration is more complicated. Invariance under left or right group action is a desired property that leads one to consider Haar measures [ Tor20 ]. It is, however, unclear what are the practical benefits of considering Haar measures over the Lebesgue measure of the underlying manifold, which often turns out to be invariant anyway."},{"id":2107,"pagetitle":"integrate on manifolds and handle probability densities","title":"Integration in charts","ref":"/manifolds/stable/tutorials/integration/#Integration-in-charts","content":" Integration in charts Integration through charts is an approach currently not supported by  Manifolds.jl . One has to define a suitable set of disjoint charts covering the entire manifold and use a method for multivariate Euclidean integration. Note that ranges of parameters have to be adjusted for each manifold and scaling based on the metric needs to be applied. See [ BST03 ] for some considerations on symmetric spaces."},{"id":2108,"pagetitle":"integrate on manifolds and handle probability densities","title":"References","ref":"/manifolds/stable/tutorials/integration/#References","content":" References"},{"id":2109,"pagetitle":"integrate on manifolds and handle probability densities","title":"Literature","ref":"/manifolds/stable/tutorials/integration/#Literature","content":" Literature [BST03] L.¬†J.¬†Boya, E.¬†Sudarshan and T.¬†Tilma.  Volumes of compact manifolds .  Reports¬†on¬†Mathematical¬†Physics  52 , 401‚Äì422  (2003). [BP19] A.¬†L.¬†Brigant and S.¬†Puechmorel.  Approximation of Densities on Riemannian Manifolds .  Entropy  21 , 43  (2019). [CLLD22] E.¬†Chevallier, D.¬†Li, Y.¬†Lu and D.¬†B.¬†Dunson.  Exponential-wrapped distributions on symmetric spaces . ArXiv¬†Preprint (2022). [LW19] N.¬†Langren√© and X.¬†Warin.  Fast and Stable Multivariate Kernel Density Estimation by Fast Sum Updating .  Journal¬†of¬†Computational¬†and¬†Graphical¬†Statistics  28 , 596‚Äì608  (2019). [Tor20] S.¬†Tornier.  Haar Measures  (2020)."},{"id":2112,"pagetitle":"work in charts","title":"Working in charts","ref":"/manifolds/stable/tutorials/working-in-charts/#Working-in-charts","content":" Working in charts In this tutorial we will learn how to use charts for basic geometric operations like exponential map, logarithmic map and parallel transport. There are two conceptually different approaches to working on a manifold: working in charts and chart-free representations. The first one, widespread in differential geometry textbooks, is based on defining an atlas on the manifold and performing computations in selected charts. This approach, while generic, is not ideally suitable in all circumstances. For example, working in charts that do not cover the entire manifold causes issues with having to switch charts when operating on a manifold. The second one is beneficial if there exists a representation of points and tangent vectors for a manifold which allows for efficient closed-form formulas for standard functions like the exponential map or Riemannian distance in this representation. These computations are then chart-free.  Manifolds.jl  supports both approaches, although the chart-free approach is the main focus of the library. In this tutorial we focus on chart-based computation. using Manifolds, RecursiveArrayTools, OrdinaryDiffEq, DiffEqCallbacks, BoundaryValueDiffEq The manifold we consider is the  M  is the torus in form of the  EmbeddedTorus , that is the representation defined as a surface of revolution of a circle of radius 2 around a circle of radius 3. The atlas we will perform computations in is its  DefaultTorusAtlas A , consisting of a family of charts indexed by two angles, that specify the base point of the chart. We will draw geodesics time between  0  and  t_end , and then sample the solution at multiples of  dt  and draw a line connecting sampled points. M = Manifolds.EmbeddedTorus(3, 2)\nA = Manifolds.DefaultTorusAtlas() Manifolds.DefaultTorusAtlas()"},{"id":2113,"pagetitle":"work in charts","title":"Setup","ref":"/manifolds/stable/tutorials/working-in-charts/#Setup","content":" Setup We will first set up our plot with an empty torus.  param_points  are points on the surface of the torus that will be used for basic surface shape in  Makie.jl . The torus will be colored according to its Gaussian curvature stored in  gcs . We later want to have a color scale that has negative curvature blue, zero curvature white and positive curvature red so  gcs_mm  is the largest absolute value of the curvature that will be needed to properly set range of curvature values. In the documentation this tutorial represents a static situation (without interactivity).  Makie.jl  rendering is turned off. # using GLMakie, Makie\n# GLMakie.activate!()\n\n\"\"\"\n    torus_figure()\n\nThis function generates a simple plot of a torus and returns the new figure containing the plot.\n\"\"\"\nfunction torus_figure()\n    fig = Figure(resolution=(1400, 1000), fontsize=16)\n    ax = LScene(fig[1, 1], show_axis=true)\n    œ¥s, œÜs = LinRange(-œÄ, œÄ, 50), LinRange(-œÄ, œÄ, 50)\n    param_points = [Manifolds._torus_param(M, Œ∏, œÜ) for Œ∏ in œ¥s, œÜ in œÜs]\n    X1, Y1, Z1 = [[p[i] for p in param_points] for i in 1:3]\n    gcs = [gaussian_curvature(M, p) for p in param_points]\n    gcs_mm = max(abs(minimum(gcs)), abs(maximum(gcs)))\n    pltobj = surface!(\n        ax,\n        X1,\n        Y1,\n        Z1;\n        shading=true,\n        ambient=Vec3f(0.65, 0.65, 0.65),\n        backlight=1.0f0,\n        color=gcs,\n        colormap=Reverse(:RdBu),\n        colorrange=(-gcs_mm, gcs_mm),\n        transparency=true,\n    )\n    wireframe!(ax, X1, Y1, Z1; transparency=true, color=:gray, linewidth=0.5)\n    zoom!(ax.scene, cameracontrols(ax.scene), 0.98)\n    Colorbar(fig[1, 2], pltobj, height=Relative(0.5), label=\"Gaussian curvature\")\n    return ax, fig\nend torus_figure"},{"id":2114,"pagetitle":"work in charts","title":"Values for the geodesic","ref":"/manifolds/stable/tutorials/working-in-charts/#Values-for-the-geodesic","content":" Values for the geodesic solve_for  is a helper function that solves a parallel transport along geodesic problem on the torus  M .  p0x  is the  $(\\theta, \\varphi)$  parametrization of the point from which we will transport the vector. We first calculate the coordinates in the embedding of  p0x  and store it as  p , and then get the initial chart from atlas  A  appropriate for starting working at point  p . The vector we transport has coordinates  Y_transp  in the induced tangent space basis of chart  i_p0x . The function returns the full solution to the parallel transport problem, containing the sequence of charts that was used and solutions of differential equations computed using  OrdinaryDiffEq . bvp_i  is needed later for a different purpose, it is the chart index we will use for solving the logarithmic map boundary value problem in. Next we solve the vector transport problem  solve_for([Œ∏‚Çö, œÜ‚Çö], [Œ∏‚Çì, œÜ‚Çì], [Œ∏y, œÜy]) , sample the result at the selected time steps and store the result in  geo . The solution includes the geodesic which we extract and convert to a sequence of points digestible by  Makie.jl ,  geo_ps .  [Œ∏‚Çö, œÜ‚Çö]  is the parametrization in chart (0, 0) of the starting point of the geodesic. The direction of the geodesic is determined by  [Œ∏‚Çì, œÜ‚Çì] , coordinates of the tangent vector at the starting point expressed in the induced basis of chart  i_p0x  (which depends on the initial point). Finally,  [Œ∏y, œÜy]  are the coordinates of the tangent vector that will be transported along the geodesic, which are also expressed in same basis as  [Œ∏‚Çì, œÜ‚Çì] . We won‚Äôt draw the transported vector at every point as there would be too many arrows, which is why we select every 100th point only for that purpose with  pt_indices . Then,  geo_ps_pt  contains points at which the transported vector is tangent to and  geo_Ys  the transported vector at that point, represented in the embedding. The logarithmic map will be solved between points with parametrization  bvp_a1  and  bvp_a2  in chart  bvp_i . The result is assigned to variable  bvp_sol  and then sampled with time step 0.05. The result of this sampling is converted from parameters in chart  bvp_i  to point in the embedding and stored in  geo_r . function solve_for(p0x, X_p0x, Y_transp, T)\n    p = [Manifolds._torus_param(M, p0x...)...]\n    i_p0x = Manifolds.get_chart_index(M, A, p)\n    p_exp = Manifolds.solve_chart_parallel_transport_ode(\n        M,\n        [0.0, 0.0],\n        X_p0x,\n        A,\n        i_p0x,\n        Y_transp;\n        final_time=T,\n    )\n    return p_exp\nend;"},{"id":2115,"pagetitle":"work in charts","title":"Solving parallel Transport ODE","ref":"/manifolds/stable/tutorials/working-in-charts/#Solving-parallel-Transport-ODE","content":" Solving parallel Transport ODE We set the end time  t_end  and time step  dt . t_end = 2.0\ndt = 1e-1 0.1 We also parametrise the start point and direction. Œ∏‚Çö = œÄ/10\nœÜ‚Çö = -œÄ/4\nŒ∏‚Çì = œÄ/2\nœÜ‚Çì = 0.7\nŒ∏y = 0.2\nœÜy = -0.1\n\ngeo = solve_for([Œ∏‚Çö, œÜ‚Çö], [Œ∏‚Çì, œÜ‚Çì], [Œ∏y, œÜy], t_end)(0.0:dt:t_end);\n# geo_ps = [Point3f(s[1]) for s in geo]\n# pt_indices = 1:div(length(geo), 10):length(geo)\n# geo_ps_pt = [Point3f(s[1]) for s in geo[pt_indices]]\n# geo_Ys = [Point3f(s[3]) for s in geo[pt_indices]]\n\n# ax1, fig1 = torus_figure()\n# arrows!(ax1, geo_ps_pt, geo_Ys, linewidth=0.05, color=:blue)\n# lines!(geo_ps; linewidth=4.0, color=:green)\n# fig1 fig-pt"},{"id":2116,"pagetitle":"work in charts","title":"Solving the logarithmic map ODE","ref":"/manifolds/stable/tutorials/working-in-charts/#Solving-the-logarithmic-map-ODE","content":" Solving the logarithmic map ODE Œ∏‚ÇÅ=œÄ/2\nœÜ‚ÇÅ=-1.0\nŒ∏‚ÇÇ=-œÄ/8\nœÜ‚ÇÇ=œÄ/2\n\nbvp_i = (0, 0)\nbvp_a1 = [Œ∏‚ÇÅ, œÜ‚ÇÅ]\nbvp_a2 = [Œ∏‚ÇÇ, œÜ‚ÇÇ]\nbvp_sol = Manifolds.solve_chart_log_bvp(M, bvp_a1, bvp_a2, A, bvp_i);\n# geo_r = [Point3f(get_point(M, A, bvp_i, p[1:2])) for p in bvp_sol(0.0:0.05:1.0)]\n\n# ax2, fig2 = torus_figure()\n# lines!(geo_r; linewidth=4.0, color=:green)\n# fig2 fig-geodesic An interactive Pluto version of this tutorial is available in file  tutorials/working-in-charts.jl ."},{"id":2119,"pagetitle":"Home","title":"Welcome to Manopt.jl","ref":"/manopt/stable/#Welcome-to-Manopt.jl","content":" Welcome to Manopt.jl"},{"id":2120,"pagetitle":"Home","title":"Manopt.Manopt","ref":"/manopt/stable/#Manopt.Manopt","content":" Manopt.Manopt  ‚Äî  Module üèîÔ∏è Manopt.jl: optimization on Manifolds in Julia. üìö Documentation:  manoptjl.org üì¶ Repository:  github.com/JuliaManifolds/Manopt.jl üí¨ Discussions:  github.com/JuliaManifolds/Manopt.jl/discussions üéØ Issues:  github.com/JuliaManifolds/Manopt.jl/issues source For a function  $f:\\mathcal M ‚Üí ‚Ñù$  defined on a  Riemannian manifold $\\mathcal M$  algorithms in this package aim to solve \\[\\operatorname*{argmin}_{p ‚àà \\mathcal M} f(p),\\] or in other words: find the point  $p$  on the manifold, where  $f$  reaches its minimal function value. Manopt.jl  provides a framework for optimization on manifolds as well as a Library of optimization algorithms in  Julia . It belongs to the ‚ÄúManopt family‚Äù, which includes  Manopt  (Matlab) and  pymanopt.org  (Python). If you want to delve right into  Manopt.jl  read the  üèîÔ∏è Get started with Manopt.jl  tutorial. Manopt.jl  makes it easy to use an algorithm for your favourite manifold as well as a manifold for your favourite algorithm. It already provides many manifolds and algorithms, which can easily be enhanced, for example to  record  certain data or  debug output  throughout iterations. If you use  Manopt.jl in your work, please cite the following @article{Bergmann2022,\n    Author    = {Ronny Bergmann},\n    Doi       = {10.21105/joss.03866},\n    Journal   = {Journal of Open Source Software},\n    Number    = {70},\n    Pages     = {3866},\n    Publisher = {The Open Journal},\n    Title     = {Manopt.jl: Optimization on Manifolds in {J}ulia},\n    Volume    = {7},\n    Year      = {2022},\n} To refer to a certain version or the source code in general cite for example @software{manoptjl-zenodo-mostrecent,\n    Author    = {Ronny Bergmann},\n    Copyright = {MIT License},\n    Doi       = {10.5281/zenodo.4290905},\n    Publisher = {Zenodo},\n    Title     = {Manopt.jl},\n    Year      = {2024},\n} for the most recent version or a corresponding version specific DOI, see  the list of all versions . If you are also using  Manifolds.jl  please consider to cite @article{AxenBaranBergmannRzecki:2023,\n    AUTHOR    = {Axen, Seth D. and Baran, Mateusz and Bergmann, Ronny and Rzecki, Krzysztof},\n    ARTICLENO = {33},\n    DOI       = {10.1145/3618296},\n    JOURNAL   = {ACM Transactions on Mathematical Software},\n    MONTH     = {dec},\n    NUMBER    = {4},\n    TITLE     = {Manifolds.Jl: An Extensible Julia Framework for Data Analysis on Manifolds},\n    VOLUME    = {49},\n    YEAR      = {2023}\n} Note that both citations are in  BibLaTeX  format."},{"id":2121,"pagetitle":"Home","title":"Main features","ref":"/manopt/stable/#Main-features","content":" Main features"},{"id":2122,"pagetitle":"Home","title":"Optimization algorithms (solvers)","ref":"/manopt/stable/#Optimization-algorithms-(solvers)","content":" Optimization algorithms (solvers) For every optimization algorithm, a  solver  is implemented based on a  AbstractManoptProblem  that describes the problem to solve and its  AbstractManoptSolverState  that set up the solver, and stores values that are required between or for the next iteration. Together they form a  plan ."},{"id":2123,"pagetitle":"Home","title":"Manifolds","ref":"/manopt/stable/#Manifolds","content":" Manifolds This project is build upon  ManifoldsBase.jl , a generic interface to implement manifolds. Certain functions are extended for specific manifolds from  Manifolds.jl , but all other manifolds from that package can be used here, too. The notation in the documentation aims to follow the same  notation  from these packages."},{"id":2124,"pagetitle":"Home","title":"Visualization","ref":"/manopt/stable/#Visualization","content":" Visualization To visualize and interpret results,  Manopt.jl  aims to provide both easy plot functions as well as  exports . Furthermore a system to get  debug  during the iterations of an algorithms as well as  record  capabilities, for example to record a specified tuple of values per iteration, most prominently  RecordCost  and  RecordIterate . Take a look at the  üèîÔ∏è Get started with Manopt.jl  tutorial on how to easily activate this."},{"id":2125,"pagetitle":"Home","title":"Literature","ref":"/manopt/stable/#Literature","content":" Literature If you want to get started with manifolds, one book is [ Car92 ], and if you want do directly dive into optimization on manifolds, good references are [ AMS08 ] and [ Bou23 ], which are both available online for free [AMS08] P.-A.¬†Absil, R.¬†Mahony and R.¬†Sepulchre.  Optimization Algorithms on Matrix Manifolds  (Princeton University Press, 2008), available online at  press.princeton.edu/chapters/absil/ . [Bou23] N.¬†Boumal.  An Introduction to Optimization on Smooth Manifolds . First¬†Edition ( Cambridge University Press, 2023 ). [Car92] M.¬†P.¬†do¬†Carmo.  Riemannian Geometry .  Mathematics: Theory & Applications  (Birkh√§user Boston, Inc., Boston, MA, 1992); p.¬†xiv+300."},{"id":2128,"pagetitle":"About","title":"About","ref":"/manopt/stable/about/#About","content":" About Manopt.jl inherited its name from  Manopt , a Matlab toolbox for optimization on manifolds. This Julia package was started and is currently maintained by  Ronny Bergmann ."},{"id":2129,"pagetitle":"About","title":"Contributors","ref":"/manopt/stable/about/#Contributors","content":" Contributors Thanks to the following contributors to  Manopt.jl : Constantin Ahlmann-Eltze  implemented the  gradient and differential  check  functions Ren√©e Dornig  implemented the  particle swarm , the  Riemannian Augmented Lagrangian Method , the  Exact Penalty Method , as well as the  NonmonotoneLinesearch . These solvers are also the first one with modular/exchangable sub solvers. Willem Diepeveen  implemented the  primal-dual Riemannian semismooth Newton  solver. Hajg Jasa  implemented the  convex bundle method  and the  proximal bundle method  and a default subsolver each of them. Even Stephansen Kjems√•s contributed to the implementation of the  Frank Wolfe Method  solver. Mathias Ravn Munkvold contributed most of the implementation of the  Adaptive Regularization with Cubics  solver as well as its  Lanczos  subsolver Sander Engen Oddsen  contributed to the implementation of the  LTMADS  solver. Tom-Christian Riemer  implemented the  trust regions  and  quasi Newton  solvers as well as the  truncated conjugate gradient descent  subsolver. Markus A. Stokkenes  contributed most of the implementation of the  Interior Point Newton Method  as well as its default  Conjugate Residual  subsolver Manuel Weiss  implemented most of the  conjugate gradient update rules as well as various  contributors  providing small extensions, finding small bugs and mistakes and fixing them by opening  PR s. Thanks to all of you. If you want to contribute a manifold or algorithm or have any questions, visit the  GitHub repository  to clone/fork the repository or open an issue."},{"id":2130,"pagetitle":"About","title":"Work using Manopt.jl","ref":"/manopt/stable/about/#Work-using-Manopt.jl","content":" Work using Manopt.jl ExponentialFamilyProjection.jl  package uses  Manopt.jl  to project arbitrary functions onto the closest exponential family distributions. The package also integrates with  RxInfer.jl  to enable Bayesian inference in a larger set of probabilistic models. Caesar.jl  within non-Gaussian factor graph inference algorithms If you are missing a package, that uses  Manopt.jl , please  open an issue . It would be great to collect anything and anyone using Manopt.jl in this list."},{"id":2131,"pagetitle":"About","title":"Further packages","ref":"/manopt/stable/about/#Further-packages","content":" Further packages Manopt.jl  belongs to the Manopt family: manopt.org  The Matlab version of Manopt, see also their :octocat:  GitHub repository pymanopt.org  The Python version of Manopt providing also several AD backends, see also their :octocat:  GitHub repository but there are also more packages providing tools on manifolds in other languages Jax Geometry  (Python/Jax) for differential geometry and stochastic dynamics with deep learning Geomstats  (Python with several backends) focusing on statistics and machine learning :octocat:  GitHub repository Geoopt  (Python & PyTorch) Riemannian ADAM & SGD. :octocat:  GitHub repository McTorch  (Python & PyToch) Riemannian SGD, Adagrad, ASA & CG. ROPTLIB  (C++) a Riemannian OPTimization LIBrary :octocat:  GitHub repository TF Riemopt  (Python & TensorFlow) Riemannian optimization using TensorFlow"},{"id":2134,"pagetitle":"Changelog","title":"Changelog","ref":"/manopt/stable/changelog/#Changelog","content":" Changelog All notable Changes to the Julia package  Manopt.jl  are documented in this file. The file was started with Version  0.4 . The format is based on  Keep a Changelog , and this project adheres to  Semantic Versioning ."},{"id":2135,"pagetitle":"Changelog","title":"0.5.20 (July 8, 2025)","ref":"/manopt/stable/changelog/#[0.5.20](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.20)-(July-8,-2025)","content":" 0.5.20  (July 8, 2025)"},{"id":2136,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added","content":" Added a  DebugWarnIfStepsizeCollapsed  DebugAction and a related  :WarnStepsize  symbol for the debug dictionary. This is to be used in conjunction with the  ProximalGradientMethodBacktracking  stepsize to warn if the backtracking procedure of the  proximal_gradient_method  hit the stepsize length threshold without converging."},{"id":2137,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed","content":" Changed bumped dependencies."},{"id":2138,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed","content":" Fixed Fixed a few typos in the docs."},{"id":2139,"pagetitle":"Changelog","title":"0.5.19 (July 4, 2025)","ref":"/manopt/stable/changelog/#[0.5.19](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.19)-(July-4,-2025)","content":" 0.5.19  (July 4, 2025)"},{"id":2140,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-2","content":" Added a function  get_differential  and  get_differential_function  for first order objectives. a  ParentEvaluationType  to indicate that a certain objective inherits it evaluation from the parent (wrapping) objective a new  AllocatingInplaceEvaluation  that is used for the functions that offer both variants simultaneously. a  differential=  keyword for providing a faster way of computing  inner(M, p, grad_f(p), X) , introduced to the algorithms  conjugate_gradient_descent ,  gradient_descent ,  Frank_Wolfe_method ,  quasi_Newton"},{"id":2141,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-2","content":" Changed the  ManifoldGradientObjective  and the  ManifoldCostGradientObjective  are now merely a const special cases of the  ManifoldFirstOrderObjective , since this type might now also represent a differential or other combinations of cost, grad, and differential, where they are computed together. the  AbstractManifoldGradientObjective  is renamed to  AbstractManifoldFirstOrderObjective , since the second function might now also represent a differential."},{"id":2142,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-2","content":" Fixed fixes a small bug where calling  mesh_adaptive_direct_search  with a start point in some cases did not initialise the state correctly with that start point. The  HestenesStiefelCoefficient  now also always returns a real value, similar the other coefficient rules. To the best of our knowledge, this might have been a bug previously."},{"id":2143,"pagetitle":"Changelog","title":"0.5.18 (June 18, 2025)","ref":"/manopt/stable/changelog/#[0.5.18](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.18)-(June-18,-2025)","content":" 0.5.18  (June 18, 2025)"},{"id":2144,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-3","content":" Added Introduce the algorithm  proximal_gradient_method  along with  ManifoldProximalGradientObjective ,  ProximalGradientMethodState , as well as an experimental  ProximalGradientMethodAcceleration . Add  ProximalGradientMethodBacktracking  stepsize. Add  StopWhenGradientMappingNormLess  stopping criterion. Introduce a  StopWhenRepeated  stopping criterion that stops when the given stopping criterion has indicated to stop  n  times (consecutively, if  consecutive=true ). Introduce a  StopWhenCriterionWithIterationCondition  stopping criterion that stops when a given stopping criterion has been satisfied together with a certain iteration condition. This can the generated even with shortcuts like  sc > 5 Introduce a  DebugCallback  that allows to add a callback function to the debug system Introduce a  callback=  keyword to all solvers. Added back functions  estimate_sectional_curvature ,  Œ∂_1 ,  Œ∂_2 ,  close_point  from  convex_bundle_method ; the function call can stay the same as before since there is a curvature estimation fallback Add back some fields and arguments such as  p_estimate ,  œ± ,  Œ± , from  ConvexBundleMethodState"},{"id":2145,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-3","content":" Changed make the  GradientDescentState  a bit more tolerant to ignore keywords it does not use."},{"id":2146,"pagetitle":"Changelog","title":"0.5.17 (June 3, 2025)","ref":"/manopt/stable/changelog/#[0.5.17](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.17)-(June-3,-2025)","content":" 0.5.17  (June 3, 2025)"},{"id":2147,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-4","content":" Added Introduce a  StopWhenCostChangeLess  stopping criterion that stops when the cost function changes less than a given value."},{"id":2148,"pagetitle":"Changelog","title":"0.5.16 (May 7, 2025)","ref":"/manopt/stable/changelog/#[0.5.16](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.16)-(May-7,-2025)","content":" 0.5.16  (May 7, 2025)"},{"id":2149,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-3","content":" Fixed fixes a bug in the  LineSearches.jl  extension, where two (old)  retract! s were still present; they were changed to  retact_fused! ."},{"id":2150,"pagetitle":"Changelog","title":"0.5.15 (May 6, 2025)","ref":"/manopt/stable/changelog/#[0.5.15](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.15)-(May-6,-2025)","content":" 0.5.15  (May 6, 2025)"},{"id":2151,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-4","content":" Fixed CMA-ES no longer errors when the covariance matrix has nonpositive eigenvalues due to numerical issues."},{"id":2152,"pagetitle":"Changelog","title":"0.5.14 (May 5, 2025)","ref":"/manopt/stable/changelog/#[0.5.14](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.14)-(May-5,-2025)","content":" 0.5.14  (May 5, 2025)"},{"id":2153,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-5","content":" Added linear_subsolver!  is added as a keyword argument to the Levenberg-Marquardt interface."},{"id":2154,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-4","content":" Changed adapt to using  default_basis  where appropriate. the tutorials are now rendered with  quarto  using the  QuartoNotebookRunner.jl  and are hence purely julia based."},{"id":2155,"pagetitle":"Changelog","title":"0.5.13 (April 25, 2025)","ref":"/manopt/stable/changelog/#[0.5.13](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.13)-(April-25,-2025)","content":" 0.5.13  (April 25, 2025)"},{"id":2156,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-6","content":" Added Allow setting  AbstractManifoldObjective  through JuMP"},{"id":2157,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-5","content":" Changed Remove dependency on  ManoptExamples.jl  which yielded a circular dependency, though only through extras Unify dummy types and several test functions into the  ManoptTestSuite  subpackage."},{"id":2158,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-5","content":" Fixed A scaling error that appeared only when calling  get_cost_function  on the new  ScaledManifoldObjective . Documentation issues for quasi-Newton solvers. fixes a scaling error in quasi newton Fixes printing of JuMP models containg Manopt solver."},{"id":2159,"pagetitle":"Changelog","title":"0.5.12 (April 13, 2025)","ref":"/manopt/stable/changelog/#[0.5.12](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.12)-(April-13,-2025)","content":" 0.5.12  (April 13, 2025)"},{"id":2160,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-7","content":" Added a  ScaledManifoldObjective  to easier build scaled versions of objectives, especially turn maximisation problems into minimisation ones using a scaling of  -1 . Introduce a  ManifoldConstrainedSetObjective Introduce a  projected_gradient_method"},{"id":2161,"pagetitle":"Changelog","title":"0.5.11 (April 8, 2025)","ref":"/manopt/stable/changelog/#[0.5.11](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.11)-(April-8,-2025)","content":" 0.5.11  (April 8, 2025)"},{"id":2162,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-8","content":" Added Configurable subsolver for the linear subproblem in Levenberg-Marquardt. The default subsolver is now also robust to numerical issues that may cause Cholesky decomposition to fail."},{"id":2163,"pagetitle":"Changelog","title":"0.5.10 (April 4, 2025)","ref":"/manopt/stable/changelog/#[0.5.10](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.10)-(April-4,-2025)","content":" 0.5.10  (April 4, 2025)"},{"id":2164,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-6","content":" Fixed a proper implementation of the preconditioning for  quasi_Newton , that can be used instead of or in combination with the initial scaling."},{"id":2165,"pagetitle":"Changelog","title":"0.5.9 (March 24, 2025)","ref":"/manopt/stable/changelog/#[0.5.9](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.9)-(March-24,-2025)","content":" 0.5.9  (March 24, 2025)"},{"id":2166,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-9","content":" Added add a  PreconditionedDirection  variant to the  direction  gradient processor keyword argument and its corresponding  PreconditionedDirectionRule make the preconditioner available in quasi Newton. in  gradient_descent  and  conjugate_gradient_descent  the rule can be added anyways."},{"id":2167,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-7","content":" Fixed the links in the AD tutorial are fixed and moved to using  extref"},{"id":2168,"pagetitle":"Changelog","title":"0.5.8 (February 28, 2025)","ref":"/manopt/stable/changelog/#[0.5.8](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.8)-(February-28,-2025)","content":" 0.5.8  (February 28, 2025)"},{"id":2169,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-8","content":" Fixed fixed a small bug in the  NonmonotoneLinesearchStepsize  hwn the injectivity radius is an irrational number. fixed a small bug in  check_gradient  where  eps  might have been called on complex types. fixed a bug in several gradient based solvers like  quasi_newton , such that they properly work with the combined cost grad objective. fixes a few typos in the docs."},{"id":2170,"pagetitle":"Changelog","title":"0.5.7 (February 20, 20265)","ref":"/manopt/stable/changelog/#[0.5.7](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.7)-(February-20,-20265)","content":" 0.5.7  (February 20, 20265)"},{"id":2171,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-10","content":" Added Adds a mesh adaptive direct search algorithm (MADS), using the LTMADS variant with a lower triangular (LT) random matrix in the mesh generation."},{"id":2172,"pagetitle":"Changelog","title":"0.5.6 (February 10, 2025)","ref":"/manopt/stable/changelog/#[0.5.6](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.6)-(February-10,-2025)","content":" 0.5.6  (February 10, 2025)"},{"id":2173,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-6","content":" Changed bump dependencies of all JuliaManifolds ecosystem packages to be consistent with ManifoldsBase 1.0"},{"id":2174,"pagetitle":"Changelog","title":"0.5.5 (January 4, 2025)","ref":"/manopt/stable/changelog/#[0.5.5](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.5)-(January-4,-2025)","content":" 0.5.5  (January 4, 2025)"},{"id":2175,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-11","content":" Added the Levenberg-Marquardt algorithm internally uses a  VectorGradientFunction , which allows to use a vector of gradients of a function returning all gradients as well for the algorithm The  VectorGradientFunction  now also have a  get_jacobian  function"},{"id":2176,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-7","content":" Changed Minimum Julia version is now 1.10 (the LTS which replaced 1.6) The vectorial functions had a bug where the original vector function for the mutating case was not always treated as mutating."},{"id":2177,"pagetitle":"Changelog","title":"Removed","ref":"/manopt/stable/changelog/#Removed","content":" Removed The geodesic regression example, first because it is not correct, second because it should become part of ManoptExamples.jl once it is correct."},{"id":2178,"pagetitle":"Changelog","title":"0.5.4 (December 11, 2024)","ref":"/manopt/stable/changelog/#[0.5.4](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.4)-(December-11,-2024)","content":" 0.5.4  (December 11, 2024)"},{"id":2179,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-12","content":" Added An automated detection whether the tutorials are present  if not an also no quarto run is done, an automated  --exclude-tutorials  option is added. Support for ManifoldDiff 0.4 icons upfront external links when they link to another package or Wikipedia."},{"id":2180,"pagetitle":"Changelog","title":"0.5.3 (October 18, 2024)","ref":"/manopt/stable/changelog/#[0.5.3](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.3)-(October-18,-2024)","content":" 0.5.3  (October 18, 2024)"},{"id":2181,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-13","content":" Added StopWhenChangeLess ,  StopWhenGradientChangeLess  and  StopWhenGradientLess  can now use the new idea (ManifoldsBase.jl 0.15.18) of different outer norms on manifolds with components like power and product manifolds and all others that support this from the  Manifolds.jl  Library, like  Euclidean"},{"id":2182,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-8","content":" Changed stabilize  max_stepsize  to also work when  injectivity_radius  dos not exist. It however would warn new users, that activate tutorial mode. Start a  ManoptTestSuite  sub package to store dummy types and common test helpers in."},{"id":2183,"pagetitle":"Changelog","title":"0.5.2 (October 5, 2024)","ref":"/manopt/stable/changelog/#[0.5.2](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.2)-(October-5,-2024)","content":" 0.5.2  (October 5, 2024)"},{"id":2184,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-14","content":" Added three new symbols to easier state to record the  :Gradient , the  :GradientNorm , and the  :Stepsize ."},{"id":2185,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-9","content":" Changed fix a few typos in the documentation improved the documentation for the initial guess of  ArmijoLinesearchStepsize ."},{"id":2186,"pagetitle":"Changelog","title":"0.5.1 (September 4, 2024)","ref":"/manopt/stable/changelog/#[0.5.1](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.1)-(September-4,-2024)","content":" 0.5.1  (September 4, 2024)"},{"id":2187,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-10","content":" Changed slightly improves the test for the  ExponentialFamilyProjection  text on the about page."},{"id":2188,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-15","content":" Added the  proximal_point  method."},{"id":2189,"pagetitle":"Changelog","title":"0.5.0 (August 29, 2024)","ref":"/manopt/stable/changelog/#[0.5.0](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.5.0)-(August-29,-2024)","content":" 0.5.0  (August 29, 2024) This breaking update is mainly concerned with improving a unified experience through all solvers and some usability improvements, such that for example the different gradient update rules are easier to specify. In general this introduces a few factories, that avoid having to pass the manifold to keyword arguments"},{"id":2190,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-16","content":" Added A  ManifoldDefaultsFactory  that postpones the creation/allocation of manifold-specific fields in for example direction updates, step sizes and stopping criteria. As a rule of thumb, internal structures, like a solver state should store the final type. Any high-level interface, like the functions to start solvers, should accept such a factory in the appropriate places and call the internal  _produce_type(factory, M) , for example before passing something to the state. a  documentation_glossary.jl  file containing a glossary of often used variables in fields, arguments, and keywords, to print them in a unified manner. The same for usual sections, text, and math notation that is often used within the doc-strings."},{"id":2191,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-11","content":" Changed Any  Stepsize  now has a  Stepsize  struct used internally as the original  struct s before. The newly exported terms aim to fit  stepsize=...  in naming and create a  ManifoldDefaultsFactory  instead, so that any stepsize can be created without explicitly specifying the manifold. ConstantStepsize  is no longer exported, use  ConstantLength  instead. The length parameter is now a positional argument following the (optional) manifold. Besides that  ConstantLength  works as before,just that omitting the manifold fills the one specified in the solver now. DecreasingStepsize  is no longer exported, use  DecreasingLength  instead.  ConstantLength  works as before,just that omitting the manifold fills the one specified in the solver now. ArmijoLinesearch  is now called  ArmijoLinesearchStepsize .  ArmijoLinesearch  works as before,just that omitting the manifold fills the one specified in the solver now. WolfePowellLinesearch  is now called  WolfePowellLinesearchStepsize , its constant  c_1  is now unified with Armijo and called  sufficient_decrease ,  c_2  was renamed to  sufficient_curvature . Besides that,  WolfePowellLinesearch  works as before, just that omitting the manifold fills the one specified in the solver now. WolfePowellBinaryLinesearch  is now called  WolfePowellBinaryLinesearchStepsize , its constant  c_1  is now unified with Armijo and called  sufficient_decrease ,  c_2  was renamed to  sufficient_curvature . Besides that,  WolfePowellBinaryLinesearch  works as before, just that omitting the manifold fills the one specified in the solver now. NonmonotoneLinesearch  is now called  NonmonotoneLinesearchStepsize .  NonmonotoneLinesearch  works as before, just that omitting the manifold fills the one specified in the solver now. AdaptiveWNGradient  is now called  AdaptiveWNGradientStepsize . Its second positional argument, the gradient function was only evaluated once for the  gradient_bound  default, so it has been replaced by the keyword  X=  accepting a tangent vector. The last positional argument  p  has also been moved to a keyword argument. Besides that,  AdaptiveWNGradient  works as before, just that omitting the manifold fills the one specified in the solver now. Any  DirectionUpdateRule  now has the  Rule  in its name, since the original name is used to create the  ManifoldDefaultsFactory  instead. The original constructor now no longer requires the manifold as a parameter, that is later done in the factory. The  Rule  is, however, also no longer exported. AverageGradient  is now called  AverageGradientRule .  AverageGradient  works as before, but the manifold as its first parameter is no longer necessary and  p  is now a keyword argument. The  IdentityUpdateRule  now accepts a manifold optionally for consistency, and you can use  Gradient()  for short as well as its factory. Hence  direction=Gradient()  is now available. MomentumGradient  is now called  MomentumGradientRule .  MomentumGradient  works as before, but the manifold as its first parameter is no longer necessary and  p  is now a keyword argument. Nesterov  is now called  NesterovRule .  Nesterov  works as before, but the manifold as its first parameter is no longer necessary and  p  is now a keyword argument. ConjugateDescentCoefficient  is now called  ConjugateDescentCoefficientRule .  ConjugateDescentCoefficient  works as before, but can now use the factory in between the  ConjugateGradientBealeRestart  is now called  ConjugateGradientBealeRestartRule . For the  ConjugateGradientBealeRestart  the manifold is now a first parameter, that is not necessary and no longer the  manifold=  keyword. DaiYuanCoefficient  is now called  DaiYuanCoefficientRule . For the  DaiYuanCoefficient  the manifold as its first parameter is no longer necessary and the vector transport has been unified/moved to the  vector_transport_method=  keyword. FletcherReevesCoefficient  is now called  FletcherReevesCoefficientRule .  FletcherReevesCoefficient  works as before, but can now use the factory in between HagerZhangCoefficient  is now called  HagerZhangCoefficientRule . For the  HagerZhangCoefficient  the manifold as its first parameter is no longer necessary and the vector transport has been unified/moved to the  vector_transport_method=  keyword. HestenesStiefelCoefficient  is now called  HestenesStiefelCoefficientRule . For the  HestenesStiefelCoefficient  the manifold as its first parameter is no longer necessary and the vector transport has been unified/moved to the  vector_transport_method=  keyword. LiuStoreyCoefficient  is now called  LiuStoreyCoefficientRule . For the  LiuStoreyCoefficient  the manifold as its first parameter is no longer necessary and the vector transport has been unified/moved to the  vector_transport_method=  keyword. PolakRibiereCoefficient  is now called  PolakRibiereCoefficientRule . For the  PolakRibiereCoefficient  the manifold as its first parameter is no longer necessary and the vector transport has been unified/moved to the  vector_transport_method=  keyword. the  SteepestDirectionUpdateRule  is now called  SteepestDescentCoefficientRule . The  SteepestDescentCoefficient  is equivalent, but creates the new factory temporarily. AbstractGradientGroupProcessor  is now called  AbstractGradientGroupDirectionRule the  StochasticGradient  is now called  StochasticGradientRule . The  StochasticGradient  is equivalent, but creates the new factory temporarily, so that the manifold is not longer necessary. the  AlternatingGradient  is now called  AlternatingGradientRule . The  AlternatingGradient  is equivalent, but creates the new factory temporarily, so that the manifold is not longer necessary. quasi_Newton  had a keyword  scale_initial_operator=  that was inconsistently declared (sometimes boolean, sometimes real) and was unused. It is now called  initial_scale=1.0  and scales the initial (diagonal, unit) matrix within the approximation of the Hessian additionally to the  $\\frac{1}{\\lVert g_k\\rVert}$  scaling with the norm of the oldest gradient for the limited memory variant. For the full matrix variant the initial identity matrix is now scaled with this parameter. Unify doc strings and presentation of keyword arguments general indexing, for example in a vector, uses  i index for inequality constraints is unified to  i  running from  1,...,m index for equality constraints is unified to  j  running from  1,...,n iterations are using now  k get_manopt_parameter  has been renamed to  get_parameter  since it is internal, so internally that is clear; accessing it from outside hence reads anyways  Manopt.get_parameter set_manopt_parameter!  has been renamed to  set_parameter!  since it is internal, so internally that is clear; accessing it from outside hence reads  Manopt.set_parameter! changed the  stabilize::Bool=  keyword in  quasi_Newton  to the more flexible  project!=  keyword, this is also more in line with the other solvers. Internally the same is done within the  QuasiNewtonLimitedMemoryDirectionUpdate . To adapt, the previous  stabilize=true  is now set with  (project!)=embed_project!  in general, and if the manifold is represented by points in the embedding, like the sphere,  (project!)=project!  suffices the new default is  (project!)=copyto! , so by default no projection/stabilization is performed. the positional argument  p  (usually the last or the third to last if sub solvers existed) has been moved to a keyword argument  p=  in all State constructors in  NelderMeadState  the  population  moved from positional to keyword argument as well, the way to initialise sub solvers in the solver states has been unified In the new variant the  sub_problem  is always a positional argument; namely the last one if the  sub_state  is given as a optional positional argument after the problem, it has to be a manopt solver state you can provide the new  ClosedFormSolverState(e::AbstractEvaluationType)  for the state to indicate that the  sub_problem  is a closed form solution (function call) and how it has to be called if you do not provide the  sub_state  as positional, the keyword  evaluation=  is used to generate the state  ClosedFormSolverState . when previously  p  and eventually  X  where positional arguments, they are now moved to keyword arguments of the same name for start point and tangent vector. in detail AdaptiveRegularizationState(M, sub_problem [, sub_state]; kwargs...)  replaces the (unused) variant to only provide the objective; both  X  and  p  moved to keyword arguments. AugmentedLagrangianMethodState(M, objective, sub_problem; evaluation=...)  was added AugmentedLagrangianMethodState(M, objective, sub_problem, sub_state; evaluation=...)  now has  p=rand(M)  as keyword argument instead of being the second positional one ExactPenaltyMethodState(M, sub_problem; evaluation=...)  was added and  ExactPenaltyMethodState(M, sub_problem, sub_state; evaluation=...)  now has  p=rand(M)  as keyword argument instead of being the second positional one DifferenceOfConvexState(M, sub_problem; evaluation=...)  was added and  DifferenceOfConvexState(M, sub_problem, sub_state; evaluation=...)  now has  p=rand(M)  as keyword argument instead of being the second positional one DifferenceOfConvexProximalState(M, sub_problem; evaluation=...)  was added and  DifferenceOfConvexProximalState(M, sub_problem, sub_state; evaluation=...)  now has  p=rand(M)  as keyword argument instead of being the second positional one bumped  Manifolds.jl to version 0.10; this mainly means that any algorithm working on a product manifold and requiring  ArrayPartition  now has to explicitly do  using RecursiveArrayTools ."},{"id":2192,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-9","content":" Fixed the  AverageGradientRule  filled its internal vector of gradients wrongly or mixed it up in parallel transport. This is now fixed."},{"id":2193,"pagetitle":"Changelog","title":"Removed","ref":"/manopt/stable/changelog/#Removed-2","content":" Removed the  convex_bundle_method  and its  ConvexBundleMethodState  no longer accept the keywords  k_size ,  p_estimate  nor  œ± , they are superseded by just providing  k_max . the  truncated_conjugate_gradient_descent(M, f, grad_f, hess_f)  has the Hessian now  a mandatory argument. To use the old variant,  provide  ApproxHessianFiniteDifference(M, copy(M, p), grad_f)  to  hess_f  directly. all deprecated keyword arguments and a few function signatures were removed: get_equality_constraints ,  get_equality_constraints! ,  get_inequality_constraints ,  get_inequality_constraints!  are removed. Use their singular forms and set the index to  :  instead. StopWhenChangeLess(Œµ)  is removed, use ` StopWhenChangeLess(M, Œµ)  instead to fill for example the retraction properly used to determine the change In the  WolfePowellLinesearch  and   WolfeBinaryLinesearch the  linesearch_stopsize=  keyword is replaced by  stop_when_stepsize_less= DebugChange  and  RecordChange  had a  manifold=  and a  invretr  keyword that were replaced by the first positional argument  M  and  inverse_retraction_method= , respectively in the  NonlinearLeastSquaresObjective  and  LevenbergMarquardt  the  jacB=  keyword is now called  jacobian_tangent_basis= in  particle_swarm  the  n=  keyword is replaced by  swarm_size= . update_stopping_criterion!  has been removed and unified with  set_parameter! . The code adaptions are to set a parameter of a stopping criterion, just replace  update_stopping_criterion!(sc, :Val, v)  with  set_parameter!(sc, :Val, v) to update a stopping criterion in a solver state, replace the old  update_stopping_criterion!(state, :Val, v)  tat passed down to the stopping criterion by the explicit pass down with  set_parameter!(state, :StoppingCriterion, :Val, v)"},{"id":2194,"pagetitle":"Changelog","title":"0.4.69 (August 3, 2024)","ref":"/manopt/stable/changelog/#[0.4.69](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.69)-(August-3,-2024)","content":" 0.4.69  (August 3, 2024)"},{"id":2195,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-12","content":" Changed Improved performance of Interior Point Newton Method."},{"id":2196,"pagetitle":"Changelog","title":"0.4.68 (August 2, 2024)","ref":"/manopt/stable/changelog/#[0.4.68](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.68)-(August-2,-2024)","content":" 0.4.68  (August 2, 2024)"},{"id":2197,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-17","content":" Added an Interior Point Newton Method, the  interior_point_newton a  conjugate_residual  Algorithm to solve a linear system on a tangent space. ArmijoLinesearch  now allows for additional  additional_decrease_condition  and  additional_increase_condition  keywords to add further conditions to accept additional conditions when to accept an decreasing or increase of the stepsize. add a  DebugFeasibility  to have a debug print about feasibility of points in constrained optimisation employing the new  is_feasible  function add a  InteriorPointCentralityCondition  that can be added for step candidates within the line search of  interior_point_newton Add Several new functors the  LagrangianCost ,  LagrangianGradient ,  LagrangianHessian , that based on a constrained objective allow to construct the Hessian objective of its Lagrangian the  CondensedKKTVectorField  and its  CondensedKKTVectorFieldJacobian , that are being used to solve a linear system within  interior_point_newton the  KKTVectorField  as well as its  KKTVectorFieldJacobian  and ` KKTVectorFieldAdjointJacobian the  KKTVectorFieldNormSq  and its  KKTVectorFieldNormSqGradient  used within the Armijo line search of  interior_point_newton New stopping criteria A  StopWhenRelativeResidualLess  for the  conjugate_residual A  StopWhenKKTResidualLess  for the  interior_point_newton"},{"id":2198,"pagetitle":"Changelog","title":"0.4.67 (July 25, 2024)","ref":"/manopt/stable/changelog/#[0.4.67](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.67)-(July-25,-2024)","content":" 0.4.67  (July 25, 2024)"},{"id":2199,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-18","content":" Added max_stepsize  methods for  Hyperrectangle ."},{"id":2200,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-10","content":" Fixed a few typos in the documentation WolfePowellLinesearch  no longer uses  max_stepsize  with invalid point by default."},{"id":2201,"pagetitle":"Changelog","title":"0.4.66 (June 27, 2024)","ref":"/manopt/stable/changelog/#[0.4.66](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.66)-(June-27,-2024)","content":" 0.4.66  (June 27, 2024)"},{"id":2202,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-13","content":" Changed Remove functions  estimate_sectional_curvature ,  Œ∂_1 ,  Œ∂_2 ,  close_point  from  convex_bundle_method Remove some unused fields and arguments such as  p_estimate ,  œ± ,  Œ± , from  ConvexBundleMethodState  in favor of jut  k_max Change parameter  R  placement in  ProximalBundleMethodState  to fifth position"},{"id":2203,"pagetitle":"Changelog","title":"0.4.65 (June 13, 2024)","ref":"/manopt/stable/changelog/#[0.4.65](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.65)-(June-13,-2024)","content":" 0.4.65  (June 13, 2024)"},{"id":2204,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-14","content":" Changed refactor stopping criteria to not store a  sc.reason  internally, but instead only generate the reason (and hence allocate a string) when actually asked for a reason."},{"id":2205,"pagetitle":"Changelog","title":"0.4.64 (June 4, 2024)","ref":"/manopt/stable/changelog/#[0.4.64](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.64)-(June-4,-2024)","content":" 0.4.64  (June 4, 2024)"},{"id":2206,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-19","content":" Added Remodel the constraints and their gradients into separate  VectorGradientFunctions  to reduce code duplication and encapsulate the inner model of these functions and their gradients Introduce a  ConstrainedManoptProblem  to model different ranges for the gradients in the new  VectorGradientFunction s beyond the default  NestedPowerRepresentation introduce a  VectorHessianFunction  to also model that one can provide the vector of Hessians to constraints introduce a more flexible indexing beyond single indexing, to also include arbitrary ranges when accessing vector functions and their gradients and hence also for constraints and their gradients."},{"id":2207,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-15","content":" Changed Remodel  ConstrainedManifoldObjective  to store an  AbstractManifoldObjective  internally instead of directly  f  and  grad_f , allowing also Hessian objectives therein and implementing access to this Hessian Fixed a bug that Lanczos produced NaNs when started exactly in a minimizer, since the algorithm initially divides by the gradient norm."},{"id":2208,"pagetitle":"Changelog","title":"Deprecated","ref":"/manopt/stable/changelog/#Deprecated","content":" Deprecated deprecate  get_grad_equality_constraints(M, o, p) , use  get_grad_equality_constraint(M, o, p, :)  from the more flexible indexing instead."},{"id":2209,"pagetitle":"Changelog","title":"0.4.63 (May 11, 2024)","ref":"/manopt/stable/changelog/#[0.4.63](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.63)-(May-11,-2024)","content":" 0.4.63  (May 11, 2024)"},{"id":2210,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-20","content":" Added :reinitialize_direction_update  option for quasi-Newton behavior when the direction is not a descent one. It is now the new default for  QuasiNewtonState . Quasi-Newton direction update rules are now initialized upon start of the solver with the new internal function  initialize_update! ."},{"id":2211,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-11","content":" Fixed ALM and EPM no longer keep a part of the quasi-Newton subsolver state between runs."},{"id":2212,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-16","content":" Changed Quasi-Newton solvers:  :reinitialize_direction_update  is the new default behavior in case of detection of non-descent direction instead of  :step_towards_negative_gradient .  :step_towards_negative_gradient  is still available when explicitly set using the  nondescent_direction_behavior  keyword argument."},{"id":2213,"pagetitle":"Changelog","title":"0.4.62 (May 3, 2024)","ref":"/manopt/stable/changelog/#[0.4.62](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.62)-(May-3,-2024)","content":" 0.4.62  (May 3, 2024)"},{"id":2214,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-17","content":" Changed bumped dependency of ManifoldsBase.jl to 0.15.9 and imported their numerical verify functions. This changes the  throw_error  keyword used internally to a  error=  with a symbol."},{"id":2215,"pagetitle":"Changelog","title":"0.4.61 (April 27, 2024)","ref":"/manopt/stable/changelog/#[0.4.61](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.61)-(April-27,-2024)","content":" 0.4.61  (April 27, 2024)"},{"id":2216,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-21","content":" Added Tests use  Aqua.jl  to spot problems in the code introduce a feature-based list of solvers and reduce the details in the alphabetical list adds a  PolyakStepsize added a  get_subgradient  for  AbstractManifoldGradientObjectives  since their gradient is a special case of a subgradient."},{"id":2217,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-12","content":" Fixed get_last_stepsize  was defined in quite different ways that caused ambiguities. That is now internally a bit restructured and should work nicer. Internally this means that the interim dispatch on  get_last_stepsize(problem, state, step, vars...)  was removed. Now the only two left are  get_last_stepsize(p, s, vars...)  and the one directly checking  get_last_stepsize(::Stepsize)  for stored values. the accidentally exported  set_manopt_parameter!  is no longer exported"},{"id":2218,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-18","content":" Changed get_manopt_parameter  and  set_manopt_parameter!  have been revised and better documented, they now use more semantic symbols (with capital letters) instead of direct field access (lower letter symbols). Since these are not exported, this is considered an internal, hence non-breaking change. semantic symbols are now all nouns in upper case letters :active  is changed to  :Activity"},{"id":2219,"pagetitle":"Changelog","title":"0.4.60 (April 10, 2024)","ref":"/manopt/stable/changelog/#[0.4.60](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.60)-(April-10,-2024)","content":" 0.4.60  (April 10, 2024)"},{"id":2220,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-22","content":" Added RecordWhenActive  to allow records to be deactivated during runtime, symbol  :WhenActive RecordSubsolver  to record the result of a subsolver recording in the main solver, symbol  :Subsolver RecordStoppingReason  to record the reason a solver stopped made the  RecordFactory  more flexible and quite similar to  DebugFactory , such that it is now also easy to specify recordings at the end of solver runs. This can especially be used to record final states of sub solvers."},{"id":2221,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-19","content":" Changed being a bit more strict with internal tools and made the factories for record non-exported, so this is the same as for debug."},{"id":2222,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-13","content":" Fixed The name  :Subsolver  to generate  DebugWhenActive  was misleading, it is now called  :WhenActive  referring to ‚Äúprint debug only when set active, that is by the parent (main) solver‚Äù. the old version of specifying  Symbol => RecordAction  for later access was ambiguous, since it could also mean to store the action in the dictionary under that symbol. Hence the order for access was switched to  RecordAction => Symbol  to resolve that ambiguity."},{"id":2223,"pagetitle":"Changelog","title":"0.4.59 (April 7, 2024)","ref":"/manopt/stable/changelog/#[0.4.59](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.59)-(April-7,-2024)","content":" 0.4.59  (April 7, 2024)"},{"id":2224,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-23","content":" Added A Riemannian variant of the CMA-ES (Covariance Matrix Adaptation Evolutionary Strategy) algorithm,  cma_es ."},{"id":2225,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-14","content":" Fixed The constructor dispatch for  StopWhenAny  with  Vector  had incorrect element type assertion which was fixed."},{"id":2226,"pagetitle":"Changelog","title":"0.4.58 (March 18, 2024)","ref":"/manopt/stable/changelog/#[0.4.58](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.58)-(March-18,-2024)","content":" 0.4.58  (March 18, 2024)"},{"id":2227,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-24","content":" Added more advanced methods to add debug to the beginning of an algorithm, a step, or the end of the algorithm with  DebugAction  entries at  :Start ,  :BeforeIteration ,  :Iteration , and  :Stop , respectively. Introduce a Pair-based format to add elements to these hooks, while all others ar now added to :Iteration (no longer to  :All ) (planned) add an easy possibility to also record the initial stage and not only after the first iteration."},{"id":2228,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-20","content":" Changed Changed the symbol for the  :Step  dictionary to be  :Iteration , to unify this with the symbols used in recording, and removed the  :All  symbol. On the fine granular scale, all but  :Start  debugs are now reset on init. Since these are merely internal entries in the debug dictionary, this is considered non-breaking. introduce a  StopWhenSwarmVelocityLess  stopping criterion for  particle_swarm  replacing the current default of the swarm change, since this is a bit more effective to compute"},{"id":2229,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-15","content":" Fixed fixed the outdated documentation of  TruncatedConjugateGradientState , that now correctly state that  p  is no longer stored, but the algorithm runs on  TpM . implemented the missing  get_iterate  for  TruncatedConjugateGradientState ."},{"id":2230,"pagetitle":"Changelog","title":"0.4.57 (March 15, 2024)","ref":"/manopt/stable/changelog/#[0.4.57](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.57)-(March-15,-2024)","content":" 0.4.57  (March 15, 2024)"},{"id":2231,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-21","content":" Changed convex_bundle_method  uses the  sectional_curvature  from  ManifoldsBase.jl . convex_bundle_method  no longer has the unused  k_min  keyword argument. ManifoldsBase.jl  now is running on Documenter 1.3,  Manopt.jl  documentation now uses  DocumenterInterLinks  to refer to sections and functions from  ManifoldsBase.jl"},{"id":2232,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-16","content":" Fixed fixes a type that when passing  sub_kwargs  to  trust_regions  caused an error in the decoration of the sub objective."},{"id":2233,"pagetitle":"Changelog","title":"0.4.56 (March 4, 2024)","ref":"/manopt/stable/changelog/#[0.4.56](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.56)-(March-4,-2024)","content":" 0.4.56  (March 4, 2024)"},{"id":2234,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-25","content":" Added The option  :step_towards_negative_gradient  for  nondescent_direction_behavior  in quasi-Newton solvers does no longer emit a warning by default. This has been moved to a  message , that can be accessed/displayed with  DebugMessages DebugMessages  now has a second positional argument, specifying whether all messages, or just the first ( :Once ) should be displayed."},{"id":2235,"pagetitle":"Changelog","title":"0.4.55 (March 3, 2024)","ref":"/manopt/stable/changelog/#[0.4.55](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.55)-(March-3,-2024)","content":" 0.4.55  (March 3, 2024)"},{"id":2236,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-26","content":" Added Option  nondescent_direction_behavior  for quasi-Newton solvers. By default it checks for non-descent direction which may not be handled well by some stepsize selection algorithms."},{"id":2237,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-17","content":" Fixed unified documentation, especially function signatures further. fixed a few typos related to math formulae in the doc strings."},{"id":2238,"pagetitle":"Changelog","title":"0.4.54 (February 28, 2024)","ref":"/manopt/stable/changelog/#[0.4.54](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.54)-(February-28,-2024)","content":" 0.4.54  (February 28, 2024)"},{"id":2239,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-27","content":" Added convex_bundle_method  optimization algorithm for non-smooth geodesically convex functions proximal_bundle_method  optimization algorithm for non-smooth functions. StopWhenSubgradientNormLess ,  StopWhenLagrangeMultiplierLess , and stopping criteria."},{"id":2240,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-18","content":" Fixed Doc strings now follow a  vale.sh  policy. Though this is not fully working, this PR improves a lot of the doc strings concerning wording and spelling."},{"id":2241,"pagetitle":"Changelog","title":"0.4.53 (February 13, 2024)","ref":"/manopt/stable/changelog/#[0.4.53](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.53)-(February-13,-2024)","content":" 0.4.53  (February 13, 2024)"},{"id":2242,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-19","content":" Fixed fixes two storage action defaults, that accidentally still tried to initialize a  :Population  (as modified back to  :Iterate  0.4.49). fix a few typos in the documentation and add a reference for the subgradient method."},{"id":2243,"pagetitle":"Changelog","title":"0.4.52 (February 5, 2024)","ref":"/manopt/stable/changelog/#[0.4.52](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.52)-(February-5,-2024)","content":" 0.4.52  (February 5, 2024)"},{"id":2244,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-28","content":" Added introduce an environment persistent way of setting global values with the  set_manopt_parameter!  function using  Preferences.jl . introduce such a value named  :Mode  to enable a  \"Tutorial\"  mode that shall often provide more warnings and information for people getting started with optimisation on manifolds"},{"id":2245,"pagetitle":"Changelog","title":"0.4.51 (January 30, 2024)","ref":"/manopt/stable/changelog/#[0.4.51](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.51)-(January-30,-2024)","content":" 0.4.51  (January 30, 2024)"},{"id":2246,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-29","content":" Added A  StopWhenSubgradientNormLess  stopping criterion for subgradient-based optimization. Allow the  message=  of the  DebugIfEntry  debug action to contain a format element to print the field in the message as well."},{"id":2247,"pagetitle":"Changelog","title":"0.4.50 (January 26, 2024)","ref":"/manopt/stable/changelog/#[0.4.50](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.50)-(January-26,-2024)","content":" 0.4.50  (January 26, 2024)"},{"id":2248,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-20","content":" Fixed Fix Quasi Newton on complex manifolds."},{"id":2249,"pagetitle":"Changelog","title":"0.4.49 (January 18, 2024)","ref":"/manopt/stable/changelog/#[0.4.49](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.49)-(January-18,-2024)","content":" 0.4.49  (January 18, 2024)"},{"id":2250,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-30","content":" Added A  StopWhenEntryChangeLess  to be able to stop on arbitrary small changes of specific fields generalises  StopWhenGradientNormLess  to accept arbitrary  norm=  functions refactor the default in  particle_swarm  to no longer ‚Äúmisuse‚Äù the iteration change, but actually the new one the  :swarm  entry"},{"id":2251,"pagetitle":"Changelog","title":"0.4.48 (January 16, 2024)","ref":"/manopt/stable/changelog/#[0.4.48](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.48)-(January-16,-2024)","content":" 0.4.48  (January 16, 2024)"},{"id":2252,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-21","content":" Fixed fixes an imprecision in the interface of  get_iterate  that sometimes led to the swarm of  particle_swarm  being returned as the iterate. refactor  particle_swarm  in naming and access functions to avoid this also in the future. To access the whole swarm, one now should use  get_manopt_parameter(pss, :Population)"},{"id":2253,"pagetitle":"Changelog","title":"0.4.47 (January 6, 2024)","ref":"/manopt/stable/changelog/#[0.4.47](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.47)-(January-6,-2024)","content":" 0.4.47  (January 6, 2024)"},{"id":2254,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-22","content":" Fixed fixed a bug, where the retraction set in  check_Hessian  was not passed on to the optional inner  check_gradient  call, which could lead to unwanted side effects, see  #342 ."},{"id":2255,"pagetitle":"Changelog","title":"0.4.46 (January 1, 2024)","ref":"/manopt/stable/changelog/#[0.4.46](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.46)-(January-1,-2024)","content":" 0.4.46  (January 1, 2024)"},{"id":2256,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-22","content":" Changed An error is thrown when a line search from  LineSearches.jl  reports search failure. Changed default stopping criterion in ALM algorithm to mitigate an issue occurring when step size is very small. Default memory length in default ALM subsolver is now capped at manifold dimension. Replaced CI testing on Julia 1.8 with testing on Julia 1.10."},{"id":2257,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-23","content":" Fixed A bug in  LineSearches.jl  extension leading to slower convergence. Fixed a bug in L-BFGS related to memory storage, which caused significantly slower convergence."},{"id":2258,"pagetitle":"Changelog","title":"0.4.45 (December 28, 2023)","ref":"/manopt/stable/changelog/#[0.4.45](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.45)-(December-28,-2023)","content":" 0.4.45  (December 28, 2023)"},{"id":2259,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-31","content":" Added Introduce  sub_kwargs  and  sub_stopping_criterion  for  trust_regions  as noticed in  #336"},{"id":2260,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-23","content":" Changed WolfePowellLineSearch ,  ArmijoLineSearch  step sizes now allocate less linesearch_backtrack!  is now available Quasi Newton Updates can work in-place of a direction vector as well. Faster  safe_indices  in L-BFGS."},{"id":2261,"pagetitle":"Changelog","title":"0.4.44 (December 12, 2023)","ref":"/manopt/stable/changelog/#[0.4.44](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.44)-(December-12,-2023)","content":" 0.4.44  (December 12, 2023) Formally one could consider this version breaking, since a few functions have been moved, that in earlier versions (0.3.x) have been used in example scripts. These examples are now available again within  ManoptExamples.jl , and with their ‚Äúreappearance‚Äù the corresponding costs, gradients, differentials, adjoint differentials, and proximal maps have been moved there as well. This is not considered breaking, since the functions were only used in the old, removed examples. Each and every moved function is still documented. They have been partly renamed, and their documentation and testing has been extended."},{"id":2262,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-24","content":" Changed Bumped and added dependencies on all 3 Project.toml files, the main one, the docs/, an the tutorials/ one. artificial_S2_lemniscate  is available as  ManoptExample.Lemniscate  and works on arbitrary manifolds now. artificial_S1_signal  is available as  ManoptExample.artificial_S1_signal artificial_S1_slope_signal  is available as  ManoptExamples.artificial_S1_slope_signal artificial_S2_composite_bezier_curve  is available as  ManoptExamples.artificial_S2_composite_Bezier_curve artificial_S2_rotation_image  is available as  ManoptExamples.artificial_S2_rotation_image artificial_S2_whirl_image  is available as  ManoptExamples.artificial_S2_whirl_image artificial_S2_whirl_patch  is available as  ManoptExamples.artificial_S2_whirl_path artificial_SAR_image  is available as  ManoptExamples.artificial_SAR_image artificial_SPD_image  is available as  ManoptExamples.artificial_SPD_image artificial_SPD_image2  is available as  ManoptExamples.artificial_SPD_image adjoint_differential_forward_logs  is available as  ManoptExamples.adjoint_differential_forward_logs adjoint:differential_bezier_control  is available as  ManoptExamples.adjoint_differential_Bezier_control_points BezierSegment  is available as  ManoptExamples.Bezi√©rSegment cost_acceleration_bezier  is available as  ManoptExamples.acceleration_Bezier cost_L2_acceleration_bezier  is available as  ManoptExamples.L2_acceleration_Bezier costIntrICTV12  is available as  ManoptExamples.Intrinsic_infimal_convolution_TV12 costL2TV  is available as  ManoptExamples.L2_Total_Variation costL2TV12  is available as  ManoptExamples.L2_Total_Variation_1_2 costL2TV2  is available as  ManoptExamples.L2_second_order_Total_Variation costTV  is available as  ManoptExamples.Total_Variation costTV2  is available as  ManoptExamples.second_order_Total_Variation de_casteljau  is available as  ManoptExamples.de_Casteljau differential_forward_logs  is available as  ManoptExamples.differential_forward_logs differential_bezier_control  is available as  ManoptExamples.differential_Bezier_control_points forward_logs  is available as  ManoptExamples.forward_logs get_bezier_degree  is available as  ManoptExamples.get_Bezier_degree get_bezier_degrees  is available as  ManoptExamples.get_Bezier_degrees get_Bezier_inner_points  is available as  ManoptExamples.get_Bezier_inner_points get_bezier_junction_tangent_vectors  is available as  ManoptExamples.get_Bezier_junction_tangent_vectors get_bezier_junctions  is available as  ManoptExamples.get_Bezier_junctions get_bezier_points  is available as  ManoptExamples.get_Bezier_points get_bezier_segments  is available as  ManoptExamples.get_Bezier_segments grad_acceleration_bezier  is available as  ManoptExamples.grad_acceleration_Bezier grad_L2_acceleration_bezier  is available as  ManoptExamples.grad_L2_acceleration_Bezier grad_Intrinsic_infimal_convolution_TV12  is available as  ManoptExamples.Intrinsic_infimal_convolution_TV12 grad_TV  is available as  ManoptExamples.grad_Total_Variation costIntrICTV12  is available as  ManoptExamples.Intrinsic_infimal_convolution_TV12 project_collaborative_TV  is available as  ManoptExamples.project_collaborative_TV prox_parallel_TV  is available as  ManoptExamples.prox_parallel_TV grad_TV2  is available as  ManoptExamples.prox_second_order_Total_Variation prox_TV  is available as  ManoptExamples.prox_Total_Variation prox_TV2  is available as  ManopExamples.prox_second_order_Total_Variation"},{"id":2263,"pagetitle":"Changelog","title":"0.4.43 (November 19, 2023)","ref":"/manopt/stable/changelog/#[0.4.43](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.43)-(November-19,-2023)","content":" 0.4.43  (November 19, 2023)"},{"id":2264,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-32","content":" Added vale.sh as a CI to keep track of a consistent documentation"},{"id":2265,"pagetitle":"Changelog","title":"0.4.42 (November 6, 2023)","ref":"/manopt/stable/changelog/#[0.4.42](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.42)-(November-6,-2023)","content":" 0.4.42  (November 6, 2023)"},{"id":2266,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-33","content":" Added add  Manopt.JuMP_Optimizer  implementing JuMP's solver interface"},{"id":2267,"pagetitle":"Changelog","title":"0.4.41 (November 2, 2023)","ref":"/manopt/stable/changelog/#[0.4.41](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.41)-(November-2,-2023)","content":" 0.4.41  (November 2, 2023)"},{"id":2268,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-25","content":" Changed trust_regions  is now more flexible and the sub solver (Steihaug-Toint tCG by default) can now be exchanged. adaptive_regularization_with_cubics  is now more flexible as well, where it previously was a bit too much tightened to the Lanczos solver as well. Unified documentation notation and bumped dependencies to use DocumenterCitations 1.3"},{"id":2269,"pagetitle":"Changelog","title":"0.4.40 (October 24, 2023)","ref":"/manopt/stable/changelog/#[0.4.40](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.40)-(October-24,-2023)","content":" 0.4.40  (October 24, 2023)"},{"id":2270,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-34","content":" Added add a  --help  argument to  docs/make.jl  to document all available command line arguments add a  --exclude-tutorials  argument to  docs/make.jl . This way, when quarto is not available on a computer, the docs can still be build with the tutorials not being added to the menu such that documenter does not expect them to exist."},{"id":2271,"pagetitle":"Changelog","title":"Changes","ref":"/manopt/stable/changelog/#Changes","content":" Changes Bump dependencies to  ManifoldsBase.jl  0.15 and  Manifolds.jl  0.9 move the ARC CG subsolver to the main package, since  TangentSpace  is now already available from  ManifoldsBase ."},{"id":2272,"pagetitle":"Changelog","title":"0.4.39 (October 9, 2023)","ref":"/manopt/stable/changelog/#[0.4.39](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.39)-(October-9,-2023)","content":" 0.4.39  (October 9, 2023)"},{"id":2273,"pagetitle":"Changelog","title":"Changes","ref":"/manopt/stable/changelog/#Changes-2","content":" Changes also use the pair of a retraction and the inverse retraction (see last update) to perform the relaxation within the Douglas-Rachford algorithm."},{"id":2274,"pagetitle":"Changelog","title":"0.4.38 (October 8, 2023)","ref":"/manopt/stable/changelog/#[0.4.38](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.38)-(October-8,-2023)","content":" 0.4.38  (October 8, 2023)"},{"id":2275,"pagetitle":"Changelog","title":"Changes","ref":"/manopt/stable/changelog/#Changes-3","content":" Changes avoid allocations when calling  get_jacobian!  within the Levenberg-Marquard Algorithm."},{"id":2276,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-24","content":" Fixed Fix a lot of typos in the documentation"},{"id":2277,"pagetitle":"Changelog","title":"0.4.37 (September 28, 2023)","ref":"/manopt/stable/changelog/#[0.4.37](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.37)-(September-28,-2023)","content":" 0.4.37  (September 28, 2023)"},{"id":2278,"pagetitle":"Changelog","title":"Changes","ref":"/manopt/stable/changelog/#Changes-4","content":" Changes add more of the Riemannian Levenberg-Marquard algorithms parameters as keywords, so they can be changed on call generalize the internal reflection of Douglas-Rachford, such that is also works with an arbitrary pair of a reflection and an inverse reflection."},{"id":2279,"pagetitle":"Changelog","title":"0.4.36 ( September 20, 2023)","ref":"/manopt/stable/changelog/#[0.4.36](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.36)-(-September-20,-2023)","content":" 0.4.36  ( September 20, 2023)"},{"id":2280,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-25","content":" Fixed Fixed a bug that caused non-matrix points and vectors to fail when working with approximate"},{"id":2281,"pagetitle":"Changelog","title":"0.4.35 ( September 14, 2023)","ref":"/manopt/stable/changelog/#[0.4.35](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.35)-(-September-14,-2023)","content":" 0.4.35  ( September 14, 2023)"},{"id":2282,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-35","content":" Added The access to functions of the objective is now unified and encapsulated in proper  get_  functions."},{"id":2283,"pagetitle":"Changelog","title":"0.4.34 ( September 02, 2023)","ref":"/manopt/stable/changelog/#[0.4.34](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.34)-(-September-02,-2023)","content":" 0.4.34  ( September 02, 2023)"},{"id":2284,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-36","content":" Added an  ManifoldEuclideanGradientObjective  to allow the cost, gradient, and Hessian and other first or second derivative based elements to be Euclidean and converted when needed. a keyword  objective_type=:Euclidean  for all solvers, that specifies that an Objective shall be created of the new type"},{"id":2285,"pagetitle":"Changelog","title":"0.4.33 (August 24, 2023)","ref":"/manopt/stable/changelog/#[0.4.33](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.33)-(August-24,-2023)","content":" 0.4.33  (August 24, 2023)"},{"id":2286,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-37","content":" Added ConstantStepsize  and  DecreasingStepsize  now have an additional field  type::Symbol  to assess whether the step-size should be relatively (to the gradient norm) or absolutely constant."},{"id":2287,"pagetitle":"Changelog","title":"0.4.32 (August 23, 2023)","ref":"/manopt/stable/changelog/#[0.4.32](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.32)-(August-23,-2023)","content":" 0.4.32  (August 23, 2023)"},{"id":2288,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-38","content":" Added The adaptive regularization with cubics (ARC) solver."},{"id":2289,"pagetitle":"Changelog","title":"0.4.31 (August 14, 2023)","ref":"/manopt/stable/changelog/#[0.4.31](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.31)-(August-14,-2023)","content":" 0.4.31  (August 14, 2023)"},{"id":2290,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-39","content":" Added A  :Subsolver  keyword in the  debug=  keyword argument, that activates the new  DebugWhenActive to de/activate subsolver debug from the main solvers DebugEvery`."},{"id":2291,"pagetitle":"Changelog","title":"0.4.30 (August 3, 2023)","ref":"/manopt/stable/changelog/#[0.4.30](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.30)-(August-3,-2023)","content":" 0.4.30  (August 3, 2023)"},{"id":2292,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-26","content":" Changed References in the documentation are now rendered using  DocumenterCitations.jl Asymptote export now also accepts a size in pixel instead of its default  4cm  size and  render  can be deactivated setting it to  nothing ."},{"id":2293,"pagetitle":"Changelog","title":"0.4.29 (July 12, 2023)","ref":"/manopt/stable/changelog/#[0.4.29](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.29)-(July-12,-2023)","content":" 0.4.29  (July 12, 2023)"},{"id":2294,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-26","content":" Fixed fixed a bug, where  cyclic_proximal_point  did not work with decorated objectives."},{"id":2295,"pagetitle":"Changelog","title":"0.4.28 (June 24, 2023)","ref":"/manopt/stable/changelog/#[0.4.28](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.28)-(June-24,-2023)","content":" 0.4.28  (June 24, 2023)"},{"id":2296,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-27","content":" Changed max_stepsize  was specialized for  FixedRankManifold  to follow Matlab Manopt."},{"id":2297,"pagetitle":"Changelog","title":"0.4.27 (June 15, 2023)","ref":"/manopt/stable/changelog/#[0.4.27](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.27)-(June-15,-2023)","content":" 0.4.27  (June 15, 2023)"},{"id":2298,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-40","content":" Added The  AdaptiveWNGrad  stepsize is available as a new stepsize functor."},{"id":2299,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-27","content":" Fixed Levenberg-Marquardt now possesses its parameters  initial_residual_values  and  initial_jacobian_f  also as keyword arguments, such that their default initialisations can be adapted, if necessary"},{"id":2300,"pagetitle":"Changelog","title":"0.4.26 (June 11, 2023)","ref":"/manopt/stable/changelog/#[0.4.26](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.26)-(June-11,-2023)","content":" 0.4.26  (June 11, 2023)"},{"id":2301,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-41","content":" Added simplify usage of gradient descent as sub solver in the DoC solvers. add a  get_state  function document  indicates_convergence ."},{"id":2302,"pagetitle":"Changelog","title":"0.4.25 (June 5, 2023)","ref":"/manopt/stable/changelog/#[0.4.25](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.25)-(June-5,-2023)","content":" 0.4.25  (June 5, 2023)"},{"id":2303,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-28","content":" Fixed Fixes an allocation bug in the difference of convex algorithm"},{"id":2304,"pagetitle":"Changelog","title":"0.4.24 (June 4, 2023)","ref":"/manopt/stable/changelog/#[0.4.24](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.24)-(June-4,-2023)","content":" 0.4.24  (June 4, 2023)"},{"id":2305,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-42","content":" Added another workflow that deletes old PR renderings from the docs to keep them smaller in overall size."},{"id":2306,"pagetitle":"Changelog","title":"Changes","ref":"/manopt/stable/changelog/#Changes-5","content":" Changes bump dependencies since the extension between Manifolds.jl and ManifoldsDiff.jl has been moved to Manifolds.jl"},{"id":2307,"pagetitle":"Changelog","title":"0.4.23 (June 4, 2023)","ref":"/manopt/stable/changelog/#[0.4.23](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.23)-(June-4,-2023)","content":" 0.4.23  (June 4, 2023)"},{"id":2308,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-43","content":" Added More details on the Count and Cache tutorial"},{"id":2309,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-28","content":" Changed loosen constraints slightly"},{"id":2310,"pagetitle":"Changelog","title":"0.4.22 (May 31, 2023)","ref":"/manopt/stable/changelog/#[0.4.22](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.22)-(May-31,-2023)","content":" 0.4.22  (May 31, 2023)"},{"id":2311,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-44","content":" Added A tutorial on how to implement a solver"},{"id":2312,"pagetitle":"Changelog","title":"0.4.21 (May 22, 2023)","ref":"/manopt/stable/changelog/#[0.4.21](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.21)-(May-22,-2023)","content":" 0.4.21  (May 22, 2023)"},{"id":2313,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-45","content":" Added A  ManifoldCacheObjective  as a decorator for objectives to cache results of calls, using LRU Caches as a weak dependency. For now this works with cost and gradient evaluations A  ManifoldCountObjective  as a decorator for objectives to enable counting of calls to for example the cost and the gradient adds a  return_objective  keyword, that switches the return of a solver to a tuple  (o, s) , where  o  is the (possibly decorated) objective, and  s  is the ‚Äúclassical‚Äù solver return (state or point). This way the counted values can be accessed and the cache can be reused. change solvers on the mid level (form  solver(M, objective, p) ) to also accept decorated objectives"},{"id":2314,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-29","content":" Changed Switch all Requires weak dependencies to actual weak dependencies starting in Julia 1.9"},{"id":2315,"pagetitle":"Changelog","title":"0.4.20 (May 11, 2023)","ref":"/manopt/stable/changelog/#[0.4.20](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.20)-(May-11,-2023)","content":" 0.4.20  (May 11, 2023)"},{"id":2316,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-30","content":" Changed the default tolerances for the numerical  check_  functions were loosened a bit, such that  check_vector  can also be changed in its tolerances."},{"id":2317,"pagetitle":"Changelog","title":"0.4.19 (May 7, 2023)","ref":"/manopt/stable/changelog/#[0.4.19](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.19)-(May-7,-2023)","content":" 0.4.19  (May 7, 2023)"},{"id":2318,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-46","content":" Added the sub solver for  trust_regions  is now customizable and can now be exchanged."},{"id":2319,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-31","content":" Changed slightly changed the definitions of the solver states for ALM and EPM to be type stable"},{"id":2320,"pagetitle":"Changelog","title":"0.4.18 (May 4, 2023)","ref":"/manopt/stable/changelog/#[0.4.18](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.18)-(May-4,-2023)","content":" 0.4.18  (May 4, 2023)"},{"id":2321,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-47","content":" Added A function  check_Hessian(M, f, grad_f, Hess_f)  to numerically verify the (Riemannian) Hessian of a function  f"},{"id":2322,"pagetitle":"Changelog","title":"0.4.17 (April 28, 2023)","ref":"/manopt/stable/changelog/#[0.4.17](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.17)-(April-28,-2023)","content":" 0.4.17  (April 28, 2023)"},{"id":2323,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-48","content":" Added A new interface of the form  alg(M, objective, p0)  to allow to reuse objectives without creating  AbstractManoptSolverState s and calling  solve! . This especially still allows for any decoration of the objective and/or the state using  debug= , or  record= ."},{"id":2324,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-32","content":" Changed All solvers now have the initial point  p  as an optional parameter making it more accessible to first time users,  gradient_descent(M, f, grad_f)  is equivalent to  gradient_descent(M, f, grad_f, rand(M))"},{"id":2325,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-29","content":" Fixed Unified the framework to work on manifold where points are represented by numbers for several solvers"},{"id":2326,"pagetitle":"Changelog","title":"0.4.16 (April 18, 2023)","ref":"/manopt/stable/changelog/#[0.4.16](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.16)-(April-18,-2023)","content":" 0.4.16  (April 18, 2023)"},{"id":2327,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-30","content":" Fixed the inner products used in  truncated_gradient_descent  now also work thoroughly on complex matrix manifolds"},{"id":2328,"pagetitle":"Changelog","title":"0.4.15 (April 13, 2023)","ref":"/manopt/stable/changelog/#[0.4.15](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.15)-(April-13,-2023)","content":" 0.4.15  (April 13, 2023)"},{"id":2329,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-33","content":" Changed trust_regions(M, f, grad_f, hess_f, p)  now has the Hessian  hess_f  as well as the start point  p0  as an optional parameter and approximate it otherwise. trust_regions!(M, f, grad_f, hess_f, p)  has the Hessian as an optional parameter and approximate it otherwise."},{"id":2330,"pagetitle":"Changelog","title":"Removed","ref":"/manopt/stable/changelog/#Removed-3","content":" Removed support for  ManifoldsBase.jl  0.13.x, since with the definition of  copy(M,p::Number) , in 0.14.4, that one is used instead of defining it ourselves."},{"id":2331,"pagetitle":"Changelog","title":"0.4.14 (April 06, 2023)","ref":"/manopt/stable/changelog/#[0.4.14](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.14)-(April-06,-2023)","content":" 0.4.14  (April 06, 2023)"},{"id":2332,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-34","content":" Changed particle_swarm  now uses much more in-place operations"},{"id":2333,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-31","content":" Fixed particle_swarm  used quite a few  deepcopy(p)  commands still, which were replaced by  copy(M, p)"},{"id":2334,"pagetitle":"Changelog","title":"0.4.13 (April 09, 2023)","ref":"/manopt/stable/changelog/#[0.4.13](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.13)-(April-09,-2023)","content":" 0.4.13  (April 09, 2023)"},{"id":2335,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-49","content":" Added get_message  to obtain messages from sub steps of a solver DebugMessages  to display the new messages in debug safeguards in Armijo line search and L-BFGS against numerical over- and underflow that report in messages"},{"id":2336,"pagetitle":"Changelog","title":"0.4.12 (April 4, 2023)","ref":"/manopt/stable/changelog/#[0.4.12](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.12)-(April-4,-2023)","content":" 0.4.12  (April 4, 2023)"},{"id":2337,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-50","content":" Added Introduce the  Difference of Convex Algorithm  (DCA)  difference_of_convex_algorithm(M, f, g, ‚àÇh, p0) Introduce the  Difference of Convex Proximal Point Algorithm  (DCPPA)  difference_of_convex_proximal_point(M, prox_g, grad_h, p0) Introduce a  StopWhenGradientChangeLess  stopping criterion"},{"id":2338,"pagetitle":"Changelog","title":"0.4.11 (March 27, 2023)","ref":"/manopt/stable/changelog/#[0.4.11](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.11)-(March-27,-2023)","content":" 0.4.11  (March 27, 2023)"},{"id":2339,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-35","content":" Changed adapt tolerances in tests to the speed/accuracy optimized distance on the sphere in  Manifolds.jl  (part II)"},{"id":2340,"pagetitle":"Changelog","title":"0.4.10 (March 26, 2023)","ref":"/manopt/stable/changelog/#[0.4.10](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.10)-(March-26,-2023)","content":" 0.4.10  (March 26, 2023)"},{"id":2341,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-36","content":" Changed adapt tolerances in tests to the speed/accuracy optimized distance on the sphere in  Manifolds.jl"},{"id":2342,"pagetitle":"Changelog","title":"0.4.9 (March 3, 2023)","ref":"/manopt/stable/changelog/#[0.4.9](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.9)-(March-3,-2023)","content":" 0.4.9  (March 3, 2023)"},{"id":2343,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-51","content":" Added introduce a wrapper that allows line searches from  LineSearches.jl  to be used within Manopt.jl, introduce the  manoptjl.org/stable/extensions/  page to explain the details."},{"id":2344,"pagetitle":"Changelog","title":"0.4.8 (February 21, 2023)","ref":"/manopt/stable/changelog/#[0.4.8](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.8)-(February-21,-2023)","content":" 0.4.8  (February 21, 2023)"},{"id":2345,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-52","content":" Added a  status_summary  that displays the main parameters within several structures of Manopt, most prominently a solver state"},{"id":2346,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-37","content":" Changed Improved storage performance by introducing separate named tuples for points and vectors changed the  show  methods of  AbstractManoptSolverState s to display their `state_summary Move tutorials to be rendered with Quarto into the documentation."},{"id":2347,"pagetitle":"Changelog","title":"0.4.7 (February 14, 2023)","ref":"/manopt/stable/changelog/#[0.4.7](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.7)-(February-14,-2023)","content":" 0.4.7  (February 14, 2023)"},{"id":2348,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-38","content":" Changed Bump  [compat]  entry of ManifoldDiff to also include 0.3"},{"id":2349,"pagetitle":"Changelog","title":"0.4.6 (February 3, 2023)","ref":"/manopt/stable/changelog/#[0.4.6](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.6)-(February-3,-2023)","content":" 0.4.6  (February 3, 2023)"},{"id":2350,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-32","content":" Fixed Fixed a few stopping criteria even indicated to stop before the algorithm started."},{"id":2351,"pagetitle":"Changelog","title":"0.4.5 (January 24, 2023)","ref":"/manopt/stable/changelog/#[0.4.5](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.5)-(January-24,-2023)","content":" 0.4.5  (January 24, 2023)"},{"id":2352,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-39","content":" Changed the new default functions that include  p  are used where possible a first step towards faster storage handling"},{"id":2353,"pagetitle":"Changelog","title":"0.4.4 (January 20, 2023)","ref":"/manopt/stable/changelog/#[0.4.4](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.4)-(January-20,-2023)","content":" 0.4.4  (January 20, 2023)"},{"id":2354,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-53","content":" Added Introduce  ConjugateGradientBealeRestart  to allow CG restarts using Beale‚Äòs rule"},{"id":2355,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-33","content":" Fixed fix a type in  HestenesStiefelCoefficient"},{"id":2356,"pagetitle":"Changelog","title":"0.4.3 (January 17, 2023)","ref":"/manopt/stable/changelog/#[0.4.3](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.3)-(January-17,-2023)","content":" 0.4.3  (January 17, 2023)"},{"id":2357,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-34","content":" Fixed the CG coefficient  Œ≤  can now be complex fix a bug in  grad_distance"},{"id":2358,"pagetitle":"Changelog","title":"0.4.2 (January 16, 2023)","ref":"/manopt/stable/changelog/#[0.4.2](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.2)-(January-16,-2023)","content":" 0.4.2  (January 16, 2023)"},{"id":2359,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-40","content":" Changed the usage of  inner  in line search methods, such that they work well with complex manifolds as well"},{"id":2360,"pagetitle":"Changelog","title":"0.4.1 (January 15, 2023)","ref":"/manopt/stable/changelog/#[0.4.1](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.1)-(January-15,-2023)","content":" 0.4.1  (January 15, 2023)"},{"id":2361,"pagetitle":"Changelog","title":"Fixed","ref":"/manopt/stable/changelog/#Fixed-35","content":" Fixed a  max_stepsize  per manifold to avoid leaving the injectivity radius, which it also defaults to"},{"id":2362,"pagetitle":"Changelog","title":"0.4.0 (January 10, 2023)","ref":"/manopt/stable/changelog/#[0.4.0](https://github.com/JuliaManifolds/Manopt.jl/releases/tag/v0.4.0)-(January-10,-2023)","content":" 0.4.0  (January 10, 2023)"},{"id":2363,"pagetitle":"Changelog","title":"Added","ref":"/manopt/stable/changelog/#Added-54","content":" Added Dependency on  ManifoldDiff.jl  and a start of moving actual derivatives, differentials, and gradients there. AbstractManifoldObjective  to store the objective within the  AbstractManoptProblem Introduce a  CostGrad  structure to store a function that computes the cost and gradient within one function. started a  changelog.md  to thoroughly keep track of changes"},{"id":2364,"pagetitle":"Changelog","title":"Changed","ref":"/manopt/stable/changelog/#Changed-41","content":" Changed AbstractManoptProblem  replaces  Problem the problem now contains a AbstractManoptSolverState  replaces  Options random_point(M)  is replaced by  rand(M)  from `ManifoldsBase.jl random_tangent(M, p)  is replaced by  rand(M; vector_at=p)"},{"id":2367,"pagetitle":"Contributing to Manopt.jl","title":"Contributing to Manopt.jl","ref":"/manopt/stable/contributing/#Contributing-to-Manopt.jl","content":" Contributing to  Manopt.jl First, thanks for taking the time to contribute. Any contribution is appreciated and welcome. The following is a set of guidelines to  Manopt.jl ."},{"id":2368,"pagetitle":"Contributing to Manopt.jl","title":"Table of contents","ref":"/manopt/stable/contributing/#Table-of-contents","content":" Table of contents Contributing to  Manopt.jl      -  Table of Contents I just have a question How can I file an issue? How can I contribute? Add a missing method Provide a new algorithm Provide a new example Code style"},{"id":2369,"pagetitle":"Contributing to Manopt.jl","title":"I just have a question","ref":"/manopt/stable/contributing/#I-just-have-a-question","content":" I just have a question The developer can most easily be reached in the Julia Slack channel  #manifolds . You can apply for the Julia Slack workspace  here  if you haven't joined yet. You can also ask your question on  discourse.julialang.org ."},{"id":2370,"pagetitle":"Contributing to Manopt.jl","title":"How can I file an issue?","ref":"/manopt/stable/contributing/#How-can-I-file-an-issue?","content":" How can I file an issue? If you found a bug or want to propose a feature, please open an issue in within the  GitHub repository ."},{"id":2371,"pagetitle":"Contributing to Manopt.jl","title":"How can I contribute?","ref":"/manopt/stable/contributing/#How-can-I-contribute?","content":" How can I contribute?"},{"id":2372,"pagetitle":"Contributing to Manopt.jl","title":"Add a missing method","ref":"/manopt/stable/contributing/#Add-a-missing-method","content":" Add a missing method There is still a lot of methods for within the optimization framework of   Manopt.jl , may it be functions, gradients, differentials, proximal maps, step size rules or stopping criteria. If you notice a method missing and can contribute an implementation, please do so, and the maintainers try help with the necessary details. Even providing a single new method is a good contribution."},{"id":2373,"pagetitle":"Contributing to Manopt.jl","title":"Provide a new algorithm","ref":"/manopt/stable/contributing/#Provide-a-new-algorithm","content":" Provide a new algorithm A main contribution you can provide is another algorithm that is not yet included in the package. An algorithm is always based on a concrete type of a  AbstractManoptProblem  storing the main information of the task and a concrete type of an  AbstractManoptSolverState  storing all information that needs to be known to the solver in general. The actual algorithm is split into an initialization phase, see  initialize_solver! , and the implementation of the  i th step of the solver itself, see  before the iterative procedure, see  step_solver! . For these two functions, it would be great if a new algorithm uses functions from the  ManifoldsBase.jl  interface as generically as possible. For example, if possible use  retract!(M,q,p,X)  in favor of  exp!(M,q,p,X)  to perform a step starting in  p  in direction  X  (in place of  q ), since the exponential map might be too expensive to evaluate or might not be available on a certain manifold. See  Retractions and inverse retractions  for more details. Further, if possible, prefer  retract!(M,q,p,X)  in favor of  retract(M,p,X) , since a computation in place of a suitable variable  q  reduces memory allocations. Usually, the methods implemented in  Manopt.jl  also have a high-level interface, that is easier to call, creates the necessary problem and options structure and calls the solver. The two technical functions  initialize_solver!  and  step_solver!  should be documented with technical details, while the high level interface should usually provide a general description and some literature references to the algorithm at hand."},{"id":2374,"pagetitle":"Contributing to Manopt.jl","title":"Provide a new example","ref":"/manopt/stable/contributing/#Provide-a-new-example","content":" Provide a new example Example problems are available at  ManoptExamples.jl , where also their reproducible Quarto-Markdown files are stored."},{"id":2375,"pagetitle":"Contributing to Manopt.jl","title":"Code style","ref":"/manopt/stable/contributing/#Code-style","content":" Code style Try to follow the  documentation guidelines  from the Julia documentation as well as  Blue Style . Run  JuliaFormatter.jl  on the repository in the way set in the  .JuliaFormatter.toml  file, which enforces a number of conventions consistent with the Blue Style. Furthermore  vale  is run on both Markdown and code files, affecting documentation and source code comments Please follow a few internal conventions: It is preferred that the  AbstractManoptProblem 's struct contains information about the general structure of the problem. Any implemented function should be accompanied by its mathematical formulae if a closed form exists. AbstractManoptProblem  and helping functions are stored within the  plan/  folder and sorted by properties of the problem and/or solver at hand. the solver state is usually stored with the solver itself Within the source code of one algorithm, following the state, the high level interface should be next, then the initialization, then the step. Otherwise an alphabetical order of functions is preferable. The preceding implies that the mutating variant of a function follows the non-mutating variant. There should be no dangling  =  signs. Always add a newline between things of different types (struct/method/const). Always add a newline between methods for different functions (including mutating/nonmutating variants). Prefer to have no newline between methods for the same function; when reasonable, merge the documentation strings. All  import / using / include  should be in the main module file. Concerning documentation if possible provide both mathematical formulae and literature references using  DocumenterCitations.jl  and BibTeX where possible Always document all input variables and keyword arguments If you implement an algorithm with a certain numerical example in mind, it would be great, if this could be added to the  ManoptExamples.jl  package as well."},{"id":2378,"pagetitle":"Extensions","title":"Extensions","ref":"/manopt/stable/extensions/#Extensions","content":" Extensions"},{"id":2379,"pagetitle":"Extensions","title":"LineSearches.jl","ref":"/manopt/stable/extensions/#LineSearches.jl","content":" LineSearches.jl Manopt can be used with line search algorithms implemented in  LineSearches.jl . This can be illustrated by the following example of optimizing Rosenbrock function constrained to the unit sphere. using Manopt, Manifolds, LineSearches\n\n# define objective function and its gradient\np = [1.0, 100.0]\nfunction rosenbrock(::AbstractManifold, x)\n    val = zero(eltype(x))\n    for i in 1:(length(x) - 1)\n        val += (p[1] - x[i])^2 + p[2] * (x[i + 1] - x[i]^2)^2\n    end\n    return val\nend\nfunction rosenbrock_grad!(M::AbstractManifold, storage, x)\n    storage .= 0.0\n    for i in 1:(length(x) - 1)\n        storage[i] += -2.0 * (p[1] - x[i]) - 4.0 * p[2] * (x[i + 1] - x[i]^2) * x[i]\n        storage[i + 1] += 2.0 * p[2] * (x[i + 1] - x[i]^2)\n    end\n    project!(M, storage, x, storage)\n    return storage\nend\n# define constraint\nn_dims = 5\nM = Manifolds.Sphere(n_dims)\n# set initial point\nx0 = vcat(zeros(n_dims - 1), 1.0)\n# use LineSearches.jl HagerZhang method with Manopt.jl quasiNewton solver\nls_hz = Manopt.LineSearchesStepsize(M, LineSearches.HagerZhang())\nx_opt = quasi_Newton(\n    M,\n    rosenbrock,\n    rosenbrock_grad!,\n    x0;\n    stepsize=ls_hz,\n    evaluation=InplaceEvaluation(),\n    stopping_criterion=StopAfterIteration(1000) | StopWhenGradientNormLess(1e-6),\n    return_state=true,\n) # Solver state for `Manopt.jl`s Quasi Newton Method\nAfter 10 iterations\n\n## Parameters\n* direction update:        limited memory InverseBFGS (size 5) initial scaling 1.0and ParallelTransport() as vector transport.\n* retraction method:       ExponentialRetraction()\n* vector transport method: ParallelTransport()\n\n## Stepsize\nLineSearchesStepsize(HagerZhang{Float64, Base.RefValue{Bool}}\n  delta: Float64 0.1\n  sigma: Float64 0.9\n  alphamax: Float64 Inf\n  rho: Float64 5.0\n  epsilon: Float64 1.0e-6\n  gamma: Float64 0.66\n  linesearchmax: Int64 50\n  psi3: Float64 0.1\n  display: Int64 0\n  mayterminate: Base.RefValue{Bool}\n  cache: Nothing nothing\n  check_flatness: Bool false\n; retraction_method=ExponentialRetraction(), vector_transport_method=ParallelTransport())\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 1000:\tnot reached\n  * |grad f| < 1.0e-6: reached\nOverall: reached\nThis indicates convergence: Yes In general this defines the following new  stepsize"},{"id":2380,"pagetitle":"Extensions","title":"Manopt.LineSearchesStepsize","ref":"/manopt/stable/extensions/#Manopt.LineSearchesStepsize","content":" Manopt.LineSearchesStepsize  ‚Äî  Type LineSearchesStepsize <: Stepsize Wrapper for line searches available in the  LineSearches.jl  library. Constructors LineSearchesStepsize(M::AbstractManifold, linesearch; kwargs...\nLineSearchesStepsize(\n    linesearch;\n    retraction_method=ExponentialRetraction(),\n    vector_transport_method=ParallelTransport(),\n) Wrap  linesearch  (for example  HagerZhang  or  MoreThuente ). The initial step selection from Linesearches.jl is not yet supported and the value 1.0 is used. Keyword Arguments retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports source"},{"id":2381,"pagetitle":"Extensions","title":"Manifolds.jl","ref":"/manopt/stable/extensions/#Manifolds.jl","content":" Manifolds.jl Loading  Manifolds.jl  introduces the following additional functions"},{"id":2382,"pagetitle":"Extensions","title":"Manopt.max_stepsize","ref":"/manopt/stable/extensions/#Manopt.max_stepsize-Tuple{FixedRankMatrices, Any}","content":" Manopt.max_stepsize  ‚Äî  Method max_stepsize(M::FixedRankMatrices, p) Return a reasonable guess of maximum step size on  FixedRankMatrices  following the choice of typical distance in Matlab Manopt, the dimension of  M . See  this note source"},{"id":2383,"pagetitle":"Extensions","title":"Manopt.max_stepsize","ref":"/manopt/stable/extensions/#Manopt.max_stepsize-Tuple{Hyperrectangle, Any}","content":" Manopt.max_stepsize  ‚Äî  Method max_stepsize(M::Hyperrectangle, p) The default maximum stepsize for  Hyperrectangle  manifold with corners is maximum of distances from  p  to each boundary. source"},{"id":2384,"pagetitle":"Extensions","title":"Manopt.max_stepsize","ref":"/manopt/stable/extensions/#Manopt.max_stepsize-Tuple{FiberBundle{ùîΩ, ManifoldsBase.TangentSpaceType, M} where {ùîΩ, M<:AbstractManifold{ùîΩ}}, Any}","content":" Manopt.max_stepsize  ‚Äî  Method max_stepsize(M::TangentBundle, p) Tangent bundle has injectivity radius of either infinity (for flat manifolds) or 0 (for non-flat manifolds). This makes a guess of what a reasonable maximum stepsize on a tangent bundle might be. source"},{"id":2385,"pagetitle":"Extensions","title":"ManifoldsBase.mid_point","ref":"/manopt/stable/extensions/#ManifoldsBase.mid_point","content":" ManifoldsBase.mid_point  ‚Äî  Function mid_point(M, p, q, x)\nmid_point!(M, y, p, q, x) Compute the mid point between  p  and  q . If there is more than one mid point of (not necessarily minimizing) geodesics (for example on the sphere), the one nearest to  x  is returned (in place of  y ). source Internally,  Manopt.jl  provides the two additional functions to choose some Euclidean space when needed as"},{"id":2386,"pagetitle":"Extensions","title":"Manopt.Rn","ref":"/manopt/stable/extensions/#Manopt.Rn","content":" Manopt.Rn  ‚Äî  Function Rn(args; kwargs...)\nRn(s::Symbol=:Manifolds, args; kwargs...) A small internal helper function to choose a Euclidean space. By default, this uses the  DefaultManifold  unless you load a more advanced Euclidean space like  Euclidean  from  Manifolds.jl source"},{"id":2387,"pagetitle":"Extensions","title":"Manopt.Rn_default","ref":"/manopt/stable/extensions/#Manopt.Rn_default","content":" Manopt.Rn_default  ‚Äî  Function Rn_default() Specify a default value to dispatch  Rn  on. This default is set to  Manifolds , indicating, that when this package is loded, it is the preferred package to ask for a vector space space. The default within  Manopt.jl  is to use the  DefaultManifold  from  ManifoldsBase.jl . If you load  Manifolds.jl  this switches to using  Euclidan . source"},{"id":2388,"pagetitle":"Extensions","title":"JuMP.jl","ref":"/manopt/stable/extensions/#JuMP.jl","content":" JuMP.jl Manopt can be used using the  JuMP.jl  interface. The manifold is provided in the  @variable  macro. Note that until now, only variables (points on manifolds) are supported, that are arrays, especially structs do not yet work. The algebraic expression of the objective function is specified in the  @objective  macro. The  descent_state_type  attribute specifies the solver. using JuMP, Manopt, Manifolds\nmodel = Model(Manopt.Optimizer)\n# Change the solver with this option, `GradientDescentState` is the default\nset_attribute(\"descent_state_type\", GradientDescentState)\n@variable(model, U[1:2, 1:2] in Stiefel(2, 2), start = 1.0)\n@objective(model, Min, sum((A - U) .^ 2))\noptimize!(model)\nsolution_summary(model)"},{"id":2389,"pagetitle":"Extensions","title":"Interface functions","ref":"/manopt/stable/extensions/#Interface-functions","content":" Interface functions"},{"id":2390,"pagetitle":"Extensions","title":"Manopt.JuMP_ArrayShape","ref":"/manopt/stable/extensions/#Manopt.JuMP_ArrayShape","content":" Manopt.JuMP_ArrayShape  ‚Äî  Type struct ArrayShape{N} <: JuMP.AbstractShape Shape of an  Array{T,N}  of size  size . source"},{"id":2391,"pagetitle":"Extensions","title":"Manopt.JuMP_VectorizedManifold","ref":"/manopt/stable/extensions/#Manopt.JuMP_VectorizedManifold","content":" Manopt.JuMP_VectorizedManifold  ‚Äî  Type struct VectorizedManifold{M} <: MOI.AbstractVectorSet\n    manifold::M\nend Representation of points of  manifold  as a vector of  R^n  where  n  is  MOI.dimension(VectorizedManifold(manifold)) . source"},{"id":2392,"pagetitle":"Extensions","title":"MathOptInterface.dimension","ref":"/manopt/stable/extensions/#MathOptInterface.dimension-Tuple{ManoptJuMPExt.VectorizedManifold}","content":" MathOptInterface.dimension  ‚Äî  Method MOI.dimension(set::VectorizedManifold) Return the representation side of points on the (vectorized in representation) manifold. As the MOI variables are real, this means if the  representation_size  yields (in product)  n , this refers to the vectorized point / tangent vector  from (a subset of  $‚Ñù^n$ ). source"},{"id":2393,"pagetitle":"Extensions","title":"Manopt.JuMP_Optimizer","ref":"/manopt/stable/extensions/#Manopt.JuMP_Optimizer","content":" Manopt.JuMP_Optimizer  ‚Äî  Type Manopt.JuMP_Optimizer() Creates a new optimizer object for the  MathOptInterface  (MOI). An alias  Manopt.JuMP_Optimizer  is defined for convenience. The minimization of a function  f(X)  of an array  X[1:n1,1:n2,...]  over a manifold  M  starting at  X0 , can be modeled as follows: using JuMP\nmodel = Model(Manopt.JuMP_Optimizer)\n@variable(model, X[i1=1:n1,i2=1:n2,...] in M, start = X0[i1,i2,...])\n@objective(model, Min, f(X)) The optimizer assumes that  M  has a  Array  shape described by  ManifoldsBase.representation_size . source"},{"id":2394,"pagetitle":"Extensions","title":"MathOptInterface.empty!","ref":"/manopt/stable/extensions/#MathOptInterface.empty!-Tuple{ManoptJuMPExt.Optimizer}","content":" MathOptInterface.empty!  ‚Äî  Method MOI.empty!(model::ManoptJuMPExt.Optimizer) Clear all model data from  model  but keep the  options  set. source"},{"id":2395,"pagetitle":"Extensions","title":"MathOptInterface.supports","ref":"/manopt/stable/extensions/#MathOptInterface.supports-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.RawOptimizerAttribute}","content":" MathOptInterface.supports  ‚Äî  Method MOI.supports(::Optimizer, attr::MOI.RawOptimizerAttribute) Return a  Bool  indicating whether  attr.name  is a valid option name for  Manopt . source"},{"id":2396,"pagetitle":"Extensions","title":"MathOptInterface.get","ref":"/manopt/stable/extensions/#MathOptInterface.get-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.RawOptimizerAttribute}","content":" MathOptInterface.get  ‚Äî  Method MOI.get(model::Optimizer, attr::MOI.RawOptimizerAttribute) Return last  value  set by  MOI.set(model, attr, value) . source"},{"id":2397,"pagetitle":"Extensions","title":"MathOptInterface.set","ref":"/manopt/stable/extensions/#MathOptInterface.set-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.RawOptimizerAttribute, Any}","content":" MathOptInterface.set  ‚Äî  Method MOI.get(model::Optimizer, attr::MOI.RawOptimizerAttribute) Set the value for the keyword argument  attr.name  to give for the constructor  model.options[DESCENT_STATE_TYPE] . source"},{"id":2398,"pagetitle":"Extensions","title":"MathOptInterface.supports_incremental_interface","ref":"/manopt/stable/extensions/#MathOptInterface.supports_incremental_interface-Tuple{ManoptJuMPExt.Optimizer}","content":" MathOptInterface.supports_incremental_interface  ‚Äî  Method MOI.supports_incremental_interface(::JuMP_Optimizer) Return  true  indicating that  Manopt.JuMP_Optimizer  implements  MOI.add_constrained_variables  and  MOI.set  for  MOI.ObjectiveFunction  so it can be used with  JuMP.direct_model  and does not require a  MOI.Utilities.CachingOptimizer . See  MOI.supports_incremental_interface . source"},{"id":2399,"pagetitle":"Extensions","title":"MathOptInterface.copy_to","ref":"/manopt/stable/extensions/#MathOptInterface.copy_to-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.ModelLike}","content":" MathOptInterface.copy_to  ‚Äî  Method MOI.copy_to(dest::Optimizer, src::MOI.ModelLike) Because  supports_incremental_interface(dest)  is  true , this simply uses  MOI.Utilities.default_copy_to  and copies the variables with  MOI.add_constrained_variables  and the objective sense with  MOI.set . source"},{"id":2400,"pagetitle":"Extensions","title":"MathOptInterface.supports_add_constrained_variables","ref":"/manopt/stable/extensions/#MathOptInterface.supports_add_constrained_variables-Tuple{ManoptJuMPExt.Optimizer, Type{<:ManoptJuMPExt.VectorizedManifold}}","content":" MathOptInterface.supports_add_constrained_variables  ‚Äî  Method MOI.supports_add_constrained_variables(::JuMP_Optimizer, ::Type{<:VectorizedManifold}) Return  true  indicating that  Manopt.JuMP_Optimizer  support optimization on variables constrained to belong in a vectorized manifold  Manopt.JuMP_VectorizedManifold . source"},{"id":2401,"pagetitle":"Extensions","title":"MathOptInterface.add_constrained_variables","ref":"/manopt/stable/extensions/#MathOptInterface.add_constrained_variables-Tuple{ManoptJuMPExt.Optimizer, ManoptJuMPExt.VectorizedManifold}","content":" MathOptInterface.add_constrained_variables  ‚Äî  Method MOI.add_constrained_variables(model::Optimizer, set::VectorizedManifold) Add  MOI.dimension(set)  variables constrained in  set  and return the list of variable indices that can be used to reference them as well a constraint index for the constraint enforcing the membership of the variables in the  Manopt.JuMP_VectorizedManifold set . source"},{"id":2402,"pagetitle":"Extensions","title":"MathOptInterface.is_valid","ref":"/manopt/stable/extensions/#MathOptInterface.is_valid-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.VariableIndex}","content":" MathOptInterface.is_valid  ‚Äî  Method MOI.is_valid(model::Optimizer, vi::MOI.VariableIndex) Return whether  vi  is a valid variable index. source"},{"id":2403,"pagetitle":"Extensions","title":"MathOptInterface.get","ref":"/manopt/stable/extensions/#MathOptInterface.get-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.NumberOfVariables}","content":" MathOptInterface.get  ‚Äî  Method MOI.get(model::Optimizer, ::MOI.NumberOfVariables) Return the number of variables added in the model, this corresponds to the  MOI.dimension  of the  Manopt.JuMP_VectorizedManifold . source"},{"id":2404,"pagetitle":"Extensions","title":"MathOptInterface.supports","ref":"/manopt/stable/extensions/#MathOptInterface.supports-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.VariablePrimalStart, Type{MathOptInterface.VariableIndex}}","content":" MathOptInterface.supports  ‚Äî  Method MOI.supports(::Manopt.JuMP_Optimizer, attr::MOI.RawOptimizerAttribute) Return  true  indicating that  Manopt.JuMP_Optimizer  supports starting values for the variables. source"},{"id":2405,"pagetitle":"Extensions","title":"MathOptInterface.set","ref":"/manopt/stable/extensions/#MathOptInterface.set-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.VariablePrimalStart, MathOptInterface.VariableIndex, Union{Nothing, Real}}","content":" MathOptInterface.set  ‚Äî  Method function MOI.set(\n    model::Optimizer,\n    ::MOI.VariablePrimalStart,\n    vi::MOI.VariableIndex,\n    value::Union{Real,Nothing},\n) Set the starting value of the variable of index  vi  to  value . Note that if  value  is  nothing  then it essentially unset any previous starting values set and hence  MOI.optimize!  unless another starting value is set. source"},{"id":2406,"pagetitle":"Extensions","title":"MathOptInterface.set","ref":"/manopt/stable/extensions/#MathOptInterface.set-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.ObjectiveSense, MathOptInterface.OptimizationSense}","content":" MathOptInterface.set  ‚Äî  Method MOI.set(model::Optimizer, ::MOI.ObjectiveSense, sense::MOI.OptimizationSense) Modify the objective sense to either  MOI.MAX_SENSE ,  MOI.MIN_SENSE  or  MOI.FEASIBILITY_SENSE . source"},{"id":2407,"pagetitle":"Extensions","title":"MathOptInterface.set","ref":"/manopt/stable/extensions/#MathOptInterface.set-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.ObjectiveFunction, MathOptInterface.AbstractScalarFunction}","content":" MathOptInterface.set  ‚Äî  Method MOI.set(model::Optimizer, ::MOI.ObjectiveFunction{F}, func::F) where {F} Set the objective function as  func  for  model . source"},{"id":2408,"pagetitle":"Extensions","title":"MathOptInterface.supports","ref":"/manopt/stable/extensions/#MathOptInterface.supports-Tuple{ManoptJuMPExt.Optimizer, Union{MathOptInterface.ObjectiveSense, MathOptInterface.ObjectiveFunction}}","content":" MathOptInterface.supports  ‚Äî  Method MOI.supports(::Optimizer, ::Union{MOI.ObjectiveSense,MOI.ObjectiveFunction}) Return  true  indicating that  Optimizer  supports being set the objective sense (that is, min, max or feasibility) and the objective function. source"},{"id":2409,"pagetitle":"Extensions","title":"JuMP.build_variable","ref":"/manopt/stable/extensions/#JuMP.build_variable-Tuple{Function, Any, AbstractManifold}","content":" JuMP.build_variable  ‚Äî  Method JuMP.build_variable(::Function, func, m::ManifoldsBase.AbstractManifold) Build a  JuMP.VariablesConstrainedOnCreation  object containing variables and the  Manopt.JuMP_VectorizedManifold  in which they should belong as well as the  shape  that can be used to go from the vectorized MOI representation to the shape of the manifold, that is,  Manopt.JuMP_ArrayShape . source"},{"id":2410,"pagetitle":"Extensions","title":"MathOptInterface.get","ref":"/manopt/stable/extensions/#MathOptInterface.get-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.ResultCount}","content":" MathOptInterface.get  ‚Äî  Method MOI.get(model::Optimizer, ::MOI.ResultCount) Return  0  if  optimize!  hasn't been called yet and  1  otherwise indicating that one solution is available. source"},{"id":2411,"pagetitle":"Extensions","title":"MathOptInterface.get","ref":"/manopt/stable/extensions/#MathOptInterface.get-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.SolverName}","content":" MathOptInterface.get  ‚Äî  Method MOI.get(::Optimizer, ::MOI.SolverName) Return the name of the  Optimizer  with the value of the  descent_state_type  option. source"},{"id":2412,"pagetitle":"Extensions","title":"MathOptInterface.get","ref":"/manopt/stable/extensions/#MathOptInterface.get-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.ObjectiveValue}","content":" MathOptInterface.get  ‚Äî  Method MOI.get(model::Optimizer, attr::MOI.ObjectiveValue) Return the value of the objective function evaluated at the solution. source"},{"id":2413,"pagetitle":"Extensions","title":"MathOptInterface.get","ref":"/manopt/stable/extensions/#MathOptInterface.get-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.PrimalStatus}","content":" MathOptInterface.get  ‚Äî  Method MOI.get(model::Optimizer, ::MOI.PrimalStatus) Return  MOI.NO_SOLUTION  if  optimize!  hasn't been called yet and  MOI.FEASIBLE_POINT  otherwise indicating that a solution is available to query with  MOI.VariablePrimalStart . source"},{"id":2414,"pagetitle":"Extensions","title":"MathOptInterface.get","ref":"/manopt/stable/extensions/#MathOptInterface.get-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.DualStatus}","content":" MathOptInterface.get  ‚Äî  Method MOI.get(::Optimizer, ::MOI.DualStatus) Returns  MOI.NO_SOLUTION  indicating that there is no dual solution available. source"},{"id":2415,"pagetitle":"Extensions","title":"MathOptInterface.get","ref":"/manopt/stable/extensions/#MathOptInterface.get-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.TerminationStatus}","content":" MathOptInterface.get  ‚Äî  Method MOI.get(model::Optimizer, ::MOI.ResultCount) Return  MOI.OPTIMIZE_NOT_CALLED  if  optimize!  hasn't been called yet and  MOI.LOCALLY_SOLVED  otherwise indicating that the solver has solved the problem to local optimality the value of  MOI.RawStatusString  for more details on why the solver stopped. source"},{"id":2416,"pagetitle":"Extensions","title":"MathOptInterface.get","ref":"/manopt/stable/extensions/#MathOptInterface.get-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.SolverVersion}","content":" MathOptInterface.get  ‚Äî  Method MOI.get(::Optimizer, ::MOI.SolverVersion) Return the version of the Manopt solver, it corresponds to the version of Manopt.jl. source"},{"id":2417,"pagetitle":"Extensions","title":"MathOptInterface.get","ref":"/manopt/stable/extensions/#MathOptInterface.get-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.ObjectiveSense}","content":" MathOptInterface.get  ‚Äî  Method MOI.get(model::Optimizer, ::MOI.ObjectiveSense) Return the objective sense, defaults to  MOI.FEASIBILITY_SENSE  if no sense has already been set. source"},{"id":2418,"pagetitle":"Extensions","title":"MathOptInterface.get","ref":"/manopt/stable/extensions/#MathOptInterface.get-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.VariablePrimal, MathOptInterface.VariableIndex}","content":" MathOptInterface.get  ‚Äî  Method MOI.get(model::Optimizer, attr::MOI.VariablePrimal, vi::MOI.VariableIndex) Return the value of the solution for the variable of index  vi . source"},{"id":2419,"pagetitle":"Extensions","title":"MathOptInterface.get","ref":"/manopt/stable/extensions/#MathOptInterface.get-Tuple{ManoptJuMPExt.Optimizer, MathOptInterface.RawStatusString}","content":" MathOptInterface.get  ‚Äî  Method MOI.get(model::Optimizer, ::MOI.RawStatusString) Return a  String  containing  Manopt.get_reason  without the ending newline character. source"},{"id":2422,"pagetitle":"Checks","title":"Verifying gradients and Hessians","ref":"/manopt/stable/helpers/checks/#Verifying-gradients-and-Hessians","content":" Verifying gradients and Hessians If you have computed a gradient or differential and you are not sure whether it is correct."},{"id":2423,"pagetitle":"Checks","title":"Manopt.check_Hessian","ref":"/manopt/stable/helpers/checks/#Manopt.check_Hessian","content":" Manopt.check_Hessian  ‚Äî  Function check_Hessian(M, f, grad_f, Hess_f, p=rand(M), X=rand(M; vector_at=p), Y=rand(M, vector_at=p); kwargs...) Verify numerically whether the Hessian  Hess_f(M,p, X)  of  f(M,p)  is correct. For this either a second-order retraction or a critical point  $p$  of  f  is required. The approximation is then \\[f(\\operatorname{retr}_p(tX)) = f(p) + t‚ü®\\operatorname{grad} f(p), X‚ü© + \\frac{t^2}{2}‚ü®\\operatorname{Hess}f(p)[X], X‚ü© + \\mathcal O(t^3)\\] or in other words, that the error between the function  $f$  and its second order Taylor behaves in error  $\\mathcal O(t^3)$ , which indicates that the Hessian is correct, cf. also [ Bou23 , Section 6.8]. Note that if the errors are below the given tolerance and the method is exact, no plot is generated. Keyword arguments check_grad=true : verify that  $\\operatorname{grad}f(p) ‚àà T_{p}\\mathcal M$ . check_linearity=true : verify that the Hessian is linear, see  is_Hessian_linear  using  a ,  b ,  X , and  Y check_symmetry=true : verify that the Hessian is symmetric, see  is_Hessian_symmetric check_vector=false : verify that  \\operatorname{Hess} f(p)[X] ‚àà T_{p}\\mathcal M using is_vector`. mode=:Default : specify the mode for the verification; the default assumption is, that the retraction provided is of second order. Otherwise one can also verify the Hessian if the point  p  is a critical point. THen set the mode to  :CritalPoint  to use  gradient_descent  to find a critical point. Note: this requires (and evaluates) new tangent vectors  X  and  Y atol ,  rtol :      (same defaults as  isapprox ) tolerances that are passed down to all checks a ,  b             two real values to verify linearity of the Hessian (if  check_linearity=true ) N=101 : number of points to verify within the  log_range  default range  $[10^{-8},10^{0}]$ exactness_tol=1e-12 : if all errors are below this tolerance, the verification is considered to be exact io=nothing : provide an  IO  to print the result to gradient=grad_f(M, p) : instead of the gradient function you can also provide the gradient at  p  directly Hessian=Hess_f(M, p, X) : instead of the Hessian function you can provide the result of  $\\operatorname{Hess} f(p)[X]$  directly. Note that evaluations of the Hessian might still be necessary for checking linearity and symmetry and/or when using  :CriticalPoint  mode. limits=(-8.0, 0.0) : specify the limits in the  log_range log_range=range(limits[1], limits[2]; length=N) : specify the range of points (in log scale) to sample the Hessian line N=101 : number of points to use within the  log_range  default range  $[10^{-8},10^{0}]$ plot=false : whether to plot the resulting verification (requires  Plots.jl  to be loaded). The plot is in log-log-scale. This is returned and can then also be saved. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions slope_tol=0.1 : tolerance for the slope (global) of the approximation error=:none : how to handle errors, possible values:  :error ,  :info ,  :warn window=nothing : specify window sizes within the  log_range  that are used for the slope estimation. the default is, to use all window sizes  2:N . The  kwargs...  are also passed down to the  check_vector  and the  check_gradient  call, such that tolerances can easily be set. While  check_vector  is also passed to the inner call to  check_gradient  as well as the  retraction_method , this inner  check_gradient  is meant to be just for inner verification, so it does not throw an error nor produce a plot itself. source"},{"id":2424,"pagetitle":"Checks","title":"Manopt.check_differential","ref":"/manopt/stable/helpers/checks/#Manopt.check_differential","content":" Manopt.check_differential  ‚Äî  Function check_differential(M, F, dF, p=rand(M), X=rand(M; vector_at=p); kwargs...) Check numerically whether the differential  dF(M,p,X)  of  F(M,p)  is correct. This implements the method described in [ Bou23 , Section 4.8]. Note that if the errors are below the given tolerance and the method is exact, no plot is generated, Keyword arguments exactness_tol=1e-12 : if all errors are below this tolerance, the differential is considered to be exact io=nothing : provide an  IO  to print the result to limits=(-8.0, 0.0) : specify the limits in the  log_range log_range=range(limits[1], limits[2]; length=N) : specify the range of points (in log scale) to sample the differential line N=101 : number of points to verify within the  log_range  default range  $[10^{-8},10^{0}]$ name=\"differential\" : name to display in the plot plot=false : whether to plot the result (if  Plots.jl  is loaded). The plot is in log-log-scale. This is returned and can then also be saved. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions slope_tol=0.1 : tolerance for the slope (global) of the approximation throw_error=false : throw an error message if the differential is wrong window=nothing : specify window sizes within the  log_range  that are used for the slope estimation. The default is, to use all window sizes  2:N . source"},{"id":2425,"pagetitle":"Checks","title":"Manopt.check_gradient","ref":"/manopt/stable/helpers/checks/#Manopt.check_gradient","content":" Manopt.check_gradient  ‚Äî  Function check_gradient(M, f, grad_f, p=rand(M), X=rand(M; vector_at=p); kwargs...) Verify numerically whether the gradient  grad_f(M,p)  of  f(M,p)  is correct, that is whether \\[f(\\operatorname{retr}_p(tX)) = f(p) + t‚ü®\\operatorname{grad} f(p), X‚ü© + \\mathcal O(t^2)\\] or in other words, that the error between the function  $f$  and its first order Taylor behaves in error  $\\mathcal O(t^2)$ , which indicates that the gradient is correct, cf. also [ Bou23 , Section 4.8]. Note that if the errors are below the given tolerance and the method is exact, no plot is generated. Keyword arguments check_vector=true : verify that  $\\operatorname{grad}f(p) ‚àà T_{p}\\mathcal M$  using  is_vector . exactness_tol=1e-12 : if all errors are below this tolerance, the gradient is considered to be exact io=nothing : provide an  IO  to print the result to gradient=grad_f(M, p) : instead of the gradient function you can also provide the gradient at  p  directly limits=(-8.0, 0.0) : specify the limits in the  log_range log_range=range(limits[1], limits[2]; length=N) : specify the range of points (in log scale) to sample the gradient line N=101 : number of points to verify within the  log_range  default range  $[10^{-8},10^{0}]$ plot=false : whether to plot the result (if  Plots.jl  is loaded). The plot is in log-log-scale. This is returned and can then also be saved. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions slope_tol=0.1 : tolerance for the slope (global) of the approximation atol ,  rtol : (same defaults as  isapprox ) tolerances that are passed down to  is_vector  if  check_vector  is set to  true error=:none : how to handle errors, possible values:  :error ,  :info ,  :warn window=nothing : specify window sizes within the  log_range  that are used for the slope estimation. the default is, to use all window sizes  2:N . The remaining keyword arguments are also passed down to the  check_vector  call, such that tolerances can easily be set. source"},{"id":2426,"pagetitle":"Checks","title":"Manopt.is_Hessian_linear","ref":"/manopt/stable/helpers/checks/#Manopt.is_Hessian_linear","content":" Manopt.is_Hessian_linear  ‚Äî  Function is_Hessian_linear(M, Hess_f, p,\n    X=rand(M; vector_at=p), Y=rand(M; vector_at=p), a=randn(), b=randn();\n    error=:none, io=nothing, kwargs...\n) Verify whether the Hessian function  Hess_f  fulfills linearity, \\[\\operatorname{Hess} f(p)[aX + bY] = b\\operatorname{Hess} f(p)[X]\n + b\\operatorname{Hess} f(p)[Y]\\] which is checked using  isapprox  and the keyword arguments are passed to this function. Optional arguments error=:none : how to handle errors, possible values:  :error ,  :info ,  :warn source"},{"id":2427,"pagetitle":"Checks","title":"Manopt.is_Hessian_symmetric","ref":"/manopt/stable/helpers/checks/#Manopt.is_Hessian_symmetric","content":" Manopt.is_Hessian_symmetric  ‚Äî  Function is_Hessian_symmetric(M, Hess_f, p=rand(M), X=rand(M; vector_at=p), Y=rand(M; vector_at=p);\nerror=:none, io=nothing, atol::Real=0, rtol::Real=atol>0 ? 0 : ‚àöeps ) Verify whether the Hessian function  Hess_f  fulfills symmetry, which means that \\[‚ü®\\operatorname{Hess} f(p)[X], Y‚ü© = ‚ü®X, \\operatorname{Hess} f(p)[Y]‚ü©\\] which is checked using  isapprox  and the  kwargs...  are passed to this function. Optional arguments atol ,  rtol    with the same defaults as the usual  isapprox error=:none : how to handle errors, possible values:  :error ,  :info ,  :warn source"},{"id":2428,"pagetitle":"Checks","title":"Literature","ref":"/manopt/stable/helpers/checks/#Literature","content":" Literature [Bou23] N.¬†Boumal.  An Introduction to Optimization on Smooth Manifolds . First¬†Edition ( Cambridge University Press, 2023 )."},{"id":2431,"pagetitle":"Exports","title":"Exports","ref":"/manopt/stable/helpers/exports/#sec-exports","content":" Exports Exports aim to provide a consistent generation of images of your results. For example if you  record  the trace your algorithm walks on the  Sphere , you can easily export this trace to a rendered image using  asymptote_export_S2_signals  and render the result with  Asymptote . Despite these, you can always  record  values during your iterations, and export these, for example to  csv ."},{"id":2432,"pagetitle":"Exports","title":"Asymptote","ref":"/manopt/stable/helpers/exports/#Asymptote","content":" Asymptote The following functions provide exports both in graphics and/or raw data using  Asymptote ."},{"id":2433,"pagetitle":"Exports","title":"Manopt.asymptote_export_S2_data","ref":"/manopt/stable/helpers/exports/#Manopt.asymptote_export_S2_data-Tuple{String}","content":" Manopt.asymptote_export_S2_data  ‚Äî  Method asymptote_export_S2_data(filename) Export given  data  as an array of points on the 2-sphere, which might be one-, two- or three-dimensional data with points on the  Sphere $\\mathbb S^2$ . Input filename                 a file to store the Asymptote code in. Optional arguments for the data data                     a point representing the 1D,2D, or 3D array of points elevation_color_scheme   A  ColorScheme  for elevation scale_axes=(1/3,1/3,1/3) : move spheres closer to each other by a factor per direction Optional arguments for asymptote arrow_head_size=1.8 : size of the arrowheads of the vectors (in mm) camera_position   position of the camera scene (default: atop the center of the data in the xy-plane) target            position the camera points at (default: center of xy-plane within data). source"},{"id":2434,"pagetitle":"Exports","title":"Manopt.asymptote_export_S2_signals","ref":"/manopt/stable/helpers/exports/#Manopt.asymptote_export_S2_signals-Tuple{String}","content":" Manopt.asymptote_export_S2_signals  ‚Äî  Method asymptote_export_S2_signals(filename; points, curves, tangent_vectors, colors, kwargs...) Export given  points ,  curves , and  tangent_vectors  on the sphere  $\\mathbb S^2$  to Asymptote. Input filename           a file to store the Asymptote code in. Keywaord arguments for the data colors=Dict{Symbol,Array{RGBA{Float64},1}}() : dictionary of color arrays, indexed by symbols  :points ,  :curves  and  :tvector , where each entry has to provide as least as many colors as the length of the corresponding sets. curves=Array{Array{Float64,1},1}(undef, 0) : an  Array  of  Arrays  of points on the sphere, where each inner array is interpreted as a curve and is accompanied by an entry within  colors . points=Array{Array{Float64,1},1}(undef, 0) : an  Array  of  Arrays  of points on the sphere where each inner array is interpreted as a set of points and is accompanied by an entry within  colors . tangent_vectors=Array{Array{Tuple{Float64,Float64},1},1}(undef, 0) : an  Array  of  Arrays  of tuples, where the first is a points, the second a tangent vector and each set of vectors is accompanied by an entry from within  colors . Keyword arguments for asymptote arrow_head_size=6.0 : size of the arrowheads of the tangent vectors arrow_head_sizes   overrides the previous value to specify a value per  tVector ` set. camera_position=(1., 1., 0.) : position of the camera in the Asymptote scene line_width=1.0 : size of the lines used to draw the curves. line_widths        overrides the previous value to specify a value per curve and  tVector ` set. dot_size=1.0 : size of the dots used to draw the points. dot_sizes          overrides the previous value to specify a value per point set. size=nothing : a tuple for the image size, otherwise a relative size  4cm  is used. sphere_color=RGBA{Float64}(0.85, 0.85, 0.85, 0.6) : color of the sphere the data is drawn on sphere_line_color=RGBA{Float64}(0.75, 0.75, 0.75, 0.6) : color of the lines on the sphere sphere_line_width=0.5 : line width of the lines on the sphere target=(0.,0.,0.) : position the camera points at source"},{"id":2435,"pagetitle":"Exports","title":"Manopt.asymptote_export_SPD","ref":"/manopt/stable/helpers/exports/#Manopt.asymptote_export_SPD-Tuple{String}","content":" Manopt.asymptote_export_SPD  ‚Äî  Method asymptote_export_SPD(filename) export given  data  as a point on a  Power(SymmetricPOsitiveDefinnite(3))}  manifold of one-, two- or three-dimensional data with points on the manifold of symmetric positive definite matrices. Input filename         a file to store the Asymptote code in. Optional arguments for the data data             a point representing the 1D, 2D, or 3D array of SPD matrices color_scheme     a  ColorScheme  for Geometric Anisotropy Index scale_axes=(1/3,1/3,1/3) : move symmetric positive definite matrices closer to each other by a factor per direction compared to the distance estimated by the maximal eigenvalue of all involved SPD points Optional arguments for asymptote camera_position   position of the camera scene (default: atop the center of the data in the xy-plane) target            position the camera points at (default: center of xy-plane within data). Both values  camera_position  and  target  are scaled by  scaledAxes*EW , where  EW  is the maximal eigenvalue in the  data . source"},{"id":2436,"pagetitle":"Exports","title":"Manopt.render_asymptote","ref":"/manopt/stable/helpers/exports/#Manopt.render_asymptote-Tuple{Any}","content":" Manopt.render_asymptote  ‚Äî  Method render_asymptote(filename; render=4, format=\"png\", ...) render an exported asymptote file specified in the  filename , which can also be given as a relative or full path Input filename     filename of the exported  asy  and rendered image Keyword arguments the default values are given in brackets render=4 : render level of asymptote passed to its  -render  option.  This can be removed from the command by setting it to  nothing . format=\"png\" : final rendered format passed to the  -f  option export_file : (the filename with format as ending) specify the export filename source"},{"id":2439,"pagetitle":"Notation","title":"Notation","ref":"/manopt/stable/notation/#Notation","content":" Notation In this package,the notation introduced in  Manifolds.jl Notation  is used with the following additional parts. Symbol Description Also used Comment $\\operatorname{arg\\,min}$ argument of a function  $f$  where a local or global minimum is attained $k$ the current iterate $√¨$ the goal is to unify this to  k $‚àá$ The  Levi-Cevita connection"},{"id":2442,"pagetitle":"Specify a Solver","title":"Plans for solvers","ref":"/manopt/stable/plans/#sec-plan","content":" Plans for solvers For any optimisation performed in  Manopt.jl  information is required about both the optimisation task or ‚Äúproblem‚Äù at hand as well as the solver and all its parameters. This together is called a  plan  in  Manopt.jl  and it consists of two data structures: The  Manopt Problem  describes all  static  data of a task, most prominently the manifold and the objective. The  Solver State  describes all  varying  data and parameters for the solver that is used. This also means that each solver has its own data structure for the state. By splitting these two parts, one problem can be define an then be solved  using different solvers. Still there might be the need to set certain parameters within any of these structures. For that there is"},{"id":2443,"pagetitle":"Specify a Solver","title":"Manopt.set_parameter!","ref":"/manopt/stable/plans/#Manopt.set_parameter!","content":" Manopt.set_parameter!  ‚Äî  Function set_parameter!(f, element::Symbol , args...) For any  f  and a  Symbol e , dispatch on its value so by default, to set some  args...  in  f  or one of uts sub elements. source set_parameter!(element::Symbol, value::Union{String,Bool,<:Number}) Set global  Manopt  parameters addressed by a symbol  element . W This first dispatches on the value of  element . The parameters are stored to the global settings using  Preferences.jl . Passing a  value  of  \"\"  deletes the corresponding entry from the preferences. Whenever the  LocalPreferences.toml  is modified, this is also issued as an  @info . source set_parameter!(amo::AbstractManifoldObjective, element::Symbol, args...) Set a certain  args...  from the  AbstractManifoldObjective amo  to  value. This function should dispatch on Val(element)`. Currently supported :Cost  passes to the  get_cost_function :Gradient  passes to the  get_gradient_function :SubGradient  passes to the  get_subgradient_function source set_parameter!(ams::AbstractManoptProblem, element::Symbol, field::Symbol , value) Set a certain field/element from the  AbstractManoptProblem ams  to  value . This function usually dispatches on  Val(element) . Instead of a single field, also a chain of elements can be provided, allowing to access encapsulated parts of the problem. Main values for  element  are  :Manifold  and  :Objective . source set_parameter!(ams::DebugSolverState, ::Val{:Debug}, args...) Set certain values specified by  args...  into the elements of the  debugDictionary source set_parameter!(ams::RecordSolverState, ::Val{:Record}, args...) Set certain values specified by  args...  into the elements of the  recordDictionary source set_parameter!(c::StopAfter, :MaxTime, v::Period) Update the time period after which an algorithm shall stop. source set_parameter!(c::StopAfterIteration, :;MaxIteration, v::Int) Update the number of iterations after which the algorithm should stop. source set_parameter!(c::StopWhenChangeLess, :MinIterateChange, v::Int) Update the minimal change below which an algorithm shall stop. source set_parameter!(c::StopWhenCostLess, :MinCost, v) Update the minimal cost below which the algorithm shall stop source set_parameter!(c::StopWhenEntryChangeLess, :Threshold, v) Update the minimal cost below which the algorithm shall stop source set_parameter!(c::StopWhenGradientChangeLess, :MinGradientChange, v) Update the minimal change below which an algorithm shall stop. source set_parameter!(c::StopWhenGradientNormLess, :MinGradNorm, v::Float64) Update the minimal gradient norm when an algorithm shall stop source set_parameter!(c::StopWhenStepsizeLess, :MinStepsize, v) Update the minimal step size below which the algorithm shall stop source set_parameter!(c::StopWhenSubgradientNormLess, :MinSubgradNorm, v::Float64) Update the minimal subgradient norm when an algorithm shall stop source set_parameter!(ams::AbstractManoptSolverState, element::Symbol, args...) Set a certain field or semantic element from the  AbstractManoptSolverState ams  to  value . This function passes to  Val(element)  and specific setters should dispatch on  Val{element} . By default, this function just does nothing. source set_parameter!(ams::DebugSolverState, ::Val{:SubProblem}, args...) Set certain values specified by  args...  to the sub problem. source set_parameter!(ams::DebugSolverState, ::Val{:SubState}, args...) Set certain values specified by  args...  to the sub state. source set_parameter!(c::StopWhenResidualIsReducedByFactorOrPower, :ResidualPower, v) Update the residual Power  Œ∏   to  v . source set_parameter!(c::StopWhenResidualIsReducedByFactorOrPower, :ResidualFactor, v) Update the residual Factor  Œ∫  to  v . source"},{"id":2444,"pagetitle":"Specify a Solver","title":"Manopt.get_parameter","ref":"/manopt/stable/plans/#Manopt.get_parameter","content":" Manopt.get_parameter  ‚Äî  Function get_parameter(f, element::Symbol, args...) Access arbitrary parameters from  f  addressed by a symbol  element . For any  f  and a  Symbol e  dispatch on its value by default, to get some element from  f  potentially further qualified by  args... . This functions returns  nothing  if  f  does not have the property  element source get_parameter(element::Symbol; default=nothing) Access global  Manopt  parameters addressed by a symbol  element . This first dispatches on the value of  element . If the value is not set,  default  is returned. The parameters are queried from the global settings using  Preferences.jl , so they are persistent within your activated Environment. Currently used settings :Mode  the mode can be set to  \"Tutorial\"  to get several hints especially in scenarios, where the optimisation on manifolds is different from the usual ‚Äúexperience‚Äù in (classical, Euclidean) optimization. Any other value has the same effect as not setting it. source"},{"id":2445,"pagetitle":"Specify a Solver","title":"Manopt.status_summary","ref":"/manopt/stable/plans/#Manopt.status_summary","content":" Manopt.status_summary  ‚Äî  Function status_summary(e) Return a string reporting about the current status of  e , where  e  is a type from Manopt. This method is similar to  show  but just returns a string. It might also be more verbose in explaining, or hide internal information. source The following symbols are used. Symbol Used in Description :Activity DebugWhenActive activity of the debug action stored within :Basepoint TangentSpace the point the tangent space is at :Cost generic the cost function (within an objective, as pass down) :Debug DebugSolverState the stored  debugDictionary :Gradient generic the gradient function (within an objective, as pass down) :Iterate generic the (current) iterate,¬†similar to  set_iterate! ,¬†within a state :Manifold generic the manifold (within a problem, as pass down) :Objective generic the objective (within a problem, as pass down) :SubProblem generic the sub problem (within a state, as pass down) :SubState generic the sub state (within a state, as pass down) :Œª ProximalDCCost ,  ProximalDCGrad set the proximal parameter within the proximal sub objective elements :Population ParticleSwarmState a certain population of points, for example  particle_swarm s swarm :Record RecordSolverState :TrustRegionRadius TrustRegionsState the trust region radius, equivalent to  :œÉ :œÅ ,  :u ExactPenaltyCost ,  ExactPenaltyGrad Parameters within the exact penalty objective :œÅ ,  :Œº ,  :Œª AugmentedLagrangianCost ,  AugmentedLagrangianGrad Parameters of the Lagrangian function :p ,  :X LinearizedDCCost ,  LinearizedDCGrad Parameters withing the linearized functional used for the sub problem of the  difference of convex algorithm Any other lower case name or letter as well as single upper case letters access fields of the corresponding first argument. for example  :p  could be used to access the field  s.p  of a state. This is often, where the iterate is stored, so the recommended way is to use  :Iterate  from before. Since the iterate is often stored in the states fields  s.p  one  could  access the iterate often also with  :p  and similarly the gradient with  :X . This is discouraged for both readability as well as to stay more generic, and it is recommended to use  :Iterate  and  :Gradient  instead in generic settings. You can further activate a ‚ÄúTutorial‚Äù mode by  set_parameter!(:Mode, \"Tutorial\") . Internally, the following convenience function is available."},{"id":2446,"pagetitle":"Specify a Solver","title":"Manopt.is_tutorial_mode","ref":"/manopt/stable/plans/#Manopt.is_tutorial_mode","content":" Manopt.is_tutorial_mode  ‚Äî  Function is_tutorial_mode() A small internal helper to indicate whether tutorial mode is active. You can set the mode by calling  set_parameter!(:Mode, \"Tutorial\")  or deactivate it by  set_parameter!(:Mode, \"\") . source"},{"id":2447,"pagetitle":"Specify a Solver","title":"A factory for providing manifold defaults","ref":"/manopt/stable/plans/#A-factory-for-providing-manifold-defaults","content":" A factory for providing manifold defaults In several cases a manifold might not yet be known at the time a (keyword) argument should be provided. Therefore, any type with a manifold default can be wrapped into a factory."},{"id":2448,"pagetitle":"Specify a Solver","title":"Manopt.ManifoldDefaultsFactory","ref":"/manopt/stable/plans/#Manopt.ManifoldDefaultsFactory","content":" Manopt.ManifoldDefaultsFactory  ‚Äî  Type ManifoldDefaultsFactory{M,T,A,K} A generic factory to postpone the instantiation of certain types from within  Manopt.jl , in order to be able to adapt it to defaults from different manifolds and/or postpone the decission on which manifold to use to a later point For now this is established for DirectionUpdateRule s Stepsize StoppingCriterion This factory stores necessary and optional parameters as well as keyword arguments provided by the user to later produce the type this factory is for. Besides a manifold as a fallback, the factory can also be used for the (maybe simpler) types from the list of types that do not require the manifold. Fields M::Union{Nothing,AbstractManifold} :  provide a manifold for defaults args::A :                             arguments ( args... ) that are passed to the type constructor kwargs::K :                           keyword arguments ( kwargs... ) that are passed to the type constructor constructor_requires_manifold::Bool : indicate whether the type construtor requires the manifold or not Constructor ManifoldDefaultsFactory(T, args...; kwargs...)\nManifoldDefaultsFactory(T, M, args...; kwargs...) Input T  a subtype of types listed above that this factory is to produce M  (optional) a manifold used for the defaults in case no manifold is provided. args...  arguments to pass to the constructor of  T kwargs...  keyword arguments to pass (overwrite) when constructing  T . Keyword arguments requires_manifold=true : indicate whether the type constructor this factory wraps requires the manifold as first argument or not. All other keyword arguments are internally stored to be used in the type constructor as well as arguments and keyword arguments for the update rule. see also _produce_type source"},{"id":2449,"pagetitle":"Specify a Solver","title":"Manopt._produce_type","ref":"/manopt/stable/plans/#Manopt._produce_type","content":" Manopt._produce_type  ‚Äî  Function _produce_type(t::T, M::AbstractManifold)\n_produce_type(t::ManifoldDefaultsFactory{T}, M::AbstractManifold) Use the  ManifoldDefaultsFactory {T}  to produce an instance of type  T . This acts transparent in the way that if you provide an instance  t::T  already, this will just be returned. source"},{"id":2452,"pagetitle":"Debug Output","title":"Debug output","ref":"/manopt/stable/plans/debug/#sec-debug","content":" Debug output Debug output can easily be added to any solver run. On the high level interfaces, like  gradient_descent , you can just use the  debug=  keyword."},{"id":2453,"pagetitle":"Debug Output","title":"Manopt.DebugAction","ref":"/manopt/stable/plans/debug/#Manopt.DebugAction","content":" Manopt.DebugAction  ‚Äî  Type DebugAction A  DebugAction  is a small functor to print/issue debug output. The usual call is given by  (p::AbstractManoptProblem, s::AbstractManoptSolverState, k) -> s , where  i  is the current iterate. By convention  i=0  is interpreted as \"For Initialization only,\" only debug info that prints initialization reacts,  i<0  triggers updates of variables internally but does not trigger any output. Fields (assumed by subtypes to exist) print  method to perform the actual print. Can for example be set to a file export, or to @info. The default is the  print  function on the default  Base.stdout . source"},{"id":2454,"pagetitle":"Debug Output","title":"Manopt.DebugCallback","ref":"/manopt/stable/plans/debug/#Manopt.DebugCallback","content":" Manopt.DebugCallback  ‚Äî  Type DebugCallback <: DebugAction Debug for a simple callback function, mainly for compatibility to other solvers and if a user already has a callback function or functor available The expected format of the is that it is a function with signature  (problem, state, iteration) -> nothing  A simple callbaclk of the signature  () -> nothing  can be specified by  simple=true . In this case the callback is wrapped in a function of the generic form Note This is for now an internal struct, since its name might still change before it is made public. The functionality with the factory ( callback=f ) will still work, but this debug actions name might still change its name in the future. Constructor DebugCallback(callback; simple=false) source"},{"id":2455,"pagetitle":"Debug Output","title":"Manopt.DebugChange","ref":"/manopt/stable/plans/debug/#Manopt.DebugChange","content":" Manopt.DebugChange  ‚Äî  Type DebugChange(M=DefaultManifold(); kwargs...) debug for the amount of change of the iterate (stored in  get_iterate(o)  of the  AbstractManoptSolverState ) during the last iteration. See  DebugEntryChange  for the general case Keyword parameters storage= StoreStateAction ( [:Gradient] )  storage of the previous action prefix=\"Last Change:\" : prefix of the debug output (ignored if you set  format ) io=stdout : default stream to print the debug to. inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses the inverse retraction   to be used for approximating distance. source"},{"id":2456,"pagetitle":"Debug Output","title":"Manopt.DebugCost","ref":"/manopt/stable/plans/debug/#Manopt.DebugCost","content":" Manopt.DebugCost  ‚Äî  Type DebugCost <: DebugAction print the current cost function value, see  get_cost . Constructors DebugCost() Parameters format=\"$prefix %f\" : format to print the output io=stdout : default stream to print the debug to. long=false : short form to set the format to  f(x):  (default) or  current cost:  and the cost source"},{"id":2457,"pagetitle":"Debug Output","title":"Manopt.DebugDivider","ref":"/manopt/stable/plans/debug/#Manopt.DebugDivider","content":" Manopt.DebugDivider  ‚Äî  Type DebugDivider <: DebugAction print a small divider (default  \" | \" ). Constructor DebugDivider(div,print) source"},{"id":2458,"pagetitle":"Debug Output","title":"Manopt.DebugEntry","ref":"/manopt/stable/plans/debug/#Manopt.DebugEntry","content":" Manopt.DebugEntry  ‚Äî  Type DebugEntry <: DebugAction print a certain fields entry during the iterates, where a  format  can be specified how to print the entry. Additional fields field : symbol the entry can be accessed with within  AbstractManoptSolverState Constructor DebugEntry(f; prefix=\"$f:\", format = \"$prefix %s\", io=stdout) source"},{"id":2459,"pagetitle":"Debug Output","title":"Manopt.DebugEntryChange","ref":"/manopt/stable/plans/debug/#Manopt.DebugEntryChange","content":" Manopt.DebugEntryChange  ‚Äî  Type DebugEntryChange{T} <: DebugAction print a certain entries change during iterates Additional fields print :    function to print the result prefix :   prefix to the print out format :   format to print (uses the  prefix  by default and scientific notation) field :    Symbol the field can be accessed with within  AbstractManoptSolverState distance : function (p,o,x1,x2) to compute the change/distance between two values of the entry storage :  a  StoreStateAction  to store the previous value of  :f Constructors DebugEntryChange(f,d) Keyword arguments io=stdout :                      an  IOStream  used for the debug prefix=\"Change of $f\" :          the prefix storage=StoreStateAction((f,)) : a  StoreStateAction initial_value=NaN :              an initial value for the change of  o.field . format=\"$prefix %e\" :            format to print the change source"},{"id":2460,"pagetitle":"Debug Output","title":"Manopt.DebugEvery","ref":"/manopt/stable/plans/debug/#Manopt.DebugEvery","content":" Manopt.DebugEvery  ‚Äî  Type DebugEvery <: DebugAction evaluate and print debug only every  $k$ th iteration. Otherwise no print is performed. Whether internal variables are updates is determined by  always_update . This method does not perform any print itself but relies on it's children's print. It also sets the sub solvers active parameter, see | DebugWhenActive }(#ref). Here, the  activattion_offset  can be used to specify whether it refers to  this  iteration, the  i th, when this call is  before  the iteration, then the offset should be 0, for the  next  iteration, that is if this is called  after  an iteration, it has to be set to 1. Since usual debug is happening after the iteration, 1 is the default. Constructor DebugEvery(d::DebugAction, every=1, always_update=true, activation_offset=1) source"},{"id":2461,"pagetitle":"Debug Output","title":"Manopt.DebugFeasibility","ref":"/manopt/stable/plans/debug/#Manopt.DebugFeasibility","content":" Manopt.DebugFeasibility  ‚Äî  Type DebugFeasibility <: DebugAction Display information about the feasibility of the current iterate Fields atol :   absolute tolerance for when either equality or inequality constraints are counted as violated format : a vector of symbols and string formatting the output io :     default stream to print the debug to. The following symbols are filled with values :Feasbile  display true or false depending on whether the iterate is feasible :FeasbileEq  display  =  or  ‚â†  equality constraints are fulfilled or not :FeasbileInEq  display  ‚â§  or  ‚â∞  inequality constraints are fulfilled or not :NumEq  display the number of equality constraints infeasible :NumEqNz  display the number of equality constraints infeasible if exists :NumIneq  display the number of inequality constraints infeasible :NumIneqNz  display the number of inequality constraints infeasible if exists :TotalEq  display the sum of how much the equality constraints are violated :TotalInEq  display the sum of how much the inequality constraints are violated format to print the output. Constructor DebugFeasibility(     format=[\"feasible: \", :Feasible];     io::IO=stdout,     atol=1e-13 ) source"},{"id":2462,"pagetitle":"Debug Output","title":"Manopt.DebugGradientChange","ref":"/manopt/stable/plans/debug/#Manopt.DebugGradientChange","content":" Manopt.DebugGradientChange  ‚Äî  Type DebugGradientChange() debug for the amount of change of the gradient (stored in  get_gradient(o)  of the  AbstractManoptSolverState o ) during the last iteration. See  DebugEntryChange  for the general case Keyword parameters storage= StoreStateAction ( (:Gradient,) ) : storage of the action for previous data prefix=\"Last Change:\" : prefix of the debug output (ignored if you set  format : io=stdout : default stream to print the debug to. format=\"$prefix %f\" : format to print the output source"},{"id":2463,"pagetitle":"Debug Output","title":"Manopt.DebugGroup","ref":"/manopt/stable/plans/debug/#Manopt.DebugGroup","content":" Manopt.DebugGroup  ‚Äî  Type DebugGroup <: DebugAction group a set of  DebugAction s into one action, where the internal prints are removed by default and the resulting strings are concatenated Constructor DebugGroup(g) construct a group consisting of an Array of  DebugAction s  g , that are evaluated  en bloque ; the method does not perform any print itself, but relies on the internal prints. It still concatenates the result and returns the complete string source"},{"id":2464,"pagetitle":"Debug Output","title":"Manopt.DebugIfEntry","ref":"/manopt/stable/plans/debug/#Manopt.DebugIfEntry","content":" Manopt.DebugIfEntry  ‚Äî  Type DebugIfEntry <: DebugAction Issue a warning, info, or error if a certain field does  not  pass a the  check . The  message  is printed in this case. If it contains a  @printf  argument identifier, that one is filled with the value of the  field . That way you can print the value in this case as well. Fields io :    an  IO  stream check : a function that takes the value of the  field  as input and returns a boolean field : symbol the entry can be accessed with within  AbstractManoptSolverState msg :   if the  check  fails, this message is displayed type : symbol specifying the type of display, possible values  :print ,  : warn ,  :info ,  :error ,           where  :print  prints to  io . Constructor DebugEntry(field, check=(>(0)); type=:warn, message=\":$f is nonnegative\", io=stdout) source"},{"id":2465,"pagetitle":"Debug Output","title":"Manopt.DebugIterate","ref":"/manopt/stable/plans/debug/#Manopt.DebugIterate","content":" Manopt.DebugIterate  ‚Äî  Type DebugIterate <: DebugAction debug for the current iterate (stored in  get_iterate(o) ). Constructor DebugIterate(; kwargs...) Keyword arguments io=stdout :           default stream to print the debug to. format=\"$prefix %s\" : format how to print the current iterate long=false :          whether to have a long ( \"current iterate:\" ) or a short ( \"p:\" ) prefix default prefix :              (see  long  for default) set a prefix to be printed before the iterate source"},{"id":2466,"pagetitle":"Debug Output","title":"Manopt.DebugIteration","ref":"/manopt/stable/plans/debug/#Manopt.DebugIteration","content":" Manopt.DebugIteration  ‚Äî  Type DebugIteration <: DebugAction Constructor DebugIteration() Keyword parameters format=\"# %-6d\" : format to print the output io=stdout : default stream to print the debug to. debug for the current iteration (prefixed with  #  by ) source"},{"id":2467,"pagetitle":"Debug Output","title":"Manopt.DebugMessages","ref":"/manopt/stable/plans/debug/#Manopt.DebugMessages","content":" Manopt.DebugMessages  ‚Äî  Type DebugMessages <: DebugAction An  AbstractManoptSolverState  or one of its sub steps like a  Stepsize  might generate warnings throughout their computations. This debug can be used to  :print  them display them as  :info  or  :warnings  or even  :error , depending on the message type. Constructor DebugMessages(mode=:Info, warn=:Once; io::IO=stdout) Initialize the messages debug to a certain  mode . Available modes are :Error :   issue the messages as an error and hence stop at any issue occurring :Info :    issue the messages as an  @info :Print :   print messages to the steam  io . :Warning : issue the messages as a warning The  warn  level can be set to  :Once  to only display only the first message, to  :Always  to report every message, one can set it to  :No , to deactivate this, then this  DebugAction  is inactive. All other symbols are handled as if they were  :Always: source"},{"id":2468,"pagetitle":"Debug Output","title":"Manopt.DebugSolverState","ref":"/manopt/stable/plans/debug/#Manopt.DebugSolverState","content":" Manopt.DebugSolverState  ‚Äî  Type DebugSolverState <: AbstractManoptSolverState The debug state appends debug to any state, they act as a decorator pattern. Internally a dictionary is kept that stores a  DebugAction  for several occasions using a  Symbol  as reference. The original options can still be accessed using the  get_state  function. Fields options :         the options that are extended by debug information debugDictionary : a  Dict{Symbol,DebugAction}  to keep track of Debug for different actions Constructors DebugSolverState(o,dA) construct debug decorated options, where  dD  can be a  DebugAction , then it is stored within the dictionary at  :Iteration an  Array  of  DebugAction s. a  Dict{Symbol,DebugAction} . an Array of Symbols, String and an Int for the  DebugFactory source"},{"id":2469,"pagetitle":"Debug Output","title":"Manopt.DebugStoppingCriterion","ref":"/manopt/stable/plans/debug/#Manopt.DebugStoppingCriterion","content":" Manopt.DebugStoppingCriterion  ‚Äî  Type DebugStoppingCriterion <: DebugAction print the Reason provided by the stopping criterion. Usually this should be empty, unless the algorithm stops. Fields prefix=\"\" : format to print the output io=stdout : default stream to print the debug to. Constructor DebugStoppingCriterion(prefix = \"\"; io::IO=stdout) source"},{"id":2470,"pagetitle":"Debug Output","title":"Manopt.DebugTime","ref":"/manopt/stable/plans/debug/#Manopt.DebugTime","content":" Manopt.DebugTime  ‚Äî  Type DebugTime() Measure time and print the intervals. Using  start=true  you can start the timer on construction, for example to measure the runtime of an algorithm overall (adding) The measured time is rounded using the given  time_accuracy  and printed after  canonicalization . Keyword parameters io=stdout :             default stream to print the debug to. format=\"$prefix %s\" :   format to print the output, where  %s  is the canonicalized time`. mode=:cumulative :      whether to display the total time or reset on every call using  :iterative . prefix=\"Last Change:\" : prefix of the debug output (ignored if you set  format : start=false :           indicate whether to start the timer on creation or not.  Otherwise it might only be started on first call. time_accuracy=Millisecond(1) : round the time to this period before printing the canonicalized time source"},{"id":2471,"pagetitle":"Debug Output","title":"Manopt.DebugWarnIfCostIncreases","ref":"/manopt/stable/plans/debug/#Manopt.DebugWarnIfCostIncreases","content":" Manopt.DebugWarnIfCostIncreases  ‚Äî  Type DebugWarnIfCostIncreases <: DebugAction print a warning if the cost increases. Note that this provides an additional warning for gradient descent with its default constant step size. Constructor DebugWarnIfCostIncreases(warn=:Once; tol=1e-13) Initialize the warning to warning level ( :Once ) and introduce a tolerance for the test of  1e-13 . The  warn  level can be set to  :Once  to only warn the first time the cost increases, to  :Always  to report an increase every time it happens, and it can be set to  :No  to deactivate the warning, then this  DebugAction  is inactive. All other symbols are handled as if they were  :Always: source"},{"id":2472,"pagetitle":"Debug Output","title":"Manopt.DebugWarnIfCostNotFinite","ref":"/manopt/stable/plans/debug/#Manopt.DebugWarnIfCostNotFinite","content":" Manopt.DebugWarnIfCostNotFinite  ‚Äî  Type DebugWarnIfCostNotFinite <: DebugAction A debug to see when a field (value or array within the AbstractManoptSolverState is or contains values that are not finite, for example  Inf  or  Nan . Constructor DebugWarnIfCostNotFinite(field::Symbol, warn=:Once) Initialize the warning to warn  :Once . This can be set to  :Once  to only warn the first time the cost is Nan. It can also be set to  :No  to deactivate the warning, but this makes this Action also useless. All other symbols are handled as if they were  :Always: source"},{"id":2473,"pagetitle":"Debug Output","title":"Manopt.DebugWarnIfFieldNotFinite","ref":"/manopt/stable/plans/debug/#Manopt.DebugWarnIfFieldNotFinite","content":" Manopt.DebugWarnIfFieldNotFinite  ‚Äî  Type DebugWarnIfFieldNotFinite <: DebugAction A debug to see when a field from the options is not finite, for example  Inf  or  Nan Constructor DebugWarnIfFieldNotFinite(field::Symbol, warn=:Once) Initialize the warning to warn  :Once . This can be set to  :Once  to only warn the first time the cost is Nan. It can also be set to  :No  to deactivate the warning, but this makes this Action also useless. All other symbols are handled as if they were  :Always: Example DebugWaranIfFieldNotFinite(:Gradient) Creates a [ DebugAction ] to track whether the gradient does not get  Nan  or  Inf . source"},{"id":2474,"pagetitle":"Debug Output","title":"Manopt.DebugWarnIfGradientNormTooLarge","ref":"/manopt/stable/plans/debug/#Manopt.DebugWarnIfGradientNormTooLarge","content":" Manopt.DebugWarnIfGradientNormTooLarge  ‚Äî  Type DebugWarnIfGradientNormTooLarge{T} <: DebugAction A debug to warn when an evaluated gradient at the current iterate is larger than (a factor times) the maximal (recommended) stepsize at the current iterate. Constructor DebugWarnIfGradientNormTooLarge(factor::T=1.0, warn=:Once) Initialize the warning to warn  :Once . This can be set to  :Once  to only warn the first time the cost is Nan. It can also be set to  :No  to deactivate the warning, but this makes this Action also useless. All other symbols are handled as if they were  :Always: Example DebugWaranIfFieldNotFinite(:Gradient) Creates a [ DebugAction ] to track whether the gradient does not get  Nan  or  Inf . source"},{"id":2475,"pagetitle":"Debug Output","title":"Manopt.DebugWhenActive","ref":"/manopt/stable/plans/debug/#Manopt.DebugWhenActive","content":" Manopt.DebugWhenActive  ‚Äî  Type DebugWhenActive <: DebugAction evaluate and print debug only if the active boolean is set. This can be set from outside and is for example triggered by  DebugEvery  on debugs on the subsolver. This method does not perform any print itself but relies on it's children's prints. For now, the main interaction is with  DebugEvery  which might activate or deactivate this debug Fields active :        a boolean that can (de-)activated from outside to turn on/off debug always_update : whether or not to call the order debugs with iteration  <=0  inactive state Constructor DebugWhenActive(d::DebugAction, active=true, always_update=true) source"},{"id":2476,"pagetitle":"Debug Output","title":"Manopt.DebugActionFactory","ref":"/manopt/stable/plans/debug/#Manopt.DebugActionFactory-Tuple{String}","content":" Manopt.DebugActionFactory  ‚Äî  Method DebugActionFactory(s) create a  DebugAction  where a  String yields the corresponding divider a  DebugAction  is passed through a [ Symbol ] creates  DebugEntry  of that symbol, with the exceptions of  :Change ,  :Iterate ,  :Iteration , and  :Cost . a  Tuple{Symbol,String}  creates a  DebugEntry  of that symbol where the String specifies the format. a  <:Function  creates a  DebugCallback  with the function as callback. source"},{"id":2477,"pagetitle":"Debug Output","title":"Manopt.DebugActionFactory","ref":"/manopt/stable/plans/debug/#Manopt.DebugActionFactory-Tuple{Symbol}","content":" Manopt.DebugActionFactory  ‚Äî  Method DebugActionFactory(s::Symbol) Convert certain Symbols in the  debug=[ ... ]  vector to  DebugAction s Currently the following ones are done. Note that the Shortcut symbols should all start with a capital letter. :Cost  creates a  DebugCost :Change  creates a  DebugChange :Gradient  creates a  DebugGradient :GradientChange  creates a  DebugGradientChange :GradientNorm  creates a  DebugGradientNorm :Iterate  creates a  DebugIterate :Iteration  creates a  DebugIteration :IterativeTime  creates a  DebugTime (:Iterative) :Stepsize  creates a  DebugStepsize :Stop  creates a  StoppingCriterion () :WarnStepsize  creates a  DebugWarnIfStepsizeCollapsed :WarnBundle  creates a  DebugWarnIfLagrangeMultiplierIncreases :WarnCost  creates a  DebugWarnIfCostNotFinite :WarnGradient  creates a  DebugWarnIfFieldNotFinite  for the  ::Gradient . :Time  creates a  DebugTime :WarningMessages  creates a  DebugMessages (:Warning) :InfoMessages  creates a  DebugMessages (:Info) :ErrorMessages  creates a  DebugMessages (:Error) :Messages  creates a  DebugMessages ()  (the same as  :InfoMessages ) any other symbol creates a  DebugEntry(s)  to print the entry (o.:s) from the options. source"},{"id":2478,"pagetitle":"Debug Output","title":"Manopt.DebugActionFactory","ref":"/manopt/stable/plans/debug/#Manopt.DebugActionFactory-Tuple{Tuple{Symbol, Any}}","content":" Manopt.DebugActionFactory  ‚Äî  Method DebugActionFactory(t::Tuple{Symbol,String) Convert certain Symbols in the  debug=[ ... ]  vector to  DebugAction s Currently the following ones are done, where the string in  t[2]  is passed as the  format  the corresponding debug. Note that the Shortcut symbols  t[1]  should all start with a capital letter. :Change  creates a  DebugChange :Cost  creates a  DebugCost :Gradient  creates a  DebugGradient :GradientChange  creates a  DebugGradientChange :GradientNorm  creates a  DebugGradientNorm :Iterate  creates a  DebugIterate :Iteration  creates a  DebugIteration :Stepsize  creates a  DebugStepsize :Stop  creates a  DebugStoppingCriterion :Time  creates a  DebugTime :IterativeTime  creates a  DebugTime (:Iterative) any other symbol creates a  DebugEntry(s)  to print the entry (o.:s) from the options. source"},{"id":2479,"pagetitle":"Debug Output","title":"Manopt.DebugFactory","ref":"/manopt/stable/plans/debug/#Manopt.DebugFactory-Tuple{Vector}","content":" Manopt.DebugFactory  ‚Äî  Method DebugFactory(a::Vector) Generate a dictionary of  DebugAction s. First all  Symbol s  String ,  DebugAction s and numbers are collected, excluding  :Stop  and  :WhenActive . This collected vector is added to the  :Iteration => [...]  pair.  :Stop  is added as  :StoppingCriterion  to the  :Stop => [...]  pair. If necessary, these pairs are created For each  Pair  of a  Symbol  and a  Vector , the  DebugGroupFactory  is called for the  Vector  and the result is added to the debug dictionary's entry with said symbol. This is wrapped into the  DebugWhenActive , when the  :WhenActive  symbol is present Return value A dictionary for the different enrty points where debug can happen, each containing a  DebugAction  to call. Note that upon the initialisation all dictionaries but the  :StartAlgorithm  one are called with an  i=0  for reset. Examples Providing a simple vector of symbols, numbers and strings like [:Iterate, \" | \", :Cost, :Stop, 10] Adds a group to :Iteration of three actions ( DebugIteration ,  DebugDivider (\" | \"),  and[ DebugCost ](@ref)) as a [ DebugGroup ](@ref) inside an [ DebugEvery ](@ref) to only be executed every 10th iteration. It also adds the [ DebugStoppingCriterion ](@ref) to the :EndAlgorithm` entry of the dictionary. The same can also be written a bit more precise as DebugFactory([:Iteration => [:Iterate, \" | \", :Cost, 10], :Stop]) We can even make the stopping criterion concrete and pass Actions directly, for example explicitly Making the stop more concrete, we get DebugFactory([:Iteration => [:Iterate, \" | \", DebugCost(), 10], :Stop => [:Stop]]) source"},{"id":2480,"pagetitle":"Debug Output","title":"Manopt.DebugGroupFactory","ref":"/manopt/stable/plans/debug/#Manopt.DebugGroupFactory-Tuple{Vector}","content":" Manopt.DebugGroupFactory  ‚Äî  Method DebugGroupFactory(a::Vector) Generate a  DebugGroup  of  DebugAction s. The following rules are used Any  Symbol  is passed to  DebugActionFactory Any  (Symbol, String)  generates similar actions as in 1., but the string is used for  format= , see  DebugActionFactory Any  String  is passed to  DebugActionFactory Any  Function  generates a  DebugCallback . Any  DebugAction  is included as is. If this results in more than one  DebugAction  a  DebugGroup  of these is build. If any integers are present, the last of these is used to wrap the group in a  DebugEvery (k) . If  :WhenActive  is present, the resulting Action is wrapped in  DebugWhenActive , making it deactivatable by its parent solver. source"},{"id":2481,"pagetitle":"Debug Output","title":"Manopt.reset!","ref":"/manopt/stable/plans/debug/#Manopt.reset!-Tuple{DebugTime}","content":" Manopt.reset!  ‚Äî  Method reset!(d::DebugTime) reset the internal time of a  DebugTime , that is start from now again. source"},{"id":2482,"pagetitle":"Debug Output","title":"Manopt.set_parameter!","ref":"/manopt/stable/plans/debug/#Manopt.set_parameter!-Tuple{DebugSolverState, Val{:Debug}, Vararg{Any}}","content":" Manopt.set_parameter!  ‚Äî  Method set_parameter!(ams::DebugSolverState, ::Val{:Debug}, args...) Set certain values specified by  args...  into the elements of the  debugDictionary source"},{"id":2483,"pagetitle":"Debug Output","title":"Manopt.stop!","ref":"/manopt/stable/plans/debug/#Manopt.stop!-Tuple{DebugTime}","content":" Manopt.stop!  ‚Äî  Method stop!(d::DebugTime) stop the reset the internal time of a  DebugTime , that is set the time to 0 (undefined) source"},{"id":2484,"pagetitle":"Debug Output","title":"Technical details","ref":"/manopt/stable/plans/debug/#Technical-details","content":" Technical details The decorator to print debug during the iterations can be activated by decorating the state of a solver and implementing your own  DebugAction s. For example printing a gradient from the  GradientDescentState  is automatically available, as explained in the  gradient_descent  solver."},{"id":2485,"pagetitle":"Debug Output","title":"Manopt.initialize_solver!","ref":"/manopt/stable/plans/debug/#Manopt.initialize_solver!-Tuple{AbstractManoptProblem, DebugSolverState}","content":" Manopt.initialize_solver!  ‚Äî  Method initialize_solver!(amp::AbstractManoptProblem, dss::DebugSolverState) Extend the initialization of the solver by a hook to run the  DebugAction  that was added to the  :Start  entry of the debug lists. All others are triggered (with iteration number  0 ) to trigger possible resets source"},{"id":2486,"pagetitle":"Debug Output","title":"Manopt.step_solver!","ref":"/manopt/stable/plans/debug/#Manopt.step_solver!-Tuple{AbstractManoptProblem, DebugSolverState, Any}","content":" Manopt.step_solver!  ‚Äî  Method step_solver!(amp::AbstractManoptProblem, dss::DebugSolverState, k) Extend the  i th step of the solver by a hook to run debug prints, that were added to the  :BeforeIteration  and  :Iteration  entries of the debug lists. source"},{"id":2487,"pagetitle":"Debug Output","title":"Manopt.stop_solver!","ref":"/manopt/stable/plans/debug/#Manopt.stop_solver!-Tuple{AbstractManoptProblem, DebugSolverState, Int64}","content":" Manopt.stop_solver!  ‚Äî  Method stop_solver!(amp::AbstractManoptProblem, dss::DebugSolverState, k) Extend the  stop_solver! , whether to stop the solver by a hook to run debug, that were added to the  :Stop  entry of the debug lists. source"},{"id":2490,"pagetitle":"Objective","title":"A manifold objective","ref":"/manopt/stable/plans/objective/#A-manifold-objective","content":" A manifold objective The Objective describes that actual cost function and all its properties."},{"id":2491,"pagetitle":"Objective","title":"Manopt.AbstractManifoldObjective","ref":"/manopt/stable/plans/objective/#Manopt.AbstractManifoldObjective","content":" Manopt.AbstractManifoldObjective  ‚Äî  Type AbstractManifoldObjective{E<:AbstractEvaluationType} Describe the collection of the optimization function  $f: \\mathcal M ‚Üí ‚Ñù$  (or even a vectorial range) and its corresponding elements, which might for example be a gradient or (one or more) proximal maps. All these elements should usually be implemented as functions  (M, p) -> ... , or  (M, X, p) -> ...  that is the first argument of these functions should be the manifold  M  they are defined on the argument  X  is present, if the computation is performed in-place of  X  (see  InplaceEvaluation ) the argument  p  is the place the function ( $f$  or one of its elements) is evaluated  at . the type  T  indicates the global  AbstractEvaluationType . source"},{"id":2492,"pagetitle":"Objective","title":"Manopt.AbstractDecoratedManifoldObjective","ref":"/manopt/stable/plans/objective/#Manopt.AbstractDecoratedManifoldObjective","content":" Manopt.AbstractDecoratedManifoldObjective  ‚Äî  Type AbstractDecoratedManifoldObjective{E<:AbstractEvaluationType,O<:AbstractManifoldObjective} A common supertype for all decorators of  AbstractManifoldObjective s to simplify dispatch.     The second parameter should refer to the undecorated objective (the most inner one). source Which has two main different possibilities for its containing functions concerning the evaluation mode, not necessarily the cost, but for example gradient in an  AbstractManifoldFirstOrderObjective ."},{"id":2493,"pagetitle":"Objective","title":"Manopt.AbstractEvaluationType","ref":"/manopt/stable/plans/objective/#Manopt.AbstractEvaluationType","content":" Manopt.AbstractEvaluationType  ‚Äî  Type AbstractEvaluationType An abstract type to specify the kind of evaluation a  AbstractManifoldObjective  supports. source"},{"id":2494,"pagetitle":"Objective","title":"Manopt.AllocatingEvaluation","ref":"/manopt/stable/plans/objective/#Manopt.AllocatingEvaluation","content":" Manopt.AllocatingEvaluation  ‚Äî  Type AllocatingEvaluation <: AbstractEvaluationType A parameter for a  AbstractManoptProblem  or a  Function  indicating that the problem contains or the function(s) allocate memory for their result, they work out of place. source"},{"id":2495,"pagetitle":"Objective","title":"Manopt.AllocatingInplaceEvaluation","ref":"/manopt/stable/plans/objective/#Manopt.AllocatingInplaceEvaluation","content":" Manopt.AllocatingInplaceEvaluation  ‚Äî  Type AllocatingInplaceEvaluation <: AbstractEvaluationType A parameter for a  AbstractManoptProblem  or a  Function  indicating that the problem contains or the function(s) that provides both an allocating variant and one, that does not allocate memory but work on their input, in place. source"},{"id":2496,"pagetitle":"Objective","title":"Manopt.InplaceEvaluation","ref":"/manopt/stable/plans/objective/#Manopt.InplaceEvaluation","content":" Manopt.InplaceEvaluation  ‚Äî  Type InplaceEvaluation <: AbstractEvaluationType A parameter for a  AbstractManoptProblem  or a  Function  indicating that the problem contains or the function(s) do not allocate memory but work on their input, in place. source"},{"id":2497,"pagetitle":"Objective","title":"Manopt.ParentEvaluationType","ref":"/manopt/stable/plans/objective/#Manopt.ParentEvaluationType","content":" Manopt.ParentEvaluationType  ‚Äî  Type ParentEvaluationType <: AbstractEvaluationType A parameter for a  AbstractManoptProblem  or a  Function  indicating that the problem contains or the function(s) do inherit their property from a parent  AbstractManoptProblem  or function. source"},{"id":2498,"pagetitle":"Objective","title":"Manopt.evaluation_type","ref":"/manopt/stable/plans/objective/#Manopt.evaluation_type","content":" Manopt.evaluation_type  ‚Äî  Function evaluation_type(mp::AbstractManoptProblem) Get the  AbstractEvaluationType  of the objective in  AbstractManoptProblem mp . source evaluation_type(::AbstractManifoldObjective{Teval}) Get the  AbstractEvaluationType  of the objective. source"},{"id":2499,"pagetitle":"Objective","title":"Decorators for objectives","ref":"/manopt/stable/plans/objective/#Decorators-for-objectives","content":" Decorators for objectives An objective can be decorated using the following trait and function to initialize"},{"id":2500,"pagetitle":"Objective","title":"Manopt.dispatch_objective_decorator","ref":"/manopt/stable/plans/objective/#Manopt.dispatch_objective_decorator","content":" Manopt.dispatch_objective_decorator  ‚Äî  Function dispatch_objective_decorator(o::AbstractManoptSolverState) Indicate internally, whether an  AbstractManifoldObjective o  to be of decorating type, it stores (encapsulates) an object in itself, by default in the field  o.objective . Decorators indicate this by returning  Val{true}  for further dispatch. The default is  Val{false} , so by default an state is not decorated. source"},{"id":2501,"pagetitle":"Objective","title":"Manopt.is_objective_decorator","ref":"/manopt/stable/plans/objective/#Manopt.is_objective_decorator","content":" Manopt.is_objective_decorator  ‚Äî  Function is_object_decorator(s::AbstractManifoldObjective) Indicate, whether  AbstractManifoldObjective s  are of decorator type. source"},{"id":2502,"pagetitle":"Objective","title":"Manopt.decorate_objective!","ref":"/manopt/stable/plans/objective/#Manopt.decorate_objective!","content":" Manopt.decorate_objective!  ‚Äî  Function decorate_objective!(M, o::AbstractManifoldObjective) decorate the  AbstractManifoldObjective o  with specific decorators. Optional arguments optional arguments provide necessary details on the decorators. A specific one is used to activate certain decorators. cache=missing : specify a cache. Currently  :Simple  is supported and  :LRU  if you load  LRUCache.jl . For this case a tuple specifying what to cache and how many can be provided, has to be specified. For example  (:LRU, [:Cost, :Gradient], 10)  states that the last 10 used cost function evaluations and gradient evaluations should be stored. See  objective_cache_factory  for details. count=missing : specify calls to the objective to be called, see  ManifoldCountObjective  for the full list objective_type=:Riemannian : specify that an objective is  :Riemannian  or  :Euclidean . The  :Euclidean  symbol is equivalent to specifying it as  :Embedded , since in the end, both refer to converting an objective from the embedding (whether its Euclidean or not) to the Riemannian one. See also objective_cache_factory source"},{"id":2503,"pagetitle":"Objective","title":"Embedded objectives","ref":"/manopt/stable/plans/objective/#subsection-embedded-objectives","content":" Embedded objectives"},{"id":2504,"pagetitle":"Objective","title":"Manopt.EmbeddedManifoldObjective","ref":"/manopt/stable/plans/objective/#Manopt.EmbeddedManifoldObjective","content":" Manopt.EmbeddedManifoldObjective  ‚Äî  Type EmbeddedManifoldObjective{P, T, E, O2, O1<:AbstractManifoldObjective{E}} <:\n   AbstractDecoratedManifoldObjective{E,O2} Declare an objective to be defined in the embedding. This also declares the gradient to be defined in the embedding, and especially being the Riesz representer with respect to the metric in the embedding. The types can be used to still dispatch on also the undecorated objective type  O2 . Fields objective : the objective that is defined in the embedding p=nothing : a point in the embedding. X=nothing : a tangent vector in the embedding When a point in the embedding  p  is provided,  embed!  is used in place of this point to reduce memory allocations. Similarly  X  is used when embedding tangent vectors source"},{"id":2505,"pagetitle":"Objective","title":"Scaled objectives","ref":"/manopt/stable/plans/objective/#subsection-scaled-objectives","content":" Scaled objectives"},{"id":2506,"pagetitle":"Objective","title":"Manopt.ScaledManifoldObjective","ref":"/manopt/stable/plans/objective/#Manopt.ScaledManifoldObjective","content":" Manopt.ScaledManifoldObjective  ‚Äî  Type ScaledManifoldObjective{E, O2, O1<:AbstractManifoldObjective{E},F} <:\n   AbstractDecoratedManifoldObjective{E,O2} Declare an objective to be defined as a scaled version of an existing objective. This rescales all involved functions. For now the functions rescaled are the cost the gradient the Hessian Fields objective : the objective that is defined in the embedding scale=1 : the scaling applied Constructors ScaledManifoldObjective(objective, scale::Real=1)\n-objective\nscale*objective Generate a scaled manifold objective based on  objective  with  scale  being  1  by default in the first,  scale=-1  in the second case. The multiplication from the left with a scalar is also overloaded. source"},{"id":2507,"pagetitle":"Objective","title":"Cache objective","ref":"/manopt/stable/plans/objective/#subsection-cache-objective","content":" Cache objective Since single function calls, for example to the cost or the gradient, might be expensive, a simple cache objective exists as a decorator, that caches one cost value or gradient. It can be activated/used with the  cache=  keyword argument available for every solver."},{"id":2508,"pagetitle":"Objective","title":"Manopt.reset_counters!","ref":"/manopt/stable/plans/objective/#Manopt.reset_counters!","content":" Manopt.reset_counters!  ‚Äî  Function reset_counters(co::ManifoldCountObjective, value::Integer=0) Reset all values in the count objective to  value . source"},{"id":2509,"pagetitle":"Objective","title":"Manopt.objective_cache_factory","ref":"/manopt/stable/plans/objective/#Manopt.objective_cache_factory","content":" Manopt.objective_cache_factory  ‚Äî  Function objective_cache_factory(M::AbstractManifold, o::AbstractManifoldObjective, cache::Symbol) Generate a cached variant of the  AbstractManifoldObjective o  on the  AbstractManifold M  based on the symbol  cache . The following caches are available :Simple  generates a  SimpleManifoldCachedObjective :LRU  generates a  ManifoldCachedObjective  where you should use the form  (:LRU, [:Cost, :Gradient])  to specify what should be cached or  (:LRU, [:Cost, :Gradient], 100)  to specify the cache size. Here this variant defaults to  (:LRU, [:Cost, :Gradient], 100) , caching up to 100 cost and gradient values. [1] source objective_cache_factory(M::AbstractManifold, o::AbstractManifoldObjective, cache::Tuple{Symbol, Array, Array})\nobjective_cache_factory(M::AbstractManifold, o::AbstractManifoldObjective, cache::Tuple{Symbol, Array}) Generate a cached variant of the  AbstractManifoldObjective o  on the  AbstractManifold M  based on the symbol  cache[1] , where the second element  cache[2]  are further arguments to the cache and the optional third is passed down as keyword arguments. For all available caches see the simpler variant with symbols. source"},{"id":2510,"pagetitle":"Objective","title":"A simple cache","ref":"/manopt/stable/plans/objective/#A-simple-cache","content":" A simple cache A first generic cache is always available, but it only caches one gradient and one cost function evaluation (for the same point)."},{"id":2511,"pagetitle":"Objective","title":"Manopt.SimpleManifoldCachedObjective","ref":"/manopt/stable/plans/objective/#Manopt.SimpleManifoldCachedObjective","content":" Manopt.SimpleManifoldCachedObjective  ‚Äî  Type  SimpleManifoldCachedObjective{O<:AbstractManifoldFirstOrderObjective{E}, P, T,C} <: AbstractDecoratedManifoldObjective{E,O} Provide a simple cache for an  AbstractManifoldFirstOrderObjective  that is, this cache stores a point  p  and a gradient  $\\operatorname{grad} f(p)$  in  X  as well as a cost value  $f(p)$  in  c . It can also easily evaluate the differential based on the cached gradient. Both  X  and  c  are accompanied by booleans to keep track of their validity. While this does not provide a cache for the differential, it uses the cached gradient as a help to evaluate the differential, if an up-to-date gradient is available. It otherwise does call the original differential. This simple cache does not take into account, that some first order objectives have a common function for cost & grad. It only caches the function that is actually called. Constructor SimpleManifoldCachedObjective(M::AbstractManifold, obj::AbstractManifoldFirstOrderObjective; kwargs...) Keyword arguments p= rand (M) : a point on the manifold to initialize the cache with X=get_gradient(M, obj, p)  or  zero_vector(M,p) : a tangent vector to store the gradient in, see also  initialize= c=[ get_cost ](@ref) (M, obj, p) or 0.0 : a value to store the cost function in initialize` initialized=true : whether to initialize the cached  X  and  c  or not. source"},{"id":2512,"pagetitle":"Objective","title":"A generic cache","ref":"/manopt/stable/plans/objective/#A-generic-cache","content":" A generic cache For the more advanced cache, you need to implement some type of cache yourself, that provides a  get!  and implement  init_caches . This is for example provided if you load  LRUCache.jl . Then you obtain"},{"id":2513,"pagetitle":"Objective","title":"Manopt.ManifoldCachedObjective","ref":"/manopt/stable/plans/objective/#Manopt.ManifoldCachedObjective","content":" Manopt.ManifoldCachedObjective  ‚Äî  Type ManifoldCachedObjective{E,P,O<:AbstractManifoldObjective{<:E},C<:NamedTuple{}} <: AbstractDecoratedManifoldObjective{E,P} Create a cache for an objective, based on a  NamedTuple  that stores some kind of cache. Constructor ManifoldCachedObjective(M, o::AbstractManifoldObjective, caches::Vector{Symbol}; kwargs...) Create a cache for the  AbstractManifoldObjective  where the Symbols in  caches  indicate, which function evaluations to cache. Supported symbols Symbol Caches calls to (incl.  !  variants) Comment :Cost get_cost :Differential get_differential (M, p, X) . :EqualityConstraint get_equality_constraint (M, p, i) :EqualityConstraints get_equality_constraint (M, p, :) :GradEqualityConstraint get_grad_equality_constraint tangent vector per (p,i) :GradInequalityConstraint get_inequality_constraint tangent vector per (p,i) :Gradient get_gradient (M,p) tangent vectors :Hessian get_hessian tangent vectors :InequalityConstraint get_inequality_constraint (M, p, j) :InequalityConstraints get_inequality_constraint (M, p, :) :Preconditioner get_preconditioner tangent vectors :ProximalMap get_proximal_map point per  (p,Œª,i) :StochasticGradients get_gradients vector of tangent vectors :StochasticGradient get_gradient (M, p, i) tangent vector per (p,i) :SubGradient get_subgradient tangent vectors :SubtrahendGradient get_subtrahend_gradient tangent vectors Keyword arguments p=rand(M) : the type of the keys to be used in the caches. Defaults to the default representation on  M . value=get_cost(M, objective, p) : the type of values for numeric values in the cache X=zero_vector(M,p) : the type of values to be cached for gradient and Hessian calls. cache=[:Cost] : a vector of symbols indicating which function calls should be cached. cache_size=10 : number of (least recently used) calls to cache cache_sizes=Dict{Symbol,Int}() : a named tuple or dictionary specifying the sizes individually for each cache. source"},{"id":2514,"pagetitle":"Objective","title":"Manopt.init_caches","ref":"/manopt/stable/plans/objective/#Manopt.init_caches","content":" Manopt.init_caches  ‚Äî  Function init_caches(caches, T::Type{LRU}; kwargs...) Given a vector of symbols  caches , this function sets up the  NamedTuple  of caches, where  T  is the type of cache to use. Keyword arguments p= rand (M) : a point on a manifold, to both infer its type for keys and initialize caches value=0.0 :  a value both typing and initialising number-caches, the default is for (Float) values like the cost. X=zero_vector(M, p) : a tangent vector at  p  to both type and initialize tangent vector caches cache_size=10 : a default cache size to use cache_sizes=Dict{Symbol,Int}() : a dictionary of sizes for the  caches  to specify different (non-default) sizes source init_caches(M::AbstractManifold, caches, T; kwargs...) Given a vector of symbols  caches , this function sets up the  NamedTuple  of caches for points/vectors on  M , where  T  is the type of cache to use. source"},{"id":2515,"pagetitle":"Objective","title":"Count objective","ref":"/manopt/stable/plans/objective/#subsection-count-objective","content":" Count objective"},{"id":2516,"pagetitle":"Objective","title":"Manopt.ManifoldCountObjective","ref":"/manopt/stable/plans/objective/#Manopt.ManifoldCountObjective","content":" Manopt.ManifoldCountObjective  ‚Äî  Type ManifoldCountObjective{E,P,O<:AbstractManifoldObjective,I<:Integer} <: AbstractDecoratedManifoldObjective{E,P} A wrapper for any  AbstractManifoldObjective  of type  O  to count different calls to parts of the objective. Fields counts  a dictionary of symbols mapping to integers keeping the counted values objective  the wrapped objective Supported symbols Symbol Counts calls to (incl.  !  variants) Comment :Cost get_cost :Differential get_differential . :EqualityConstraint get_equality_constraint requires vector of counters :EqualityConstraints get_equality_constraint when evaluating all of them with  : :GradEqualityConstraint get_grad_equality_constraint requires vector of counters :GradEqualityConstraints get_grad_equality_constraint when evaluating all of them with  : :GradInequalityConstraint get_inequality_constraint requires vector of counters :GradInequalityConstraints get_inequality_constraint when evaluating all of them with  : :Gradient get_gradient (M,p) :Hessian get_hessian :InequalityConstraint get_inequality_constraint requires vector of counters :InequalityConstraints get_inequality_constraint when evaluating all of them with  : :Preconditioner get_preconditioner :ProximalMap get_proximal_map :StochasticGradients get_gradients :StochasticGradient get_gradient (M, p, i) :SubGradient get_subgradient :SubtrahendGradient get_subtrahend_gradient Constructors ManifoldCountObjective(objective::AbstractManifoldObjective, counts::Dict{Symbol, <:Integer}) Initialise the  ManifoldCountObjective  to wrap  objective  initializing the set of counts ManifoldCountObjective(M::AbstractManifold, objective::AbstractManifoldObjective, count::AbstractVecor{Symbol}, init=0) Count function calls on  objective  using the symbols in  count  initialising all entries to  init . source"},{"id":2517,"pagetitle":"Objective","title":"Internal decorators and functions","ref":"/manopt/stable/plans/objective/#Internal-decorators-and-functions","content":" Internal decorators and functions"},{"id":2518,"pagetitle":"Objective","title":"Manopt.ReturnManifoldObjective","ref":"/manopt/stable/plans/objective/#Manopt.ReturnManifoldObjective","content":" Manopt.ReturnManifoldObjective  ‚Äî  Type ReturnManifoldObjective{E,O2,O1<:AbstractManifoldObjective{E}} <:\n   AbstractDecoratedManifoldObjective{E,O2} A wrapper to indicate that  get_solver_result  should return the inner objective. The types are such that one can still dispatch on the undecorated type  O2  of the original objective as well. source"},{"id":2519,"pagetitle":"Objective","title":"Specific Objective typed and their access functions","ref":"/manopt/stable/plans/objective/#Specific-Objective-typed-and-their-access-functions","content":" Specific Objective typed and their access functions"},{"id":2520,"pagetitle":"Objective","title":"Cost objective","ref":"/manopt/stable/plans/objective/#Cost-objective","content":" Cost objective"},{"id":2521,"pagetitle":"Objective","title":"Manopt.AbstractManifoldCostObjective","ref":"/manopt/stable/plans/objective/#Manopt.AbstractManifoldCostObjective","content":" Manopt.AbstractManifoldCostObjective  ‚Äî  Type AbstractManifoldCostObjective{T<:AbstractEvaluationType} <: AbstractManifoldObjective{T} Representing objectives on manifolds with a cost function implemented. source"},{"id":2522,"pagetitle":"Objective","title":"Manopt.ManifoldCostObjective","ref":"/manopt/stable/plans/objective/#Manopt.ManifoldCostObjective","content":" Manopt.ManifoldCostObjective  ‚Äî  Type ManifoldCostObjective{T, TC} <: AbstractManifoldCostObjective{T, TC} specify an  AbstractManifoldObjective  that does only have information about the cost function  $f:  \\mathbb M ‚Üí ‚Ñù$  implemented as a function  (M, p) -> c  to compute the cost value  c  at  p  on the manifold  M . cost : a function  $f: \\mathcal M ‚Üí ‚Ñù$  to minimize Constructors ManifoldCostObjective(f) Generate a problem. While this Problem does not have any allocating functions, the type  T  can be set for consistency reasons with other problems. Used with NelderMead ,  particle_swarm source"},{"id":2523,"pagetitle":"Objective","title":"Access functions","ref":"/manopt/stable/plans/objective/#Access-functions","content":" Access functions"},{"id":2524,"pagetitle":"Objective","title":"Manopt.get_cost","ref":"/manopt/stable/plans/objective/#Manopt.get_cost","content":" Manopt.get_cost  ‚Äî  Function get_cost(amp::AbstractManoptProblem, p) evaluate the cost function  f  stored within the  AbstractManifoldObjective  of an  AbstractManoptProblem amp  at the point  p . source get_cost(M::AbstractManifold, obj::AbstractManifoldObjective, p) evaluate the cost function  f  defined on  M  stored within the  AbstractManifoldObjective  at the point  p . source get_cost(M::AbstractManifold, mco::AbstractManifoldCostObjective, p) Evaluate the cost function from within the  AbstractManifoldCostObjective  on  M  at  p . By default this implementation assumed that the cost is stored within  mco.cost . source get_cost(TpM, trmo::TrustRegionModelObjective, X) Evaluate the tangent space  TrustRegionModelObjective \\[m(X) = f(p) + ‚ü®\\operatorname{grad} f(p), X ‚ü©_p + \\frac{1}{2} ‚ü®\\operatorname{Hess} f(p)[X], X‚ü©_p.\\] source get_cost(TpM, trmo::AdaptiveRagularizationWithCubicsModelObjective, X) Evaluate the tangent space  AdaptiveRagularizationWithCubicsModelObjective \\[m(X) = f(p) + ‚ü®\\operatorname{grad} f(p), X ‚ü©_p + \\frac{1}{2} ‚ü®\\operatorname{Hess} f(p)[X], X‚ü©_p\n       +  \\frac{œÉ}{3} \\lVert X \\rVert^3,\\] at  X , cf. Eq. (33) in [ ABBC20 ]. source get_cost(TpM::TangentSpace, slso::SymmetricLinearSystemObjective, X) evaluate the cost \\[f(X) = \\frac{1}{2} \\lVert \\mathcal A[X] + b \\rVert_{p}^2,\\qquad X ‚àà T_{p}\\mathcal M,\\] at  X . source get_cost(M::AbstractManifold, sgo::ManifoldStochasticGradientObjective, p, i) Evaluate the  i th summand of the cost. If you use a single function for the stochastic cost, then only the index  √¨=1 ` is available to evaluate the whole cost. source get_cost(M::AbstractManifold,emo::EmbeddedManifoldObjective, p) Evaluate the cost function of an objective defined in the embedding by first embedding  p  before calling the cost function stored in the  EmbeddedManifoldObjective . source get_cost(M::AbstractManifold, scaled_objective::ScaledManifoldObjective, p) Evaluate the scaled objective.  $s*f(p)$ source and internally"},{"id":2525,"pagetitle":"Objective","title":"Manopt.get_cost_function","ref":"/manopt/stable/plans/objective/#Manopt.get_cost_function","content":" Manopt.get_cost_function  ‚Äî  Function get_cost_function(amco::AbstractManifoldCostObjective; recursive=false) return the function to evaluate (just) the cost  $f(p)=c$  as a function  (M,p) -> c . If  amco  has more than one decorator,  recursive  determines whether just one ( false ) or all wrappers ( true ) should be ‚Äúunwrapped‚Äù at once. source"},{"id":2526,"pagetitle":"Objective","title":"First order objectives","ref":"/manopt/stable/plans/objective/#First-order-objectives","content":" First order objectives"},{"id":2527,"pagetitle":"Objective","title":"Manopt.AbstractManifoldFirstOrderObjective","ref":"/manopt/stable/plans/objective/#Manopt.AbstractManifoldFirstOrderObjective","content":" Manopt.AbstractManifoldFirstOrderObjective  ‚Äî  Type AbstractManifoldFirstOrderObjective{E<:AbstractEvaluationType, FGD} <: AbstractManifoldCostObjective{E, FGD} An abstract type for all objectives that provide a cost first order information, so either a (full) gradient or a differential, where E  is a  AbstractEvaluationType  for the gradient function. source"},{"id":2528,"pagetitle":"Objective","title":"Manopt.ManifoldFirstOrderObjective","ref":"/manopt/stable/plans/objective/#Manopt.ManifoldFirstOrderObjective","content":" Manopt.ManifoldFirstOrderObjective  ‚Äî  Type ManifoldFirstOrderObjective{E<:AbstractEvaluationType, F} <: AbstractManifoldFirstOrderObjective{E, F} specify an objective containing a cost and its gradient or differential, where the  AbstractEvaluationType E  indicates the type of evaluation for a gradient. Fields functions::F : a function or a tuple of functions containing the cost and first order information. Currently the following cases are covered, sorted by their popularity a single function  fg , i.e. a function or a functor, represents a combined  function  (M, p) -> (c, X)  that computes the cost  c=cost(M,p)  and gradient  X=grad_f(M,p) ; a single function  fdf , i.e. a function or a functor, represents a combined function   (M, p) -> (c, d)  that computes the cost  c=cost(M,p)  and differential  d=diff_f(M,p) ; pairs of single functions  (f, g) ,  (f, df)  of a cost function  f  and either its  gradient  g  or its differential  d , respectively The function  (fg, d)  and  (fdf, g)   from 1 and 2, respectively joined by  the other missing third information, the differential for the first or the gradient for the second a tuple  (f, g, d)  of three functions, computing cost,  f , gradient  g ,  and  differential d` separately a  (f, gd)  of a cost function and a combined function  (X, d) = gd(M, p, X)   to compute gradient and differential together a single function  (c, X, d) = fgd(M, p,X) For all cases where a gradient is present, also an in-place variant is possible, where the signature has the result  Y  in second place. The cases of a common  fg  function for cost and gradient and the tuple  (f,g)  are the most common one. They can also be addressed by their alternate constructors  ManifoldCostGradientObjective (fg)  and  ManifoldGradientObjective (f,g) , respectively. Constructors ManifoldFirstOrderObjective(; kwargs...) Keyword arguments cost = nothing  the cost function  c = f(M,p) differential = nothing  the differential  d = df(M, p, X) gradient=nothing  the gradient function  g(M, p)  or in-place  g!(M, X, p) costgradient = nothing  the combined cost and gradient function  fg(M,p)  or in-place  fg!(M, X, p)) costdifferential = nothing  the combined cost and differential function   fdf(M, p, X) evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. Where: At least one of  cost ,  costgradient  or  costdifferential  must be provided. Either  gradient ,  costgradient ,  differential  or  costdifferential  must be provided. If more than one function provides the same thing (e.g. cost), it is assumed that all such functions return the same value. Optimization algorithms will attempt to make the most efficient use of provided functions. Used with gradient_descent ,  conjugate_gradient_descent ,  quasi_Newton source"},{"id":2529,"pagetitle":"Objective","title":"Manopt.ManifoldAlternatingGradientObjective","ref":"/manopt/stable/plans/objective/#Manopt.ManifoldAlternatingGradientObjective","content":" Manopt.ManifoldAlternatingGradientObjective  ‚Äî  Type ManifoldAlternatingGradientObjective{E<:AbstractEvaluationType,F,G} <: AbstractManifoldFirstOrderObjective{E, Tuple{F,G}} An alternating gradient objective consists of a cost function  $F(x)$ a gradient  $\\operatorname{grad}F$  that is either given as one function  $\\operatorname{grad}F$  returning a tangent vector  X  on  M  or an array of gradient functions  $\\operatorname{grad}F_i$ ,  √¨=1,‚Ä¶,n  s each returning a component of the gradient which might be allocating or mutating variants, but not a mix of both. Note This Objective is usually defined using the  ProductManifold  from  Manifolds.jl , so  Manifolds.jl  to be loaded. Constructors ManifoldAlternatingGradientObjective(F, gradF::Function;\n    evaluation=AllocatingEvaluation()\n)\nManifoldAlternatingGradientObjective(F, gradF::AbstractVector{<:Function};\n    evaluation=AllocatingEvaluation()\n) Create a alternating gradient problem with an optional  cost  and the gradient either as one function (returning an array) or a vector of functions. source"},{"id":2530,"pagetitle":"Objective","title":"Manopt.ManifoldStochasticGradientObjective","ref":"/manopt/stable/plans/objective/#Manopt.ManifoldStochasticGradientObjective","content":" Manopt.ManifoldStochasticGradientObjective  ‚Äî  Type ManifoldStochasticGradientObjective{E<:AbstractEvaluationType, F, G} <: AbstractManifoldFirstOrderObjective{E, Tuple{F,G}} A stochastic gradient objective consists of a(n optional) cost function  $f(p) = \\displaystyle\\sum_{i=1}^{n} f_i(p)$ an array of gradients,  $\\operatorname{grad} f_i(p), i=1,‚Ä¶,n$  which can be given in two forms as one single function  $(\\mathcal M, p) ‚Ü¶ (X_1,‚Ä¶,X_n) ‚àà (T_{p}\\mathcal M)^n$ as a vector of functions  $\\bigl( (\\mathcal M, p) ‚Ü¶ X_1, ‚Ä¶, (\\mathcal M, p) ‚Ü¶ X_n\\bigr)$ . Where both variants can also be provided as  InplaceEvaluation  functions  (M, X, p) -> X , where  X  is the vector of  X1,...,Xn  and  (M, X1, p) -> X1, ..., (M, Xn, p) -> Xn , respectively. Constructors ManifoldStochasticGradientObjective(\n    grad_f::Function;\n    cost=Missing(),\n    evaluation=AllocatingEvaluation()\n)\nManifoldStochasticGradientObjective(\n    grad_f::AbstractVector{<:Function};\n    cost=Missing(), evaluation=AllocatingEvaluation()\n) Create a Stochastic gradient problem with the gradient either as one function (returning an array of tangent vectors) or a vector of functions (each returning one tangent vector). The optional cost can also be given as either a single function (returning a number) pr a vector of functions, each returning a value. Used with stochastic_gradient_descent Note that this can also be used with a  gradient_descent , since the (complete) gradient is just the sums of the single gradients. source"},{"id":2531,"pagetitle":"Objective","title":"Manopt.NonlinearLeastSquaresObjective","ref":"/manopt/stable/plans/objective/#Manopt.NonlinearLeastSquaresObjective","content":" Manopt.NonlinearLeastSquaresObjective  ‚Äî  Type NonlinearLeastSquaresObjective{E<:AbstractEvaluationType} <: AbstractManifoldObjective{T} An objective to model the nonlinear least squares problem \\[\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} \\frac{1}{2} \\sum_{}^{}_{i=1}^m \\lvert f_i(p) \\rvert^2\\] where  $f: \\mathcal M ‚Üí ‚Ñù^m$  is written with component functions  $f_i: \\mathcal M ‚Üí ‚Ñù$ ,  $i=1,‚Ä¶,m$ , and each component function is continuously differentiable. Specify a nonlinear least squares problem Fields objective : a  AbstractVectorGradientFunction {E}  containing both the vector of cost functions  $f_i$  (or a function returning a vector of costs) as well as their gradients  $\\operatorname{grad} f_i$  (or Jacobian of the vector-valued function). This  NonlinearLeastSquaresObjective  then has the same  AbstractEvaluationType T  as the (inner)  objective . Constructors NonlinearLeastSquaresObjective(f, jacobian, range_dimension::Integer; kwargs...)\nNonlinearLeastSquaresObjective(vf::AbstractVectorGradientFunction) Arguments f  the vectorial cost function  $f: \\mathcal M ‚Üí ‚Ñù^m$ jacobian  the Jacobian, might also be a vector of gradients of the component functions of  f range_dimension::Integer  the number of dimensions  m  the function  f  maps into These three can also be passed as a  AbstractVectorGradientFunction vf  already. Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. function_type:: AbstractVectorialType = FunctionVectorialType () : specify the format the residuals are given in. By default a function returning a vector. jacobian_tangent_basis::AbstractBasis=DefaultOrthonormalBasis() ; shortcut to specify the basis the Jacobian matrix is build with. jacobian_type:: AbstractVectorialType = CoordinateVectorialType (jacobian_tangent_basis) : specify the format the Jacobian is given in. By default a matrix of the differential with respect to a certain basis of the tangent space. See also LevenbergMarquardt ,  LevenbergMarquardtState source While the  ManifoldFirstOrderObjective  allows to provide different first order information, there are also its shortcuts, mainly for historical reasons, but also since these are the most commonly used ones."},{"id":2532,"pagetitle":"Objective","title":"Manopt.ManifoldGradientObjective","ref":"/manopt/stable/plans/objective/#Manopt.ManifoldGradientObjective","content":" Manopt.ManifoldGradientObjective  ‚Äî  Type ManifoldGradientObjective(cost, gradient; evaluation::E=AllocatingEvaluation() kwargs...) Generate an objective with a function  cost  and its  gradient . Depending on the  AbstractEvaluationType E  the gradient can have to forms as a function  (M, p) -> X  that allocates memory for  X , an  AllocatingEvaluation as a function  (M, X, p) -> X  that work in place of  X , an  InplaceEvaluation Internally this is stored in a  ManifoldFirstOrderObjective . The  kwargs...  are also passed to this representation, which allows to add a special function to evaluate the  differential . Used with gradient_descent ,  conjugate_gradient_descent ,  quasi_Newton source"},{"id":2533,"pagetitle":"Objective","title":"Manopt.ManifoldCostGradientObjective","ref":"/manopt/stable/plans/objective/#Manopt.ManifoldCostGradientObjective","content":" Manopt.ManifoldCostGradientObjective  ‚Äî  Type ManifoldCostGradientObjective(costgrad; evaluation::E=AllocatingEvaluation(), kwargs...) create an objective containing one function to perform a combined computation of cost and its gradient Depending on the  AbstractEvaluationType E  the gradient can have to forms as a function  (M, p) -> (c, X)  that allocates memory for the gradient  X , an  AllocatingEvaluation as a function  (M, X, p) -> (c, X)  that work in place of  X , an  InplaceEvaluation Internally this is stored in a  ManifoldFirstOrderObjective . The  kwargs...  are also passed to this representation, which allows to add a special function to evaluate the  differential . Used with gradient_descent ,  conjugate_gradient_descent ,  quasi_Newton source"},{"id":2534,"pagetitle":"Objective","title":"Access functions","ref":"/manopt/stable/plans/objective/#Access-functions-2","content":" Access functions"},{"id":2535,"pagetitle":"Objective","title":"Manopt.get_gradient","ref":"/manopt/stable/plans/objective/#Manopt.get_gradient","content":" Manopt.get_gradient  ‚Äî  Function get_gradient(s::AbstractManoptSolverState) return the (last stored) gradient within  AbstractManoptSolverState s`. By default also undecorates the state beforehand source get_gradient(amp::AbstractManoptProblem, p)\nget_gradient!(amp::AbstractManoptProblem, X, p) evaluate the gradient of an  AbstractManoptProblem amp  at the point  p . The evaluation is done in place of  X  for the  ! -variant. source get_gradient(agst::AbstractGradientSolverState) return the gradient stored within gradient options. THe default returns  agst.X . source get_gradient(M::AbstractManifold, mgo::ManifoldProximalGradientObjective, p)\nget_gradient!(M::AbstractManifold, X, mgo::ManifoldProximalGradientObjective, p) Evaluate the gradient of the smooth part of a  ManifoldProximalGradientObjective mgo  at  p . source get_gradient(M::AbstractManifold, vgf::VectorGradientFunction, p, i)\nget_gradient(M::AbstractManifold, vgf::VectorGradientFunction, p, i, range)\nget_gradient!(M::AbstractManifold, X, vgf::VectorGradientFunction, p, i)\nget_gradient!(M::AbstractManifold, X, vgf::VectorGradientFunction, p, i, range) Evaluate the gradients of the vector function  vgf  on the manifold  M  at  p  and the values given in  range , specifying the representation of the gradients. Since  i  is assumed to be a linear index, you can provide a single integer a  UnitRange  to specify a range to be returned like  1:3 a  BitVector  specifying a selection a  AbstractVector{<:Integer}  to specify indices :  to return the vector of all gradients source get_gradient(TpM, trmo::TrustRegionModelObjective, X) Evaluate the gradient of the  TrustRegionModelObjective \\[\\operatorname{grad} m(X) = \\operatorname{grad} f(p) + \\operatorname{Hess} f(p)[X].\\] source get_gradient(TpM, trmo::AdaptiveRagularizationWithCubicsModelObjective, X) Evaluate the gradient of the  AdaptiveRagularizationWithCubicsModelObjective \\[\\operatorname{grad} m(X) = \\operatorname{grad} f(p) + \\operatorname{Hess} f(p)[X]\n       + œÉ\\lVert X \\rVert X,\\] at  X , cf. Eq. (37) in [ ABBC20 ]. source get_gradient(TpM::TangentSpace, slso::SymmetricLinearSystemObjective, X)\nget_gradient!(TpM::TangentSpace, Y, slso::SymmetricLinearSystemObjective, X) evaluate the gradient of \\[f(X) = \\frac{1}{2} \\lVert \\mathcal A[X] + b \\rVert_{p}^2,\\qquad X ‚àà T_{p}\\mathcal M,\\] Which is  $\\operatorname{grad} f(X) = \\mathcal A[X]+b$ . This can be computed in-place of  Y . source get_gradient(M::AbstractManifold, sgo::ManifoldStochasticGradientObjective, p, k)\nget_gradient!(M::AbstractManifold, sgo::ManifoldStochasticGradientObjective, Y, p, k) Evaluate one of the summands gradients  $\\operatorname{grad}f_k$ ,  $k‚àà\\{1,‚Ä¶,n\\}$ , at  x  (in place of  Y ). If you use a single function for the stochastic gradient, that works in-place, then  get_gradient  is not available, since the length (or number of elements of the gradient required for allocation) can not be determined. source get_gradient(M::AbstractManifold, sgo::ManifoldStochasticGradientObjective, p)\nget_gradient!(M::AbstractManifold, sgo::ManifoldStochasticGradientObjective, X, p) Evaluate the complete gradient  $\\operatorname{grad} f = \\displaystyle\\sum_{i=1}^n \\operatorname{grad} f_i(p)$  at  p  (in place of  X ). If you use a single function for the stochastic gradient, that works in-place, then  get_gradient  is not available, since the length (or number of elements of the gradient required for allocation) can not be determined. source get_gradient(M::AbstractManifold, emo::EmbeddedManifoldObjective, p)\nget_gradient!(M::AbstractManifold, X, emo::EmbeddedManifoldObjective, p) Evaluate the gradient function of an objective defined in the embedding, that is embed  p  before calling the gradient function stored in the  EmbeddedManifoldObjective . The returned gradient is then converted to a Riemannian gradient calling  riemannian_gradient . source get_gradient(M::AbstractManifold, scaled_objective::ScaledManifoldObjective, p)\nget_gradient!(M::AbstractManifold, X, scaled_objective::ScaledManifoldObjective, p) Evaluate the scaled gradient.  $s*\\operatorname{grad}f(p)$ source"},{"id":2536,"pagetitle":"Objective","title":"Manopt.get_gradients","ref":"/manopt/stable/plans/objective/#Manopt.get_gradients","content":" Manopt.get_gradients  ‚Äî  Function get_gradients(M::AbstractManifold, sgo::ManifoldStochasticGradientObjective, p)\nget_gradients!(M::AbstractManifold, X, sgo::ManifoldStochasticGradientObjective, p) Evaluate all summands gradients  $\\{\\operatorname{grad}f_i\\}_{i=1}^n$  at  p  (in place of  X ). If you use a single function for the stochastic gradient, that works in-place, then  get_gradient  is not available, since the length (or number of elements of the gradient) can not be determined. source"},{"id":2537,"pagetitle":"Objective","title":"Manopt.get_differential","ref":"/manopt/stable/plans/objective/#Manopt.get_differential","content":" Manopt.get_differential  ‚Äî  Function  get_differential(amp::AbstractManoptProblem, p, X; kwargs...)\n get_differential(M::AbstractManifold, amfo:AbstractManifoldFirstOrderObjective, p, X; kwargs...)\n get_differential(M::AbstractManifold, amfo:AbstractDecoratedManifoldObjective, p, X; kwargs...) Evaluate the differential  $Df(p)[X]$  of the function  $f$  represented by the  AbstractManifoldFirstOrderObjective . For  AbstractManoptProblem  the inner manifold and objectives are used, similarly, any objective decorator would ‚Äúpass though‚Äù to its inner objective. By default this falls back to ``Df(p)[X] = ‚ü®\\operatorname{grad}f(p), X‚ü© Keyword arguments gradient=nothing  ‚Äì pass a tangent vector to be used internally as interims memory, e.g. in the default variant to evaluate the gradient in-place in. evaluated=false  ‚Äì indicate whether  gradient  is just memory ( false , default) or already contains the evaluated gradient ( true ). source"},{"id":2538,"pagetitle":"Objective","title":"Manopt.get_residuals","ref":"/manopt/stable/plans/objective/#Manopt.get_residuals","content":" Manopt.get_residuals  ‚Äî  Function get_residuals(M::AbstractManifold, nlso::NonlinearLeastSquaresObjective, p)\nget_residuals!(M::AbstractManifold, V, nlso::NonlinearLeastSquaresObjective, p) Compute the vector of residuals  $f_i(p)$ ,  $i=1,‚Ä¶,m$  given the manifold  M , the  NonlinearLeastSquaresObjective nlso  and a current point  $p$  on  M . source"},{"id":2539,"pagetitle":"Objective","title":"Manopt.get_residuals!","ref":"/manopt/stable/plans/objective/#Manopt.get_residuals!","content":" Manopt.get_residuals!  ‚Äî  Function get_residuals(M::AbstractManifold, nlso::NonlinearLeastSquaresObjective, p)\nget_residuals!(M::AbstractManifold, V, nlso::NonlinearLeastSquaresObjective, p) Compute the vector of residuals  $f_i(p)$ ,  $i=1,‚Ä¶,m$  given the manifold  M , the  NonlinearLeastSquaresObjective nlso  and a current point  $p$  on  M . source and internally"},{"id":2540,"pagetitle":"Objective","title":"Manopt.get_differential_function","ref":"/manopt/stable/plans/objective/#Manopt.get_differential_function","content":" Manopt.get_differential_function  ‚Äî  Function  get_differential_function(admo::AbstractManifoldFirstOrderObjective, recursive::Bool=false) Return the function to evaluate (just) the differential  $Df(p)[X]$ . For a decorated objective, the  recursive  positional parameter determines whether to directly call this function on the next decorator or whether to get the ‚Äúmost inner‚Äù objective. source"},{"id":2541,"pagetitle":"Objective","title":"Manopt.get_gradient_function","ref":"/manopt/stable/plans/objective/#Manopt.get_gradient_function","content":" Manopt.get_gradient_function  ‚Äî  Function get_gradient_function(amgo::AbstractManifoldFirstOrderObjective, recursive=false) return the function to evaluate (just) the gradient  $\\operatorname{grad} f(p)$ , where either the gradient function using the decorator or without the decorator is used. By default  recursive  is set to  false , since usually to just pass the gradient function somewhere, one still wants for example the cached one or the one that still counts calls. Depending on the  AbstractEvaluationType E  this is a function (M, p) -> X  for the  AllocatingEvaluation  case (M, X, p) -> X  for the  InplaceEvaluation  working in-place of  X . source"},{"id":2542,"pagetitle":"Objective","title":"Subgradient objective","ref":"/manopt/stable/plans/objective/#Subgradient-objective","content":" Subgradient objective"},{"id":2543,"pagetitle":"Objective","title":"Manopt.ManifoldSubgradientObjective","ref":"/manopt/stable/plans/objective/#Manopt.ManifoldSubgradientObjective","content":" Manopt.ManifoldSubgradientObjective  ‚Äî  Type ManifoldSubgradientObjective{T<:AbstractEvaluationType,C,S} <:AbstractManifoldCostObjective{T, C} A structure to store information about a objective for a subgradient based optimization problem Fields cost :        the function  $f$  to be minimized subgradient : a function returning a subgradient  $‚àÇf$  of  $f$ Constructor ManifoldSubgradientObjective(f, ‚àÇf) Generate the  ManifoldSubgradientObjective  for a subgradient objective, consisting of a (cost) function  f(M, p)  and a function  ‚àÇf(M, p)  that returns a not necessarily deterministic element from the subdifferential at  p  on a manifold  M . source"},{"id":2544,"pagetitle":"Objective","title":"Access functions","ref":"/manopt/stable/plans/objective/#Access-functions-3","content":" Access functions"},{"id":2545,"pagetitle":"Objective","title":"Manopt.get_subgradient","ref":"/manopt/stable/plans/objective/#Manopt.get_subgradient","content":" Manopt.get_subgradient  ‚Äî  Function X = get_subgradient(M::AbstractManifold, sgo::AbstractManifoldFirstOrderObjective, p)\nget_subgradient!(M::AbstractManifold, X, sgo::AbstractManifoldFirstOrderObjective, p) Evaluate the subgradient, which for the case of a objective having a gradient, means evaluating the gradient itself. While in general, the result might not be deterministic, for this case it is. source get_subgradient(amp::AbstractManoptProblem, p)\nget_subgradient!(amp::AbstractManoptProblem, X, p) evaluate the subgradient of an  AbstractManoptProblem amp  at point  p . The evaluation is done in place of  X  for the  ! -variant. The result might not be deterministic,  one  element of the subdifferential is returned. source X = get_subgradient(M;;AbstractManifold, sgo::ManifoldSubgradientObjective, p)\nget_subgradient!(M;;AbstractManifold, X, sgo::ManifoldSubgradientObjective, p) Evaluate the (sub)gradient of a  ManifoldSubgradientObjective sgo  at the point  p . The evaluation is done in place of  X  for the  ! -variant. The result might not be deterministic,  one  element of the subdifferential is returned. source and internally"},{"id":2546,"pagetitle":"Objective","title":"Manopt.get_subgradient_function","ref":"/manopt/stable/plans/objective/#Manopt.get_subgradient_function","content":" Manopt.get_subgradient_function  ‚Äî  Function get_subgradient_function(amgo::ManifoldSubgradientObjective, recursive=false) return the function to evaluate (just) the gradient  $\\operatorname{grad} f(p)$ , where either the gradient function using the decorator or without the decorator is used. By default  recursive  is set to  false , since usually to just pass the gradient function somewhere, one still wants for example the cached one or the one that still counts calls. Depending on the  AbstractEvaluationType E  this is a function (M, p) -> X  for the  AllocatingEvaluation  case (M, X, p) -> X  for the  InplaceEvaluation  working in-place of  X . source"},{"id":2547,"pagetitle":"Objective","title":"Proximal map objective","ref":"/manopt/stable/plans/objective/#Proximal-map-objective","content":" Proximal map objective"},{"id":2548,"pagetitle":"Objective","title":"Manopt.ManifoldProximalMapObjective","ref":"/manopt/stable/plans/objective/#Manopt.ManifoldProximalMapObjective","content":" Manopt.ManifoldProximalMapObjective  ‚Äî  Type ManifoldProximalMapObjective{E<:AbstractEvaluationType, TC, TP, V <: Vector{<:Integer}} <: AbstractManifoldCostObjective{E, TC} specify a problem for solvers based on the evaluation of proximal maps, which represents proximal maps  $\\operatorname{prox}_{Œªf_i}$  for summands  $f = f_1 + f_2+ ‚Ä¶ + f_N$  of the cost function  $f$ . Fields cost : a function  $f:\\mathcal M‚Üí‚Ñù$  to minimize proxes : proximal maps  $\\operatorname{prox}_{Œªf_i}:\\mathcal M ‚Üí \\mathcal M$  as functions  (M, Œª, p) -> q  or in-place  (M, q, Œª, p) . number_of_proxes : number of proximal maps per function, to specify when one of the maps is a combined one such that the proximal maps functions return more than one entry per function, you have to adapt this value. if not specified, it is set to one prox per function. Constructor ManifoldProximalMapObjective(f, proxes_f::Union{Tuple,AbstractVector}, numer_of_proxes=onex(length(proxes));\n   evaluation=Allocating) Generate a proximal problem with a tuple or vector of funtions, where by default every function computes a single prox of one component of  $f$ . ManifoldProximalMapObjective(f, prox_f); evaluation=Allocating) Generate a proximal objective for  $f$  and its proxial map  $\\operatorname{prox}_{Œªf}$ See also cyclic_proximal_point ,  get_cost ,  get_proximal_map source"},{"id":2549,"pagetitle":"Objective","title":"Access functions","ref":"/manopt/stable/plans/objective/#Access-functions-4","content":" Access functions"},{"id":2550,"pagetitle":"Objective","title":"Manopt.get_proximal_map","ref":"/manopt/stable/plans/objective/#Manopt.get_proximal_map","content":" Manopt.get_proximal_map  ‚Äî  Function q = get_proximal_map(M::AbstractManifold, mpo::ManifoldProximalMapObjective, Œª, p)\nget_proximal_map!(M::AbstractManifold, q, mpo::ManifoldProximalMapObjective, Œª, p)\nq = get_proximal_map(M::AbstractManifold, mpo::ManifoldProximalMapObjective, Œª, p, i)\nget_proximal_map!(M::AbstractManifold, q, mpo::ManifoldProximalMapObjective, Œª, p, i) evaluate the ( i th) proximal map of the  ManifoldProximalMapObjective mpo  at the point  p  of  M  with parameter  $Œª>0$ . source q = get_proximal_map(M::AbstractManifold, mpo::ManifoldProximalGradientObjective, Œª, p)\nget_proximal_map!(M::AbstractManifold, q, mpo::ManifoldProximalGradientObjective, Œª, p) Evaluate proximal map of the nonsmooth component  $h$  of the  ManifoldProximalGradientObjective mpo  at the point  p  on  M  with parameter  $Œª>0$ . source"},{"id":2551,"pagetitle":"Objective","title":"Hessian objective","ref":"/manopt/stable/plans/objective/#Hessian-objective","content":" Hessian objective"},{"id":2552,"pagetitle":"Objective","title":"Manopt.AbstractManifoldHessianObjective","ref":"/manopt/stable/plans/objective/#Manopt.AbstractManifoldHessianObjective","content":" Manopt.AbstractManifoldHessianObjective  ‚Äî  Type AbstractManifoldHessianObjective{E<:AbstractEvaluationType,F, G, H} <: AbstractManifoldFirstOrderObjective{E,Tuple{F,G}} An abstract type for all objectives that provide a (full) Hessian, where  T  is a  AbstractEvaluationType  for the gradient and Hessian functions. source"},{"id":2553,"pagetitle":"Objective","title":"Manopt.ManifoldHessianObjective","ref":"/manopt/stable/plans/objective/#Manopt.ManifoldHessianObjective","content":" Manopt.ManifoldHessianObjective  ‚Äî  Type ManifoldHessianObjective{T<:AbstractEvaluationType,C,G,H,Pre} <: AbstractManifoldHessianObjective{T,C,G,H} specify a problem for Hessian based algorithms. Fields cost :           a function  $f:\\mathcal M‚Üí‚Ñù$  to minimize gradient :       the gradient  $\\operatorname{grad}f:\\mathcal M ‚Üí \\mathcal T\\mathcal M$  of the cost function  $f$ hessian :        the Hessian  $\\operatorname{Hess}f(x)[‚ãÖ]: \\mathcal T_{x} \\mathcal M ‚Üí \\mathcal T_{x} \\mathcal M$  of the cost function  $f$ preconditioner : the symmetric, positive definite preconditioner as an approximation of the inverse of the Hessian of  $f$ , a map with the same input variables as the  hessian  to numerically stabilize iterations when the Hessian is ill-conditioned Depending on the  AbstractEvaluationType T  the gradient and can have to forms as a function  (M, p) -> X   and  (M, p, X) -> Y , resp., an  AllocatingEvaluation as a function  (M, X, p) -> X  and (M, Y, p, X), resp., an  InplaceEvaluation Constructor ManifoldHessianObjective(f, grad_f, Hess_f, preconditioner = (M, p, X) -> X;\n    evaluation=AllocatingEvaluation()) See also truncated_conjugate_gradient_descent ,  trust_regions source"},{"id":2554,"pagetitle":"Objective","title":"Access functions","ref":"/manopt/stable/plans/objective/#Access-functions-5","content":" Access functions"},{"id":2555,"pagetitle":"Objective","title":"Manopt.get_hessian","ref":"/manopt/stable/plans/objective/#Manopt.get_hessian","content":" Manopt.get_hessian  ‚Äî  Function Y = get_hessian(amp::AbstractManoptProblem{T}, p, X)\nget_hessian!(amp::AbstractManoptProblem{T}, Y, p, X) evaluate the Hessian of an  AbstractManoptProblem amp  at  p  applied to a tangent vector  X , computing  $\\operatorname{Hess}f(q)[X]$ , which can also happen in-place of  Y . source get_hessian(M::AbstractManifold, vgf::VectorHessianFunction, p, X, i)\nget_hessian(M::AbstractManifold, vgf::VectorHessianFunction, p, X, i, range)\nget_hessian!(M::AbstractManifold, X, vgf::VectorHessianFunction, p, X, i)\nget_hessian!(M::AbstractManifold, X, vgf::VectorHessianFunction, p, X, i, range) Evaluate the Hessians of the vector function  vgf  on the manifold  M  at  p  in direction  X  and the values given in  range , specifying the representation of the gradients. Since  i  is assumed to be a linear index, you can provide a single integer a  UnitRange  to specify a range to be returned like  1:3 a  BitVector  specifying a selection a  AbstractVector{<:Integer}  to specify indices :  to return the vector of all Hessian evaluations source get_hessian(TpM, trmo::TrustRegionModelObjective, X) Evaluate the Hessian of the  TrustRegionModelObjective \\[\\operatorname{Hess} m(X)[Y] = \\operatorname{Hess} f(p)[Y].\\] source get_Hessian(TpM::TangentSpace, slso::SymmetricLinearSystemObjective, X, V)\nget_Hessian!(TpM::TangentSpace, W, slso::SymmetricLinearSystemObjective, X, V) evaluate the Hessian of \\[f(X) = \\frac{1}{2} \\lVert \\mathcal A[X] + b \\rVert_{p}^2,\\qquad X ‚àà T_{p}\\mathcal M,\\] Which is  $\\operatorname{Hess} f(X)[Y] = \\mathcal A[V]$ . This can be computed in-place of  W . source get_hessian(M::AbstractManifold, emo::EmbeddedManifoldObjective, p, X)\nget_hessian!(M::AbstractManifold, Y, emo::EmbeddedManifoldObjective, p, X) Evaluate the Hessian of an objective defined in the embedding, that is embed  p  and  X  before calling the Hessian function stored in the  EmbeddedManifoldObjective . The returned Hessian is then converted to a Riemannian Hessian calling   riemannian_Hessian . source get_hessian(M::AbstractManifold, scaled_objective::ScaledManifoldObjective, p, X)\nget_hessian!(M::AbstractManifold, Y, scaled_objective::ScaledManifoldObjective, p, X) Evaluate the scaled Hessian  $s*\\operatorname{Hess}f(p)$ source"},{"id":2556,"pagetitle":"Objective","title":"Manopt.get_preconditioner","ref":"/manopt/stable/plans/objective/#Manopt.get_preconditioner","content":" Manopt.get_preconditioner  ‚Äî  Function get_preconditioner(amp::AbstractManoptProblem, p, X) evaluate the symmetric, positive definite preconditioner (approximation of the inverse of the Hessian of the cost function  f ) of a  AbstractManoptProblem amp s objective at the point  p  applied to a tangent vector  X . source get_preconditioner(M::AbstractManifold, mho::ManifoldHessianObjective, p, X) evaluate the symmetric, positive definite preconditioner (approximation of the inverse of the Hessian of the cost function  F ) of a  ManifoldHessianObjective mho  at the point  p  applied to a tangent vector  X . source and internally"},{"id":2557,"pagetitle":"Objective","title":"Manopt.get_hessian_function","ref":"/manopt/stable/plans/objective/#Manopt.get_hessian_function","content":" Manopt.get_hessian_function  ‚Äî  Function get_hessian_function(amgo::ManifoldHessianObjective{E<:AbstractEvaluationType}) return the function to evaluate (just) the Hessian  $\\operatorname{Hess} f(p)$ . Depending on the  AbstractEvaluationType E  this is a function (M, p, X) -> Y  for the  AllocatingEvaluation  case (M, Y, p, X) -> X  for the  InplaceEvaluation , working in-place of  Y . source"},{"id":2558,"pagetitle":"Objective","title":"Primal-dual based objectives","ref":"/manopt/stable/plans/objective/#Primal-dual-based-objectives","content":" Primal-dual based objectives"},{"id":2559,"pagetitle":"Objective","title":"Manopt.AbstractPrimalDualManifoldObjective","ref":"/manopt/stable/plans/objective/#Manopt.AbstractPrimalDualManifoldObjective","content":" Manopt.AbstractPrimalDualManifoldObjective  ‚Äî  Type AbstractPrimalDualManifoldObjective{E<:AbstractEvaluationType,C,P} <: AbstractManifoldCostObjective{E,C} A common abstract super type for objectives that consider primal-dual problems. source"},{"id":2560,"pagetitle":"Objective","title":"Manopt.PrimalDualManifoldObjective","ref":"/manopt/stable/plans/objective/#Manopt.PrimalDualManifoldObjective","content":" Manopt.PrimalDualManifoldObjective  ‚Äî  Type PrimalDualManifoldObjective{T<:AbstractEvaluationType} <: AbstractPrimalDualManifoldObjective{T} Describes an Objective linearized or exact Chambolle-Pock algorithm, cf. [ BHS+21 ], [ CP11 ] Fields All fields with  !!  can either be in-place or allocating functions, which should be set depending on the  evaluation=  keyword in the constructor and stored in  T <: AbstractEvaluationType . cost :                           $F + G(Œõ(‚ãÖ))$  to evaluate interim cost function values linearized_forward_operator!! : linearized operator for the forward operation in the algorithm  $DŒõ$ linearized_adjoint_operator!! : the adjoint differential  $(DŒõ)^* : \\mathcal N ‚Üí T\\mathcal M$ prox_f!! :                      the proximal map belonging to  $f$ prox_G_dual!! :                 the proximal map belonging to  $g_n^*$ Œõ!! :                           the  forward operator (if given)  $Œõ: \\mathcal M ‚Üí \\mathcal N$ Either the linearized operator  $DŒõ$  or  $Œõ$  are required usually. Constructor PrimalDualManifoldObjective(cost, prox_f, prox_G_dual, adjoint_linearized_operator;\n    linearized_forward_operator::Union{Function,Missing}=missing,\n    Œõ::Union{Function,Missing}=missing,\n    evaluation::AbstractEvaluationType=AllocatingEvaluation()\n) The last optional argument can be used to provide the 4 or 5 functions as allocating or mutating (in place computation) ones. Note that the first argument is always the manifold under consideration, the mutated one is the second. source"},{"id":2561,"pagetitle":"Objective","title":"Manopt.PrimalDualManifoldSemismoothNewtonObjective","ref":"/manopt/stable/plans/objective/#Manopt.PrimalDualManifoldSemismoothNewtonObjective","content":" Manopt.PrimalDualManifoldSemismoothNewtonObjective  ‚Äî  Type PrimalDualManifoldSemismoothNewtonObjective{E<:AbstractEvaluationType, TC, LO, ALO, PF, DPF, PG, DPG, L} <: AbstractPrimalDualManifoldObjective{E, TC, PF} Describes a Problem for the Primal-dual Riemannian semismooth Newton algorithm. [ DL21 ] Fields cost :                         $F + G(Œõ(‚ãÖ))$  to evaluate interim cost function values linearized_operator :         the linearization  $DŒõ(‚ãÖ)[‚ãÖ]$  of the operator  $Œõ(‚ãÖ)$ . linearized_adjoint_operator : the adjoint differential  $(DŒõ)^* : \\mathcal N ‚Üí T\\mathcal M$ prox_F :                      the proximal map belonging to  $F$ diff_prox_F :                 the (Clarke Generalized) differential of the proximal maps of  $F$ prox_G_dual :                 the proximal map belonging to  G^\\ast_n ` diff_prox_dual_G :            the (Clarke Generalized) differential of the proximal maps of  $G^\\ast_n$ Œõ :                           the exact forward operator. This operator is required if  Œõ(m)=n  does not hold. Constructor PrimalDualManifoldSemismoothNewtonObjective(cost, prox_F, prox_G_dual, forward_operator, adjoint_linearized_operator,Œõ) source"},{"id":2562,"pagetitle":"Objective","title":"Access functions","ref":"/manopt/stable/plans/objective/#Access-functions-6","content":" Access functions"},{"id":2563,"pagetitle":"Objective","title":"Manopt.adjoint_linearized_operator","ref":"/manopt/stable/plans/objective/#Manopt.adjoint_linearized_operator","content":" Manopt.adjoint_linearized_operator  ‚Äî  Function X = adjoint_linearized_operator(N::AbstractManifold, apdmo::AbstractPrimalDualManifoldObjective, m, n, Y)\nadjoint_linearized_operator(N::AbstractManifold, X, apdmo::AbstractPrimalDualManifoldObjective, m, n, Y) Evaluate the adjoint of the linearized forward operator of  $(DŒõ(m))^*[Y]$  stored within the  AbstractPrimalDualManifoldObjective  (in place of  X ). Since  $Y‚ààT_n\\mathcal N$ , both  $m$  and  $n=Œõ(m)$  are necessary arguments, mainly because the forward operator  $Œõ$  might be  missing  in  p . source"},{"id":2564,"pagetitle":"Objective","title":"Manopt.forward_operator","ref":"/manopt/stable/plans/objective/#Manopt.forward_operator","content":" Manopt.forward_operator  ‚Äî  Function q = forward_operator(M::AbstractManifold, N::AbstractManifold, apdmo::AbstractPrimalDualManifoldObjective, p)\nforward_operator!(M::AbstractManifold, N::AbstractManifold, q, apdmo::AbstractPrimalDualManifoldObjective, p) Evaluate the forward operator of  $Œõ(x)$  stored within the  TwoManifoldProblem  (in place of  q ). source"},{"id":2565,"pagetitle":"Objective","title":"Manopt.get_differential_dual_prox","ref":"/manopt/stable/plans/objective/#Manopt.get_differential_dual_prox","content":" Manopt.get_differential_dual_prox  ‚Äî  Function Œ∑ = get_differential_dual_prox(N::AbstractManifold, pdsno::PrimalDualManifoldSemismoothNewtonObjective, n, œÑ, X, Œæ)\nget_differential_dual_prox!(N::AbstractManifold, pdsno::PrimalDualManifoldSemismoothNewtonObjective, Œ∑, n, œÑ, X, Œæ) Evaluate the differential proximal map of  $G_n^*$  stored within  PrimalDualManifoldSemismoothNewtonObjective \\[D\\operatorname{prox}_{œÑG_n^*}(X)[Œæ]\\] which can also be computed in place of  Œ∑ . source"},{"id":2566,"pagetitle":"Objective","title":"Manopt.get_differential_primal_prox","ref":"/manopt/stable/plans/objective/#Manopt.get_differential_primal_prox","content":" Manopt.get_differential_primal_prox  ‚Äî  Function y = get_differential_primal_prox(M::AbstractManifold, pdsno::PrimalDualManifoldSemismoothNewtonObjective œÉ, x)\nget_differential_primal_prox!(p::TwoManifoldProblem, y, œÉ, x) Evaluate the differential proximal map of  $F$  stored within  AbstractPrimalDualManifoldObjective \\[D\\operatorname{prox}_{œÉF}(x)[X]\\] which can also be computed in place of  y . source"},{"id":2567,"pagetitle":"Objective","title":"Manopt.get_dual_prox","ref":"/manopt/stable/plans/objective/#Manopt.get_dual_prox","content":" Manopt.get_dual_prox  ‚Äî  Function Y = get_dual_prox(N::AbstractManifold, apdmo::AbstractPrimalDualManifoldObjective, n, œÑ, X)\nget_dual_prox!(N::AbstractManifold, apdmo::AbstractPrimalDualManifoldObjective, Y, n, œÑ, X) Evaluate the proximal map of  $g_n^*$  stored within  AbstractPrimalDualManifoldObjective \\[  Y = \\operatorname{prox}}_{œÑG_n^*}(X)\\] which can also be computed in place of  Y . source"},{"id":2568,"pagetitle":"Objective","title":"Manopt.get_primal_prox","ref":"/manopt/stable/plans/objective/#Manopt.get_primal_prox","content":" Manopt.get_primal_prox  ‚Äî  Function q = get_primal_prox(M::AbstractManifold, p::AbstractPrimalDualManifoldObjective, œÉ, p)\nget_primal_prox!(M::AbstractManifold, p::AbstractPrimalDualManifoldObjective, q, œÉ, p) Evaluate the proximal map of  $F$  stored within  AbstractPrimalDualManifoldObjective \\[\\operatorname{prox}_{œÉF}(x)\\] which can also be computed in place of  y . source"},{"id":2569,"pagetitle":"Objective","title":"Manopt.linearized_forward_operator","ref":"/manopt/stable/plans/objective/#Manopt.linearized_forward_operator","content":" Manopt.linearized_forward_operator  ‚Äî  Function Y = linearized_forward_operator(M::AbstractManifold, N::AbstractManifold, apdmo::AbstractPrimalDualManifoldObjective, m, X, n)\nlinearized_forward_operator!(M::AbstractManifold, N::AbstractManifold, Y, apdmo::AbstractPrimalDualManifoldObjective, m, X, n) Evaluate the linearized operator (differential)  $DŒõ(m)[X]$  stored within the  AbstractPrimalDualManifoldObjective  (in place of  Y ), where  n = Œõ(m) . source"},{"id":2570,"pagetitle":"Objective","title":"Constrained objective","ref":"/manopt/stable/plans/objective/#Constrained-objective","content":" Constrained objective"},{"id":2571,"pagetitle":"Objective","title":"Manopt.ConstrainedManifoldObjective","ref":"/manopt/stable/plans/objective/#Manopt.ConstrainedManifoldObjective","content":" Manopt.ConstrainedManifoldObjective  ‚Äî  Type ConstrainedManifoldObjective{T<:AbstractEvaluationType, C<:ConstraintType} <: AbstractManifoldObjective{T} Describes the constrained objective \\[\\begin{aligned}\n \\operatorname*{arg\\,min}_{p ‚àà\\mathcal{M}} & f(p)\\\\\n \\text{subject to } &g_i(p)\\leq0 \\quad \\text{ for all } i=1,‚Ä¶,m,\\\\\n \\quad &h_j(p)=0 \\quad \\text{ for all } j=1,‚Ä¶,n.\n\\end{aligned}\\] Fields objective : an  AbstractManifoldObjective  representing the unconstrained objective, that is containing cost  $f$ , the gradient of the cost  $f$  and maybe the Hessian. equality_constraints : an  AbstractManifoldObjective  representing the equality constraints $h: \\mathcal M ‚Üí \\mathbb R^n$  also possibly containing its gradient and/or Hessian equality_constraints : an  AbstractManifoldObjective  representing the equality constraints $h: \\mathcal M ‚Üí \\mathbb R^n$  also possibly containing its gradient and/or Hessian Constructors ConstrainedManifoldObjective(M::AbstractManifold, f, grad_f;\n    g=nothing,\n    grad_g=nothing,\n    h=nothing,\n    grad_h=nothing;\n    hess_f=nothing,\n    hess_g=nothing,\n    hess_h=nothing,\n    equality_constraints=nothing,\n    inequality_constraints=nothing,\n    evaluation=AllocatingEvaluation(),\n    M = nothing,\n    p = isnothing(M) ? nothing : rand(M),\n) Generate the constrained objective based on all involved single functions  f ,  grad_f ,  g ,  grad_g ,  h ,  grad_h , and optionally a Hessian for each of these. With  equality_constraints  and  inequality_constraints  you have to provide the dimension of the ranges of  h  and  g , respectively. You can also provide a manifold  M  and a point  p  to use one evaluation of the constraints to automatically try to determine these sizes. ConstrainedManifoldObjective(M::AbstractManifold, mho::AbstractManifoldObjective;\n    equality_constraints = nothing,\n    inequality_constraints = nothing\n) Generate the constrained objective either with explicit constraints  $g$  and  $h$ , and their gradients, or in the form where these are already encapsulated in  VectorGradientFunction s. Both variants require that at least one of the constraints (and its gradient) is provided. If any of the three parts provides a Hessian, the corresponding object, that is a  ManifoldHessianObjective  for  f  or a  VectorHessianFunction  for  g  or  h , respectively, is created. source"},{"id":2572,"pagetitle":"Objective","title":"Manopt.ManifoldConstrainedSetObjective","ref":"/manopt/stable/plans/objective/#Manopt.ManifoldConstrainedSetObjective","content":" Manopt.ManifoldConstrainedSetObjective  ‚Äî  Type ManifoldConstrainedSetObjective{E, MO, PF, IF} <: AbstractManifoldObjective{E} Model a constrained objective restricted to a set \\[\\operatorname*{arg\\,min}_{p ‚àà \\mathcal C} f(p)\\] where  $\\mathcal C ‚äÇ \\mathcal M$  is a convex closed subset. Fields objective::AbstractManifoldObjective  the (unconstrained) objective, which contains  $f$  and for example ist gradient  $\\operatorname{grad} f$ . project!!::PF  a projection function  $\\operatorname{proj}_{\\mathcal C}: \\mathcal M ‚Üí \\mathcal C$  that projects onto the set  $\\mathcal C$ . indicator::IF  the indicator function ``Œπ_{\\mathcal C}(p) = \\begin{cases}   0 &\\text{ for }p‚àà\\mathcal C\\\\    ‚àû &\\text{ else.}\\end{cases} Constructor ManifoldConstrainedSetObjective(f, grad_f, project!!; kwargs...) Generate the constrained objective for a given function  f  its gradient  grad_f  and a projection  project!! $\\operatorname{proj}_{\\mathcal C}$ . Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. indicator=nothing : the indicator function  $Œπ_{\\mathcal C}(p)$ . If not provided a test, whether the projection yields the same point is performed. For the  InplaceEvaluation  this required one allocation. source It might be beneficial to use the adapted problem to specify different ranges for the gradients of the constraints"},{"id":2573,"pagetitle":"Objective","title":"Manopt.ConstrainedManoptProblem","ref":"/manopt/stable/plans/objective/#Manopt.ConstrainedManoptProblem","content":" Manopt.ConstrainedManoptProblem  ‚Äî  Type ConstrainedProblem{\n    TM <: AbstractManifold,\n    O <: AbstractManifoldObjective\n    HR<:Union{AbstractPowerRepresentation,Nothing},\n    GR<:Union{AbstractPowerRepresentation,Nothing},\n    HHR<:Union{AbstractPowerRepresentation,Nothing},\n    GHR<:Union{AbstractPowerRepresentation,Nothing},\n} <: AbstractManoptProblem{TM} A constrained problem might feature different ranges for the (vectors of) gradients of the equality and inequality constraints. The ranges are required in a few places to allocate memory and access elements correctly, they work as follows: Assume the objective is \\[\\begin{aligned}\n \\operatorname*{arg\\,min}_{p ‚àà\\mathcal{M}} & f(p)\\\\\n \\text{subject to } &g_i(p)\\leq0 \\quad \\text{ for all } i=1,‚Ä¶,m,\\\\\n \\quad &h_j(p)=0 \\quad \\text{ for all } j=1,‚Ä¶,n.\n\\end{aligned}\\] then the gradients can (classically) be considered as vectors of the components gradients, for example  $\\bigl(\\operatorname{grad} g_1(p), \\operatorname{grad} g_2(p), ‚Ä¶, \\operatorname{grad} g_m(p) \\bigr)$ . In another interpretation, this can be considered a point on the tangent space at  $P = (p,‚Ä¶,p) \\in \\mathcal M^m$ , so in the tangent space to the  PowerManifold $\\mathcal M^m$ . The case where this is a  NestedPowerRepresentation  this agrees with the interpretation from before, but on power manifolds, more efficient representations exist. To then access the elements, the range has to be specified. That is what this problem is for. Constructor ConstrainedManoptProblem(\n    M::AbstractManifold,\n    co::ConstrainedManifoldObjective;\n    range=NestedPowerRepresentation(),\n    gradient_equality_range=range,\n    gradient_inequality_range=range\n    hessian_equality_range=range,\n    hessian_inequality_range=range\n) Creates a constrained Manopt problem specifying an  AbstractPowerRepresentation  for both the  gradient_equality_range  and the  gradient_inequality_range , respectively. source as well as the helper functions"},{"id":2574,"pagetitle":"Objective","title":"Manopt.AbstractConstrainedFunctor","ref":"/manopt/stable/plans/objective/#Manopt.AbstractConstrainedFunctor","content":" Manopt.AbstractConstrainedFunctor  ‚Äî  Type AbstractConstrainedFunctor{T} A common supertype for functors that model constraint functions. This supertype provides access for the fields  $Œª$  and  $Œº$ , the dual variables of constraints of type  T . source"},{"id":2575,"pagetitle":"Objective","title":"Manopt.AbstractConstrainedSlackFunctor","ref":"/manopt/stable/plans/objective/#Manopt.AbstractConstrainedSlackFunctor","content":" Manopt.AbstractConstrainedSlackFunctor  ‚Äî  Type AbstractConstrainedSlackFunctor{T,R} A common supertype for functors that model constraint functions with slack. This supertype additionally provides access for the fields Œº::T  the dual for the inequality constraints s::T  the slack parameter, and Œ≤::R  the  the barrier parameter which is also of type  T . source"},{"id":2576,"pagetitle":"Objective","title":"Manopt.LagrangianCost","ref":"/manopt/stable/plans/objective/#Manopt.LagrangianCost","content":" Manopt.LagrangianCost  ‚Äî  Type LagrangianCost{CO,T} <: AbstractConstrainedFunctor{T} Implement the Lagrangian of a  ConstrainedManifoldObjective co . \\[\\mathcal L(p; Œº, Œª)\n= f(p) +  \\sum_{i=1}^m Œº_ig_i(p) + \\sum_{j=1}^n Œª_jh_j(p)\\] Fields co::CO ,  Œº::T ,  Œª::T  as mentioned, where  T  represents a vector type. Constructor LagrangianCost(co, Œº, Œª) Create a functor for the Lagrangian with fixed dual variables. Example When you directly want to evaluate the Lagrangian  $\\mathcal L$  you can also call LagrangianCost(co, Œº, Œª)(M,p) source"},{"id":2577,"pagetitle":"Objective","title":"Manopt.LagrangianGradient","ref":"/manopt/stable/plans/objective/#Manopt.LagrangianGradient","content":" Manopt.LagrangianGradient  ‚Äî  Type LagrangianGradient{CO,T} The gradient of the Lagrangian of a  ConstrainedManifoldObjective co  with respect to the variable  $p$ . The formula reads \\[\\operatorname{grad}_p \\mathcal L(p; Œº, Œª)\n= \\operatorname{grad} f(p) +  \\sum_{i=1}^m Œº_i \\operatorname{grad} g_i(p) + \\sum_{j=1}^n Œª_j \\operatorname{grad} h_j(p)\\] Fields co::CO ,  Œº::T ,  Œª::T  as mentioned, where  T  represents a vector type. Constructor LagrangianGradient(co, Œº, Œª) Create a functor for the Lagrangian with fixed dual variables. Example When you directly want to evaluate the gradient of the Lagrangian  $\\operatorname{grad}_p \\mathcal L$  you can also call  LagrangianGradient(co, Œº, Œª)(M,p)  or  LagrangianGradient(co, Œº, Œª)(M,X,p)  for the in-place variant. source"},{"id":2578,"pagetitle":"Objective","title":"Manopt.LagrangianHessian","ref":"/manopt/stable/plans/objective/#Manopt.LagrangianHessian","content":" Manopt.LagrangianHessian  ‚Äî  Type LagrangianHessian{CO, V, T} The Hesian of the Lagrangian of a  ConstrainedManifoldObjective co  with respect to the variable  $p$ . The formula reads \\[\\operatorname{Hess}_p \\mathcal L(p; Œº, Œª)[X]\n= \\operatorname{Hess} f(p) +  \\sum_{i=1}^m Œº_i \\operatorname{Hess} g_i(p)[X] + \\sum_{j=1}^n Œª_j \\operatorname{Hess} h_j(p)[X]\\] Fields co::CO ,  Œº::T ,  Œª::T  as mentioned, where  T  represents a vector type. Constructor LagrangianHessian(co, Œº, Œª) Create a functor for the Lagrangian with fixed dual variables. Example When you directly want to evaluate the Hessian of the Lagrangian  $\\operatorname{Hess}_p \\mathcal L$  you can also call  LagrangianHessian(co, Œº, Œª)(M, p, X)  or  LagrangianHessian(co, Œº, Œª)(M, Y, p, X)  for the in-place variant. source"},{"id":2579,"pagetitle":"Objective","title":"Access functions","ref":"/manopt/stable/plans/objective/#Access-functions-7","content":" Access functions"},{"id":2580,"pagetitle":"Objective","title":"Manopt.equality_constraints_length","ref":"/manopt/stable/plans/objective/#Manopt.equality_constraints_length","content":" Manopt.equality_constraints_length  ‚Äî  Function equality_constraints_length(co::ConstrainedManifoldObjective) Return the number of equality constraints of an  ConstrainedManifoldObjective . This acts transparently through  AbstractDecoratedManifoldObjective s source"},{"id":2581,"pagetitle":"Objective","title":"Manopt.inequality_constraints_length","ref":"/manopt/stable/plans/objective/#Manopt.inequality_constraints_length","content":" Manopt.inequality_constraints_length  ‚Äî  Function inequality_constraints_length(cmo::ConstrainedManifoldObjective) Return the number of inequality constraints of an  ConstrainedManifoldObjective cmo . This acts transparently through  AbstractDecoratedManifoldObjective s source"},{"id":2582,"pagetitle":"Objective","title":"Manopt.get_equality_constraint","ref":"/manopt/stable/plans/objective/#Manopt.get_equality_constraint","content":" Manopt.get_equality_constraint  ‚Äî  Function get_equality_constraint(amp::AbstractManoptProblem, p, j=:)\nget_equality_constraint(M::AbstractManifold, objective, p, j=:) Evaluate equality constraints of a  ConstrainedManifoldObjective objective  at point  p  and indices  j  (by default  :  which corresponds to all indices). source"},{"id":2583,"pagetitle":"Objective","title":"Manopt.get_grad_equality_constraint","ref":"/manopt/stable/plans/objective/#Manopt.get_grad_equality_constraint","content":" Manopt.get_grad_equality_constraint  ‚Äî  Function get_grad_equality_constraint(amp::AbstractManoptProblem, p, j)\nget_grad_equality_constraint(M::AbstractManifold, co::ConstrainedManifoldObjective, p, j, range=NestedPowerRepresentation())\nget_grad_equality_constraint!(amp::AbstractManoptProblem, X, p, j)\nget_grad_equality_constraint!(M::AbstractManifold, X, co::ConstrainedManifoldObjective, p, j, range=NestedPowerRepresentation()) Evaluate the gradient or gradients  of the equality constraint  $(\\operatorname{grad} h(p))_j$  or  $\\operatorname{grad} h_j(p)$ , See also the  ConstrainedManoptProblem  to specify the range of the gradient. source"},{"id":2584,"pagetitle":"Objective","title":"Manopt.get_grad_inequality_constraint","ref":"/manopt/stable/plans/objective/#Manopt.get_grad_inequality_constraint","content":" Manopt.get_grad_inequality_constraint  ‚Äî  Function get_grad_inequality_constraint(amp::AbstractManoptProblem, p, j=:)\nget_grad_inequality_constraint(M::AbstractManifold, co::ConstrainedManifoldObjective, p, j=:, range=NestedPowerRepresentation())\nget_grad_inequality_constraint!(amp::AbstractManoptProblem, X, p, j=:)\nget_grad_inequality_constraint!(M::AbstractManifold, X, co::ConstrainedManifoldObjective, p, j=:, range=NestedPowerRepresentation()) Evaluate the gradient or gradients of the inequality constraint  $(\\operatorname{grad} g(p))_j$  or  $\\operatorname{grad} g_j(p)$ , See also the  ConstrainedManoptProblem  to specify the range of the gradient. source"},{"id":2585,"pagetitle":"Objective","title":"Manopt.get_hess_equality_constraint","ref":"/manopt/stable/plans/objective/#Manopt.get_hess_equality_constraint","content":" Manopt.get_hess_equality_constraint  ‚Äî  Function get_hess_equality_constraint(amp::AbstractManoptProblem, p, j=:)\nget_hess_equality_constraint(M::AbstractManifold, co::ConstrainedManifoldObjective, p, j, range=NestedPowerRepresentation())\nget_hess_equality_constraint!(amp::AbstractManoptProblem, X, p, j=:)\nget_hess_equality_constraint!(M::AbstractManifold, X, co::ConstrainedManifoldObjective, p, j, range=NestedPowerRepresentation()) Evaluate the Hessian or Hessians of the equality constraint  $(\\operatorname{Hess} h(p))_j$  or  $\\operatorname{Hess} h_j(p)$ , See also the  ConstrainedManoptProblem  to specify the range of the Hessian. source"},{"id":2586,"pagetitle":"Objective","title":"Manopt.get_hess_inequality_constraint","ref":"/manopt/stable/plans/objective/#Manopt.get_hess_inequality_constraint","content":" Manopt.get_hess_inequality_constraint  ‚Äî  Function get_hess_inequality_constraint(amp::AbstractManoptProblem, p, X, j=:)\nget_hess_inequality_constraint(M::AbstractManifold, co::ConstrainedManifoldObjective, p, j=:, range=NestedPowerRepresentation())\nget_hess_inequality_constraint!(amp::AbstractManoptProblem, Y, p, j=:)\nget_hess_inequality_constraint!(M::AbstractManifold, Y, co::ConstrainedManifoldObjective, p, X, j=:, range=NestedPowerRepresentation()) Evaluate the Hessian or Hessians of the inequality constraint  $(\\operatorname{Hess} g(p)[X])_j$  or  $\\operatorname{Hess} g_j(p)[X]$ , See also the  ConstrainedManoptProblem  to specify the range of the Hessian. source"},{"id":2587,"pagetitle":"Objective","title":"Manopt.get_inequality_constraint","ref":"/manopt/stable/plans/objective/#Manopt.get_inequality_constraint","content":" Manopt.get_inequality_constraint  ‚Äî  Function get_inequality_constraint(amp::AbstractManoptProblem, p, j=:)\nget_inequality_constraint(M::AbstractManifold, co::ConstrainedManifoldObjective, p, j=:, range=NestedPowerRepresentation()) Evaluate inequality constraints of a  ConstrainedManifoldObjective objective  at point  p  and indices  j  (by default  :  which corresponds to all indices). source"},{"id":2588,"pagetitle":"Objective","title":"Manopt.get_projected_point","ref":"/manopt/stable/plans/objective/#Manopt.get_projected_point","content":" Manopt.get_projected_point  ‚Äî  Function get_projected_point(amp::AbstractManoptProblem, p)\nget_projected_point!(amp::AbstractManoptProblem, q, p)\nget_projected_point(M::AbstractManifold, cso::ManifoldConstrainedSetObjective, p)\nget_projected_point!(M::AbstractManifold, q, cso::ManifoldConstrainedSetObjective, p) Project  p  with the projection that is stored within the  ManifoldConstrainedSetObjective . This can be done in-place of  q . source get_projected_point(amp::AbstractManoptProblem, p)\nget_projected_point!(amp::AbstractManoptProblem, q, p)\nget_projected_point(M::AbstractManifold, cso::ManifoldConstrainedSetObjective, p)\nget_projected_point!(M::AbstractManifold, q, cso::ManifoldConstrainedSetObjective, p) Project  p  with the projection that is stored within the  ManifoldConstrainedSetObjective . This can be done in-place of  q . source"},{"id":2589,"pagetitle":"Objective","title":"Manopt.get_projected_point!","ref":"/manopt/stable/plans/objective/#Manopt.get_projected_point!","content":" Manopt.get_projected_point!  ‚Äî  Function get_projected_point(amp::AbstractManoptProblem, p)\nget_projected_point!(amp::AbstractManoptProblem, q, p)\nget_projected_point(M::AbstractManifold, cso::ManifoldConstrainedSetObjective, p)\nget_projected_point!(M::AbstractManifold, q, cso::ManifoldConstrainedSetObjective, p) Project  p  with the projection that is stored within the  ManifoldConstrainedSetObjective . This can be done in-place of  q . source get_projected_point(amp::AbstractManoptProblem, p)\nget_projected_point!(amp::AbstractManoptProblem, q, p)\nget_projected_point(M::AbstractManifold, cso::ManifoldConstrainedSetObjective, p)\nget_projected_point!(M::AbstractManifold, q, cso::ManifoldConstrainedSetObjective, p) Project  p  with the projection that is stored within the  ManifoldConstrainedSetObjective . This can be done in-place of  q . source"},{"id":2590,"pagetitle":"Objective","title":"Manopt.get_unconstrained_objective","ref":"/manopt/stable/plans/objective/#Manopt.get_unconstrained_objective","content":" Manopt.get_unconstrained_objective  ‚Äî  Function get_unconstrained_objective(co::ConstrainedManifoldObjective) Returns the internally stored unconstrained  AbstractManifoldObjective  within the  ConstrainedManifoldObjective . source"},{"id":2591,"pagetitle":"Objective","title":"Manopt.is_feasible","ref":"/manopt/stable/plans/objective/#Manopt.is_feasible","content":" Manopt.is_feasible  ‚Äî  Function is_feasible(M::AbstractManifold, cmo::ConstrainedManifoldObjective, p, kwargs...) Evaluate whether a boint  p  on  M  is feasible with respect to the  ConstrainedManifoldObjective cmo . That is for the provided inequality constaints  $g: \\mathcal M ‚Üí ‚Ñù^m$  and equality constaints  $h: \\mathcal M \\to ‚Ñù^m$  from within  cmo , the point  $p ‚àà \\mathcal M$  is feasible if \\[g_i(p) ‚â§ 0, \\text{ for all } i=1,‚Ä¶,m\\quad\\text{ and }\\quad h_j(p) = 0, \\text{ for all } j=1,‚Ä¶,n.\\] Keyword arguments check_point::Bool=true : whether to also verify that ` p‚àà\\mathcal M  holds, using  is_point error::Symbol=:none : if the point is not feasible, this symbol determines how to report the error. :error : throws an error :info : displays the error message as an @info :none : (default) the function just returns true/false :warn : displays the error message as a @warning. The keyword  error=  and all other  kwargs...  are passed on to  is_point  if the point is verfied (see  check_point ). All other keywords are passed on to  is_poi source"},{"id":2592,"pagetitle":"Objective","title":"Internal functions","ref":"/manopt/stable/plans/objective/#Internal-functions","content":" Internal functions"},{"id":2593,"pagetitle":"Objective","title":"Manopt.get_feasibility_status","ref":"/manopt/stable/plans/objective/#Manopt.get_feasibility_status","content":" Manopt.get_feasibility_status  ‚Äî  Function get_feasibility_status(\n    M::AbstractManifold,\n    cmo::ConstrainedManifoldObjective,\n    g = get_inequality_constraints(M, cmo, p),\n    h = get_equality_constraints(M, cmo, p),\n) Generate a message about the feasibiliy of  p  with respect to the  ConstrainedManifoldObjective . You can also provide the evaluated vectors for the values of  g  and  h  as keyword arguments, in case you had them evaluated before. source"},{"id":2594,"pagetitle":"Objective","title":"Vectorial objectives","ref":"/manopt/stable/plans/objective/#Vectorial-objectives","content":" Vectorial objectives"},{"id":2595,"pagetitle":"Objective","title":"Manopt.AbstractVectorFunction","ref":"/manopt/stable/plans/objective/#Manopt.AbstractVectorFunction","content":" Manopt.AbstractVectorFunction  ‚Äî  Type AbstractVectorFunction{E, FT} <: Function Represent an abstract vectorial function  $f:\\mathcal M ‚Üí ‚Ñù^n$  with an  AbstractEvaluationType E  and an  AbstractVectorialType  to specify the format  $f$  is implemented as. Representations of  $f$ There are three different representations of  $f$ , which might be beneficial in one or the other situation: the  FunctionVectorialType  storing a single function  $f$  that returns a vector, the  ComponentVectorialType  storing a vector of functions  $f_i$  that return a single value each, the  CoordinateVectorialType  storing functions with respect to a specific basis of the tangent space for gradients and Hessians. Gradients of this type are usually referred to as Jacobians. For the  ComponentVectorialType  imagine that  $f$  could also be written using its component functions, \\[f(p) = \\bigl( f_1(p), f_2(p), \\ldots, f_n(p) \\bigr)^{\\mathrm{T}}\\] In this representation  f  is given as a vector  [f1(M,p), f2(M,p), ..., fn(M,p)]  of its component functions. An advantage is that the single components can be evaluated and from this representation one even can directly read of the number  n . A disadvantage might be, that one has to implement a lot of individual (component) functions. For the   FunctionVectorialType $f$  is implemented as a single function  f(M, p) , that returns an  AbstractArray . And advantage here is, that this is a single function. A disadvantage might be, that if this is expensive even to compute a single component, all of  f  has to be evaluated source"},{"id":2596,"pagetitle":"Objective","title":"Manopt.AbstractVectorGradientFunction","ref":"/manopt/stable/plans/objective/#Manopt.AbstractVectorGradientFunction","content":" Manopt.AbstractVectorGradientFunction  ‚Äî  Type VectorGradientFunction{E, FT, JT, F, J, I} <: AbstractManifoldObjective{E} Represent an abstract vectorial function  $f:\\mathcal M ‚Üí ‚Ñù^n$  that provides a (component wise) gradient. The  AbstractEvaluationType E  indicates the evaluation type, and the  AbstractVectorialType s  FT  and  JT  the formats in which the function and the gradient are provided, see  AbstractVectorFunction  for an explanation. source"},{"id":2597,"pagetitle":"Objective","title":"Manopt.VectorGradientFunction","ref":"/manopt/stable/plans/objective/#Manopt.VectorGradientFunction","content":" Manopt.VectorGradientFunction  ‚Äî  Type VectorGradientFunction{E, FT, JT, F, J, I} <: AbstractVectorGradientFunction{E, FT, JT} Represent a function  $f:\\mathcal M ‚Üí ‚Ñù^n$  including it first derivative, either as a vector of gradients of a Jacobian And hence has a gradient ` \\operatorname{grad} f_i(p) ‚àà T_{p}\\mathcal M . Putting these gradients into a vector the same way as the functions, yields a  ComponentVectorialType \\[\\operatorname{grad} f(p) = \\Bigl( \\operatorname{grad} f_1(p), \\operatorname{grad} f_2(p), ‚Ä¶, \\operatorname{grad} f_n(p) \\Bigr)^\\mathrm{T}\n‚àà (T_{p}\\mathcal M)^n\\] And advantage here is, that again the single components can be evaluated individually Fields value!!::F :          the cost function  $f$ , which can take different formats cost_type:: AbstractVectorialType :     indicating / storing data for the type of  f jacobian!!::G :     the Jacobian of  $f$ jacobian_type:: AbstractVectorialType : indicating / storing data for the type of  $J_f$ parameters :    the number  n  from, the size of the vector  $f$  returns. Constructor VectorGradientFunction(f, Jf, range_dimension;\n    evaluation::AbstractEvaluationType=AllocatingEvaluation(),\n    function_type::AbstractVectorialType=FunctionVectorialType(),\n    jacobian_type::AbstractVectorialType=FunctionVectorialType(),\n) Create a  VectorGradientFunction  of  f   and its Jacobian (vector of gradients)  Jf , where  f  maps into the Euclidean space of dimension  range_dimension . Their types are specified by the  function_type , and  jacobian_type , respectively. The Jacobian can further be given as an allocating variant or an in-place variant, specified by the  evaluation=  keyword. source"},{"id":2598,"pagetitle":"Objective","title":"Manopt.VectorHessianFunction","ref":"/manopt/stable/plans/objective/#Manopt.VectorHessianFunction","content":" Manopt.VectorHessianFunction  ‚Äî  Type VectorHessianFunction{E, FT, JT, HT, F, J, H, I} <: AbstractVectorGradientFunction{E, FT, JT} Represent a function  $f:\\mathcal M M ‚Üí ‚Ñù^n$  including it first derivative, either as a vector of gradients of a Jacobian, and the Hessian, as a vector of Hessians of the component functions. Both the Jacobian and the Hessian can map into either a sequence of tangent spaces or a single tangent space of the power manifold of length  n . Fields value!!::F :          the cost function  $f$ , which can take different formats cost_type:: AbstractVectorialType :     indicating / string data for the type of  f jacobian!!::G :     the Jacobian  $J_f$  of  $f$ jacobian_type:: AbstractVectorialType : indicating / storing data for the type of  $J_f$ hessians!!::H :     the Hessians of  $f$  (in a component wise sense) hessian_type:: AbstractVectorialType :  indicating / storing data for the type of  $H_f$ range_dimension :    the number  n  from, the size of the vector  $f$  returns. Constructor VectorHessianFunction(f, Jf, Hess_f, range_dimension;\n    evaluation::AbstractEvaluationType=AllocatingEvaluation(),\n    function_type::AbstractVectorialType=FunctionVectorialType(),\n    jacobian_type::AbstractVectorialType=FunctionVectorialType(),\n    hessian_type::AbstractVectorialType=FunctionVectorialType(),\n) Create a  VectorHessianFunction  of  f   and its Jacobian (vector of gradients)  Jf  and (vector of) Hessians, where  f  maps into the Euclidean space of dimension  range_dimension . Their types are specified by the  function_type , and  jacobian_type , and  hessian_type , respectively. The Jacobian and Hessian can further be given as an allocating variant or an inplace-variant, specified by the  evaluation=  keyword. source"},{"id":2599,"pagetitle":"Objective","title":"Manopt.AbstractVectorialType","ref":"/manopt/stable/plans/objective/#Manopt.AbstractVectorialType","content":" Manopt.AbstractVectorialType  ‚Äî  Type AbstractVectorialType An abstract type for different representations of a vectorial function  $f: \\mathcal M ‚Üí ‚Ñù^m$  and its (component-wise) gradient/Jacobian source"},{"id":2600,"pagetitle":"Objective","title":"Manopt.CoordinateVectorialType","ref":"/manopt/stable/plans/objective/#Manopt.CoordinateVectorialType","content":" Manopt.CoordinateVectorialType  ‚Äî  Type CoordinateVectorialType{B<:AbstractBasis} <: AbstractVectorialType A type to indicate that gradient of the constraints is implemented as a Jacobian matrix with respect to a certain basis, that is if the vector function is  $f: \\mathcal M ‚Üí ‚Ñù^m$  and we have a basis  $\\mathcal B$  of  $T_p\\mathcal M$ , at  $p‚àà \\mathcal M$  This can be written as  $J_g(p) = (c_1^{\\mathrm{T}},‚Ä¶,c_m^{\\mathrm{T}})^{\\mathrm{T}} \\in ‚Ñù^{m,d}$ , that is, every row  $c_i$  of this matrix is a set of coefficients such that  get_coefficients(M, p, c, B)  is the tangent vector  $\\oepratorname{grad} g_i(p)$  for example  $g_i(p) ‚àà ‚Ñù^m$  or  $\\operatorname{grad} g_i(p) ‚àà T_p\\mathcal M$ ,      $i=1,‚Ä¶,m$ . Fields basis  an  AbstractBasis  to indicate the basis in which Jacobian is expressed. Constructor CoordinateVectorialType(basis=DefaultOrthonormalBasis()) source"},{"id":2601,"pagetitle":"Objective","title":"Manopt.ComponentVectorialType","ref":"/manopt/stable/plans/objective/#Manopt.ComponentVectorialType","content":" Manopt.ComponentVectorialType  ‚Äî  Type ComponentVectorialType <: AbstractVectorialType A type to indicate that constraints are implemented as component functions, for example  $g_i(p) ‚àà ‚Ñù^m$  or  $\\operatorname{grad} g_i(p) ‚àà T_p\\mathcal M$ ,  $i=1,‚Ä¶,m$ . source"},{"id":2602,"pagetitle":"Objective","title":"Manopt.FunctionVectorialType","ref":"/manopt/stable/plans/objective/#Manopt.FunctionVectorialType","content":" Manopt.FunctionVectorialType  ‚Äî  Type FunctionVectorialType{P<:AbstractPowerRepresentation} <: AbstractVectorialType A type to indicate that constraints are implemented one whole functions, for example  $g(p) ‚àà ‚Ñù^m$  or  $\\operatorname{grad} g(p) ‚àà (T_p\\mathcal M)^m$ . This type internally stores the  AbstractPowerRepresentation , when it makes sense, especially for Hessian and gradient functions. source"},{"id":2603,"pagetitle":"Objective","title":"Access functions","ref":"/manopt/stable/plans/objective/#Access-functions-8","content":" Access functions"},{"id":2604,"pagetitle":"Objective","title":"Manopt.get_jacobian","ref":"/manopt/stable/plans/objective/#Manopt.get_jacobian","content":" Manopt.get_jacobian  ‚Äî  Function get_jacobian(M::AbstractManifold, vgf::AbstractVectorGradientFunction, p; kwargs...)\nget_jacobian(M::AbstractManifold, J, vgf::AbstractVectorGradientFunction, p; kwargs...) Compute the Jacobian  $J_F ‚àà ‚Ñù^{m√ón}$  of the  AbstractVectorGradientFunction $F$  at  p  on the  M . There are two interpretations of the Jacobian of a vectorial function  $F: \\mathcal M ‚Üí ‚Ñù^m$  on a manifold. Both depend on choosing a basis on the tangent space  $T_{p}\\mathcal M$  which we denote by  $Y_1,‚Ä¶,Y_n$ , where  n  is the  manifold_dimension (M) (M) . We can write any tangent vector  $X = \\displaystyle\\sum_{}^{}_i c_iY_i$ The Jacobian  $J_F$  is the matrix with respect to the basis  $Y_1,‚Ä¶,Y_n$  such that for any  $X‚ààT_{p}\\mathcal M$  we have the equality of the differential  $DF(p)[X] = Jc$ .   In other words, the  j th column of  $J$  is given by  $DF(p)[Y_j]$ Given the gradients  $\\operatorname{grad} F_i(p)$  of the component functions  $F_i: \\mathcal M ‚Üí ‚Ñù$ , we define the jacobian function as $math   J(X) = \\begin{pmatrix} ‚ü®\\operatorname{grad} F_1, X‚ü©_p\\\\ ‚ü®\\operatorname{grad} F_1, X‚ü©_p\\\\ ‚ãÆ\\\\ ‚ü®\\operatorname{grad} F_1, X‚ü©_p\\end{pmatrix}$ Then either the  $j$ th column of  $J_F$  is given by  $J(Y_i)$  or the  $i$ th row is given by all inner products  $\\operatorname{grad} F_1, Y_j‚ü©_p$  of the  $i$ th gradient function with all basis vectors  $Y_j$ . The computation can be computed in-place of  J . Keyword arguments basis::AbstractBasis = get_basis (vgf)  for the  CoordinateVectorialType  of the vectorial functions gradient, this might lead to a change of basis, if this basis and the one the coordinates are given in do not agree. range::AbstractPowerRepresentation = get_range (vgf.jacobian_type)  specify the range of the gradients in the case of a  FunctionVectorialType , that is, on which type of power manifold the gradient is given on. source"},{"id":2605,"pagetitle":"Objective","title":"Manopt.get_jacobian!","ref":"/manopt/stable/plans/objective/#Manopt.get_jacobian!","content":" Manopt.get_jacobian!  ‚Äî  Function get_jacobian(M::AbstractManifold, vgf::AbstractVectorGradientFunction, p; kwargs...)\nget_jacobian(M::AbstractManifold, J, vgf::AbstractVectorGradientFunction, p; kwargs...) Compute the Jacobian  $J_F ‚àà ‚Ñù^{m√ón}$  of the  AbstractVectorGradientFunction $F$  at  p  on the  M . There are two interpretations of the Jacobian of a vectorial function  $F: \\mathcal M ‚Üí ‚Ñù^m$  on a manifold. Both depend on choosing a basis on the tangent space  $T_{p}\\mathcal M$  which we denote by  $Y_1,‚Ä¶,Y_n$ , where  n  is the  manifold_dimension (M) (M) . We can write any tangent vector  $X = \\displaystyle\\sum_{}^{}_i c_iY_i$ The Jacobian  $J_F$  is the matrix with respect to the basis  $Y_1,‚Ä¶,Y_n$  such that for any  $X‚ààT_{p}\\mathcal M$  we have the equality of the differential  $DF(p)[X] = Jc$ .   In other words, the  j th column of  $J$  is given by  $DF(p)[Y_j]$ Given the gradients  $\\operatorname{grad} F_i(p)$  of the component functions  $F_i: \\mathcal M ‚Üí ‚Ñù$ , we define the jacobian function as $math   J(X) = \\begin{pmatrix} ‚ü®\\operatorname{grad} F_1, X‚ü©_p\\\\ ‚ü®\\operatorname{grad} F_1, X‚ü©_p\\\\ ‚ãÆ\\\\ ‚ü®\\operatorname{grad} F_1, X‚ü©_p\\end{pmatrix}$ Then either the  $j$ th column of  $J_F$  is given by  $J(Y_i)$  or the  $i$ th row is given by all inner products  $\\operatorname{grad} F_1, Y_j‚ü©_p$  of the  $i$ th gradient function with all basis vectors  $Y_j$ . The computation can be computed in-place of  J . Keyword arguments basis::AbstractBasis = get_basis (vgf)  for the  CoordinateVectorialType  of the vectorial functions gradient, this might lead to a change of basis, if this basis and the one the coordinates are given in do not agree. range::AbstractPowerRepresentation = get_range (vgf.jacobian_type)  specify the range of the gradients in the case of a  FunctionVectorialType , that is, on which type of power manifold the gradient is given on. source"},{"id":2606,"pagetitle":"Objective","title":"Manopt.get_value","ref":"/manopt/stable/plans/objective/#Manopt.get_value","content":" Manopt.get_value  ‚Äî  Function get_value(M::AbstractManifold, vgf::AbstractVectorFunction, p[, i=:])\nget_value!(M::AbstractManifold, V, vgf::AbstractVectorFunction, p[, i=:]) Evaluate the vector function  VectorGradientFunction vgf  at  p . The  range  can be used to specify a potential range, but is currently only present for consistency. The  i  can be a linear index, you can provide a single integer a  UnitRange  to specify a range to be returned like  1:3 a  BitVector  specifying a selection a  AbstractVector{<:Integer}  to specify indices :  to return the vector of all gradients, which is also the default This function can perform the evaluation inplace of  V . source"},{"id":2607,"pagetitle":"Objective","title":"Manopt.get_value_function","ref":"/manopt/stable/plans/objective/#Manopt.get_value_function","content":" Manopt.get_value_function  ‚Äî  Function get_value_function(vgf::VectorGradientFunction, recursive=false) return the internally stored function computing  get_value . source"},{"id":2608,"pagetitle":"Objective","title":"Base.length","ref":"/manopt/stable/plans/objective/#Base.length-Tuple{VectorGradientFunction}","content":" Base.length  ‚Äî  Method length(vgf::AbstractVectorFunction) Return the length of the vector the function  $f: \\mathcal M ‚Üí ‚Ñù^n$  maps into, that is the number  n . source"},{"id":2609,"pagetitle":"Objective","title":"Internal functions","ref":"/manopt/stable/plans/objective/#Internal-functions-2","content":" Internal functions"},{"id":2610,"pagetitle":"Objective","title":"Manopt._to_iterable_indices","ref":"/manopt/stable/plans/objective/#Manopt._to_iterable_indices","content":" Manopt._to_iterable_indices  ‚Äî  Function _to_iterable_indices(A::AbstractVector, i) Convert index  i  (integer, colon, vector of indices, etc.) for array  A  into an iterable structure of indices. source"},{"id":2611,"pagetitle":"Objective","title":"Manopt._change_basis!","ref":"/manopt/stable/plans/objective/#Manopt._change_basis!","content":" Manopt._change_basis!  ‚Äî  Function _change_basis!(M::AbstractManifold, JF, p, from_basis::B1, to_basis::B; X=zero_vector(M,p)) Given a jacobian matrix  JF  on a manifold  M  at  p  with respect to the  from_basis  in the tangent space of  p  on  M . Change the basis of the Jacobian to  to_basis  in place of  JF . Keyword Arguments X  a temporary vector to store a generated vector, before decomposing it again with respect to the new basis source"},{"id":2612,"pagetitle":"Objective","title":"ManifoldsBase.get_basis","ref":"/manopt/stable/plans/objective/#ManifoldsBase.get_basis","content":" ManifoldsBase.get_basis  ‚Äî  Function get_basis(::AbstractVectorialType) Return a basis that fits a vector function representation. For the case, where some vectorial data is stored with respect to a basis, this function returns the corresponding basis, most prominently for the  CoordinateVectorialType . If a type is not with respect to a certain basis, the  DefaultOrthonormalBasis  is returned. source"},{"id":2613,"pagetitle":"Objective","title":"Manopt.get_range","ref":"/manopt/stable/plans/objective/#Manopt.get_range","content":" Manopt.get_range  ‚Äî  Function get_range(::AbstractVectorialType) Return an abstract power manifold representation that fits a vector function's range. Most prominently a  FunctionVectorialType  returns its internal range. Otherwise the default  NestedPowerRepresentation ()  is used to work on a vector of data. source"},{"id":2614,"pagetitle":"Objective","title":"Subproblem objective","ref":"/manopt/stable/plans/objective/#Subproblem-objective","content":" Subproblem objective This objective can be use when the objective of a sub problem solver still needs access to the (outer/main) objective."},{"id":2615,"pagetitle":"Objective","title":"Manopt.AbstractManifoldSubObjective","ref":"/manopt/stable/plans/objective/#Manopt.AbstractManifoldSubObjective","content":" Manopt.AbstractManifoldSubObjective  ‚Äî  Type AbstractManifoldSubObjective{O<:AbstractManifoldObjective} <: AbstractManifoldObjective An abstract type for objectives of sub problems within a solver but still store the original objective internally to generate generic objectives for sub solvers. source"},{"id":2616,"pagetitle":"Objective","title":"Access functions","ref":"/manopt/stable/plans/objective/#Access-functions-9","content":" Access functions"},{"id":2617,"pagetitle":"Objective","title":"Manopt.get_objective_cost","ref":"/manopt/stable/plans/objective/#Manopt.get_objective_cost","content":" Manopt.get_objective_cost  ‚Äî  Function get_objective_cost(M, amso::AbstractManifoldSubObjective, p) Evaluate the cost of the (original) objective stored within the sub objective. source"},{"id":2618,"pagetitle":"Objective","title":"Manopt.get_objective_gradient","ref":"/manopt/stable/plans/objective/#Manopt.get_objective_gradient","content":" Manopt.get_objective_gradient  ‚Äî  Function X = get_objective_gradient(M, amso::AbstractManifoldSubObjective, p)\nget_objective_gradient!(M, X, amso::AbstractManifoldSubObjective, p) Evaluate the gradient of the (original) objective stored within the sub objective  amso . source"},{"id":2619,"pagetitle":"Objective","title":"Manopt.get_objective_hessian","ref":"/manopt/stable/plans/objective/#Manopt.get_objective_hessian","content":" Manopt.get_objective_hessian  ‚Äî  Function Y = get_objective_Hessian(M, amso::AbstractManifoldSubObjective, p, X)\nget_objective_Hessian!(M, Y, amso::AbstractManifoldSubObjective, p, X) Evaluate the Hessian of the (original) objective stored within the sub objective  amso . source"},{"id":2620,"pagetitle":"Objective","title":"Manopt.get_objective_preconditioner","ref":"/manopt/stable/plans/objective/#Manopt.get_objective_preconditioner","content":" Manopt.get_objective_preconditioner  ‚Äî  Function Y = get_objective_preconditioner(M, amso::AbstractManifoldSubObjective, p, X)\nget_objective_preconditioner(M, Y, amso::AbstractManifoldSubObjective, p, X) Evaluate the Hessian of the (original) objective stored within the sub objective  amso . source"},{"id":2621,"pagetitle":"Objective","title":"Proximal gradient objective","ref":"/manopt/stable/plans/objective/#Proximal-gradient-objective","content":" Proximal gradient objective"},{"id":2622,"pagetitle":"Objective","title":"Manopt.ManifoldProximalGradientObjective","ref":"/manopt/stable/plans/objective/#Manopt.ManifoldProximalGradientObjective","content":" Manopt.ManifoldProximalGradientObjective  ‚Äî  Type ManifoldProximalGradientObjective{E,<:AbstractEvaluationType, TC, TG, TGG, TP} <: AbstractManifoldObjective{E,TC,TGG} Model an objective of the form \\[    f(p) = g(p) + h(p),\\qquad p \\in \\mathcal M,\\] where  $g: \\mathcal M ‚Üí \\bar{‚Ñù}$  is a differentiable function and  $h: ‚Üí \\bar{‚Ñù}$  is a (possibly) lower semicontinous, and proper function. This objective provides the total cost  $f$ , its smooth component  $g$ , as well as  $\\operatorname{grad} g$  and  $\\operatorname{prox}_{Œª} h$ . Fields cost : the overall cost  $f = g + h$ cost_smooth : the smooth cost component  $g$ gradient_g!! : the gradient  $\\operatorname{grad} g$ proximal_map_h!! : the proximal map  $\\operatorname{prox}_{Œª} h$ Constructor ManifoldProximalGradientObjective(f, g, grad_g, prox_h;\n    evalauation=[`AllocatingEvaluation`](@ref)\n) Generate the proximal gradient objective given the total cost  f = g + h , smooth cost  g , the gradient of the smooth component  grad_g , and the proximal map of the nonsmooth component  prox_h . Keyword arguments evaluation= AllocatingEvaluation : whether the gradient and proximal map is given as an allocation function or an in-place ( InplaceEvaluation ). source 1 This cache requires  LRUCache.jl  to be loaded as well."},{"id":2625,"pagetitle":"Problem","title":"A Manopt problem","ref":"/manopt/stable/plans/problem/#sec-problem","content":" A Manopt problem A problem describes all static data of an optimisation task and has as a super type"},{"id":2626,"pagetitle":"Problem","title":"Manopt.AbstractManoptProblem","ref":"/manopt/stable/plans/problem/#Manopt.AbstractManoptProblem","content":" Manopt.AbstractManoptProblem  ‚Äî  Type AbstractManoptProblem{M<:AbstractManifold} Describe a Riemannian optimization problem with all static (not-changing) properties. The most prominent features that should always be stated here are the  AbstractManifold $\\mathcal M$ the cost function  $f:  \\mathcal M ‚Üí ‚Ñù$ Usually the cost should be within an  AbstractManifoldObjective . source"},{"id":2627,"pagetitle":"Problem","title":"Manopt.get_objective","ref":"/manopt/stable/plans/problem/#Manopt.get_objective","content":" Manopt.get_objective  ‚Äî  Function get_objective(o::AbstractManifoldObjective, recursive=true) return the (one step) undecorated  AbstractManifoldObjective  of the (possibly) decorated  o . As long as your decorated objective stores the objective within  o.objective  and the  dispatch_objective_decorator  is set to  Val{true} , the internal state are extracted automatically. By default the objective that is stored within a decorated objective is assumed to be at  o.objective . Overwrite  _get_objective(o, ::Val{true}, recursive) to change this behaviour for your objective o` for both the recursive and the direct case. If  recursive  is set to  false , only the most outer decorator is taken away instead of all. source get_objective(mp::AbstractManoptProblem, recursive=false) return the objective  AbstractManifoldObjective  stored within an  AbstractManoptProblem . If  recursive is set to true, it additionally unwraps all decorators of the objective source get_objective(amso::AbstractManifoldSubObjective) Return the (original) objective stored the sub objective is build on. source"},{"id":2628,"pagetitle":"Problem","title":"Manopt.get_manifold","ref":"/manopt/stable/plans/problem/#Manopt.get_manifold","content":" Manopt.get_manifold  ‚Äî  Function get_manifold(amp::AbstractManoptProblem) return the manifold stored within an  AbstractManoptProblem source Usually, such a problem is determined by the manifold or domain of the optimisation and the objective with all its properties used within an algorithm, see  The Objective . For that one can just use"},{"id":2629,"pagetitle":"Problem","title":"Manopt.DefaultManoptProblem","ref":"/manopt/stable/plans/problem/#Manopt.DefaultManoptProblem","content":" Manopt.DefaultManoptProblem  ‚Äî  Type DefaultManoptProblem{TM <: AbstractManifold, Objective <: AbstractManifoldObjective} Model a default manifold problem, that (just) consists of the domain of optimisation, that is an  AbstractManifold  and an  AbstractManifoldObjective source For the constraint optimisation, there are different possibilities to represent the gradients of the constraints. This can be done with a ConstraintProblem The primal dual-based solvers ( Chambolle-Pock  and the  PD Semi-smooth Newton ), both need two manifolds as their domains, hence there also exists a"},{"id":2630,"pagetitle":"Problem","title":"Manopt.TwoManifoldProblem","ref":"/manopt/stable/plans/problem/#Manopt.TwoManifoldProblem","content":" Manopt.TwoManifoldProblem  ‚Äî  Type TwoManifoldProblem{\n    MT<:AbstractManifold,NT<:AbstractManifold,O<:AbstractManifoldObjective\n} <: AbstractManoptProblem{MT} An abstract type for primal-dual-based problems. source From the two ingredients here, you can find more information about the  ManifoldsBase.AbstractManifold  in  ManifoldsBase.jl the  AbstractManifoldObjective  on the  page about the objective ."},{"id":2633,"pagetitle":"Recording values","title":"Record values","ref":"/manopt/stable/plans/record/#sec-record","content":" Record values To record values during the iterations of a solver run, there are in general two possibilities. On the one hand, the high-level interfaces provide a  record=  keyword, that accepts several different inputs. For more details see  How to record ."},{"id":2634,"pagetitle":"Recording values","title":"Record Actions & the solver state decorator","ref":"/manopt/stable/plans/record/#subsec-record-states","content":" Record Actions & the solver state decorator"},{"id":2635,"pagetitle":"Recording values","title":"Manopt.RecordAction","ref":"/manopt/stable/plans/record/#Manopt.RecordAction","content":" Manopt.RecordAction  ‚Äî  Type RecordAction A  RecordAction  is a small functor to record values. The usual call is given by (amp::AbstractManoptProblem, ams::AbstractManoptSolverState, k) -> s that performs the record for the current problem and solver combination, and where  k  is the current iteration. By convention  i=0  is interpreted as \"For Initialization only,\" so only initialize internal values, but not trigger any record, that the record is called from within  stop_solver!  which returns true afterwards. Any negative value is interpreted as a ‚Äúreset‚Äù, and should hence delete all stored recordings, for example when reusing a  RecordAction . The start of a solver calls the  :Iteration  and  :Stop  dictionary entries with  -1 , to reset those recordings. By default any  RecordAction  is assumed to record its values in a field  recorded_values , an  Vector  of recorded values. See  get_record (ra) . source"},{"id":2636,"pagetitle":"Recording values","title":"Manopt.RecordChange","ref":"/manopt/stable/plans/record/#Manopt.RecordChange","content":" Manopt.RecordChange  ‚Äî  Type RecordChange <: RecordAction debug for the amount of change of the iterate (see  get_iterate (s)  of the  AbstractManoptSolverState ) during the last iteration. Fields storage                    : a  StoreStateAction  to store (at least) the last iterate to use this as the last value (to compute the change) serving as a potential cache shared with other components of the solver. inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses recorded_values            : to store the recorded values Constructor RecordChange(M=DefaultManifold();\n    inverse_retraction_method = default_inverse_retraction_method(M),\n    storage                   = StoreStateAction(M; store_points=Tuple{:Iterate})\n) with the previous fields as keywords. For the  DefaultManifold  only the field storage is used. Providing the actual manifold moves the default storage to the efficient point storage. source"},{"id":2637,"pagetitle":"Recording values","title":"Manopt.RecordCost","ref":"/manopt/stable/plans/record/#Manopt.RecordCost","content":" Manopt.RecordCost  ‚Äî  Type RecordCost <: RecordAction Record the current cost function value, see  get_cost . Fields recorded_values  : to store the recorded values Constructor RecordCost() source"},{"id":2638,"pagetitle":"Recording values","title":"Manopt.RecordEntry","ref":"/manopt/stable/plans/record/#Manopt.RecordEntry","content":" Manopt.RecordEntry  ‚Äî  Type RecordEntry{T} <: RecordAction record a certain fields entry of type {T} during the iterates Fields recorded_values  : the recorded Iterates field            : Symbol the entry can be accessed with within  AbstractManoptSolverState Constructor RecordEntry(::T, f::Symbol)\nRecordEntry(T::DataType, f::Symbol) Initialize the record action to record the state field  f , and initialize the  recorded_values  to be a vector of element type  T . Examples RecordEntry(rand(M), :q)  to record the points from  M  stored in some states  s.q RecordEntry(SVDMPoint, :p)  to record the field  s.p  which takes values of type  SVDMPoint . source"},{"id":2639,"pagetitle":"Recording values","title":"Manopt.RecordEntryChange","ref":"/manopt/stable/plans/record/#Manopt.RecordEntryChange","content":" Manopt.RecordEntryChange  ‚Äî  Type RecordEntryChange{T} <: RecordAction record a certain entries change during iterates Additional fields recorded_values  : the recorded Iterates field            : Symbol the field can be accessed with within  AbstractManoptSolverState distance         : function (p,o,x1,x2) to compute the change/distance between two values of the entry storage          : a  StoreStateAction  to store (at least)  getproperty(o, d.field) Constructor RecordEntryChange(f::Symbol, d, a::StoreStateAction=StoreStateAction([f])) source"},{"id":2640,"pagetitle":"Recording values","title":"Manopt.RecordEvery","ref":"/manopt/stable/plans/record/#Manopt.RecordEvery","content":" Manopt.RecordEvery  ‚Äî  Type RecordEvery <: RecordAction record only every  $k$ th iteration. Otherwise (optionally, but activated by default) just update internal tracking values. This method does not perform any record itself but relies on it's children's methods source"},{"id":2641,"pagetitle":"Recording values","title":"Manopt.RecordGroup","ref":"/manopt/stable/plans/record/#Manopt.RecordGroup","content":" Manopt.RecordGroup  ‚Äî  Type RecordGroup <: RecordAction group a set of  RecordAction s into one action, where the internal  RecordAction s act independently, but the results can be collected in a grouped fashion, a tuple per calls of this group. The entries can be later addressed either by index or semantic Symbols Constructors RecordGroup(g::Array{<:RecordAction, 1}) construct a group consisting of an Array of  RecordAction s  g , RecordGroup(g, symbols) Examples g1 = RecordGroup([RecordIteration(), RecordCost()]) A RecordGroup to record the current iteration and the cost. The cost can then be accessed using  get_record(r,2)  or  r[2] . g2 = RecordGroup([RecordIteration(), RecordCost()], Dict(:Cost => 2)) A RecordGroup to record the current iteration and the cost, which can then be accessed using  get_record(:Cost)  or  r[:Cost] . g3 = RecordGroup([RecordIteration(), RecordCost() => :Cost]) A RecordGroup identical to the previous constructor, just a little easier to use. To access all recordings of the second entry of this last  g3  you can do either  g4[2]  or  g[:Cost] , the first one can only be accessed by  g4[1] , since no symbol was given here. source"},{"id":2642,"pagetitle":"Recording values","title":"Manopt.RecordIterate","ref":"/manopt/stable/plans/record/#Manopt.RecordIterate","content":" Manopt.RecordIterate  ‚Äî  Type RecordIterate <: RecordAction record the iterate Constructors RecordIterate(x0) initialize the iterate record array to the type of  x0 , which indicates the kind of iterate RecordIterate(P) initialize the iterate record array to the data type  T . source"},{"id":2643,"pagetitle":"Recording values","title":"Manopt.RecordIteration","ref":"/manopt/stable/plans/record/#Manopt.RecordIteration","content":" Manopt.RecordIteration  ‚Äî  Type RecordIteration <: RecordAction record the current iteration source"},{"id":2644,"pagetitle":"Recording values","title":"Manopt.RecordSolverState","ref":"/manopt/stable/plans/record/#Manopt.RecordSolverState","content":" Manopt.RecordSolverState  ‚Äî  Type RecordSolverState <: AbstractManoptSolverState append to any  AbstractManoptSolverState  the decorator with record capability, Internally a dictionary is kept that stores a  RecordAction  for several concurrent modes using a  Symbol  as reference. The default mode is  :Iteration , which is used to store information that is recorded during the iterations. RecordActions might be added to  :Start  or  :Stop  to record values at the beginning or for the stopping time point, respectively The original options can still be accessed using the  get_state  function. Fields options           the options that are extended by debug information recordDictionary  a  Dict{Symbol,RecordAction}  to keep track of all different recorded values Constructors RecordSolverState(o,dR) construct record decorated  AbstractManoptSolverState , where  dR  can be a  RecordAction , then it is stored within the dictionary at  :Iteration an  Array  of  RecordAction s, then it is stored as a  recordDictionary (@ref). a  Dict{Symbol,RecordAction} . source"},{"id":2645,"pagetitle":"Recording values","title":"Manopt.RecordStoppingReason","ref":"/manopt/stable/plans/record/#Manopt.RecordStoppingReason","content":" Manopt.RecordStoppingReason  ‚Äî  Type RecordStoppingReason <: RecordAction Record reason the solver stopped, see  get_reason . source"},{"id":2646,"pagetitle":"Recording values","title":"Manopt.RecordSubsolver","ref":"/manopt/stable/plans/record/#Manopt.RecordSubsolver","content":" Manopt.RecordSubsolver  ‚Äî  Type RecordSubsolver <: RecordAction Record the current sub solvers recording, by calling  get_record  on the sub state with Fields records : an array to store the recorded values symbols : arguments for  get_record . Defaults to just one symbol  :Iteration , but could be set to also record the  :Stop  action. Constructor RecordSubsolver(; record=[:Iteration,], record_type=eltype([])) source"},{"id":2647,"pagetitle":"Recording values","title":"Manopt.RecordTime","ref":"/manopt/stable/plans/record/#Manopt.RecordTime","content":" Manopt.RecordTime  ‚Äî  Type RecordTime <: RecordAction record the time elapsed during the current iteration. The three possible modes are :cumulative  record times without resetting the timer :iterative  record times with resetting the timer :total  record a time only at the end of an algorithm (see  stop_solver! ) The default is  :cumulative , and any non-listed symbol default to using this mode. Constructor RecordTime(; mode::Symbol=:cumulative) source"},{"id":2648,"pagetitle":"Recording values","title":"Manopt.RecordWhenActive","ref":"/manopt/stable/plans/record/#Manopt.RecordWhenActive","content":" Manopt.RecordWhenActive  ‚Äî  Type RecordWhenActive <: RecordAction record action that only records if the  active  boolean is set to true. This can be set from outside and is for example triggered by | RecordEvery ](@ref) on recordings of the subsolver. While this is for sub solvers maybe not completely necessary, recording values that are never accessible, is not that useful. Fields active :        a boolean that can (de-)activated from outside to turn on/off debug always_update : whether or not to call the inner debugs with nonpositive iterates (init/reset) Constructor RecordWhenActive(r::RecordAction, active=true, always_update=true) source"},{"id":2649,"pagetitle":"Recording values","title":"Access functions","ref":"/manopt/stable/plans/record/#Access-functions","content":" Access functions"},{"id":2650,"pagetitle":"Recording values","title":"Base.getindex","ref":"/manopt/stable/plans/record/#Base.getindex-Tuple{RecordGroup, Vararg{Any}}","content":" Base.getindex  ‚Äî  Method getindex(r::RecordGroup, s::Symbol)\nr[s]\ngetindex(r::RecordGroup, sT::NTuple{N,Symbol})\nr[sT]\ngetindex(r::RecordGroup, i)\nr[i] return an array of recorded values with respect to the  s , the symbols from the tuple  sT  or the index  i . See  get_record  for details. source"},{"id":2651,"pagetitle":"Recording values","title":"Base.getindex","ref":"/manopt/stable/plans/record/#Base.getindex-Tuple{RecordSolverState, Symbol}","content":" Base.getindex  ‚Äî  Method get_index(rs::RecordSolverState, s::Symbol)\nro[s] Get the recorded values for recorded type  s , see  get_record  for details. get_index(rs::RecordSolverState, s::Symbol, i...)\nro[s, i...] Access the recording type of type  s  and call its  RecordAction  with  [i...] . source"},{"id":2652,"pagetitle":"Recording values","title":"Manopt.get_record","ref":"/manopt/stable/plans/record/#Manopt.get_record","content":" Manopt.get_record  ‚Äî  Function get_record(s::AbstractManoptSolverState, [,symbol=:Iteration])\nget_record(s::RecordSolverState, [,symbol=:Iteration]) return the recorded values from within the  RecordSolverState s  that where recorded with respect to the  Symbol symbol  as an  Array . The default refers to any recordings during an  :Iteration . When called with arbitrary  AbstractManoptSolverState , this method looks for the  RecordSolverState  decorator and calls  get_record  on the decorator. source"},{"id":2653,"pagetitle":"Recording values","title":"Manopt.get_record","ref":"/manopt/stable/plans/record/#Manopt.get_record-Tuple{RecordAction}","content":" Manopt.get_record  ‚Äî  Method get_record(r::RecordAction) return the recorded values stored within a  RecordAction r . source"},{"id":2654,"pagetitle":"Recording values","title":"Manopt.get_record","ref":"/manopt/stable/plans/record/#Manopt.get_record-Tuple{RecordGroup}","content":" Manopt.get_record  ‚Äî  Method get_record(r::RecordGroup) return an array of tuples, where each tuple is a recorded set per iteration or record call. get_record(r::RecordGruop, k::Int) return an array of values corresponding to the  i th entry in this record group get_record(r::RecordGruop, s::Symbol) return an array of recorded values with respect to the  s , see  RecordGroup . get_record(r::RecordGroup, s1::Symbol, s2::Symbol,...) return an array of tuples, where each tuple is a recorded set corresponding to the symbols  s1, s2,...  per iteration / record call. source"},{"id":2655,"pagetitle":"Recording values","title":"Manopt.get_record_action","ref":"/manopt/stable/plans/record/#Manopt.get_record_action","content":" Manopt.get_record_action  ‚Äî  Function get_record_action(s::AbstractManoptSolverState, s::Symbol) return the action contained in the (first)  RecordSolverState  decorator within the  AbstractManoptSolverState o . source"},{"id":2656,"pagetitle":"Recording values","title":"Manopt.get_record_state","ref":"/manopt/stable/plans/record/#Manopt.get_record_state-Tuple{AbstractManoptSolverState}","content":" Manopt.get_record_state  ‚Äî  Method get_record_state(s::AbstractManoptSolverState) return the  RecordSolverState  among the decorators from the  AbstractManoptSolverState o source"},{"id":2657,"pagetitle":"Recording values","title":"Manopt.has_record","ref":"/manopt/stable/plans/record/#Manopt.has_record-Tuple{RecordSolverState}","content":" Manopt.has_record  ‚Äî  Method has_record(s::AbstractManoptSolverState) Indicate whether the  AbstractManoptSolverState s  are decorated with  RecordSolverState source"},{"id":2658,"pagetitle":"Recording values","title":"Internal factory functions","ref":"/manopt/stable/plans/record/#Internal-factory-functions","content":" Internal factory functions"},{"id":2659,"pagetitle":"Recording values","title":"Manopt.RecordActionFactory","ref":"/manopt/stable/plans/record/#Manopt.RecordActionFactory-Tuple{AbstractManoptSolverState, RecordAction}","content":" Manopt.RecordActionFactory  ‚Äî  Method RecordActionFactory(s::AbstractManoptSolverState, a) create a  RecordAction  where a  RecordAction  is passed through a [ Symbol ] creates :Change         to record the change of the iterates, see  RecordChange :Gradient       to record the gradient, see  RecordGradient :GradientNorm   to record the norm of the gradient, see [ RecordGradientNorm`](@ref) :Iterate        to record the iterate :Iteration      to record the current iteration number IterativeTime   to record the time iteratively :Cost           to record the current cost function value :Stepsize       to record the current step size :Time           to record the total time taken after every iteration :IterativeTime  to record the times taken for each iteration. and every other symbol is passed to  RecordEntry , which results in recording the field of the state with the symbol indicating the field of the solver to record. source"},{"id":2660,"pagetitle":"Recording values","title":"Manopt.RecordActionFactory","ref":"/manopt/stable/plans/record/#Manopt.RecordActionFactory-Union{Tuple{T}, Tuple{AbstractManoptSolverState, Tuple{Symbol, T}}} where T","content":" Manopt.RecordActionFactory  ‚Äî  Method RecordActionFactory(s::AbstractManoptSolverState, t::Tuple{Symbol, T}) where {T} create a  RecordAction  where ( :Subsolver , s) creates a  RecordSubsolver  with  record=  set to the second tuple entry For other symbol the second entry is ignored and the symbol is used to generate a  RecordEntry  recording the field with the name  symbol  of  s . source"},{"id":2661,"pagetitle":"Recording values","title":"Manopt.RecordFactory","ref":"/manopt/stable/plans/record/#Manopt.RecordFactory-Tuple{AbstractManoptSolverState, Vector}","content":" Manopt.RecordFactory  ‚Äî  Method RecordFactory(s::AbstractManoptSolverState, a) Generate a dictionary of  RecordAction s. First all  Symbol s  String ,  RecordAction s and numbers are collected, excluding  :Stop  and  :WhenActive . This collected vector is added to the  :Iteration => [...]  pair.  :Stop  is added as  :StoppingCriterion  to the  :Stop => [...]  pair. If any of these two pairs does not exist, it is pairs are created when adding the corresponding symbols For each  Pair  of a  Symbol  and a  Vector , the  RecordGroupFactory  is called for the  Vector  and the result is added to the debug dictionary's entry with said symbol. This is wrapped into the  RecordWhenActive , when the  :WhenActive  symbol is present Return value A dictionary for the different entry points where debug can happen, each containing a  RecordAction  to call. Note that upon the initialisation all dictionaries but the  :StartAlgorithm  one are called with an  i=0  for reset. source"},{"id":2662,"pagetitle":"Recording values","title":"Manopt.RecordGroupFactory","ref":"/manopt/stable/plans/record/#Manopt.RecordGroupFactory-Tuple{AbstractManoptSolverState, Vector}","content":" Manopt.RecordGroupFactory  ‚Äî  Method RecordGroupFactory(s::AbstractManoptSolverState, a) Generate a [ RecordGroup ] of  RecordAction s. The following rules are used Any  Symbol  contained in  a  is passed to  RecordActionFactory Any  RecordAction  is included as is. Any Pair of a  RecordAction  and a symbol, that is in order  RecordCost() => :A  is handled, that the corresponding record action can later be accessed as  g[:A] , where  g is the record group generated here. If this results in more than one  RecordAction  a  RecordGroup  of these is build. If any integers are present, the last of these is used to wrap the group in a  RecordEvery (k) . If  :WhenActive  is present, the resulting Action is wrapped in  RecordWhenActive , making it deactivatable by its parent solver. source"},{"id":2663,"pagetitle":"Recording values","title":"Manopt.record_or_reset!","ref":"/manopt/stable/plans/record/#Manopt.record_or_reset!-Tuple{RecordAction, Any, Int64}","content":" Manopt.record_or_reset!  ‚Äî  Method record_or_reset!(r, v, k) either record ( k>0  and not  Inf ) the value  v  within the  RecordAction r  or reset ( k<0 ) the internal storage, where  v  has to match the internal value type of the corresponding  RecordAction . source"},{"id":2664,"pagetitle":"Recording values","title":"Manopt.set_parameter!","ref":"/manopt/stable/plans/record/#Manopt.set_parameter!-Tuple{RecordSolverState, Val{:Record}, Vararg{Any}}","content":" Manopt.set_parameter!  ‚Äî  Method set_parameter!(ams::RecordSolverState, ::Val{:Record}, args...) Set certain values specified by  args...  into the elements of the  recordDictionary source Further specific  RecordAction s can be found when specific types of  AbstractManoptSolverState  define them on their corresponding site."},{"id":2665,"pagetitle":"Recording values","title":"Technical details","ref":"/manopt/stable/plans/record/#Technical-details","content":" Technical details"},{"id":2666,"pagetitle":"Recording values","title":"Manopt.initialize_solver!","ref":"/manopt/stable/plans/record/#Manopt.initialize_solver!-Tuple{AbstractManoptProblem, RecordSolverState}","content":" Manopt.initialize_solver!  ‚Äî  Method initialize_solver!(ams::AbstractManoptProblem, rss::RecordSolverState) Extend the initialization of the solver by a hook to run records that were added to the  :Start  entry. source"},{"id":2667,"pagetitle":"Recording values","title":"Manopt.step_solver!","ref":"/manopt/stable/plans/record/#Manopt.step_solver!-Tuple{AbstractManoptProblem, RecordSolverState, Any}","content":" Manopt.step_solver!  ‚Äî  Method step_solver!(amp::AbstractManoptProblem, rss::RecordSolverState, k) Extend the  i th step of the solver by a hook to run records, that were added to the  :Iteration  entry. source"},{"id":2668,"pagetitle":"Recording values","title":"Manopt.stop_solver!","ref":"/manopt/stable/plans/record/#Manopt.stop_solver!-Tuple{AbstractManoptProblem, RecordSolverState, Any}","content":" Manopt.stop_solver!  ‚Äî  Method stop_solver!(amp::AbstractManoptProblem, rss::RecordSolverStatek k) Extend the call to the stopping criterion by a hook to run records, that were added to the  :Stop  entry. source"},{"id":2671,"pagetitle":"Solver State","title":"Solver state","ref":"/manopt/stable/plans/state/#sec-solver-state","content":" Solver state Given an  AbstractManoptProblem , that is a certain optimisation task, the state specifies the solver to use. It contains the parameters of a solver and all fields necessary during the algorithm, for example the current iterate, a  StoppingCriterion  or a  Stepsize ."},{"id":2672,"pagetitle":"Solver State","title":"Manopt.AbstractManoptSolverState","ref":"/manopt/stable/plans/state/#Manopt.AbstractManoptSolverState","content":" Manopt.AbstractManoptSolverState  ‚Äî  Type AbstractManoptSolverState A general super type for all solver states. Fields The following fields are assumed to be default. If you use different ones, adapt the the access functions  get_iterate  and  get_stopping_criterion  accordingly p::P : a point on the manifold  $\\mathcal M$  storing the current iterate stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled source"},{"id":2673,"pagetitle":"Solver State","title":"Manopt.get_state","ref":"/manopt/stable/plans/state/#Manopt.get_state","content":" Manopt.get_state  ‚Äî  Function get_state(s::AbstractManoptSolverState, recursive::Bool=true) return the (one step) undecorated  AbstractManoptSolverState  of the (possibly) decorated  s . As long as your decorated state stores the state within  s.state  and the  dispatch_objective_decorator  is set to  Val{true} , the internal state are extracted automatically. By default the state that is stored within a decorated state is assumed to be at  s.state . Overwrite  _get_state(s, ::Val{true}, recursive) to change this behaviour for your state s` for both the recursive and the direct case. If  recursive  is set to  false , only the most outer decorator is taken away instead of all. source"},{"id":2674,"pagetitle":"Solver State","title":"Manopt.get_count","ref":"/manopt/stable/plans/state/#Manopt.get_count","content":" Manopt.get_count  ‚Äî  Function get_count(ams::AbstractManoptSolverState, ::Symbol) Obtain the count for a certain countable size, for example the  :Iterations . This function returns 0 if there was nothing to count Available symbols from within the solver state :Iterations  is passed on to the  stop  field to obtain the iteration at which the solver stopped. source get_count(co::ManifoldCountObjective, s::Symbol, mode::Symbol=:None) Get the number of counts for a certain symbol  s . Depending on the  mode  different results appear if the symbol does not exist in the dictionary :None :  (default) silent mode, returns  -1  for non-existing entries :warn :  issues a warning if a field does not exist :error : issues an error if a field does not exist source Since every subtype of an  AbstractManoptSolverState  directly relate to a solver, the concrete states are documented together with the corresponding  solvers . This page documents the general features available for every state. A first example is to obtain or set, the current iterate. This might be useful to continue investigation at the current iterate, or to set up a solver for a next experiment, respectively."},{"id":2675,"pagetitle":"Solver State","title":"Manopt.get_iterate","ref":"/manopt/stable/plans/state/#Manopt.get_iterate","content":" Manopt.get_iterate  ‚Äî  Function get_iterate(O::AbstractManoptSolverState) return the (last stored) iterate within  AbstractManoptSolverState s`. This should usually refer to a single point on the manifold the solver is working on By default this also removes all decorators of the state beforehand. source get_iterate(agst::AbstractGradientSolverState) return the iterate stored within gradient options. THe default returns  agst.p . source"},{"id":2676,"pagetitle":"Solver State","title":"Manopt.set_iterate!","ref":"/manopt/stable/plans/state/#Manopt.set_iterate!","content":" Manopt.set_iterate!  ‚Äî  Function set_iterate!(s::AbstractManoptSolverState, M::AbstractManifold, p) set the iterate within an  AbstractManoptSolverState  to some (start) value  p . source set_iterate!(agst::AbstractGradientSolverState, M, p) set the (current) iterate stored within an  AbstractGradientSolverState  to  p . The default function modifies  s.p . source"},{"id":2677,"pagetitle":"Solver State","title":"Manopt.get_gradient","ref":"/manopt/stable/plans/state/#Manopt.get_gradient-Tuple{AbstractManoptSolverState}","content":" Manopt.get_gradient  ‚Äî  Method get_gradient(s::AbstractManoptSolverState) return the (last stored) gradient within  AbstractManoptSolverState s`. By default also undecorates the state beforehand source"},{"id":2678,"pagetitle":"Solver State","title":"Manopt.set_gradient!","ref":"/manopt/stable/plans/state/#Manopt.set_gradient!","content":" Manopt.set_gradient!  ‚Äî  Function set_gradient!(s::AbstractManoptSolverState, M::AbstractManifold, p, X) set the gradient within an (possibly decorated)  AbstractManoptSolverState  to some (start) value  X  in the tangent space at  p . source set_gradient!(agst::AbstractGradientSolverState, M, p, X) set the (current) gradient stored within an  AbstractGradientSolverState  to  X . The default function modifies  s.X . source An internal function working on the state and elements within a state is used to pass messages from (sub) activities of a state to the corresponding  DebugMessages"},{"id":2679,"pagetitle":"Solver State","title":"Manopt.get_message","ref":"/manopt/stable/plans/state/#Manopt.get_message","content":" Manopt.get_message  ‚Äî  Function get_message(du::AbstractManoptSolverState) get a message (String) from internal functors, in a summary. This should return any message a sub-step might have issued as well. source Furthermore, to access the stopping criterion use"},{"id":2680,"pagetitle":"Solver State","title":"Manopt.get_stopping_criterion","ref":"/manopt/stable/plans/state/#Manopt.get_stopping_criterion","content":" Manopt.get_stopping_criterion  ‚Äî  Function get_stopping_criterion(ams::AbstractManoptSolverState) Return the  StoppingCriterion  stored within the  AbstractManoptSolverState ams . For an undecorated state, this is assumed to be in  ams.stop . Overwrite  _get_stopping_criterion(yms::YMS)  to change this for your manopt solver ( yms ) assuming it has type YMS`. source"},{"id":2681,"pagetitle":"Solver State","title":"Decorators for AbstractManoptSolverStates","ref":"/manopt/stable/plans/state/#Decorators-for-AbstractManoptSolverStates","content":" Decorators for  AbstractManoptSolverState s A solver state can be decorated using the following trait and function to initialize"},{"id":2682,"pagetitle":"Solver State","title":"Manopt.dispatch_state_decorator","ref":"/manopt/stable/plans/state/#Manopt.dispatch_state_decorator","content":" Manopt.dispatch_state_decorator  ‚Äî  Function dispatch_state_decorator(s::AbstractManoptSolverState) Indicate internally, whether an  AbstractManoptSolverState s  is of decorating type, and stores (encapsulates) a state in itself, by default in the field  s.state . Decorators indicate this by returning  Val{true}  for further dispatch. The default is  Val{false} , so by default a state is not decorated. source"},{"id":2683,"pagetitle":"Solver State","title":"Manopt.is_state_decorator","ref":"/manopt/stable/plans/state/#Manopt.is_state_decorator","content":" Manopt.is_state_decorator  ‚Äî  Function is_state_decorator(s::AbstractManoptSolverState) Indicate, whether  AbstractManoptSolverState s  are of decorator type. source"},{"id":2684,"pagetitle":"Solver State","title":"Manopt.decorate_state!","ref":"/manopt/stable/plans/state/#Manopt.decorate_state!","content":" Manopt.decorate_state!  ‚Äî  Function decorate_state!(s::AbstractManoptSolverState) decorate the  AbstractManoptSolverState s  with specific decorators. Optional arguments optional arguments provide necessary details on the decorators. callback=missing  add an arbitrary (simple) callback function  cb()  to be called every iteration. debug=Array{Union{Symbol,DebugAction,String,Int, Function},1}() : a set of symbols representing  DebugAction s,  Strings  used as dividers and a sub-sampling integer. These are passed as a  DebugGroup  within  :Iteration  to the  DebugSolverState  decorator dictionary. A function is added as a (non-simple) callback within a  DebugCallback . Only exception is  :Stop  that is passed to  :Stop . record=Array{Union{Symbol,RecordAction,Int},1}() : specify recordings by using  Symbol s or  RecordAction s directly. An integer can again be used for only recording every  $i$ th iteration. return_state=false : indicate whether to wrap the options in a  ReturnSolverState , indicating that the solver should return options and not (only) the minimizer. other keywords are ignored. See also DebugSolverState ,  RecordSolverState ,  ReturnSolverState source A simple example is the"},{"id":2685,"pagetitle":"Solver State","title":"Manopt.ReturnSolverState","ref":"/manopt/stable/plans/state/#Manopt.ReturnSolverState","content":" Manopt.ReturnSolverState  ‚Äî  Type ReturnSolverState{O<:AbstractManoptSolverState} <: AbstractManoptSolverState This internal type is used to indicate that the contained  AbstractManoptSolverState state  should be returned at the end of a solver instead of the usual minimizer. See also get_solver_result source as well as  DebugSolverState  and  RecordSolverState ."},{"id":2686,"pagetitle":"Solver State","title":"State actions","ref":"/manopt/stable/plans/state/#State-actions","content":" State actions A state action is a struct for callback functions that can be attached within for example the just mentioned debug decorator or the record decorator."},{"id":2687,"pagetitle":"Solver State","title":"Manopt.AbstractStateAction","ref":"/manopt/stable/plans/state/#Manopt.AbstractStateAction","content":" Manopt.AbstractStateAction  ‚Äî  Type AbstractStateAction a common  Type  for  AbstractStateActions  that might be triggered in decorators, for example within the  DebugSolverState  or within the  RecordSolverState . source Several state decorators or actions might store intermediate values like the (last) iterate to compute some change or the last gradient. In order to minimise the storage of these, there is a generic  StoreStateAction  that acts as generic common storage that can be shared among different actions."},{"id":2688,"pagetitle":"Solver State","title":"Manopt.StoreStateAction","ref":"/manopt/stable/plans/state/#Manopt.StoreStateAction","content":" Manopt.StoreStateAction  ‚Äî  Type StoreStateAction <: AbstractStateAction internal storage for  AbstractStateAction s to store a tuple of fields from an  AbstractManoptSolverState s This functor possesses the usual interface of functions called during an iteration and acts on  (p, s, k) , where  p  is a  AbstractManoptProblem ,  s  is an  AbstractManoptSolverState  and  k  is the current iteration. Fields values :        a dictionary to store interim values based on certain  Symbols keys :          a  Vector  of  Symbols  to refer to fields of  AbstractManoptSolverState point_values :  a  NamedTuple  of mutable values of points on a manifold to be stored in  StoreStateAction . Manifold is later determined by  AbstractManoptProblem  passed to  update_storage! . point_init :    a  NamedTuple  of boolean values indicating whether a point in  point_values  with matching key has been already initialized to a value. When it is false, it corresponds to a general value not being stored for the key present in the vector  keys . vector_values : a  NamedTuple  of mutable values of tangent vectors on a manifold to be stored in  StoreStateAction . Manifold is later determined by  AbstractManoptProblem  passed to  update_storage! . It is not specified at which point the vectors are tangent but for storage it should not matter. vector_init :   a  NamedTuple  of boolean values indicating whether a tangent vector in  vector_values : with matching key has been already initialized to a value. When it is false, it corresponds to a general value not being stored for the key present in the vector  keys . once :          whether to update the internal values only once per iteration lastStored :    last iterate, where this  AbstractStateAction  was called (to determine  once ) To handle the general storage, use  get_storage  and  has_storage  with keys as  Symbol s. For the point storage use  PointStorageKey . For tangent vector storage use  VectorStorageKey . Point and tangent storage have been optimized to be more efficient. Constructors StoreStateAction(s::Vector{Symbol}) This is equivalent as providing  s  to the keyword  store_fields , just that here, no manifold is necessity for the construction. StoreStateAction(M) Keyword arguments store_fields  ( Symbol[] ) store_points  ( Symbol[] ) store_vectors  ( Symbol[] ) as vectors of symbols each referring to fields of the state (lower case symbols) or semantic ones (upper case). p_init  ( rand(M) ) but making sure this is not a number but a (mutatable) array X_init  ( zero_vector(M, p_init) ) are used to initialize the point and vector storage, change these if you use other types (than the default) for your points/vectors on  M . once  ( true ) whether to update internal storage only once per iteration or on every update call source"},{"id":2689,"pagetitle":"Solver State","title":"Manopt.get_storage","ref":"/manopt/stable/plans/state/#Manopt.get_storage","content":" Manopt.get_storage  ‚Äî  Function get_storage(a::AbstractStateAction, key::Symbol) Return the internal value of the  AbstractStateAction a  at the  Symbol key . source get_storage(a::AbstractStateAction, ::PointStorageKey{key}) where {key} Return the internal value of the  AbstractStateAction a  at the  Symbol key  that represents a point. source get_storage(a::AbstractStateAction, ::VectorStorageKey{key}) where {key} Return the internal value of the  AbstractStateAction a  at the  Symbol key  that represents a vector. source"},{"id":2690,"pagetitle":"Solver State","title":"Manopt.has_storage","ref":"/manopt/stable/plans/state/#Manopt.has_storage","content":" Manopt.has_storage  ‚Äî  Function has_storage(a::AbstractStateAction, key::Symbol) Return whether the  AbstractStateAction a  has a value stored at the  Symbol key . source has_storage(a::AbstractStateAction, ::PointStorageKey{key}) where {key} Return whether the  AbstractStateAction a  has a point value stored at the  Symbol key . source has_storage(a::AbstractStateAction, ::VectorStorageKey{key}) where {key} Return whether the  AbstractStateAction a  has a point value stored at the  Symbol key . source"},{"id":2691,"pagetitle":"Solver State","title":"Manopt.update_storage!","ref":"/manopt/stable/plans/state/#Manopt.update_storage!","content":" Manopt.update_storage!  ‚Äî  Function update_storage!(a::AbstractStateAction, amp::AbstractManoptProblem, s::AbstractManoptSolverState) Update the  AbstractStateAction a  internal values to the ones given on the  AbstractManoptSolverState s . Optimized using the information from  amp source update_storage!(a::AbstractStateAction, d::Dict{Symbol,<:Any}) Update the  AbstractStateAction a  internal values to the ones given in the dictionary  d . The values are merged, where the values from  d  are preferred. source"},{"id":2692,"pagetitle":"Solver State","title":"Manopt.PointStorageKey","ref":"/manopt/stable/plans/state/#Manopt.PointStorageKey","content":" Manopt.PointStorageKey  ‚Äî  Type struct PointStorageKey{key} end Refer to point storage of  StoreStateAction  in  get_storage  and  has_storage  functions source"},{"id":2693,"pagetitle":"Solver State","title":"Manopt.VectorStorageKey","ref":"/manopt/stable/plans/state/#Manopt.VectorStorageKey","content":" Manopt.VectorStorageKey  ‚Äî  Type struct VectorStorageKey{key} end Refer to tangent storage of  StoreStateAction  in  get_storage  and  has_storage  functions source as well as two internal functions"},{"id":2694,"pagetitle":"Solver State","title":"Manopt._storage_copy_vector","ref":"/manopt/stable/plans/state/#Manopt._storage_copy_vector","content":" Manopt._storage_copy_vector  ‚Äî  Function _storage_copy_vector(M::AbstractManifold, X) Make a copy of tangent vector  X  from manifold  M  for storage in  StoreStateAction . source"},{"id":2695,"pagetitle":"Solver State","title":"Manopt._storage_copy_point","ref":"/manopt/stable/plans/state/#Manopt._storage_copy_point","content":" Manopt._storage_copy_point  ‚Äî  Function _storage_copy_point(M::AbstractManifold, p) Make a copy of point  p  from manifold  M  for storage in  StoreStateAction . source"},{"id":2696,"pagetitle":"Solver State","title":"Abstract states","ref":"/manopt/stable/plans/state/#Abstract-states","content":" Abstract states In a few cases it is useful to have a hierarchy of types. These are"},{"id":2697,"pagetitle":"Solver State","title":"Manopt.AbstractSubProblemSolverState","ref":"/manopt/stable/plans/state/#Manopt.AbstractSubProblemSolverState","content":" Manopt.AbstractSubProblemSolverState  ‚Äî  Type AbstractSubProblemSolverState <: AbstractManoptSolverState An abstract type for solvers that involve a subsolver. source"},{"id":2698,"pagetitle":"Solver State","title":"Manopt.AbstractGradientSolverState","ref":"/manopt/stable/plans/state/#Manopt.AbstractGradientSolverState","content":" Manopt.AbstractGradientSolverState  ‚Äî  Type AbstractGradientSolverState <: AbstractManoptSolverState A generic  AbstractManoptSolverState  type for gradient based options data. It assumes that the iterate is stored in the field  p the gradient at  p  is stored in  X . See also GradientDescentState ,  StochasticGradientDescentState ,  SubGradientMethodState ,  QuasiNewtonState . source"},{"id":2699,"pagetitle":"Solver State","title":"Manopt.AbstractHessianSolverState","ref":"/manopt/stable/plans/state/#Manopt.AbstractHessianSolverState","content":" Manopt.AbstractHessianSolverState  ‚Äî  Type AbstractHessianSolverState <: AbstractGradientSolverState An  AbstractManoptSolverState  type to represent algorithms that employ the Hessian. These options are assumed to have a field ( gradient ) to store the current gradient  $\\operatorname{grad}f(x)$ source"},{"id":2700,"pagetitle":"Solver State","title":"Manopt.AbstractPrimalDualSolverState","ref":"/manopt/stable/plans/state/#Manopt.AbstractPrimalDualSolverState","content":" Manopt.AbstractPrimalDualSolverState  ‚Äî  Type AbstractPrimalDualSolverState A general type for all primal dual based options to be used within primal dual based algorithms source For the sub problem state, there are two access functions"},{"id":2701,"pagetitle":"Solver State","title":"Manopt.get_sub_problem","ref":"/manopt/stable/plans/state/#Manopt.get_sub_problem","content":" Manopt.get_sub_problem  ‚Äî  Function get_sub_problem(ams::AbstractSubProblemSolverState) Access the sub problem of a solver state that involves a sub optimisation task. By default this returns  ams.sub_problem . source"},{"id":2702,"pagetitle":"Solver State","title":"Manopt.get_sub_state","ref":"/manopt/stable/plans/state/#Manopt.get_sub_state","content":" Manopt.get_sub_state  ‚Äî  Function get_sub_state(ams::AbstractSubProblemSolverState) Access the sub state of a solver state that involves a sub optimisation task. By default this returns  ams.sub_state . source"},{"id":2705,"pagetitle":"Stepsize","title":"Stepsize and line search","ref":"/manopt/stable/plans/stepsize/#Stepsize","content":" Stepsize and line search Most iterative algorithms determine a direction along which the algorithm shall proceed and determine a step size to find the next iterate. How advanced the step size computation can be implemented depends (among others) on the properties the corresponding problem provides. Within  Manopt.jl , the step size determination is implemented as a  functor  which is a subtype of  Stepsize  based on"},{"id":2706,"pagetitle":"Stepsize","title":"Manopt.Stepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.Stepsize","content":" Manopt.Stepsize  ‚Äî  Type Stepsize An abstract type for the functors representing step sizes. These are callable structures. The naming scheme is  TypeOfStepSize , for example  ConstantStepsize . Every Stepsize has to provide a constructor and its function has to have the interface  (p,o,i)  where a  AbstractManoptProblem  as well as  AbstractManoptSolverState  and the current number of iterations are the arguments and returns a number, namely the stepsize to use. For most it is advisable to employ a  ManifoldDefaultsFactory . Then the function creating the factory should either be called  TypeOf  or if that is confusing or too generic,  TypeOfLength See also Linesearch source Usually, a constructor should take the manifold  M  as its first argument, for consistency, to allow general step size functors to be set up based on default values that might depend on the manifold currently under consideration. Currently, the following step sizes are available"},{"id":2707,"pagetitle":"Stepsize","title":"Manopt.AdaptiveWNGradient","ref":"/manopt/stable/plans/stepsize/#Manopt.AdaptiveWNGradient","content":" Manopt.AdaptiveWNGradient  ‚Äî  Function AdaptiveWNGradient(; kwargs...)\nAdaptiveWNGradient(M::AbstractManifold; kwargs...) A stepsize based on the adaptive gradient method introduced by [ GS23 ]. Given a positive threshold  $\\hat{c} ‚àà ‚Ñï$ , an minimal bound  $b_{\\text{min}} > 0$ , an initial  $b_0 ‚â• b_{\\text{min}}$ , and a gradient reduction factor threshold  $Œ± ‚àà [0,1)$ . Set  $c_0=0$  and use  $œâ_0 = \\lVert \\operatorname{grad} f(p_0) \\rVert_{p_0}$ . For the first iterate use the initial step size  $s_0 = \\frac{1}{b_0}$ . Then, given the last gradient  $X_{k-1} = \\operatorname{grad} f(x_{k-1})$ , and a previous  $œâ_{k-1}$ , the values  $(b_k, œâ_k, c_k)$  are computed using  $X_k = \\operatorname{grad} f(p_k)$  and the following cases If  $\\lVert X_k \\rVert_{p_k} ‚â§ Œ±œâ_{k-1}$ , then let  $\\hat{b}_{k-1} ‚àà [b_{\\text{min}},b_{k-1}]$  and set \\[(b_k, œâ_k, c_k) = \\begin{cases}   \\bigl(\\hat{b}_{k-1}, \\lVert X_k \\rVert_{p_k}, 0 \\bigr) & \\text{ if } c_{k-1}+1 = \\hat{c}\\\\\\\\    \\bigl( b_{k-1} + \\frac{\\lVert X_k \\rVert_{p_k}^2}{b_{k-1}}, œâ_{k-1}, c_{k-1}+1 \\Bigr) & \\text{ if } c_{k-1}+1<\\hat{c}\\end{cases}\\] If  $\\lVert X_k \\rVert_{p_k} > Œ±œâ_{k-1}$ , the set \\[(b_k, œâ_k, c_k) = \\Bigl( b_{k-1} + \\frac{\\lVert X_k \\rVert_{p_k}^2}{b_{k-1}}, œâ_{k-1}, 0 \\Bigr)\\] and return the step size  $s_k = \\frac{1}{b_k}$ . Note that for  $Œ±=0$  this is the Riemannian variant of  WNGRad . Keyword arguments adaptive=true : switches the  gradient_reduction Œ± (if true ) to 0`. alternate_bound = (bk, hat_c) ->  min(gradient_bound == 0 ? 1.0 : gradient_bound, max(minimal_bound, bk / (3 * hat_c)) : how to determine  $\\hat{k}_k$  as a function of  (bmin, bk, hat_c) -> hat_bk count_threshold=4 :  an  Integer  for  $\\hat{c}$ gradient_reduction::R=adaptive ? 0.9 : 0.0 : the gradient reduction factor threshold  $Œ± ‚àà [0,1)$ gradient_bound=norm(M, p, X) : the bound  $b_k$ . minimal_bound=1e-4 : the value  $b_{\\text{min}}$ p= rand (M) : a point on the manifold  $\\mathcal M$ only used to define the  gradient_bound X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ only used to define the  gradient_bound source"},{"id":2708,"pagetitle":"Stepsize","title":"Manopt.ArmijoLinesearch","ref":"/manopt/stable/plans/stepsize/#Manopt.ArmijoLinesearch","content":" Manopt.ArmijoLinesearch  ‚Äî  Function ArmijoLinesearch(; kwargs...)\nArmijoLinesearch(M::AbstractManifold; kwargs...) Specify a step size that performs an Armijo line search. Given a Function  $f:\\mathcal M‚Üí‚Ñù$  and its Riemannian Gradient  $\\operatorname{grad}f: \\mathcal M‚ÜíT\\mathcal M$ , the curent point  $p‚àà\\mathcal M$  and a search direction  $X‚ààT_{p}\\mathcal M$ . Then the step size  $s$  is found by reducing the initial step size  $s$  until \\[f(\\operatorname{retr}_p(sX)) ‚â§ f(p) - œÑs ‚ü® X, \\operatorname{grad}f(p) ‚ü©_p\\] is fulfilled. for a sufficient decrease value  $œÑ ‚àà (0,1)$ . To be a bit more optimistic, if  $s$  already fulfils this, a first search is done,  increasing  the given  $s$  until for a first time this step does not hold. Overall, we look for step size, that provides  enough decrease , see [ Bou23 , p. 58] for more information. Keyword arguments additional_decrease_condition=(M, p) -> true : specify an additional criterion that has to be met to accept a step size in the decreasing loop additional_increase_condition::IF=(M, p) -> true : specify an additional criterion that has to be met to accept a step size in the (initial) increase loop candidate_point=allocate_result(M, rand) : speciy a point to be used as memory for the candidate points. contraction_factor=0.95 : how to update  $s$  in the decrease step initial_stepsize=1.0 `: specify an initial step size initial_guess= armijo_initial_guess : Compute the initial step size of a line search based on this function. The funtion required is  (p,s,k,l) -> Œ±  and computes the initial step size  $Œ±$  based on a  AbstractManoptProblem p ,  AbstractManoptSolverState s , the current iterate  k  and a last step size  l . retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stop_when_stepsize_less=0.0 : a safeguard, stop when the decreasing step is below this (nonnegative) bound. stop_when_stepsize_exceeds=max_stepsize(M) : a safeguard to not choose a too long step size when initially increasing stop_increasing_at_step=100 : stop the initial increasing loop after this amount of steps. Set to  0  to never increase in the beginning stop_decreasing_at_step=1000 : maximal number of Armijo decreases / tests to perform sufficient_decrease=0.1 : the sufficient decrease parameter  $œÑ$ For the stop safe guards you can pass  :Messages  to a  debug=  to see  @info  messages when these happen. Info This function generates a  ManifoldDefaultsFactory  for  ArmijoLinesearchStepsize . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2709,"pagetitle":"Stepsize","title":"Manopt.ConstantLength","ref":"/manopt/stable/plans/stepsize/#Manopt.ConstantLength","content":" Manopt.ConstantLength  ‚Äî  Function ConstantLength(s; kwargs...)\nConstantLength(M::AbstractManifold, s; kwargs...) Specify a  Stepsize  that is constant. Input M  (optional) s=min( injectivity_radius(M)/2, 1.0)  : the length to use. Keyword argument type::Symbol=relative  specify the type of constant step size. :relative  ‚Äì scale the gradient tangent vector  $X$  to  $s*X$ :absolute  ‚Äì scale the gradient to an absolute step length  $s$ , that is  $\\frac{s}{\\lVert X \\rVert_{}}X$ Info This function generates a  ManifoldDefaultsFactory  for  ConstantStepsize . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2710,"pagetitle":"Stepsize","title":"Manopt.DecreasingLength","ref":"/manopt/stable/plans/stepsize/#Manopt.DecreasingLength","content":" Manopt.DecreasingLength  ‚Äî  Function DegreasingLength(; kwargs...)\nDecreasingLength(M::AbstractManifold; kwargs...) Specify a [ Stepsize ]  that is decreasing as ``s_k = \\frac{(l - ak)f^i}{(k+s)^e} with the following Keyword arguments exponent=1.0 :   the exponent  $e$  in the denominator factor=1.0 :     the factor  $f$  in the nominator length=min(injectivity_radius(M)/2, 1.0) : the initial step size  $l$ . subtrahend=0.0 : a value  $a$  that is subtracted every iteration shift=0.0 :      shift the denominator iterator  $k$  by  $s$ . type::Symbol=relative  specify the type of constant step size. :relative  ‚Äì scale the gradient tangent vector  $X$  to  $s_k*X$ :absolute  ‚Äì scale the gradient to an absolute step length  $s_k$ , that is  $\\frac{s_k}{\\lVert X \\rVert_{}}X$ Info This function generates a  ManifoldDefaultsFactory  for  DecreasingStepsize . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2711,"pagetitle":"Stepsize","title":"Manopt.NonmonotoneLinesearch","ref":"/manopt/stable/plans/stepsize/#Manopt.NonmonotoneLinesearch","content":" Manopt.NonmonotoneLinesearch  ‚Äî  Function NonmonotoneLinesearch(; kwargs...)\nNonmonotoneLinesearch(M::AbstractManifold; kwargs...) A functor representing a nonmonotone line search using the Barzilai-Borwein step size [ IP17 ]. This method first computes (x -> p, F-> f) \\[y_{k} = \\operatorname{grad}f(p_{k}) - \\mathcal T_{p_k‚Üêp_{k-1}}\\operatorname{grad}f(p_{k-1})\\] and \\[s_{k} = - Œ±_{k-1} ‚ãÖ \\mathcal T_{p_k‚Üêp_{k-1}}\\operatorname{grad}f(p_{k-1}),\\] where  $Œ±_{k-1}$  is the step size computed in the last iteration and  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  is a vector transport. Then the Barzilai‚ÄîBorwein step size is \\[Œ±_k^{\\text{BB}} = \\begin{cases}   \\min(Œ±_{\\text{max}}, \\max(Œ±_{\\text{min}}, œÑ_{k})), & \\text{if} ‚ü®s_{k}, y_{k}‚ü©_{p_k} > 0,\\\\\\\\    Œ±_{\\text{max}}, & \\text{else,}\\end{cases}\\] where \\[œÑ_{k} = \\frac{‚ü®s_{k}, s_{k}‚ü©_{p_k}}{‚ü®s_{k}, y_{k}‚ü©_{p_k}},\\] if the direct strategy is chosen, or \\[œÑ_{k} =  \\frac{‚ü®s_{k}, y_{k}‚ü©_{p_k}}{‚ü®y_{k}, y_{k}‚ü©_{p_k}},\\] in case of the inverse strategy or an alternation between the two in cases for the alternating strategy. Then find the smallest  $h = 0, 1, 2, ‚Ä¶$  such that \\[f(\\operatorname{retr}_{p_k}(- œÉ^h Œ±_k^{\\text{BB}} \\operatorname{grad}f(p_k)))  ‚â§\n\\max_{1 ‚â§ j ‚â§ \\max(k+1,m)} f(p_{k+1-j}) - Œ≥ œÉ^h Œ±_k^{\\text{BB}} ‚ü®\\operatorname{grad}F(p_k), \\operatorname{grad}F(p_k)‚ü©_{p_k},\\] where  $œÉ ‚àà (0,1)$  is a step length reduction factor ,  $m$  is the number of iterations after which the function value has to be lower than the current one and  $Œ≥ ‚àà (0,1)$  is the sufficient decrease parameter. Finally the step size is computed as \\[Œ±_k = œÉ^h Œ±_k^{\\text{BB}}.\\] Keyword arguments p= rand (M) : a point on the manifold  $\\mathcal M$ to store an interim result p=allocate_result(M, rand) : to store an interim result initial_stepsize=1.0 : the step size to start the search with memory_size=10 : number of iterations after which the cost value needs to be lower than the current one bb_min_stepsize=1e-3 : lower bound for the Barzilai-Borwein step size greater than zero bb_max_stepsize=1e3 : upper bound for the Barzilai-Borwein step size greater than min_stepsize retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions strategy=direct : defines if the new step size is computed using the  :direct ,  :indirect  or  :alternating  strategy storage= StoreStateAction (M; store_fields=[:Iterate, :Gradient]) : increase efficiency by using a  StoreStateAction  for  :Iterate  and  :Gradient . stepsize_reduction=0.5 :  step size reduction factor contained in the interval  $(0,1)$ sufficient_decrease=1e-4 : sufficient decrease parameter contained in the interval  $(0,1)$ stop_when_stepsize_less=0.0 : smallest stepsize when to stop (the last one before is taken) stop_when_stepsize_exceeds= max_stepsize (M, p) ): largest stepsize when to stop to avoid leaving the injectivity radius stop_increasing_at_step=100 :  last step to increase the stepsize (phase 1), stop_decreasing_at_step=1000 : last step size to decrease the stepsize (phase 2), source"},{"id":2712,"pagetitle":"Stepsize","title":"Manopt.Polyak","ref":"/manopt/stable/plans/stepsize/#Manopt.Polyak","content":" Manopt.Polyak  ‚Äî  Function Polyak(; kwargs...)\nPolyak(M::AbstractManifold; kwargs...) Compute a step size according to a method propsed by Polyak, cf. the Dynamic step size discussed in Section 3.2 of [ Ber15 ]. This has been generalised here to both the Riemannian case and to approximate the minimum cost value. Let  $f_{\\text{best}$  be the best cost value seen until now during some iterative optimisation algorithm and let  $Œ≥_k$  be a sequence of numbers that is square summable, but not summable. Then the step size computed here reads \\[s_k = \\frac{f(p^{(k)}) - f_{\\text{best} + Œ≥_k}{\\lVert ‚àÇf(p^{(k)})} \\rVert_{}},\\] where  $‚àÇf$  denotes a nonzero-subgradient of  $f$  at the current iterate  $p^{(k)}$ . Constructor Polyak(; Œ≥ = k -> 1/k, initial_cost_estimate=0.0) initialize the Polyak stepsize to a certain sequence and an initial estimate of  $f_{\text{best}}$ . Info This function generates a  ManifoldDefaultsFactory  for  PolyakStepsize . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2713,"pagetitle":"Stepsize","title":"Manopt.WolfePowellLinesearch","ref":"/manopt/stable/plans/stepsize/#Manopt.WolfePowellLinesearch","content":" Manopt.WolfePowellLinesearch  ‚Äî  Function WolfePowellLinesearch(; kwargs...)\nWolfePowellLinesearch(M::AbstractManifold; kwargs...) Perform a lineseach to fulfull both the Armijo-Goldstein conditions \\[f\\bigl( \\operatorname{retr}_{p}(Œ±X) \\bigr) ‚â§ f(p) + c_1 Œ±_k ‚ü®\\operatorname{grad} f(p), X‚ü©_{p}\\] as well as the Wolfe conditions \\[\\frac{\\mathrm{d}}{\\mathrm{d}t} f\\bigl(\\operatorname{retr}_{p}(tX)\\bigr)\n\\Big\\vert_{t=Œ±}\n‚â• c_2 \\frac{\\mathrm{d}}{\\mathrm{d}t} f\\bigl(\\operatorname{retr}_{p}(tX)\\bigr)\\Big\\vert_{t=0}.\\] for some given sufficient decrease coefficient  $c_1$  and some sufficient curvature condition coefficient $c_2$ . This is adopted from [ NW06 , Section 3.1] Keyword arguments sufficient_decrease=10^(-4) sufficient_curvature=0.999 p::P : a point on the manifold  $\\mathcal M$ as temporary storage for candidates X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ as type of memory allocated for the candidates direction and tangent max_stepsize= max_stepsize (M, p) : largest stepsize allowed here. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stop_when_stepsize_less=0.0 : smallest stepsize when to stop (the last one before is taken) vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports source"},{"id":2714,"pagetitle":"Stepsize","title":"Manopt.WolfePowellBinaryLinesearch","ref":"/manopt/stable/plans/stepsize/#Manopt.WolfePowellBinaryLinesearch","content":" Manopt.WolfePowellBinaryLinesearch  ‚Äî  Function WolfePowellBinaryLinesearch(; kwargs...)\nWolfePowellBinaryLinesearch(M::AbstractManifold; kwargs...) Perform a lineseach to fulfull both the Armijo-Goldstein conditions for some given sufficient decrease coefficient  $c_1$  and some sufficient curvature condition coefficient $c_2$ . Compared to  WolfePowellLinesearch  which tries a simpler method, this linesearch performs the following algorithm With \\[A(t) = f(p_+) ‚â§ c_1 t ‚ü®\\operatorname{grad}f(p), X‚ü©_{x}\n\\quad\\text{ and }\\quad\nW(t) = ‚ü®\\operatorname{grad}f(x_+), \\mathcal T_{p_+‚Üêp}X‚ü©_{p_+} ‚â• c_2 ‚ü®X, \\operatorname{grad}f(x)‚ü©_x,\\] where  $p_+ =\\operatorname{retr}_p(tX)$  is the current trial point, and  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  denotes a vector transport. Then the following Algorithm is performed similar to Algorithm 7 from [ Hua14 ] set  $Œ±=0$ ,  $Œ≤=‚àû$  and  $t=1$ . While either  $A(t)$  does not hold or  $W(t)$  does not hold do steps 3-5. If  $A(t)$  fails, set  $Œ≤=t$ . If  $A(t)$  holds but  $W(t)$  fails, set  $Œ±=t$ . If  $Œ≤<‚àû$  set  $t=\\frac{Œ±+Œ≤}{2}$ , otherwise set  $t=2Œ±$ . Keyword arguments sufficient_decrease=10^(-4) sufficient_curvature=0.999 max_stepsize= max_stepsize (M, p) : largest stepsize allowed here. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stop_when_stepsize_less=0.0 : smallest stepsize when to stop (the last one before is taken) vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports source Some step sizes use  max_stepsize  function as a rough upper estimate for the trust region size. It is by default equal to injectivity radius of the exponential map but in some cases a different value is used. For the  FixedRankMatrices  manifold an estimate from Manopt is used. Tangent bundle with the Sasaki metric has 0 injectivity radius, so the maximum stepsize of the underlying manifold is used instead.  Hyperrectangle  also has 0 injectivity radius and an estimate based on maximum of dimensions along each index is used instead. For manifolds with corners, however, a line search capable of handling break points along the projected search direction should be used, and such algorithms do not call  max_stepsize . Internally these step size functions create a  ManifoldDefaultsFactory . Internally these use"},{"id":2715,"pagetitle":"Stepsize","title":"Manopt.armijo_initial_guess","ref":"/manopt/stable/plans/stepsize/#Manopt.armijo_initial_guess-Tuple{AbstractManoptProblem, AbstractManoptSolverState, Int64, Real}","content":" Manopt.armijo_initial_guess  ‚Äî  Method armijo_initial_guess(mp::AbstractManoptProblem, s::AbstractManoptSolverState, k, l) Input mp : the  AbstractManoptProblem  we are aiminig to minimize s :  the  AbstractManoptSolverState  for the current solver k :  the current iteration l :  the last step size computed in the previous iteration. Return an initial guess for the  ArmijoLinesearchStepsize . The default provided is based on the  max_stepsize (M) , which we denote by  $m$ . Let further  $X$  be the current descent direction with norm  $n=\\lVert X \\rVert_{p}$  its length. Then this (default) initial guess returns $l$  if  $m$  is not finite $\\min(l, \\frac{m}{n})$  otherwise This ensures that the initial guess does not yield to large (initial) steps. source"},{"id":2716,"pagetitle":"Stepsize","title":"Manopt.default_stepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.default_stepsize-Tuple{AbstractManifold, Type{<:AbstractManoptSolverState}}","content":" Manopt.default_stepsize  ‚Äî  Method default_stepsize(M::AbstractManifold, ams::AbstractManoptSolverState) Returns the default  Stepsize  functor used when running the solver specified by the  AbstractManoptSolverState ams  running with an objective on the  AbstractManifold M . source"},{"id":2717,"pagetitle":"Stepsize","title":"Manopt.get_last_stepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.get_last_stepsize-Tuple{AbstractManoptProblem, AbstractManoptSolverState, Vararg{Any}}","content":" Manopt.get_last_stepsize  ‚Äî  Method get_last_stepsize(amp::AbstractManoptProblem, ams::AbstractManoptSolverState, vars...) return the last computed stepsize stored within  AbstractManoptSolverState ams  when solving the  AbstractManoptProblem amp . This method takes into account that  ams  might be decorated. In case this returns  NaN , a concrete call to the stored stepsize is performed. For this, usually, the first of the  vars...  should be the current iterate. source"},{"id":2718,"pagetitle":"Stepsize","title":"Manopt.get_last_stepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.get_last_stepsize-Tuple{Stepsize, Vararg{Any}}","content":" Manopt.get_last_stepsize  ‚Äî  Method get_last_stepsize(::Stepsize, vars...) return the last computed stepsize from within the stepsize. If no last step size is stored, this returns  NaN . source"},{"id":2719,"pagetitle":"Stepsize","title":"Manopt.get_stepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.get_stepsize-Tuple{AbstractManoptProblem, AbstractManoptSolverState, Vararg{Any}}","content":" Manopt.get_stepsize  ‚Äî  Method get_stepsize(amp::AbstractManoptProblem, ams::AbstractManoptSolverState, vars...) return the stepsize stored within  AbstractManoptSolverState ams  when solving the  AbstractManoptProblem amp . This method also works for decorated options and the  Stepsize  function within the options, by default stored in  ams.stepsize . source"},{"id":2720,"pagetitle":"Stepsize","title":"Manopt.linesearch_backtrack!","ref":"/manopt/stable/plans/stepsize/#Manopt.linesearch_backtrack!-Union{Tuple{T}, Tuple{TF}, Tuple{AbstractManifold, Any, TF, Any, T, Any, Any, Any}, Tuple{AbstractManifold, Any, TF, Any, T, Any, Any, Any, T}, Tuple{AbstractManifold, Any, TF, Any, T, Any, Any, Any, T, Any}} where {TF, T}","content":" Manopt.linesearch_backtrack!  ‚Äî  Method (s, msg) = linesearch_backtrack!(M, q, F, p, X, s, decrease, contract Œ∑ = -X, f0 = f(p)) Perform a line search backtrack in-place of  q . For all details and options, see  linesearch_backtrack source"},{"id":2721,"pagetitle":"Stepsize","title":"Manopt.linesearch_backtrack","ref":"/manopt/stable/plans/stepsize/#Manopt.linesearch_backtrack-Union{Tuple{T}, Tuple{AbstractManifold, Any, Any, T, Any, Any, Any}, Tuple{AbstractManifold, Any, Any, T, Any, Any, Any, T}, Tuple{AbstractManifold, Any, Any, T, Any, Any, Any, T, Any}} where T","content":" Manopt.linesearch_backtrack  ‚Äî  Method (s, msg) = linesearch_backtrack(M, F, p, X, s, decrease, contract Œ∑ = -X, f0 = f(p); kwargs...)\n(s, msg) = linesearch_backtrack!(M, q, F, p, X, s, decrease, contract Œ∑ = -X, f0 = f(p); kwargs...) perform a line search on manifold  M for the cost function  f , at the current point  p with current gradient provided in  X an initial stepsize  s a sufficient  decrease a  contract ion factor  $œÉ$ a search direction  $Œ∑ = -X$ an offset,  $f_0 = F(x)$ Keyword arguments retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stop_when_stepsize_less=0.0 : to avoid numerical underflow stop_when_stepsize_exceeds= max_stepsize (M, p) / norm(M, p, Œ∑) ) to avoid leaving the injectivity radius on a manifold stop_increasing_at_step=100 : stop the initial increase of step size after these many steps stop_decreasing_at_step= 1000`: stop the decreasing search after these many steps additional_increase_condition=(M,p) -> true : impose an additional condition for an increased step size to be accepted additional_decrease_condition=(M,p) -> true : impose an additional condition for an decreased step size to be accepted These keywords are used as safeguards, where only the max stepsize is a very manifold specific one. Return value A stepsize  s  and a message  msg  (in case any of the 4 criteria hit) source"},{"id":2722,"pagetitle":"Stepsize","title":"Manopt.max_stepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.max_stepsize-Tuple{AbstractManifold, Any}","content":" Manopt.max_stepsize  ‚Äî  Method max_stepsize(M::AbstractManifold, p)\nmax_stepsize(M::AbstractManifold) Get the maximum stepsize (at point  p ) on manifold  M . It should be used to limit the distance an algorithm is trying to move in a single step. By default, this returns  injectivity_radius (M) , if this exists. If this is not available on the the method returns  Inf . source"},{"id":2723,"pagetitle":"Stepsize","title":"Manopt.AdaptiveWNGradientStepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.AdaptiveWNGradientStepsize","content":" Manopt.AdaptiveWNGradientStepsize  ‚Äî  Type AdaptiveWNGradientStepsize{I<:Integer,R<:Real,F<:Function} <: Stepsize A functor  problem, state, k, X) -> s to an adaptive gradient method introduced by [GrapigliaStella:2023](@cite). See [ AdaptiveWNGradient`](@ref) for the mathematical details. Fields count_threshold::I : an  Integer  for  $\\hat{c}$ minimal_bound::R : the value for  $b_{\\text{min}}$ alternate_bound::F : how to determine  $\\hat{k}_k$  as a function of  (bmin, bk, hat_c) -> hat_bk gradient_reduction::R : the gradient reduction factor threshold  $Œ± ‚àà [0,1)$ gradient_bound::R : the bound  $b_k$ . weight::R :  $œâ_k$  initialised to  $œâ_0 =$ norm(M, p, X)  if this is not zero,  1.0  otherwise. count::I :  $c_k$ , initialised to  $c_0 = 0$ . Constructor AdaptiveWNGrad(M::AbstractManifold; kwargs...) Keyword arguments adaptive=true : switches the  gradient_reduction Œ± (if true ) to 0`. alternate_bound = (bk, hat_c) ->  min(gradient_bound == 0 ? 1.0 : gradient_bound, max(minimal_bound, bk / (3 * hat_c)) count_threshold=4 gradient_reduction::R=adaptive ? 0.9 : 0.0 gradient_bound=norm(M, p, X) minimal_bound=1e-4 p= rand (M) : a point on the manifold  $\\mathcal M$ only used to define the  gradient_bound X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ only used to define the  gradient_bound source"},{"id":2724,"pagetitle":"Stepsize","title":"Manopt.ArmijoLinesearchStepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.ArmijoLinesearchStepsize","content":" Manopt.ArmijoLinesearchStepsize  ‚Äî  Type ArmijoLinesearchStepsize <: Linesearch A functor  problem, state, k, X) -> s to provide an Armijo line search to compute step size, based on the search direction X` Fields candidate_point :               to store an interim result initial_stepsize :              and initial step size retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions contraction_factor :            exponent for line search reduction sufficient_decrease :           gain within Armijo's rule last_stepsize :                 the last step size to start the search with initial_guess :                 a function to provide an initial guess for the step size, it maps  (m,p,k,l) -> Œ±  based on a  AbstractManoptProblem p ,  AbstractManoptSolverState s , the current iterate  k  and a last step size  l . It returns the initial guess  Œ± . additional_decrease_condition : specify a condition a new point has to additionally fulfill. The default accepts all points. additional_increase_condition : specify a condtion that additionally to checking a valid increase has to be fulfilled. The default accepts all points. stop_when_stepsize_less :       smallest stepsize when to stop (the last one before is taken) stop_when_stepsize_exceeds :    largest stepsize when to stop. stop_increasing_at_step :       last step to increase the stepsize (phase 1), stop_decreasing_at_step :       last step size to decrease the stepsize (phase 2), Pass  :Messages  to a  debug=  to see  @info s when these happen. Constructor ArmijoLinesearchStepsize(M::AbstractManifold; kwarg...) with the fields keyword arguments and the retraction is set to the default retraction on  M . Keyword arguments candidate_point= ( allocate_result(M, rand) ) initial_stepsize=1.0 retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions contraction_factor=0.95 sufficient_decrease=0.1 last_stepsize=initialstepsize initial_guess= armijo_initial_guess ‚Äì (p,s,i,l) -> l stop_when_stepsize_less=0.0 : stop when the stepsize decreased below this version. stop_when_stepsize_exceeds=[ max_step ](@ref) (M)`: provide an absolute maximal step size. stop_increasing_at_step=100 : for the initial increase test, stop after these many steps stop_decreasing_at_step=1000 : in the backtrack, stop after these many steps source"},{"id":2725,"pagetitle":"Stepsize","title":"Manopt.ConstantStepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.ConstantStepsize","content":" Manopt.ConstantStepsize  ‚Äî  Type ConstantStepsize <: Stepsize A functor  (problem, state, ...) -> s  to provide a constant step size  s . Fields length : constant value for the step size type :   a symbol that indicates whether the stepsize is relatively (:relative),   with respect to the gradient norm, or absolutely (:absolute) constant. Constructors ConstantStepsize(s::Real, t::Symbol=:relative) initialize the stepsize to a constant  s  of type  t . ConstantStepsize(\n    M::AbstractManifold=DefaultManifold(),\n    s=min(1.0, injectivity_radius(M)/2);\n    type::Symbol=:relative\n) source"},{"id":2726,"pagetitle":"Stepsize","title":"Manopt.DecreasingStepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.DecreasingStepsize","content":" Manopt.DecreasingStepsize  ‚Äî  Type DecreasingStepsize() A functor  (problem, state, ...) -> s  to provide a constant step size  s . Fields exponent :   a value  $e$  the current iteration numbers  $e$ th exponential is taken of factor :     a value  $f$  to multiply the initial step size with every iteration length :     the initial step size  $l$ . subtrahend : a value  $a$  that is subtracted every iteration shift :      shift the denominator iterator  $i$  by  $s$ `. type :       a symbol that indicates whether the stepsize is relatively (:relative),   with respect to the gradient norm, or absolutely (:absolute) constant. In total the complete formulae reads for the  $i$ th iterate as \\[s_i = \\frac{(l - i a)f^i}{(i+s)^e}\\] and hence the default simplifies to just  $s_i = \frac{l}{i}$ Constructor DecreasingStepsize(M::AbstractManifold;\n    length=min(injectivity_radius(M)/2, 1.0),\n    factor=1.0,\n    subtrahend=0.0,\n    exponent=1.0,\n    shift=0.0,\n    type=:relative,\n) initializes all fields, where none of them is mandatory and the length is set to half and to  $1$  if the injectivity radius is infinite. source"},{"id":2727,"pagetitle":"Stepsize","title":"Manopt.Linesearch","ref":"/manopt/stable/plans/stepsize/#Manopt.Linesearch","content":" Manopt.Linesearch  ‚Äî  Type Linesearch <: Stepsize An abstract functor to represent line search type step size determinations, see  Stepsize  for details. One example is the  ArmijoLinesearchStepsize  functor. Compared to simple step sizes, the line search functors provide an interface of the form  (p,o,i,X) -> s  with an additional (but optional) fourth parameter to provide a search direction; this should default to something reasonable, most prominently the negative gradient. source"},{"id":2728,"pagetitle":"Stepsize","title":"Manopt.NonmonotoneLinesearchStepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.NonmonotoneLinesearchStepsize","content":" Manopt.NonmonotoneLinesearchStepsize  ‚Äî  Type NonmonotoneLinesearchStepsize{P,T,R<:Real} <: Linesearch A functor representing a nonmonotone line search using the Barzilai-Borwein step size [ IP17 ]. Fields initial_stepsize=1.0 :     the step size to start the search with memory_size=10 :           number of iterations after which the cost value needs to be lower than the current one bb_min_stepsize=1e-3 :     lower bound for the Barzilai-Borwein step size greater than zero bb_max_stepsize=1e3 :      upper bound for the Barzilai-Borwein step size greater than min_stepsize retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions strategy=direct :          defines if the new step size is computed using the  :direct ,  :indirect  or  :alternating  strategy storage :                  (for  :Iterate  and  :Gradient ) a  StoreStateAction stepsize_reduction :       step size reduction factor contained in the interval (0,1) sufficient_decrease :     sufficient decrease parameter contained in the interval (0,1) vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports candidate_point :          to store an interim result stop_when_stepsize_less :    smallest stepsize when to stop (the last one before is taken) stop_when_stepsize_exceeds : largest stepsize when to stop. stop_increasing_at_step :    last step to increase the stepsize (phase 1), stop_decreasing_at_step :    last step size to decrease the stepsize (phase 2), Constructor NonmonotoneLinesearchStepsize(M::AbstractManifold; kwargs...) Keyword arguments p=allocate_result(M, rand) : to store an interim result initial_stepsize=1.0 memory_size=10 bb_min_stepsize=1e-3 bb_max_stepsize=1e3 retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions strategy=direct storage=[ StoreStateAction ](@ref) (M; store_fields=[:Iterate, :Gradient])`` stepsize_reduction=0.5 sufficient_decrease=1e-4 stop_when_stepsize_less=0.0 stop_when_stepsize_exceeds= max_stepsize (M, p) ) stop_increasing_at_step=100 stop_decreasing_at_step=1000 vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports source"},{"id":2729,"pagetitle":"Stepsize","title":"Manopt.PolyakStepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.PolyakStepsize","content":" Manopt.PolyakStepsize  ‚Äî  Type PolyakStepsize <: Stepsize A functor  (problem, state, ...) -> s  to provide a step size due to Polyak, cf. Section 3.2 of [ Ber15 ]. Fields Œ≥                : a function  k -> ...  representing a seuqnce. best_cost_value  : storing the best cost value Constructor PolyakStepsize(;\n    Œ≥ = i -> 1/i,\n    initial_cost_estimate=0.0\n) Construct a stepsize of Polyak type. See also Polyak source"},{"id":2730,"pagetitle":"Stepsize","title":"Manopt.WolfePowellBinaryLinesearchStepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.WolfePowellBinaryLinesearchStepsize","content":" Manopt.WolfePowellBinaryLinesearchStepsize  ‚Äî  Type WolfePowellBinaryLinesearchStepsize{R} <: Linesearch Do a backtracking line search to find a step size  $Œ±$  that fulfils the Wolfe conditions along a search direction  $X$  starting from  $p$ . See  WolfePowellBinaryLinesearch  for the math details. Fields sufficient_decrease::R ,  sufficient_curvature::R  two constants in the line search last_stepsize::R max_stepsize::R retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stop_when_stepsize_less::R : a safeguard to stop when the stepsize gets too small vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Keyword arguments sufficient_decrease=10^(-4) sufficient_curvature=0.999 max_stepsize= max_stepsize (M, p) : largest stepsize allowed here. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stop_when_stepsize_less=0.0 : smallest stepsize when to stop (the last one before is taken) vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports source"},{"id":2731,"pagetitle":"Stepsize","title":"Manopt.WolfePowellLinesearchStepsize","ref":"/manopt/stable/plans/stepsize/#Manopt.WolfePowellLinesearchStepsize","content":" Manopt.WolfePowellLinesearchStepsize  ‚Äî  Type WolfePowellLinesearchStepsize{R<:Real} <: Linesearch Do a backtracking line search to find a step size  $Œ±$  that fulfils the Wolfe conditions along a search direction  $X$  starting from  $p$ . See  WolfePowellLinesearch  for the math details Fields sufficient_decrease::R ,  sufficient_curvature::R  two constants in the line search candidate_direction::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ candidate_point::P : a point on the manifold  $\\mathcal M$ as temporary storage for candidates candidate_tangent::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ last_stepsize::R max_stepsize::R retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stop_when_stepsize_less::R : a safeguard to stop when the stepsize gets too small vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Keyword arguments sufficient_decrease=10^(-4) sufficient_curvature=0.999 p::P : a point on the manifold  $\\mathcal M$ as temporary storage for candidates X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ as type of memory allocated for the candidates direction and tangent max_stepsize= max_stepsize (M, p) : largest stepsize allowed here. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stop_when_stepsize_less=0.0 : smallest stepsize when to stop (the last one before is taken) vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports source Some solvers have a different iterate from the one used for the line search. Then the following state can be used to wrap these locally"},{"id":2732,"pagetitle":"Stepsize","title":"Manopt.StepsizeState","ref":"/manopt/stable/plans/stepsize/#Manopt.StepsizeState","content":" Manopt.StepsizeState  ‚Äî  Type StepsizeState{P,T} <: AbstractManoptSolverState A state to store a point and a descent direction used within a linesearch, if these are different from the iterate and search direction of the main solver. Fields p::P : a point on a manifold X::T : a tangent vector at  p . Constructor StepsizeState(p,X)\nStepsizeState(M::AbstractManifold; p=rand(M), x=zero_vector(M,p) See also interior_point_Newton source"},{"id":2733,"pagetitle":"Stepsize","title":"Literature","ref":"/manopt/stable/plans/stepsize/#Literature","content":" Literature [Ber15] D.¬†P.¬†Bertsekas.  Convex Optimization Algorithms  (Athena Scientific, 2015); p.¬†576. [Bou23] N.¬†Boumal.  An Introduction to Optimization on Smooth Manifolds . First¬†Edition ( Cambridge University Press, 2023 ). [GS23] G.¬†N.¬†Grapiglia and G.¬†F.¬†Stella.  An Adaptive Riemannian Gradient Method Without Function Evaluations .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  197 , 1140‚Äì1160  (2023). [Hua14] W.¬†Huang.  Optimization algorithms on Riemannian manifolds with applications . Ph.D. Thesis, Flordia State University (2014). [IP17] B.¬†Iannazzo and M.¬†Porcelli.  The Riemannian Barzilai‚ÄìBorwein method with nonmonotone line search and the matrix geometric mean computation .  IMA¬†Journal¬†of¬†Numerical¬†Analysis  38 , 495‚Äì517  (2017). [NW06] J.¬†Nocedal and S.¬†J.¬†Wright.  Numerical Optimization . 2¬†Edition (Springer, New York, 2006)."},{"id":2736,"pagetitle":"Stopping Criteria","title":"Stopping criteria","ref":"/manopt/stable/plans/stopping_criteria/#sec-stopping-criteria","content":" Stopping criteria Stopping criteria are implemented as a  functor  and inherit from the base type"},{"id":2737,"pagetitle":"Stopping Criteria","title":"Manopt.StoppingCriterion","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StoppingCriterion","content":" Manopt.StoppingCriterion  ‚Äî  Type StoppingCriterion An abstract type for the functors representing stopping criteria, so they are callable structures. The naming Scheme follows functions, see for example  StopAfterIteration . Every StoppingCriterion has to provide a constructor and its function has to have the interface  (p,o,i)  where a  AbstractManoptProblem  as well as  AbstractManoptSolverState  and the current number of iterations are the arguments and returns a boolean whether to stop or not. By default each  StoppingCriterion  should provide a fields  reason  to provide details when a criterion is met (and that is empty otherwise). source They can also be grouped, which is summarized in the type of a set of criteria"},{"id":2738,"pagetitle":"Stopping Criteria","title":"Manopt.StoppingCriterionSet","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StoppingCriterionSet","content":" Manopt.StoppingCriterionSet  ‚Äî  Type StoppingCriterionGroup <: StoppingCriterion An abstract type for a Stopping Criterion that itself consists of a set of Stopping criteria. In total it acts as a stopping criterion itself. Examples are  StopWhenAny  and  StopWhenAll  that can be used to combine stopping criteria. source The stopping criteria  s  might have certain internal values/fields it uses to verify against. This is done when calling them as a function  s(amp::AbstractManoptProblem, ams::AbstractManoptSolverState) , where the  AbstractManoptProblem  and the  AbstractManoptSolverState  together represent the current state of the solver. The functor returns either  false  when the stopping criterion is not fulfilled or  true  otherwise. One field all criteria should have is the  s.at_iteration , to indicate at which iteration the stopping criterion (last) indicated to stop.  0  refers to an indication  before  starting the algorithm, while any negative number meant the stopping criterion is not (yet) fulfilled. To can access a string giving the reason of stopping see  get_reason ."},{"id":2739,"pagetitle":"Stopping Criteria","title":"Generic stopping criteria","ref":"/manopt/stable/plans/stopping_criteria/#Generic-stopping-criteria","content":" Generic stopping criteria The following generic stopping criteria are available. Some require that, for example, the corresponding  AbstractManoptSolverState  have a field  gradient  when the criterion should access that. Further stopping criteria might be available for individual solvers."},{"id":2740,"pagetitle":"Stopping Criteria","title":"Manopt.StopAfter","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopAfter","content":" Manopt.StopAfter  ‚Äî  Type StopAfter <: StoppingCriterion store a threshold when to stop looking at the complete runtime. It uses  time_ns()  to measure the time and you provide a  Period  as a time limit, for example  Minute(15) . Fields threshold  stores the  Period  after which to stop start  stores the starting time when the algorithm is started, that is a call with  i=0 . time  stores the elapsed time at_iteration  indicates at which iteration (including  i=0 ) the stopping criterion was fulfilled and is  -1  while it is not fulfilled. Constructor StopAfter(t) initialize the stopping criterion to a  Period t  to stop after. source"},{"id":2741,"pagetitle":"Stopping Criteria","title":"Manopt.StopAfterIteration","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopAfterIteration","content":" Manopt.StopAfterIteration  ‚Äî  Type StopAfterIteration <: StoppingCriterion A functor for a stopping criterion to stop after a maximal number of iterations. Fields max_iterations   stores the maximal iteration number where to stop at at_iteration  indicates at which iteration (including  i=0 ) the stopping criterion was fulfilled and is  -1  while it is not fulfilled. Constructor StopAfterIteration(maxIter) initialize the functor to indicate to stop after  maxIter  iterations. source"},{"id":2742,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenAll","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenAll","content":" Manopt.StopWhenAll  ‚Äî  Type StopWhenAll <: StoppingCriterionSet store an array of  StoppingCriterion  elements and indicates to stop, when  all  indicate to stop. The  reason  is given by the concatenation of all reasons. Constructor StopWhenAll(c::NTuple{N,StoppingCriterion} where N)\nStopWhenAll(c::StoppingCriterion,...) source"},{"id":2743,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenAny","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenAny","content":" Manopt.StopWhenAny  ‚Äî  Type StopWhenAny <: StoppingCriterionSet store an array of  StoppingCriterion  elements and indicates to stop, when  any  single one indicates to stop. The  reason  is given by the concatenation of all reasons (assuming that all non-indicating return  \"\" ). Constructor StopWhenAny(c::NTuple{N,StoppingCriterion} where N)\nStopWhenAny(c::StoppingCriterion...) source"},{"id":2744,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenChangeLess","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenChangeLess","content":" Manopt.StopWhenChangeLess  ‚Äî  Type StopWhenChangeLess <: StoppingCriterion stores a threshold when to stop looking at the norm of the change of the optimization variable from within a  AbstractManoptSolverState s . That ism by accessing  get_iterate(s)  and comparing successive iterates. For the storage a  StoreStateAction  is used. Fields at_iteration::Int : an integer indicating at which the stopping criterion last indicted to stop, which might also be before the solver started ( 0 ). Any negative value indicates that this was not yet the case; last_change::Real : the last change recorded in this stopping criterion inverse_retraction_method::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses storage::StoreStateAction : a storage to access the previous iterate at_iteration::Int : indicate at which iteration this stopping criterion was last active. inverse_retraction : An  AbstractInverseRetractionMethod  that can be passed to approximate the distance by this inverse retraction and a norm on the tangent space. This can be used if neither the distance nor the logarithmic map are availannle on  M . last_change : store the last change storage : A  StoreStateAction  to access the previous iterate. threshold : the threshold for the change to check (run under to stop) outer_norm : if  M  is a manifold with components, this can be used to specify the norm, that is used to compute the overall distance based on the element-wise distance. You can deactivate this, but setting this value to  missing . Example On an  AbstractPowerManifold  like  $\\mathcal M = \\mathcal N^n$  any point  $p = (p_1,‚Ä¶,p_n) ‚àà \\mathcal M$  is a vector of length  $n$  with of points  $p_i ‚àà \\mathcal N$ . Then, denoting the  outer_norm  by  $r$ , the distance of two points  $p,q ‚àà \\mathcal M$  is given by \\mathrm{d}(p,q) = \\Bigl( \\sum_{}^{}_{k=1}^n \\mathrm{d}(p_k,q_k)^r \\Bigr)^{\\frac{1}{r}}, where the sum turns into a maximum for the case  $r=‚àû$ . The  outer_norm  has no effect on manifolds that do not consist of components. If the manifold does not have components, the outer norm is ignored. Constructor StopWhenChangeLess(\n    M::AbstractManifold,\n    threshold::Float64;\n    storage::StoreStateAction=StoreStateAction([:Iterate]),\n    inverse_retraction_method::IRT=default_inverse_retraction_method(M)\n    outer_norm::Union{Missing,Real}=missing\n) initialize the stopping criterion to a threshold  Œµ  using the  StoreStateAction a , which is initialized to just store  :Iterate  by default. You can also provide an inverse retraction method for the  distance  or a manifold to use its default inverse retraction. source"},{"id":2745,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenCostChangeLess","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenCostChangeLess","content":" Manopt.StopWhenCostChangeLess  ‚Äî  Type StopWhenCostChangeLess <: StoppingCriterion A stopping criterion to stop when the change of the cost function is less than a certain threshold. Fields at_iteration::Int : an integer indicating at which the stopping criterion last indicted to stop, which might also be before the solver started ( 0 ). Any negative value indicates that this was not yet the case; last_change::Real : the last change recorded in this stopping criterion last_cost `: the last cost value Constructor StopWhenCostChangeLess(tolerance::F) Initialize the stopping criterion to a threshold  tolerance  for the change of the cost function. source"},{"id":2746,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenCostLess","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenCostLess","content":" Manopt.StopWhenCostLess  ‚Äî  Type StopWhenCostLess <: StoppingCriterion store a threshold when to stop looking at the cost function of the optimization problem from within a  AbstractManoptProblem , i.e  get_cost(p,get_iterate(o)) . Constructor StopWhenCostLess(Œµ) initialize the stopping criterion to a threshold  Œµ . source"},{"id":2747,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenCostNaN","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenCostNaN","content":" Manopt.StopWhenCostNaN  ‚Äî  Type StopWhenCostNaN <: StoppingCriterion stop looking at the cost function of the optimization problem from within a  AbstractManoptProblem , i.e  get_cost(p,get_iterate(o)) . Constructor StopWhenCostNaN() initialize the stopping criterion to NaN. source"},{"id":2748,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenCriterionWithIterationCondition","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenCriterionWithIterationCondition","content":" Manopt.StopWhenCriterionWithIterationCondition  ‚Äî  Type StopWhenCriterionWithIterationCondition <: StoppingCriterion A stopping criterion that indicates to stop when the (internal) stopping criterion it wraps, has indicated to stop with an additional check compared to the current iteration, e.g. >(n)  only (stricly) after iteration  n >=(n)  only including and after iteration  n ==(n)  only exactly at  n <=(n)  only including and before iteration  n <(n)  only (strictly) before iteration  n any functor  f(k) -> Bool  indicating to evaluate the stopping criterion at iteration  k . Fields criterion : the  StoppingCriterion  to wrap comp : the number of times the criterion has to indicate to stop Constructor StopWhenRepeated(criterion::StoppingCriterion, n=0; comp = (>(n))) Create a stopping criterion that indicates to stop when the  comp  has indicated to check the inner criterion. The  n  is ignored if you provide a manual functor  comp . As well as for a given [ StoppingCriterion ]  sc  the shortcuts  sc > n ,  sc >= n ,  sc == n ,  sc <= n ,  sc < n  for the cases above. Examples A stopping criterion that indicates to stop when the gradient norm is small but only after the third iteration     StopWhenCriterionWithIterationCondition(StopWhenGradientNormLess(1e-6), 3)     StopWhenGradientNormLess(1e-6) > 3 source"},{"id":2749,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenEntryChangeLess","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenEntryChangeLess","content":" Manopt.StopWhenEntryChangeLess  ‚Äî  Type StopWhenEntryChangeLess Evaluate whether a certain fields change is less than a certain threshold Fields field :     a symbol addressing the corresponding field in a certain subtype of  AbstractManoptSolverState  to track distance :  a function  (problem, state, v1, v2) -> R  that computes the distance between two possible values of the  field storage :   a  StoreStateAction  to store the previous value of the  field threshold : the threshold to indicate to stop when the distance is below this value Internal fields at_iteration : store the iteration at which the stop indication happened stores a threshold when to stop looking at the norm of the change of the optimization variable from within a  AbstractManoptSolverState , i.e  get_iterate(o) . For the storage a  StoreStateAction  is used Constructor StopWhenEntryChangeLess(\n    field::Symbol\n    distance,\n    threshold;\n    storage::StoreStateAction=StoreStateAction([field]),\n) source"},{"id":2750,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenGradientChangeLess","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenGradientChangeLess","content":" Manopt.StopWhenGradientChangeLess  ‚Äî  Type StopWhenGradientChangeLess <: StoppingCriterion A stopping criterion based on the change of the gradient. Fields at_iteration::Int : an integer indicating at which the stopping criterion last indicted to stop, which might also be before the solver started ( 0 ). Any negative value indicates that this was not yet the case; last_change::Real : the last change recorded in this stopping criterion vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports storage::StoreStateAction : a storage to access the previous iterate threshold : the threshold for the change to check (run under to stop) outer_norm : if  M  is a manifold with components, this can be used to specify the norm, that is used to compute the overall distance based on the element-wise distance. You can deactivate this, but setting this value to  missing . Example On an  AbstractPowerManifold  like  $\\mathcal M = \\mathcal N^n$  any point  $p = (p_1,‚Ä¶,p_n) ‚àà \\mathcal M$  is a vector of length  $n$  with of points  $p_i ‚àà \\mathcal N$ . Then, denoting the  outer_norm  by  $r$ , the norm of the difference of tangent vectors like the last and current gradien  $X,Y ‚àà \\mathcal M$  is given by \\lVert X-Y \\rVert_{p} = \\Bigl( \\sum_{}^{}_{k=1}^n \\lVert X_k-Y_k \\rVert_{p_k}^r \\Bigr)^{\\frac{1}{r}}, where the sum turns into a maximum for the case  $r=‚àû$ . The  outer_norm  has no effect on manifols, that do not consist of components. Constructor StopWhenGradientChangeLess(\n    M::AbstractManifold,\n    Œµ::Float64;\n    storage::StoreStateAction=StoreStateAction([:Iterate]),\n    vector_transport_method::IRT=default_vector_transport_method(M),\n    outer_norm::N=missing\n) Create a stopping criterion with threshold  Œµ  for the change gradient, that is, this criterion indicates to stop when  get_gradient  is in (norm of) its change less than  Œµ , where  vector_transport_method  denotes the vector transport  $\\mathcal T$  used. source"},{"id":2751,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenGradientNormLess","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenGradientNormLess","content":" Manopt.StopWhenGradientNormLess  ‚Äî  Type StopWhenGradientNormLess <: StoppingCriterion A stopping criterion based on the current gradient norm. Fields norm :      a function  (M::AbstractManifold, p, X) -> ‚Ñù  that computes a norm of the gradient  X  in the tangent space at  p  on  M . For manifolds with components provide (M::AbstractManifold, p, X, r) -> ‚Ñù`. threshold : the threshold to indicate to stop when the distance is below this value outer_norm : if  M  is a manifold with components, this can be used to specify the norm, that is used to compute the overall distance based on the element-wise distance. Internal fields last_change  store the last change at_iteration  store the iteration at which the stop indication happened Example On an  AbstractPowerManifold  like  $\\mathcal M = \\mathcal N^n$  any point  $p = (p_1,‚Ä¶,p_n) ‚àà \\mathcal M$  is a vector of length  $n$  with of points  $p_i ‚àà \\mathcal N$ . Then, denoting the  outer_norm  by  $r$ , the norm of a tangent vector like the current gradient  $X ‚àà \\mathcal M$  is given by \\lVert X \\rVert_{p} = \\Bigl( \\sum_{}^{}_{k=1}^n \\lVert X_k \\rVert_{p_k}^r \\Bigr)^{\\frac{1}{r}}, where the sum turns into a maximum for the case  $r=‚àû$ . The  outer_norm  has no effect on manifolds that do not consist of components. If you pass in your individual norm, this can be deactivated on such manifolds by passing  missing  to  outer_norm . Constructor StopWhenGradientNormLess(Œµ; norm=ManifoldsBase.norm, outer_norm=missing) Create a stopping criterion with threshold  Œµ  for the gradient, that is, this criterion indicates to stop when  get_gradient  returns a gradient vector of norm less than  Œµ , where the norm to use can be specified in the  norm=  keyword. source"},{"id":2752,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenIterateNaN","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenIterateNaN","content":" Manopt.StopWhenIterateNaN  ‚Äî  Type StopWhenIterateNaN <: StoppingCriterion stop looking at the cost function of the optimization problem from within a  AbstractManoptProblem , i.e  get_cost(p,get_iterate(o)) . Constructor StopWhenIterateNaN() initialize the stopping criterion to NaN. source"},{"id":2753,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenRepeated","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenRepeated","content":" Manopt.StopWhenRepeated  ‚Äî  Type StopWhenRepeated <: StoppingCriterion A stopping Criterion that indicates to stop when the (internal) stoppoing criterion it wraps, has indicated to stop for  n  (consecutive) times Fields criterion : the  StoppingCriterion  to wrap n : the number of times the criterion has to indicate to stop count : the number of times the criterion has indicated to stop so far consecutive::Bool : indicate whether to count consecutive indications to stop or arbitrary. Constructor StopWhenRepeated(criterion::StoppingCriterion, n::Int; consecutive::Bool=true)\ncriterion √ó n\ncross(sc::StoppingCriterion, n::Int) Create a stopping criterion that indicates to stop when the  criterion  has indicated to stop  n  times (consecutively, if  consecutive=true  for the first constructor). Note that the cross product is in general noncommutative, and here only the order  sc √ó n ` is possible. Examples A stopping criterion that indicates to stop whenever the gradient norm is less that  1e-6  for three consecutive iterations:     StopWhenRepeated(StopWhenGradientNormLess(1e-6), 3)     StopWhenGradientNormLess(1e-6) √ó 3 A stopping criterion that indicates to stop whenever the gradient norm is less that  1e-6  at three iterations (not necessarily consecutive):     StopWhenRepeated(StopWhenGradientNormLess(1e-6), 3; consecutive=false) source"},{"id":2754,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenSmallerOrEqual","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenSmallerOrEqual","content":" Manopt.StopWhenSmallerOrEqual  ‚Äî  Type StopWhenSmallerOrEqual <: StoppingCriterion A functor for an stopping criterion, where the algorithm if stopped when a variable is smaller than or equal to its minimum value. Fields value     stores the variable which has to fall under a threshold for the algorithm to stop minValue  stores the threshold where, if the value is smaller or equal to this threshold, the algorithm stops Constructor StopWhenSmallerOrEqual(value, minValue) initialize the functor to indicate to stop after  value  is smaller than or equal to  minValue . source"},{"id":2755,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenStepsizeLess","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenStepsizeLess","content":" Manopt.StopWhenStepsizeLess  ‚Äî  Type StopWhenStepsizeLess <: StoppingCriterion stores a threshold when to stop looking at the last step size determined or found during the last iteration from within a  AbstractManoptSolverState . Constructor StopWhenStepsizeLess(Œµ) initialize the stopping criterion to a threshold  Œµ . source"},{"id":2756,"pagetitle":"Stopping Criteria","title":"Manopt.StopWhenSubgradientNormLess","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.StopWhenSubgradientNormLess","content":" Manopt.StopWhenSubgradientNormLess  ‚Äî  Type StopWhenSubgradientNormLess <: StoppingCriterion A stopping criterion based on the current subgradient norm. Constructor StopWhenSubgradientNormLess(Œµ::Float64) Create a stopping criterion with threshold  Œµ  for the subgradient, that is, this criterion indicates to stop when  get_subgradient  returns a subgradient vector of norm less than  Œµ . source"},{"id":2757,"pagetitle":"Stopping Criteria","title":"Functions for stopping criteria","ref":"/manopt/stable/plans/stopping_criteria/#Functions-for-stopping-criteria","content":" Functions for stopping criteria There are a few functions to update, combine, and modify stopping criteria, especially to update internal values even for stopping criteria already being used within an  AbstractManoptSolverState  structure."},{"id":2758,"pagetitle":"Stopping Criteria","title":"Base.:&","ref":"/manopt/stable/plans/stopping_criteria/#Base.:&-Union{Tuple{T}, Tuple{S}, Tuple{S, T}} where {S<:StoppingCriterion, T<:StoppingCriterion}","content":" Base.:&  ‚Äî  Method &(s1,s2)\ns1 & s2 Combine two  StoppingCriterion  within an  StopWhenAll . If either  s1  (or  s2 ) is already an  StopWhenAll , then  s2  (or  s1 ) is appended to the list of  StoppingCriterion  within  s1  (or  s2 ). Example a = StopAfterIteration(200) & StopWhenChangeLess(M, 1e-6)\nb = a & StopWhenGradientNormLess(1e-6) Is the same as a = StopWhenAll(StopAfterIteration(200), StopWhenChangeLess(M, 1e-6))\nb = StopWhenAll(StopAfterIteration(200), StopWhenChangeLess(M, 1e-6), StopWhenGradientNormLess(1e-6)) source"},{"id":2759,"pagetitle":"Stopping Criteria","title":"Base.:|","ref":"/manopt/stable/plans/stopping_criteria/#Base.:|-Union{Tuple{T}, Tuple{S}, Tuple{S, T}} where {S<:StoppingCriterion, T<:StoppingCriterion}","content":" Base.:|  ‚Äî  Method |(s1,s2)\ns1 | s2 Combine two  StoppingCriterion  within an  StopWhenAny . If either  s1  (or  s2 ) is already an  StopWhenAny , then  s2  (or  s1 ) is appended to the list of  StoppingCriterion  within  s1  (or  s2 ) Example a = StopAfterIteration(200) | StopWhenChangeLess(M, 1e-6)\nb = a | StopWhenGradientNormLess(1e-6) Is the same as a = StopWhenAny(StopAfterIteration(200), StopWhenChangeLess(M, 1e-6))\nb = StopWhenAny(StopAfterIteration(200), StopWhenChangeLess(M, 1e-6), StopWhenGradientNormLess(1e-6)) source"},{"id":2760,"pagetitle":"Stopping Criteria","title":"Manopt.get_active_stopping_criteria","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.get_active_stopping_criteria-Tuple{sCS} where sCS<:StoppingCriterionSet","content":" Manopt.get_active_stopping_criteria  ‚Äî  Method get_active_stopping_criteria(c) returns all active stopping criteria, if any, that are within a  StoppingCriterion c , and indicated a stop, that is their reason is nonempty. To be precise for a simple stopping criterion, this returns either an empty array if no stop is indicated or the stopping criterion as the only element of an array. For a  StoppingCriterionSet  all internal (even nested) criteria that indicate to stop are returned. source"},{"id":2761,"pagetitle":"Stopping Criteria","title":"Manopt.get_reason","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.get_reason-Tuple{AbstractManoptSolverState}","content":" Manopt.get_reason  ‚Äî  Method get_reason(s::AbstractManoptSolverState) return the current reason stored within the  StoppingCriterion  from within the  AbstractManoptSolverState . This reason is empty ( \"\" ) if the criterion has never been met. source"},{"id":2762,"pagetitle":"Stopping Criteria","title":"Manopt.get_stopping_criteria","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.get_stopping_criteria-Tuple{S} where S<:StoppingCriterionSet","content":" Manopt.get_stopping_criteria  ‚Äî  Method get_stopping_criteria(c) return the array of internally stored  StoppingCriterion s for a  StoppingCriterionSet c . source"},{"id":2763,"pagetitle":"Stopping Criteria","title":"Manopt.indicates_convergence","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.indicates_convergence-Tuple{StoppingCriterion}","content":" Manopt.indicates_convergence  ‚Äî  Method indicates_convergence(c::StoppingCriterion) Return whether (true) or not (false) a  StoppingCriterion  does  always  mean that, when it indicates to stop, the solver has converged to a minimizer or critical point. Note that this is independent of the actual state of the stopping criterion, whether some of them indicate to stop, but a purely type-based, static decision. Examples With  s1=StopAfterIteration(20)  and  s2=StopWhenGradientNormLess(1e-7)  the indicator yields indicates_convergence(s1)  is  false indicates_convergence(s2)  is  true indicates_convergence(s1 | s2)  is  false , since this might also stop after 20 iterations indicates_convergence(s1 & s2)  is  true , since  s2  is fulfilled if this stops. source"},{"id":2764,"pagetitle":"Stopping Criteria","title":"Manopt.set_parameter!","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.set_parameter!-Tuple{StopAfter, Val{:MaxTime}, Dates.Period}","content":" Manopt.set_parameter!  ‚Äî  Method set_parameter!(c::StopAfter, :MaxTime, v::Period) Update the time period after which an algorithm shall stop. source"},{"id":2765,"pagetitle":"Stopping Criteria","title":"Manopt.set_parameter!","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.set_parameter!-Tuple{StopAfterIteration, Val{:MaxIteration}, Int64}","content":" Manopt.set_parameter!  ‚Äî  Method set_parameter!(c::StopAfterIteration, :;MaxIteration, v::Int) Update the number of iterations after which the algorithm should stop. source"},{"id":2766,"pagetitle":"Stopping Criteria","title":"Manopt.set_parameter!","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.set_parameter!-Tuple{StopWhenChangeLess, Val{:MinIterateChange}, Any}","content":" Manopt.set_parameter!  ‚Äî  Method set_parameter!(c::StopWhenChangeLess, :MinIterateChange, v::Int) Update the minimal change below which an algorithm shall stop. source"},{"id":2767,"pagetitle":"Stopping Criteria","title":"Manopt.set_parameter!","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.set_parameter!-Tuple{StopWhenCostLess, Val{:MinCost}, Any}","content":" Manopt.set_parameter!  ‚Äî  Method set_parameter!(c::StopWhenCostLess, :MinCost, v) Update the minimal cost below which the algorithm shall stop source"},{"id":2768,"pagetitle":"Stopping Criteria","title":"Manopt.set_parameter!","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.set_parameter!-Tuple{StopWhenEntryChangeLess, Val{:Threshold}, Any}","content":" Manopt.set_parameter!  ‚Äî  Method set_parameter!(c::StopWhenEntryChangeLess, :Threshold, v) Update the minimal cost below which the algorithm shall stop source"},{"id":2769,"pagetitle":"Stopping Criteria","title":"Manopt.set_parameter!","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.set_parameter!-Tuple{StopWhenGradientChangeLess, Val{:MinGradientChange}, Any}","content":" Manopt.set_parameter!  ‚Äî  Method set_parameter!(c::StopWhenGradientChangeLess, :MinGradientChange, v) Update the minimal change below which an algorithm shall stop. source"},{"id":2770,"pagetitle":"Stopping Criteria","title":"Manopt.set_parameter!","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.set_parameter!-Tuple{StopWhenGradientNormLess, Val{:MinGradNorm}, Float64}","content":" Manopt.set_parameter!  ‚Äî  Method set_parameter!(c::StopWhenGradientNormLess, :MinGradNorm, v::Float64) Update the minimal gradient norm when an algorithm shall stop source"},{"id":2771,"pagetitle":"Stopping Criteria","title":"Manopt.set_parameter!","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.set_parameter!-Tuple{StopWhenStepsizeLess, Val{:MinStepsize}, Any}","content":" Manopt.set_parameter!  ‚Äî  Method set_parameter!(c::StopWhenStepsizeLess, :MinStepsize, v) Update the minimal step size below which the algorithm shall stop source"},{"id":2772,"pagetitle":"Stopping Criteria","title":"Manopt.set_parameter!","ref":"/manopt/stable/plans/stopping_criteria/#Manopt.set_parameter!-Tuple{StopWhenSubgradientNormLess, Val{:MinSubgradNorm}, Float64}","content":" Manopt.set_parameter!  ‚Äî  Method set_parameter!(c::StopWhenSubgradientNormLess, :MinSubgradNorm, v::Float64) Update the minimal subgradient norm when an algorithm shall stop source"},{"id":2775,"pagetitle":"References","title":"Literature","ref":"/manopt/stable/references/#Literature","content":" Literature This is all literature mentioned / referenced in the  Manopt.jl  documentation. Usually you find a small reference section at the end of every documentation page that contains the corresponding references as well. [ABG06] P.-A.¬†Absil, C.¬†Baker and K.¬†Gallivan.  Trust-Region Methods on Riemannian Manifolds .  Foundations¬†of¬†Computational¬†Mathematics  7 , 303‚Äì330  (2006). [AMS08] P.-A.¬†Absil, R.¬†Mahony and R.¬†Sepulchre.  Optimization Algorithms on Matrix Manifolds  (Princeton University Press, 2008), available online at  press.princeton.edu/chapters/absil/ . [AOT22] S.¬†Adachi, T.¬†Okuno and A.¬†Takeda.  Riemannian Levenberg-Marquardt Method with Global and Local Convergence Properties . ArXiv¬†Preprint (2022). [ABBC20] N.¬†Agarwal, N.¬†Boumal, B.¬†Bullins and C.¬†Cartis.  Adaptive regularization with cubics on manifolds .  Mathematical¬†Programming  (2020). [ACOO20] Y.¬†T.¬†Almeida, J.¬†X.¬†Cruz Neto, P.¬†R.¬†Oliveira and J.¬†C.¬†Oliveira Souza.  A modified proximal point method for DC functions on Hadamard manifolds .  Computational¬†Optimization¬†and¬†Applications  76 , 649‚Äì673  (2020). [Bac14] M.¬†Baƒç√°k.  Computing medians and means in Hadamard spaces .  SIAM¬†Journal¬†on¬†Optimization  24 , 1542‚Äì1566  (2014),  arXiv:1210.2145 . [Bea72] E.¬†M.¬†Beale.  A derivation of conjugate gradients . In:  Numerical methods for nonlinear optimization , edited by F.¬†A.¬†Lootsma (Academic Press, London, London, 1972); pp.¬†39‚Äì43. [BFNZ25] R.¬†Bergmann, O.¬†P.¬†Ferreira, S.¬†Z.¬†N√©meth and J.¬†Zhu.  On projection mappings and the gradient projection method on hyperbolic space forms . Preprint,¬†in¬†preparation (2025). [BFSS24] R.¬†Bergmann, O.¬†P.¬†Ferreira, E.¬†M.¬†Santos and J.¬†C.¬†Souza.  The difference of convex algorithm on Hadamard manifolds .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  (2024). [BH19] R.¬†Bergmann and R.¬†Herzog.  Intrinsic formulation of KKT conditions and constraint qualifications on smooth manifolds .  SIAM¬†Journal¬†on¬†Optimization  29 , 2423‚Äì2444  (2019),  arXiv:1804.06214 . [BHJ24] R.¬†Bergmann, R.¬†Herzog and H.¬†Jasa.  The Riemannian Convex Bundle Method , preprint (2024),  arXiv:2402.13670 . [BHS+21] R.¬†Bergmann, R.¬†Herzog, M.¬†Silva Louzeiro, D.¬†Tenbrinck and J.¬†Vidal-N√∫√±ez.  Fenchel duality theory and a primal-dual algorithm on Riemannian manifolds .  Foundations¬†of¬†Computational¬†Mathematics  21 , 1465‚Äì1504  (2021),  arXiv:1908.02022 . [BJJP25] R.¬†Bergmann, H.¬†Jasa, P.¬†John and M.¬†Pfeffer.  The Intrinsic Riemannian Proximal Gradient Method for Nonconvex Optimization , preprint (2025),  arXiv:2506.09775 . [BPS16] R.¬†Bergmann, J.¬†Persch and G.¬†Steidl.  A parallel Douglas Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  9 , 901‚Äì937  (2016),  arXiv:1512.02814 . [Ber15] D.¬†P.¬†Bertsekas.  Convex Optimization Algorithms  (Athena Scientific, 2015); p.¬†576. [BIA10] P.¬†B.¬†Borckmans, M.¬†Ishteva and P.-A.¬†Absil.  A Modified Particle Swarm Optimization Algorithm for the Best Low Multilinear Rank Approximation of Higher-Order Tensors . In:  7th International Conference on Swarm INtelligence  (Springer Berlin Heidelberg, 2010); pp.¬†13‚Äì23. [Bou23] N.¬†Boumal.  An Introduction to Optimization on Smooth Manifolds . First¬†Edition ( Cambridge University Press, 2023 ). [Car92] M.¬†P.¬†do¬†Carmo.  Riemannian Geometry .  Mathematics: Theory & Applications  (Birkh√§user Boston, Inc., Boston, MA, 1992); p.¬†xiv+300. [CP11] A.¬†Chambolle and T.¬†Pock.  A first-order primal-dual algorithm for convex problems with applications to imaging .  Journal¬†of¬†Mathematical¬†Imaging¬†and¬†Vision  40 , 120‚Äì145  (2011). [CFFS10] S.¬†Colutto, F.¬†Fruhauf, M.¬†Fuchs and O.¬†Scherzer.  The CMA-ES on Riemannian Manifolds to Reconstruct Shapes in 3-D Voxel Images .  IEEE¬†Transactions¬†on¬†Evolutionary¬†Computation  14 , 227‚Äì245  (2010). [CGT00] A.¬†R.¬†Conn, N.¬†I.¬†Gould and P.¬†L.¬†Toint.  Trust Region Methods  (Society for Industrial and Applied Mathematics, 2000). [DY99] Y.¬†H.¬†Dai and Y.¬†Yuan.  A Nonlinear Conjugate Gradient Method with a Strong Global Convergence Property .  SIAM¬†Journal¬†on¬†Optimization  10 , 177‚Äì182  (1999). [DL21] W.¬†Diepeveen and J.¬†Lellmann.  An Inexact Semismooth Newton Method on Riemannian Manifolds with Application to Duality-Based Total Variation Denoising .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  14 , 1565‚Äì1600  (2021),  arXiv:2102.10309 . [Dre07] D.¬†W.¬†Dreisigmeyer.  Direct Search Alogirthms over Riemannian Manifolds  (Optimization Online, 2007). [ETTZ96] A.¬†S.¬†El-Bakry, R.¬†A.¬†Tapia, T.¬†Tsuchiya and Y.¬†Zhang.  On the formulation and theory of the Newton interior-point method for nonlinear programming .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  89 , 507‚Äì541  (1996). [FO98] O.¬†Ferreira and P.¬†R.¬†Oliveira.  Subgradient algorithm on Riemannian manifolds .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  97 , 93‚Äì104  (1998). [FO02] O.¬†Ferreira and P.¬†R.¬†Oliveira.  Proximal point algorithm on Riemannian manifolds .  Optimization.¬†A¬†Journal¬†of¬†Mathematical¬†Programming¬†and¬†Operations¬†Research  51 , 257‚Äì270  (2002). [Fle87] R.¬†Fletcher.  Practical Methods of Optimization . 2¬†Edition,  A Wiley-Interscience Publication  (John Wiley & Sons Ltd., 1987). [FR64] R.¬†Fletcher and C.¬†M.¬†Reeves.  Function minimization by conjugate gradients .  The¬†Computer¬†Journal  7 , 149‚Äì154  (1964). [GS23] G.¬†N.¬†Grapiglia and G.¬†F.¬†Stella.  An Adaptive Riemannian Gradient Method Without Function Evaluations .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  197 , 1140‚Äì1160  (2023). [HZ06] W.¬†W.¬†Hager and H.¬†Zhang.  A survey of nonlinear conjugate gradient methods . Pacific¬†Journal¬†of¬†Optimization  2 , 35‚Äì58 (2006). [HZ05] W.¬†W.¬†Hager and H.¬†Zhang.  A New Conjugate Gradient Method with Guaranteed Descent and an Efficient Line Search .  SIAM¬†Journal¬†on¬†Optimization  16 , 170‚Äì192  (2005). [Han23] N.¬†Hansen.  The CMA Evolution Strategy: A Tutorial . ArXiv¬†Preprint (2023). [HS52] M.¬†Hestenes and E.¬†Stiefel.  Methods of conjugate gradients for solving linear systems .  Journal¬†of¬†Research¬†of¬†the¬†National¬†Bureau¬†of¬†Standards  49 , 409  (1952). [HNP23] N.¬†Hoseini Monjezi, S.¬†Nobakhtian and M.¬†R.¬†Pouryayevali.  A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds .  IMA¬†Journal¬†of¬†Numerical¬†Analysis  43 , 293‚Äì325  (2023). [Hua14] W.¬†Huang.  Optimization algorithms on Riemannian manifolds with applications . Ph.D. Thesis, Flordia State University (2014). [HAG18] W.¬†Huang, P.-A.¬†Absil and K.¬†A.¬†Gallivan.  A Riemannian BFGS method without differentiated retraction for nonconvex optimization problems .  SIAM¬†Journal¬†on¬†Optimization  28 , 470‚Äì495  (2018). [HGA15] W.¬†Huang, K.¬†A.¬†Gallivan and P.-A.¬†Absil.  A Broyden class of quasi-Newton methods for Riemannian optimization .  SIAM¬†Journal¬†on¬†Optimization  25 , 1660‚Äì1685  (2015). [IP17] B.¬†Iannazzo and M.¬†Porcelli.  The Riemannian Barzilai‚ÄìBorwein method with nonmonotone line search and the matrix geometric mean computation .  IMA¬†Journal¬†of¬†Numerical¬†Analysis  38 , 495‚Äì517  (2017). [Kar77] H.¬†Karcher.  Riemannian center of mass and mollifier smoothing .  Communications¬†on¬†Pure¬†and¬†Applied¬†Mathematics  30 , 509‚Äì541  (1977). [LY24] Z.¬†Lai and A.¬†Yoshise.  Riemannian Interior Point Methods for Constrained Optimization on Manifolds .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  201 , 433‚Äì469  (2024),  arXiv:2203.09762 . [LB19] C.¬†Liu and N.¬†Boumal.  Simple algorithms for optimization on Riemannian manifolds with constraints .  Applied¬†Mathematics¬†&¬†Optimization  (2019),  arXiv:1091.10000 . [LS91] Y.¬†Liu and C.¬†Storey.  Efficient generalized conjugate gradient algorithms,  part 1: Theory .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  69 , 129‚Äì137  (1991). [Ngu23] D.¬†Nguyen.  Operator-Valued Formulas for Riemannian Gradient and Hessian and Families of Tractable Metrics in Riemannian Optimization .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  198 , 135‚Äì164  (2023),  arXiv:2009.10159 . [NW06] J.¬†Nocedal and S.¬†J.¬†Wright.  Numerical Optimization . 2¬†Edition (Springer, New York, 2006). [Pee93] R.¬†Peeters.  On a Riemannian version of the Levenberg-Marquardt algorithm . Serie Research Memoranda¬†0011 (VU University Amsterdam, Faculty of Economics, Business Administration and Econometrics, 1993). [PR69] E.¬†Polak and G.¬†Ribi√®re.  Note sur la convergence de m√©thodes de directions conjugu√©es .  Revue¬†fran√ßaise¬†d‚Äôinformatique¬†et¬†de¬†recherche¬†op√©rationnelle  3 , 35‚Äì43  (1969). [Pow77] M.¬†J.¬†Powell.  Restart procedures for the conjugate gradient method .  Mathematical¬†Programming  12 , 241‚Äì254  (1977). [SO15] J.¬†C.¬†Souza and P.¬†R.¬†Oliveira.  A proximal point algorithm for DC fuctions on Hadamard manifolds .  Journal¬†of¬†Global¬†Optimization  63 , 797‚Äì810  (2015). [WS22] M.¬†Weber and S.¬†Sra.  Riemannian Optimization via Frank-Wolfe Methods .  Mathematical¬†Programming  199 , 525‚Äì556  (2022). [ZS18] H.¬†Zhang and S.¬†Sra.  Towards Riemannian accelerated gradient methods , arXiv¬†Preprint,¬†1806.02812 (2018)."},{"id":2778,"pagetitle":"List of Solvers","title":"Available solvers in Manopt.jl","ref":"/manopt/stable/solvers/#Available-solvers-in-Manopt.jl","content":" Available solvers in Manopt.jl Optimisation problems can be classified with respect to several criteria. The following list of the algorithms is a grouped with respect to the ‚Äúinformation‚Äù available about a optimisation problem \\[\\operatorname*{arg\\,min}_{p‚àà\\mathbb M} f(p)\\] Within each group short notes on advantages of the individual solvers, and required properties the cost  $f$  should have, are provided. In that list a üèÖ is used to indicate state-of-the-art solvers, that usually perform best in their corresponding group and ü´è for a maybe not so fast, maybe not so state-of-the-art method, that nevertheless gets the job done most reliably."},{"id":2779,"pagetitle":"List of Solvers","title":"Derivative free","ref":"/manopt/stable/solvers/#Derivative-free","content":" Derivative free For derivative free only function evaluations of  $f$  are used. Nelder-Mead  a simplex based variant, that is using  $d+1$  points, where  $d$  is the dimension of the manifold. Particle Swarm  ü´è use the evolution of a set of points, called swarm, to explore the domain of the cost and find a minimizer. Mesh adaptive direct search  performs a mesh based exploration (poll) and search. CMA-ES  uses a stochastic evolutionary strategy to perform minimization robust to local minima of the objective."},{"id":2780,"pagetitle":"List of Solvers","title":"First order","ref":"/manopt/stable/solvers/#First-order","content":" First order"},{"id":2781,"pagetitle":"List of Solvers","title":"Gradient","ref":"/manopt/stable/solvers/#Gradient","content":" Gradient Gradient Descent  uses the gradient from  $f$  to determine a descent direction. Here, the direction can also be changed to be Averaged, Momentum-based, based on Nesterovs rule. Conjugate Gradient Descent  uses information from the previous descent direction to improve the current (gradient-based) one including several such update rules. The  Quasi-Newton Method  üèÖ uses gradient evaluations to approximate the Hessian, which is then used in a Newton-like scheme, where both a limited memory and a full Hessian approximation are available with several different update rules. Steihaug-Toint Truncated Conjugate-Gradient Method  a solver for a constrained problem defined on a tangent space."},{"id":2782,"pagetitle":"List of Solvers","title":"Subgradient","ref":"/manopt/stable/solvers/#Subgradient","content":" Subgradient The following methods require the Riemannian subgradient  $‚àÇf$  to be available. While the subgradient might be set-valued, the function should provide one of the subgradients. The  Subgradient Method  takes the negative subgradient as a step direction and can be combined with a step size. The  Convex Bundle Method  (CBM) uses a former collection of sub gradients at the previous iterates and iterate candidates to solve a local approximation to  f  in every iteration by solving a quadratic problem in the tangent space. The  Proximal Bundle Method  works similar to CBM, but solves a proximal map-based problem in every iteration."},{"id":2783,"pagetitle":"List of Solvers","title":"Second order","ref":"/manopt/stable/solvers/#Second-order","content":" Second order Adaptive Regularisation with Cubics  üèÖ locally builds a cubic model to determine the next descent direction. The  Riemannian Trust-Regions Solver  builds a quadratic model within a trust region to determine the next descent direction."},{"id":2784,"pagetitle":"List of Solvers","title":"Splitting based","ref":"/manopt/stable/solvers/#Splitting-based","content":" Splitting based For splitting methods, the algorithms are based on splitting the cost into different parts, usually in a sum of two or more summands. This is usually very well tailored for non-smooth objectives."},{"id":2785,"pagetitle":"List of Solvers","title":"Smooth","ref":"/manopt/stable/solvers/#Smooth","content":" Smooth The following methods require that the splitting, for example into several summands, is smooth in the sense that for every summand of the cost, the gradient should still exist everywhere Levenberg-Marquardt  minimizes the square norm of  $f: \\mathcal M‚Üí‚Ñù^d$  provided the gradients of the component functions, or in other words the Jacobian of  $f$ . Stochastic Gradient Descent  is based on a splitting of  $f$  into a sum of several components  $f_i$  whose gradients are provided. Steps are performed according to gradients of randomly selected components. The  Alternating Gradient Descent  alternates gradient descent steps on the components of the product manifold. All these components should be smooth as it is required, that the gradient exists, and is (locally) convex."},{"id":2786,"pagetitle":"List of Solvers","title":"Nonsmooth","ref":"/manopt/stable/solvers/#Nonsmooth","content":" Nonsmooth If the gradient does not exist everywhere, that is if the splitting yields summands that are nonsmooth, usually methods based on proximal maps are used. The  Chambolle-Pock  algorithm uses a splitting  $f(p) = F(p) + G(Œõ(p))$ , where  $G$  is defined on a manifold  $\\mathcal N$  and the proximal map of its Fenchel dual is required. Both these functions can be non-smooth. The  Cyclic Proximal Point  ü´è uses proximal maps of the functions from splitting  $f$  into summands  $f_i$ Difference of Convex Algorithm  (DCA) uses a splitting of the (non-convex) function  $f = g - h$  into a difference of two functions; for each of these it is required to have access to the gradient of  $g$  and the subgradient of  $h$  to state a sub problem in every iteration to be solved. Difference of Convex Proximal Point  uses a splitting of the (non-convex) function  $f = g - h$  into a difference of two functions; provided the proximal map of  $g$  and the subgradient of  $h$ , the next iterate is computed. Compared to DCA, the corresponding sub problem is here written in a form that yields the proximal map. Douglas‚ÄîRachford  uses a splitting  $f(p) = F(x) + G(x)$  and their proximal maps to compute a minimizer of  $f$ , which can be non-smooth. Primal-dual Riemannian semismooth Newton Algorithm  extends Chambolle-Pock and requires the differentials of the proximal maps additionally. The  Proximal Point  uses the proximal map of  $f$  iteratively."},{"id":2787,"pagetitle":"List of Solvers","title":"Constrained","ref":"/manopt/stable/solvers/#Constrained","content":" Constrained Constrained problems of the form \\[\\begin{align*}\n\\operatorname*{arg\\,min}_{p‚àà\\mathbb M}& f(p)\\\\\n\\text{such that } & g(p) \\leq 0\\\\&h(p) = 0\n\\end{align*}\\] For these you can use The  Augmented Lagrangian Method  (ALM), where both  g  and  grad_g  as well as  h  and  grad_h  are keyword arguments, and one of these pairs is mandatory. The  Exact Penalty Method  (EPM) uses a penalty term instead of augmentation, but has the same interface as ALM. The  Interior Point Newton Method  (IPM) rephrases the KKT system of a constrained problem into an Newton iteration being performed in every iteration. Frank-Wolfe algorithm , where besides the gradient of  $f$  either a closed form solution or a (maybe even automatically generated) sub problem solver for  $\\operatorname*{arg\\,min}_{q ‚àà C} ‚ü®\\operatorname{grad} f(p_k), \\log_{p_k}q‚ü©$  is required, where  $p_k$  is a fixed point on the manifold (changed in every iteration). Gradient Projection Method"},{"id":2788,"pagetitle":"List of Solvers","title":"On the tangent space","ref":"/manopt/stable/solvers/#On-the-tangent-space","content":" On the tangent space Conjugate Residual  a solver for a linear system  $\\mathcal A[X] + b = 0$  on a tangent space. Steihaug-Toint Truncated Conjugate-Gradient Method  a solver for a constrained problem defined on a tangent space."},{"id":2789,"pagetitle":"List of Solvers","title":"Alphabetical list of algorithms","ref":"/manopt/stable/solvers/#Alphabetical-list-of-algorithms","content":" Alphabetical list of algorithms Solver Function State Adaptive Regularisation with Cubics adaptive_regularization_with_cubics AdaptiveRegularizationState Augmented Lagrangian Method augmented_Lagrangian_method AugmentedLagrangianMethodState Chambolle-Pock ChambollePock ChambollePockState Conjugate Gradient Descent conjugate_gradient_descent ConjugateGradientDescentState Conjugate Residual conjugate_residual ConjugateResidualState Convex Bundle Method convex_bundle_method ConvexBundleMethodState Cyclic Proximal Point cyclic_proximal_point CyclicProximalPointState Difference of Convex Algorithm difference_of_convex_algorithm DifferenceOfConvexState Difference of Convex Proximal Point difference_of_convex_proximal_point DifferenceOfConvexProximalState Douglas‚ÄîRachford DouglasRachford DouglasRachfordState Exact Penalty Method exact_penalty_method ExactPenaltyMethodState Frank-Wolfe algorithm Frank_Wolfe_method FrankWolfeState Gradient Descent gradient_descent GradientDescentState Interior Point Newton interior_point_Newton Levenberg-Marquardt LevenbergMarquardt LevenbergMarquardtState Nelder-Mead NelderMead NelderMeadState Particle Swarm particle_swarm ParticleSwarmState Primal-dual Riemannian semismooth Newton Algorithm primal_dual_semismooth_Newton PrimalDualSemismoothNewtonState Proximal Bundle Method proximal_bundle_method ProximalBundleMethodState Proximal Point proximal_point ProximalPointState Quasi-Newton Method quasi_Newton QuasiNewtonState Steihaug-Toint Truncated Conjugate-Gradient Method truncated_conjugate_gradient_descent TruncatedConjugateGradientState Subgradient Method subgradient_method SubGradientMethodState Stochastic Gradient Descent stochastic_gradient_descent StochasticGradientDescentState Riemannian Trust-Regions trust_regions TrustRegionsState Note that the solvers (their  AbstractManoptSolverState , to be precise) can also be decorated to enhance your algorithm by general additional properties, see  debug output  and  recording values . This is done using the  debug=  and  record=  keywords in the function calls. Similarly, a  cache=  keyword is available in any of the function calls, that wraps the  AbstractManoptProblem  in a cache for certain parts of the objective."},{"id":2790,"pagetitle":"List of Solvers","title":"Technical details","ref":"/manopt/stable/solvers/#Technical-details","content":" Technical details The main function a solver calls is"},{"id":2791,"pagetitle":"List of Solvers","title":"Manopt.solve!","ref":"/manopt/stable/solvers/#Manopt.solve!-Tuple{AbstractManoptProblem, AbstractManoptSolverState}","content":" Manopt.solve!  ‚Äî  Method solve!(p::AbstractManoptProblem, s::AbstractManoptSolverState) run the solver implemented for the  AbstractManoptProblem p  and the  AbstractManoptSolverState s  employing  initialize_solver! ,  step_solver! , as well as the  stop_solver!  of the solver. source which is a framework that you in general should not change or redefine. It uses the following methods, which also need to be implemented on your own algorithm, if you want to provide one."},{"id":2792,"pagetitle":"List of Solvers","title":"Manopt.initialize_solver!","ref":"/manopt/stable/solvers/#Manopt.initialize_solver!","content":" Manopt.initialize_solver!  ‚Äî  Function initialize_solver!(ams::AbstractManoptProblem, amp::AbstractManoptSolverState) Initialize the solver to the optimization  AbstractManoptProblem amp  by initializing the necessary values in the  AbstractManoptSolverState amp . source initialize_solver!(amp::AbstractManoptProblem, dss::DebugSolverState) Extend the initialization of the solver by a hook to run the  DebugAction  that was added to the  :Start  entry of the debug lists. All others are triggered (with iteration number  0 ) to trigger possible resets source initialize_solver!(ams::AbstractManoptProblem, rss::RecordSolverState) Extend the initialization of the solver by a hook to run records that were added to the  :Start  entry. source"},{"id":2793,"pagetitle":"List of Solvers","title":"Manopt.step_solver!","ref":"/manopt/stable/solvers/#Manopt.step_solver!","content":" Manopt.step_solver!  ‚Äî  Function step_solver!(amp::AbstractManoptProblem, ams::AbstractManoptSolverState, k) Do one iteration step (the  i th) for an  AbstractManoptProblem p  by modifying the values in the  AbstractManoptSolverState ams . source step_solver!(amp::AbstractManoptProblem, dss::DebugSolverState, k) Extend the  i th step of the solver by a hook to run debug prints, that were added to the  :BeforeIteration  and  :Iteration  entries of the debug lists. source step_solver!(amp::AbstractManoptProblem, rss::RecordSolverState, k) Extend the  i th step of the solver by a hook to run records, that were added to the  :Iteration  entry. source"},{"id":2794,"pagetitle":"List of Solvers","title":"Manopt.get_solver_result","ref":"/manopt/stable/solvers/#Manopt.get_solver_result","content":" Manopt.get_solver_result  ‚Äî  Function get_solver_result(ams::AbstractManoptSolverState)\nget_solver_result(tos::Tuple{AbstractManifoldObjective,AbstractManoptSolverState})\nget_solver_result(o::AbstractManifoldObjective, s::AbstractManoptSolverState) Return the final result after all iterations that is stored within the  AbstractManoptSolverState ams , which was modified during the iterations. For the case the objective is passed as well, but default, the objective is ignored, and the solver result for the state is called. source"},{"id":2795,"pagetitle":"List of Solvers","title":"Manopt.get_solver_return","ref":"/manopt/stable/solvers/#Manopt.get_solver_return","content":" Manopt.get_solver_return  ‚Äî  Function get_solver_return(s::AbstractManoptSolverState)\nget_solver_return(o::AbstractManifoldObjective, s::AbstractManoptSolverState) determine the result value of a call to a solver. By default this returns the same as  get_solver_result . get_solver_return(s::ReturnSolverState)\nget_solver_return(o::AbstractManifoldObjective, s::ReturnSolverState) return the internally stored state of the  ReturnSolverState  instead of the minimizer. This means that when the state are decorated like this, the user still has to call  get_solver_result  on the internal state separately. get_solver_return(o::ReturnManifoldObjective, s::AbstractManoptSolverState) return both the objective and the state as a tuple. source"},{"id":2796,"pagetitle":"List of Solvers","title":"Manopt.stop_solver!","ref":"/manopt/stable/solvers/#Manopt.stop_solver!-Tuple{AbstractManoptProblem, AbstractManoptSolverState, Any}","content":" Manopt.stop_solver!  ‚Äî  Method stop_solver!(amp::AbstractManoptProblem, ams::AbstractManoptSolverState, k) depending on the current  AbstractManoptProblem amp , the current state of the solver stored in  AbstractManoptSolverState ams  and the current iterate  i  this function determines whether to stop the solver, which by default means to call the internal  StoppingCriterion .  ams.stop source"},{"id":2797,"pagetitle":"List of Solvers","title":"API for solvers","ref":"/manopt/stable/solvers/#API-for-solvers","content":" API for solvers this is a short overview of the different types of high-level functions are usually available for a solver. Assume the solver is called  new_solver  and requires a cost  f  and some first order information  df  as well as a starting point  p  on  M .  f  and  df  form the objective together called  obj . Then there are basically two different variants to call"},{"id":2798,"pagetitle":"List of Solvers","title":"The easy to access call","ref":"/manopt/stable/solvers/#The-easy-to-access-call","content":" The easy to access call new_solver(M, f, df, p=rand(M); kwargs...)\nnew_solver!(M, f, df, p; kwargs...) Where the start point should be optional. Keyword arguments include the type of evaluation, decorators like  debug=  or  record=  as well as algorithm specific ones. If you provide an immutable point  p  or the  rand(M)  point is immutable, like on the  Circle()  this method should turn the point into a mutable one as well. The third variant works in place of  p , so it is mandatory. This first interface would set up the objective and pass all keywords on the objective based call."},{"id":2799,"pagetitle":"List of Solvers","title":"Objective based calls to solvers","ref":"/manopt/stable/solvers/#Objective-based-calls-to-solvers","content":" Objective based calls to solvers new_solver(M, obj, p=rand(M); kwargs...)\nnew_solver!(M, obj, p; kwargs...) Here the objective would be created beforehand for example to compare different solvers on the same objective, and for the first variant the start point is optional. Keyword arguments include decorators like  debug=  or  record=  as well as algorithm specific ones. This variant would generate the  problem  and the  state  and verify validity of all provided keyword arguments that affect the state. Then it would call the iterate process."},{"id":2800,"pagetitle":"List of Solvers","title":"Manual calls","ref":"/manopt/stable/solvers/#Manual-calls","content":" Manual calls If you generate the corresponding  problem  and  state  as the previous step does, you can also use the third (lowest level) and just call solve!(problem, state)"},{"id":2801,"pagetitle":"List of Solvers","title":"Closed-form sub solvers","ref":"/manopt/stable/solvers/#Closed-form-sub-solvers","content":" Closed-form sub solvers If a subsolver solution is available in closed form,  ClosedFormSubSolverState  is used to indicate that."},{"id":2802,"pagetitle":"List of Solvers","title":"Manopt.ClosedFormSubSolverState","ref":"/manopt/stable/solvers/#Manopt.ClosedFormSubSolverState","content":" Manopt.ClosedFormSubSolverState  ‚Äî  Type ClosedFormSubSolverState{E<:AbstractEvaluationType} <: AbstractManoptSolverState Subsolver state indicating that a closed-form solution is available with  AbstractEvaluationType E . Constructor ClosedFormSubSolverState(; evaluation=AllocatingEvaluation()) source"},{"id":2805,"pagetitle":"Chambolle-Pock","title":"The Riemannian Chambolle-Pock algorithm","ref":"/manopt/stable/solvers/ChambollePock/#The-Riemannian-Chambolle-Pock-algorithm","content":" The Riemannian Chambolle-Pock algorithm The Riemannian Chambolle‚ÄîPock is a generalization of the Chambolle‚ÄîPock algorithm  Chambolle and Pock [CP11]  It is also known as primal-dual hybrid gradient (PDHG) or primal-dual proximal splitting (PDPS) algorithm. In order to minimize over  $p‚àà\\mathcal M$  the cost function consisting of In order to minimize a cost function consisting of \\[F(p) + G(Œõ(p)),\\] over  $p‚àà\\mathcal M$ where  $F:\\mathcal M ‚Üí \\overline{‚Ñù}$ ,  $G:\\mathcal N ‚Üí \\overline{‚Ñù}$ , and  $Œõ:\\mathcal M ‚Üí\\mathcal N$ . If the manifolds  $\\mathcal M$  or  $\\mathcal N$  are not Hadamard, it has to be considered locally only, that is on geodesically convex sets  $\\mathcal C \\subset \\mathcal M$  and  $\\mathcal D \\subset\\mathcal N$  such that  $Œõ(\\mathcal C) \\subset \\mathcal D$ . The algorithm is available in four variants: exact versus linearized (see  variant ) as well as with primal versus dual relaxation (see  relax ). For more details, see  Bergmann, Herzog, Silva Louzeiro, Tenbrinck and Vidal-N√∫√±ez [BHS+21] . In the following description is the case of the exact, primal relaxed Riemannian Chambolle‚ÄîPock algorithm. Given base points  $m‚àà\\mathcal C$ ,  $n=Œõ(m)‚àà\\mathcal D$ , initial primal and dual values  $p^{(0)} ‚àà\\mathcal C$ ,  $Œæ_n^{(0)} ‚ààT_n^*\\mathcal N$ , and primal and dual step sizes  $\\sigma_0$ ,  $\\tau_0$ , relaxation  $\\theta_0$ , as well as acceleration  $\\gamma$ . As an initialization, perform  $\\bar p^{(0)} \\gets p^{(0)}$ . The algorithms performs the steps  $k=1,‚Ä¶,$  (until a  StoppingCriterion  is fulfilled with) \\[Œæ^{(k+1)}_n = \\operatorname{prox}_{\\tau_k G_n^*}\\Bigl(Œæ_n^{(k)} + \\tau_k \\bigl(\\log_n Œõ (\\bar p^{(k)})\\bigr)^\\flat\\Bigr)\\] \\[p^{(k+1)} = \\operatorname{prox}_{\\sigma_k F}\\biggl(\\exp_{p^{(k)}}\\Bigl( \\operatorname{PT}_{p^{(k)}\\gets m}\\bigl(-\\sigma_k DŒõ(m)^*[Œæ_n^{(k+1)}]\\bigr)^\\sharp\\Bigr)\\biggr)\\] Update $\\theta_k = (1+2\\gamma\\sigma_k)^{-\\frac{1}{2}}$ $\\sigma_{k+1} = \\sigma_k\\theta_k$ $\\tau_{k+1} =  \\frac{\\tau_k}{\\theta_k}$ \\[\\bar p^{(k+1)}  = \\exp_{p^{(k+1)}}\\bigl(-\\theta_k \\log_{p^{(k+1)}} p^{(k)}\\bigr)\\] Furthermore you can exchange the exponential map, the logarithmic map, and the parallel transport by a retraction, an inverse retraction, and a vector transport. Finally you can also update the base points  $m$  and  $n$  during the iterations. This introduces a few additional vector transports. The same holds for the case  $Œõ(m^{(k)})\\neq n^{(k)}$  at some point. All these cases are covered in the algorithm."},{"id":2806,"pagetitle":"Chambolle-Pock","title":"Manopt.ChambollePock","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.ChambollePock","content":" Manopt.ChambollePock  ‚Äî  Function ChambollePock(M, N, f, p, X, m, n, prox_G, prox_G_dual, adjoint_linear_operator; kwargs...)\nChambollePock!(M, N, f, p, X, m, n, prox_G, prox_G_dual, adjoint_linear_operator; kwargs...) Perform the Riemannian Chambolle‚ÄîPock algorithm. Given a  cost  function  $\\mathcal E:\\mathcal M ‚Üí ‚Ñù$  of the form \\[\\mathcal f(p) = F(p) + G( Œõ(p) ),\\] where  $F:\\mathcal M ‚Üí ‚Ñù$ ,  $G:\\mathcal N ‚Üí ‚Ñù$ , and  $Œõ:\\mathcal M ‚Üí \\mathcal N$ . This can be done inplace of  $p$ . Input parameters M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ N:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ p : a point on the manifold  $\\mathcal M$ X : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ m : a point on the manifold  $\\mathcal M$ n : a point on the manifold  $\\mathcal N$ adjoint_linearized_operator :  the adjoint  $DŒõ^*$  of the linearized operator  $DŒõ: T_{m}\\mathcal M ‚Üí T_{Œõ(m)}\\mathcal N)$ prox_F, prox_G_Dual :          the proximal maps of  $F$  and  $G^\\ast_n$ note that depending on the  AbstractEvaluationType evaluation  the last three parameters as well as the forward operator  Œõ  and the  linearized_forward_operator  can be given as allocating functions  (Manifolds, parameters) -> result   or as mutating functions  (Manifold, result, parameters)  -> result` to spare allocations. By default, this performs the exact Riemannian Chambolle Pock algorithm, see the optional parameter  DŒõ  for their linearized variant. For more details on the algorithm, see [ BHS+21 ]. Keyword Arguments acceleration=0.05 : acceleration parameter dual_stepsize=1/sqrt(8) : proximal parameter of the primal prox evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses inverse_retraction_method_dual= default_inverse_retraction_method (N, typeof(n)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses Œõ=missing : the (forward) operator  $Œõ(‚ãÖ)$  (required for the  :exact  variant) linearized_forward_operator=missing : its linearization  $DŒõ(‚ãÖ)[‚ãÖ]$  (required for the  :linearized  variant) primal_stepsize=1/sqrt(8) : proximal parameter of the dual prox relaxation=1. : the relaxation parameter  $Œ≥$ relax=:primal : whether to relax the primal or dual variant=:exact  if  Œõ  is missing, otherwise  :linearized : variant to use. Note that this changes the arguments the  forward_operator  is called with. stopping_criterion= StopAfterIteration` (100) : a functor indicating that the stopping criterion is fulfilled update_primal_base=missing : function to update  m  (identity by default/missing) update_dual_base=missing : function to update  n  (identity by default/missing) retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports vector_transport_method_dual= default_vector_transport_method (N, typeof(n)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2807,"pagetitle":"Chambolle-Pock","title":"Manopt.ChambollePock!","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.ChambollePock!","content":" Manopt.ChambollePock!  ‚Äî  Function ChambollePock(M, N, f, p, X, m, n, prox_G, prox_G_dual, adjoint_linear_operator; kwargs...)\nChambollePock!(M, N, f, p, X, m, n, prox_G, prox_G_dual, adjoint_linear_operator; kwargs...) Perform the Riemannian Chambolle‚ÄîPock algorithm. Given a  cost  function  $\\mathcal E:\\mathcal M ‚Üí ‚Ñù$  of the form \\[\\mathcal f(p) = F(p) + G( Œõ(p) ),\\] where  $F:\\mathcal M ‚Üí ‚Ñù$ ,  $G:\\mathcal N ‚Üí ‚Ñù$ , and  $Œõ:\\mathcal M ‚Üí \\mathcal N$ . This can be done inplace of  $p$ . Input parameters M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ N:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ p : a point on the manifold  $\\mathcal M$ X : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ m : a point on the manifold  $\\mathcal M$ n : a point on the manifold  $\\mathcal N$ adjoint_linearized_operator :  the adjoint  $DŒõ^*$  of the linearized operator  $DŒõ: T_{m}\\mathcal M ‚Üí T_{Œõ(m)}\\mathcal N)$ prox_F, prox_G_Dual :          the proximal maps of  $F$  and  $G^\\ast_n$ note that depending on the  AbstractEvaluationType evaluation  the last three parameters as well as the forward operator  Œõ  and the  linearized_forward_operator  can be given as allocating functions  (Manifolds, parameters) -> result   or as mutating functions  (Manifold, result, parameters)  -> result` to spare allocations. By default, this performs the exact Riemannian Chambolle Pock algorithm, see the optional parameter  DŒõ  for their linearized variant. For more details on the algorithm, see [ BHS+21 ]. Keyword Arguments acceleration=0.05 : acceleration parameter dual_stepsize=1/sqrt(8) : proximal parameter of the primal prox evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses inverse_retraction_method_dual= default_inverse_retraction_method (N, typeof(n)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses Œõ=missing : the (forward) operator  $Œõ(‚ãÖ)$  (required for the  :exact  variant) linearized_forward_operator=missing : its linearization  $DŒõ(‚ãÖ)[‚ãÖ]$  (required for the  :linearized  variant) primal_stepsize=1/sqrt(8) : proximal parameter of the dual prox relaxation=1. : the relaxation parameter  $Œ≥$ relax=:primal : whether to relax the primal or dual variant=:exact  if  Œõ  is missing, otherwise  :linearized : variant to use. Note that this changes the arguments the  forward_operator  is called with. stopping_criterion= StopAfterIteration` (100) : a functor indicating that the stopping criterion is fulfilled update_primal_base=missing : function to update  m  (identity by default/missing) update_dual_base=missing : function to update  n  (identity by default/missing) retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports vector_transport_method_dual= default_vector_transport_method (N, typeof(n)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2808,"pagetitle":"Chambolle-Pock","title":"State","ref":"/manopt/stable/solvers/ChambollePock/#State","content":" State"},{"id":2809,"pagetitle":"Chambolle-Pock","title":"Manopt.ChambollePockState","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.ChambollePockState","content":" Manopt.ChambollePockState  ‚Äî  Type ChambollePockState <: AbstractPrimalDualSolverState stores all options and variables within a linearized or exact Chambolle Pock. Fields acceleration::R :    acceleration factor dual_stepsize::R :   proximal parameter of the dual prox inverse_retraction_method::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses inverse_retraction_method_dual::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses m::P :               base point on  $\\mathcal M$ n::Q :               base point on  $\\mathcal N$ p::P :               an initial point on  $p^{(0)} ‚àà \\mathcal M$ pbar::P :            the relaxed iterate used in the next dual update step (when using  :primal  relaxation) primal_stepsize::R : proximal parameter of the primal prox X::T :               an initial tangent vector  $X^{(0)} ‚àà T_{p^{(0)}}\\mathcal M$ Xbar::T :            the relaxed iterate used in the next primal update step (when using  :dual  relaxation) relaxation::R :      relaxation in the primal relaxation step (to compute  pbar : relax::Symbol:       which variable to relax ( :primal or :dual`: retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled variant :            whether to perform an  :exact  or  :linearized  Chambolle-Pock update_primal_base : function  (pr, st, k) -> m  to update the primal base update_dual_base :  function  (pr, st, k) -> n  to update the dual base vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports vector_transport_method_dual::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Here,  P  is a point type on  $\\mathcal M$ ,  T  its tangent vector type,  Q  a point type on  $\\mathcal N$ , and  R<:Real  is a real number type where for the last two the functions a  AbstractManoptProblem p ,  AbstractManoptSolverState o  and the current iterate  i  are the arguments. If you activate these to be different from the default identity, you have to provide  p.Œõ  for the algorithm to work (which might be  missing  in the linearized case). Constructor ChambollePockState(M::AbstractManifold, N::AbstractManifold;\n    kwargs...\n) where {P, Q, T, R <: Real} Keyword arguments n= [ rand ](@extref Base.rand-Tuple{AbstractManifold}) (N)` p= rand (M) m= rand (M) X= zero_vector (M, p) acceleration=0.0 dual_stepsize=1/sqrt(8) primal_stepsize=1/sqrt(8) inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses inverse_retraction_method_dual= default_inverse_retraction_method (N, typeof(n)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses relaxation=1.0 relax=:primal : relax the primal variable by default retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopAfterIteration (300) : a functor indicating that the stopping criterion is fulfilled variant=:exact : run the exact Chambolle Pock by default update_primal_base=missing update_dual_base=missing vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports vector_transport_method_dual= default_vector_transport_method (N, typeof(n)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports if  Manifolds.jl  is loaded,  N  is also a keyword argument and set to  TangentBundle(M)  by default. source"},{"id":2810,"pagetitle":"Chambolle-Pock","title":"Useful terms","ref":"/manopt/stable/solvers/ChambollePock/#Useful-terms","content":" Useful terms"},{"id":2811,"pagetitle":"Chambolle-Pock","title":"Manopt.primal_residual","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.primal_residual","content":" Manopt.primal_residual  ‚Äî  Function primal_residual(p, o, x_old, X_old, n_old) Compute the primal residual at current iterate  $k$  given the necessary values  $x_{k-1}, X_{k-1}$ , and  $n_{k-1}$  from the previous iterate. \\[\\Bigl\\lVert\n\\frac{1}{œÉ}\\operatorname{retr}^{-1}_{x_{k}}x_{k-1} -\nV_{x_k\\gets m_k}\\bigl(DŒõ^*(m_k)\\bigl[V_{n_k\\gets n_{k-1}}X_{k-1} - X_k \\bigr]\n\\Bigr\\rVert\\] where  $V_{‚ãÖ\\gets‚ãÖ}$  is the vector transport used in the  ChambollePockState source"},{"id":2812,"pagetitle":"Chambolle-Pock","title":"Manopt.dual_residual","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.dual_residual","content":" Manopt.dual_residual  ‚Äî  Function dual_residual(p, o, x_old, X_old, n_old) Compute the dual residual at current iterate  $k$  given the necessary values  $x_{k-1}, X_{k-1}$ , and  $n_{k-1}$  from the previous iterate. The formula is slightly different depending on the  o.variant  used: For the  :linearized  it reads \\[\\Bigl\\lVert\n\\frac{1}{œÑ}\\bigl(\nV_{n_{k}\\gets n_{k-1}}(X_{k-1})\n- X_k\n\\bigr)\n-\nDŒõ(m_k)\\bigl[\nV_{m_k\\gets x_k}\\operatorname{retr}^{-1}_{x_{k}}x_{k-1}\n\\bigr]\n\\Bigr\\rVert\\] and for the  :exact  variant \\[\\Bigl\\lVert\n\\frac{1}{œÑ} V_{n_{k}\\gets n_{k-1}}(X_{k-1})\n-\n\\operatorname{retr}^{-1}_{n_{k}}\\bigl(\nŒõ(\\operatorname{retr}_{m_{k}}(V_{m_k\\gets x_k}\\operatorname{retr}^{-1}_{x_{k}}x_{k-1}))\n\\bigr)\n\\Bigr\\rVert\\] where in both cases  $V_{‚ãÖ\\gets‚ãÖ}$  is the vector transport used in the  ChambollePockState . source"},{"id":2813,"pagetitle":"Chambolle-Pock","title":"Debug","ref":"/manopt/stable/solvers/ChambollePock/#Debug","content":" Debug"},{"id":2814,"pagetitle":"Chambolle-Pock","title":"Manopt.DebugDualBaseIterate","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.DebugDualBaseIterate","content":" Manopt.DebugDualBaseIterate  ‚Äî  Function DebugDualBaseIterate(io::IO=stdout) Print the dual base variable by using  DebugEntry , see their constructors for detail. This method is further set display  o.n . source"},{"id":2815,"pagetitle":"Chambolle-Pock","title":"Manopt.DebugDualBaseChange","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.DebugDualBaseChange","content":" Manopt.DebugDualBaseChange  ‚Äî  Function DebugDualChange(; storage=StoreStateAction([:n]), io::IO=stdout) Print the change of the dual base variable by using  DebugEntryChange , see their constructors for detail, on  o.n . source"},{"id":2816,"pagetitle":"Chambolle-Pock","title":"Manopt.DebugPrimalBaseIterate","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.DebugPrimalBaseIterate","content":" Manopt.DebugPrimalBaseIterate  ‚Äî  Function DebugPrimalBaseIterate() Print the primal base variable by using  DebugEntry , see their constructors for detail. This method is further set display  o.m . source"},{"id":2817,"pagetitle":"Chambolle-Pock","title":"Manopt.DebugPrimalBaseChange","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.DebugPrimalBaseChange","content":" Manopt.DebugPrimalBaseChange  ‚Äî  Function DebugPrimalBaseChange(a::StoreStateAction=StoreStateAction([:m]),io::IO=stdout) Print the change of the primal base variable by using  DebugEntryChange , see their constructors for detail, on  o.n . source"},{"id":2818,"pagetitle":"Chambolle-Pock","title":"Manopt.DebugDualChange","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.DebugDualChange","content":" Manopt.DebugDualChange  ‚Äî  Type DebugDualChange(opts...) Print the change of the dual variable, similar to  DebugChange , see their constructors for detail, but with a different calculation of the change, since the dual variable lives in (possibly different) tangent spaces. source"},{"id":2819,"pagetitle":"Chambolle-Pock","title":"Manopt.DebugDualIterate","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.DebugDualIterate","content":" Manopt.DebugDualIterate  ‚Äî  Function DebugDualIterate(e) Print the dual variable by using  DebugEntry , see their constructors for detail. This method is further set display  o.X . source"},{"id":2820,"pagetitle":"Chambolle-Pock","title":"Manopt.DebugDualResidual","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.DebugDualResidual","content":" Manopt.DebugDualResidual  ‚Äî  Type DebugDualResidual <: DebugAction A Debug action to print the dual residual. The constructor accepts a printing function and some (shared) storage, which should at least record  :Iterate ,  :X  and  :n . Constructor DebugDualResidual(; kwargs...) Keyword warguments io= stdout`: stream to perform the debug to format=\"$prefix%s\" : format to print the dual residual, using the prefix=\"Dual Residual: \" : short form to just set the prefix storage  (a new  StoreStateAction ) to store values for the debug. source"},{"id":2821,"pagetitle":"Chambolle-Pock","title":"Manopt.DebugPrimalChange","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.DebugPrimalChange","content":" Manopt.DebugPrimalChange  ‚Äî  Function DebugPrimalChange(opts...) Print the change of the primal variable by using  DebugChange , see their constructors for detail. source"},{"id":2822,"pagetitle":"Chambolle-Pock","title":"Manopt.DebugPrimalIterate","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.DebugPrimalIterate","content":" Manopt.DebugPrimalIterate  ‚Äî  Function DebugPrimalIterate(opts...;kwargs...) Print the change of the primal variable by using  DebugIterate , see their constructors for detail. source"},{"id":2823,"pagetitle":"Chambolle-Pock","title":"Manopt.DebugPrimalResidual","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.DebugPrimalResidual","content":" Manopt.DebugPrimalResidual  ‚Äî  Type DebugPrimalResidual <: DebugAction A Debug action to print the primal residual. The constructor accepts a printing function and some (shared) storage, which should at least record  :Iterate ,  :X  and  :n . Constructor DebugPrimalResidual(; kwargs...) Keyword warguments io= stdout`: stream to perform the debug to format=\"$prefix%s\" : format to print the dual residual, using the prefix=\"Primal Residual: \" : short form to just set the prefix storage  (a new  StoreStateAction ) to store values for the debug. source"},{"id":2824,"pagetitle":"Chambolle-Pock","title":"Manopt.DebugPrimalDualResidual","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.DebugPrimalDualResidual","content":" Manopt.DebugPrimalDualResidual  ‚Äî  Type DebugPrimalDualResidual <: DebugAction A Debug action to print the primal dual residual. The constructor accepts a printing function and some (shared) storage, which should at least record  :Iterate ,  :X  and  :n . Constructor DebugPrimalDualResidual() with the keywords Keyword warguments io= stdout`: stream to perform the debug to format=\"$prefix%s\" : format to print the dual residual, using the prefix=\"PD Residual: \" : short form to just set the prefix storage  (a new  StoreStateAction ) to store values for the debug. source"},{"id":2825,"pagetitle":"Chambolle-Pock","title":"Record","ref":"/manopt/stable/solvers/ChambollePock/#Record","content":" Record"},{"id":2826,"pagetitle":"Chambolle-Pock","title":"Manopt.RecordDualBaseIterate","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.RecordDualBaseIterate","content":" Manopt.RecordDualBaseIterate  ‚Äî  Function RecordDualBaseIterate(n) Create an  RecordAction  that records the dual base point, an  RecordEntry  of  o.n . source"},{"id":2827,"pagetitle":"Chambolle-Pock","title":"Manopt.RecordDualBaseChange","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.RecordDualBaseChange","content":" Manopt.RecordDualBaseChange  ‚Äî  Function RecordDualBaseChange(e) Create an  RecordAction  that records the dual base point change, an  RecordEntryChange  of  o.n  with distance to the last value to store a value. source"},{"id":2828,"pagetitle":"Chambolle-Pock","title":"Manopt.RecordDualChange","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.RecordDualChange","content":" Manopt.RecordDualChange  ‚Äî  Function RecordDualChange() Create the action either with a given (shared) Storage, which can be set to the  values  Tuple, if that is provided). source"},{"id":2829,"pagetitle":"Chambolle-Pock","title":"Manopt.RecordDualIterate","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.RecordDualIterate","content":" Manopt.RecordDualIterate  ‚Äî  Function RecordDualIterate(X) Create an  RecordAction  that records the dual base point, an  RecordEntry  of  o.X . source"},{"id":2830,"pagetitle":"Chambolle-Pock","title":"Manopt.RecordPrimalBaseIterate","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.RecordPrimalBaseIterate","content":" Manopt.RecordPrimalBaseIterate  ‚Äî  Function RecordPrimalBaseIterate(x) Create an  RecordAction  that records the primal base point, an  RecordEntry  of  o.m . source"},{"id":2831,"pagetitle":"Chambolle-Pock","title":"Manopt.RecordPrimalBaseChange","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.RecordPrimalBaseChange","content":" Manopt.RecordPrimalBaseChange  ‚Äî  Function RecordPrimalBaseChange() Create an  RecordAction  that records the primal base point change, an  RecordEntryChange  of  o.m  with distance to the last value to store a value. source"},{"id":2832,"pagetitle":"Chambolle-Pock","title":"Manopt.RecordPrimalChange","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.RecordPrimalChange","content":" Manopt.RecordPrimalChange  ‚Äî  Function RecordPrimalChange(a) Create an  RecordAction  that records the primal value change,  RecordChange , to record the change of  o.x . source"},{"id":2833,"pagetitle":"Chambolle-Pock","title":"Manopt.RecordPrimalIterate","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.RecordPrimalIterate","content":" Manopt.RecordPrimalIterate  ‚Äî  Function RecordDualBaseIterate(x) Create an  RecordAction  that records the dual base point, an  RecordIterate  of  o.x . source"},{"id":2834,"pagetitle":"Chambolle-Pock","title":"Internals","ref":"/manopt/stable/solvers/ChambollePock/#Internals","content":" Internals"},{"id":2835,"pagetitle":"Chambolle-Pock","title":"Manopt.update_prox_parameters!","ref":"/manopt/stable/solvers/ChambollePock/#Manopt.update_prox_parameters!","content":" Manopt.update_prox_parameters!  ‚Äî  Function update_prox_parameters!(o) update the prox parameters as described in Algorithm 2 of [ CP11 ], $Œ∏_{n} = \\frac{1}{\\sqrt{1+2Œ≥œÑ_n}}$ $œÑ_{n+1} = Œ∏_nœÑ_n$ $œÉ_{n+1} = \\frac{œÉ_n}{Œ∏_n}$ source"},{"id":2836,"pagetitle":"Chambolle-Pock","title":"Technical details","ref":"/manopt/stable/solvers/ChambollePock/#sec-cp-technical-details","content":" Technical details The  ChambollePock  solver requires the following functions of a manifold to be available for both the manifold  $\\mathcal M$ and  $\\mathcal N$ A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  or  retraction_method_dual=  (for  $\\mathcal N$ ) does not have to be specified. An  inverse_retract! (M, X, p, q) ; it is recommended to set the  default_inverse_retraction_method  to a favourite retraction. If this default is set, a  inverse_retraction_method=  or  inverse_retraction_method_dual=  (for  $\\mathcal N$ ) does not have to be specified. A  vector_transport_to! M, Y, p, X, q) ; it is recommended to set the  default_vector_transport_method  to a favourite retraction. If this default is set, a  vector_transport_method=  or  vector_transport_method_dual=  (for  $\\mathcal N$ ) does not have to be specified. A  copyto! (M, q, p)  and  copy (M,p)  for points."},{"id":2837,"pagetitle":"Chambolle-Pock","title":"Literature","ref":"/manopt/stable/solvers/ChambollePock/#Literature","content":" Literature [BHS+21] R.¬†Bergmann, R.¬†Herzog, M.¬†Silva Louzeiro, D.¬†Tenbrinck and J.¬†Vidal-N√∫√±ez.  Fenchel duality theory and a primal-dual algorithm on Riemannian manifolds .  Foundations¬†of¬†Computational¬†Mathematics  21 , 1465‚Äì1504  (2021),  arXiv:1908.02022 . [CP11] A.¬†Chambolle and T.¬†Pock.  A first-order primal-dual algorithm for convex problems with applications to imaging .  Journal¬†of¬†Mathematical¬†Imaging¬†and¬†Vision  40 , 120‚Äì145  (2011)."},{"id":2840,"pagetitle":"Douglas‚ÄîRachford","title":"Douglas‚ÄîRachford algorithm","ref":"/manopt/stable/solvers/DouglasRachford/#Douglas‚ÄîRachford-algorithm","content":" Douglas‚ÄîRachford algorithm The (Parallel) Douglas‚ÄîRachford ((P)DR) algorithm was generalized to Hadamard manifolds in [ BPS16 ]. The aim is to minimize the sum \\[f(p) = g(p) + h(p)\\] on a manifold, where the two summands have proximal maps  $\\operatorname{prox}_{Œª g}, \\operatorname{prox}_{Œª h}$  that are easy to evaluate (maybe in closed form, or not too costly to approximate). Further, define the reflection operator at the proximal map as \\[\\operatorname{refl}_{Œª g}(p) = \\operatorname{retr}_{\\operatorname{prox}_{Œª g}(p)} \\bigl( -\\operatorname{retr}^{-1}_{\\operatorname{prox}_{Œª g}(p)} p \\bigr).\\] Let  $\\alpha_k ‚àà  [0,1]$  with  $\\sum_{k ‚àà ‚Ñï} \\alpha_k(1-\\alpha_k) =  \\infty$  and  $Œª > 0$  (which might depend on iteration  $k$  as well) be given. Then the (P)DRA algorithm for initial data  $p^{(0)} ‚àà \\mathcal M$  as"},{"id":2841,"pagetitle":"Douglas‚ÄîRachford","title":"Initialization","ref":"/manopt/stable/solvers/DouglasRachford/#Initialization","content":" Initialization Initialize  $q^{(0)} = p^{(0)}$  and  $k=0$"},{"id":2842,"pagetitle":"Douglas‚ÄîRachford","title":"Iteration","ref":"/manopt/stable/solvers/DouglasRachford/#Iteration","content":" Iteration Repeat until a convergence criterion is reached Compute  $r^{(k)} = \\operatorname{refl}_{Œª g}\\operatorname{refl}_{Œª h}(q^{(k)})$ Within that operation, store  $p^{(k+1)} = \\operatorname{prox}_{Œª h}(q^{(k)})$  which is the prox the inner reflection reflects at. Compute  $q^{(k+1)} = g(\\alpha_k; q^{(k)}, r^{(k)})$ , where  $g$  is a curve approximating the shortest geodesic, provided by a retraction and its inverse Set  $k = k+1$"},{"id":2843,"pagetitle":"Douglas‚ÄîRachford","title":"Result","ref":"/manopt/stable/solvers/DouglasRachford/#Result","content":" Result The result is given by the last computed  $p^{(K)}$  at the last iterate  $K$ . For the parallel version, the first proximal map is a vectorial version where in each component one prox is applied to the corresponding copy of  $t_k$  and the second proximal map corresponds to the indicator function of the set, where all copies are equal (in  $\\mathcal M^n$ , where  $n$  is the number of copies), leading to the second prox being the Riemannian mean."},{"id":2844,"pagetitle":"Douglas‚ÄîRachford","title":"Interface","ref":"/manopt/stable/solvers/DouglasRachford/#Interface","content":" Interface"},{"id":2845,"pagetitle":"Douglas‚ÄîRachford","title":"Manopt.DouglasRachford","ref":"/manopt/stable/solvers/DouglasRachford/#Manopt.DouglasRachford","content":" Manopt.DouglasRachford  ‚Äî  Function DouglasRachford(M, f, proxes_f, p)\nDouglasRachford(M, mpo, p)\nDouglasRachford!(M, f, proxes_f, p)\nDouglasRachford!(M, mpo, p) Compute the Douglas-Rachford algorithm on the manifold  $\\mathcal M$ , starting from  p given the (two) proximal maps proxes_f`, see [ BPS16 ]. For  $k>2$  proximal maps, the problem is reformulated using the parallel Douglas Rachford: a vectorial proximal map on the power manifold  $\\mathcal M^k$  is introduced as the first proximal map and the second proximal map of the is set to the  mean  (Riemannian center of mass). This hence also boils down to two proximal maps, though each evaluates proximal maps in parallel, that is, component wise in a vector. Note The parallel Douglas Rachford does not work in-place for now, since    while creating the new staring point  p'  on the power manifold, a copy of  p     Is created If you provide a  ManifoldProximalMapObjective mpo  instead, the proximal maps are kept unchanged. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v proxes_f : functions of the form  (M, Œª, p)-> q  performing a proximal maps, where  ‚Å†Œª  denotes the proximal parameter, for each of the summands of  F . These can also be given in the  InplaceEvaluation  variants  (M, q, Œª p) -> q  computing in place of  q . p : a point on the manifold  $\\mathcal M$ Keyword arguments Œ±= k -> 0.9 : relaxation of the step from old to new iterate, to be precise  $p^{(k+1)} = g(Œ±_k; p^{(k)}, q^{(k)})$ , where  $q^{(k)}$  is the result of the double reflection involved in the DR algorithm and  $g$  is a curve induced by the retraction and its inverse. evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses  This is used both in the relaxation step as well as in the reflection, unless you set  R  yourself. Œª= k -> 1.0 : function to provide the value for the proximal parameter  $Œª_k$ R=reflect(!) :           method employed in the iteration to perform the reflection of  p  at the prox of  p . This uses by default  reflect  or  reflect!  depending on  reflection_evaluation  and the retraction and inverse retraction specified by  retraction_method  and  inverse_retraction_method , respectively. reflection_evaluation : ( AllocatingEvaluation  whether  R  works in-place or allocating retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions  This is used both in the relaxation step as well as in the reflection, unless you set  R  yourself. stopping_criterion= StopAfterIteration (200) | StopWhenChangeLess (1e-5) : a functor indicating that the stopping criterion is fulfilled parallel=false : indicate whether to use a parallel Douglas-Rachford or not. All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source DouglasRachford(M, f, proxes_f, p; kwargs...) a doc string with some math  $t_{k+1} = g(Œ±_k; t_k, s_k)$ source"},{"id":2846,"pagetitle":"Douglas‚ÄîRachford","title":"Manopt.DouglasRachford!","ref":"/manopt/stable/solvers/DouglasRachford/#Manopt.DouglasRachford!","content":" Manopt.DouglasRachford!  ‚Äî  Function DouglasRachford(M, f, proxes_f, p)\nDouglasRachford(M, mpo, p)\nDouglasRachford!(M, f, proxes_f, p)\nDouglasRachford!(M, mpo, p) Compute the Douglas-Rachford algorithm on the manifold  $\\mathcal M$ , starting from  p given the (two) proximal maps proxes_f`, see [ BPS16 ]. For  $k>2$  proximal maps, the problem is reformulated using the parallel Douglas Rachford: a vectorial proximal map on the power manifold  $\\mathcal M^k$  is introduced as the first proximal map and the second proximal map of the is set to the  mean  (Riemannian center of mass). This hence also boils down to two proximal maps, though each evaluates proximal maps in parallel, that is, component wise in a vector. Note The parallel Douglas Rachford does not work in-place for now, since    while creating the new staring point  p'  on the power manifold, a copy of  p     Is created If you provide a  ManifoldProximalMapObjective mpo  instead, the proximal maps are kept unchanged. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v proxes_f : functions of the form  (M, Œª, p)-> q  performing a proximal maps, where  ‚Å†Œª  denotes the proximal parameter, for each of the summands of  F . These can also be given in the  InplaceEvaluation  variants  (M, q, Œª p) -> q  computing in place of  q . p : a point on the manifold  $\\mathcal M$ Keyword arguments Œ±= k -> 0.9 : relaxation of the step from old to new iterate, to be precise  $p^{(k+1)} = g(Œ±_k; p^{(k)}, q^{(k)})$ , where  $q^{(k)}$  is the result of the double reflection involved in the DR algorithm and  $g$  is a curve induced by the retraction and its inverse. evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses  This is used both in the relaxation step as well as in the reflection, unless you set  R  yourself. Œª= k -> 1.0 : function to provide the value for the proximal parameter  $Œª_k$ R=reflect(!) :           method employed in the iteration to perform the reflection of  p  at the prox of  p . This uses by default  reflect  or  reflect!  depending on  reflection_evaluation  and the retraction and inverse retraction specified by  retraction_method  and  inverse_retraction_method , respectively. reflection_evaluation : ( AllocatingEvaluation  whether  R  works in-place or allocating retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions  This is used both in the relaxation step as well as in the reflection, unless you set  R  yourself. stopping_criterion= StopAfterIteration (200) | StopWhenChangeLess (1e-5) : a functor indicating that the stopping criterion is fulfilled parallel=false : indicate whether to use a parallel Douglas-Rachford or not. All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2847,"pagetitle":"Douglas‚ÄîRachford","title":"State","ref":"/manopt/stable/solvers/DouglasRachford/#State","content":" State"},{"id":2848,"pagetitle":"Douglas‚ÄîRachford","title":"Manopt.DouglasRachfordState","ref":"/manopt/stable/solvers/DouglasRachford/#Manopt.DouglasRachfordState","content":" Manopt.DouglasRachfordState  ‚Äî  Type DouglasRachfordState <: AbstractManoptSolverState Store all options required for the DouglasRachford algorithm, Fields Œ± :                         relaxation of the step from old to new iterate, to be precise  $x^{(k+1)} = g(Œ±(k); x^{(k)}, t^{(k)})$ , where  $t^{(k)}$  is the result of the double reflection involved in the DR algorithm inverse_retraction_method::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses Œª :                         function to provide the value for the proximal parameter during the calls parallel :                  indicate whether to use a parallel Douglas-Rachford or not. R :                          method employed in the iteration to perform the reflection of  x  at the prox  p . p::P : a point on the manifold  $\\mathcal M$  storing the current iterate For the parallel Douglas-Rachford, this is not a value from the  PowerManifold  manifold but the mean. reflection_evaluation :     whether  R  works in-place or allocating retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions s :                         the last result of the double reflection at the proximal maps relaxed by  Œ± . stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled Constructor DouglasRachfordState(M::AbstractManifold; kwargs...) Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ Keyword arguments Œ±= k -> 0.9 : relaxation of the step from old to new iterate, to be precise  $x^{(k+1)} = g(Œ±(k); x^{(k)}, t^{(k)})$ , where  $t^{(k)}$  is the result of the double reflection involved in the DR algorithm inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses Œª= k -> 1.0 : function to provide the value for the proximal parameter during the calls p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value R= reflect (!) : method employed in the iteration to perform the reflection of  p  at the prox of  p , which function is used depends on  reflection_evaluation . reflection_evaluation= AllocatingEvaluation () ) specify whether the reflection works in-place or allocating (default) retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopAfterIteration (300) : a functor indicating that the stopping criterion is fulfilled parallel=false : indicate whether to use a parallel Douglas-Rachford or not. source For specific  DebugAction s and  RecordAction s see also  Cyclic Proximal Point . Furthermore, this solver has a short hand notation for the involved  reflect ion."},{"id":2849,"pagetitle":"Douglas‚ÄîRachford","title":"Manopt.reflect","ref":"/manopt/stable/solvers/DouglasRachford/#Manopt.reflect","content":" Manopt.reflect  ‚Äî  Function reflect(M, f, x; kwargs...)\nreflect!(M, q, f, x; kwargs...) reflect the point  x  from the manifold  M  at the point  f(x)  of the function  $f: \\mathcal M ‚Üí \\mathcal M$ , given by \\[    \\operatorname{refl}_f(x) = \\operatorname{refl}_{f(x)}(x),\\] Compute the result in  q . see also  reflect (M,p,x) , to which the keywords are also passed to. source reflect(M, p, x, kwargs...)\nreflect!(M, q, p, x, kwargs...) Reflect the point  x  from the manifold  M  at point  p , given by \\[\\operatorname{refl}\\] where  $\\operatorname{retr}$  and  $\\operatorname{retr}^{-1}$  denote a retraction and an inverse retraction, respectively. This can also be done in place of  q . Keyword arguments retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses and for the  reflect!  additionally X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$  as temporary memory to compute the inverse retraction in place. otherwise this is the memory that would be allocated anyways. source reflect(M, f, x; kwargs...)\nreflect!(M, q, f, x; kwargs...) reflect the point  x  from the manifold  M  at the point  f(x)  of the function  $f: \\mathcal M ‚Üí \\mathcal M$ , given by \\[    \\operatorname{refl}_f(x) = \\operatorname{refl}_{f(x)}(x),\\] Compute the result in  q . see also  reflect (M,p,x) , to which the keywords are also passed to. source reflect(M, p, x, kwargs...)\nreflect!(M, q, p, x, kwargs...) Reflect the point  x  from the manifold  M  at point  p , given by \\[\\operatorname{refl}_p(q) = \\operatorname{retr}_p(-\\operatorname{retr}^{-1}_p q),\\] where  $\\operatorname{retr}$  and  $\\operatorname{retr}^{-1}$  denote a retraction and an inverse retraction, respectively. This can also be done in place of  q . Keyword arguments retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses and for the  reflect!  additionally X=zero_vector(M,p) : a temporary memory to compute the inverse retraction in place. otherwise this is the memory that would be allocated anyways. source"},{"id":2850,"pagetitle":"Douglas‚ÄîRachford","title":"Technical details","ref":"/manopt/stable/solvers/DouglasRachford/#sec-dr-technical-details","content":" Technical details The  DouglasRachford  solver requires the following functions of a manifold to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified. An  inverse_retract! (M, X, p, q) ; it is recommended to set the  default_inverse_retraction_method  to a favourite retraction. If this default is set, a  inverse_retraction_method=  does not have to be specified. A  copyto! (M, q, p)  and  copy (M,p)  for points. By default, one of the stopping criteria is  StopWhenChangeLess , which requires An  inverse_retract! (M, X, p, q) ; it is recommended to set the  default_inverse_retraction_method  to a favourite retraction. If this default is set, a  inverse_retraction_method=  or  inverse_retraction_method_dual=  (for  $\\mathcal N$ ) does not have to be specified or the  distance (M, p, q)  for said default inverse retraction."},{"id":2851,"pagetitle":"Douglas‚ÄîRachford","title":"Literature","ref":"/manopt/stable/solvers/DouglasRachford/#Literature","content":" Literature"},{"id":2854,"pagetitle":"Frank-Wolfe","title":"Frank‚ÄîWolfe method","ref":"/manopt/stable/solvers/FrankWolfe/#Frank‚ÄîWolfe-method","content":" Frank‚ÄîWolfe method"},{"id":2855,"pagetitle":"Frank-Wolfe","title":"Manopt.Frank_Wolfe_method","ref":"/manopt/stable/solvers/FrankWolfe/#Manopt.Frank_Wolfe_method","content":" Manopt.Frank_Wolfe_method  ‚Äî  Function Frank_Wolfe_method(M, f, grad_f, p=rand(M))\nFrank_Wolfe_method(M, gradient_objective, p=rand(M); kwargs...)\nFrank_Wolfe_method!(M, f, grad_f, p; kwargs...)\nFrank_Wolfe_method!(M, gradient_objective, p; kwargs...) Perform the Frank-Wolfe algorithm to compute for  $\\mathcal C ‚äÇ \\mathcal M$  the constrained problem \\[    \\operatorname*{arg\\,min}_{p‚àà\\mathcal C} f(p),\\] where the main step is a constrained optimisation is within the algorithm, that is the sub problem (Oracle) \\[   \\operatorname*{arg\\,min}_{q ‚àà C} ‚ü®\\operatorname{grad} f(p_k), \\log_{p_k}q‚ü©.\\] for every iterate  $p_k$  together with a stepsize  $s_k‚â§1$ . The algorhtm can be performed in-place of  p . This algorithm is inspired by but slightly more general than [ WS22 ]. The next iterate is then given by  $p_{k+1} = Œ≥_{p_k,q_k}(s_k)$ , where by default  $Œ≥$  is the shortest geodesic between the two points but can also be changed to use a retraction and its inverse. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place p : a point on the manifold  $\\mathcal M$ Alternatively to  f  and  grad_f  you can provide the corresponding  AbstractManifoldFirstOrderObjective gradient_objective  directly. Keyword arguments differential= nothing : specify a specific function to evaluate the differential. By default,  $Df(p)[X] = ‚ü®\\operatorname{grad}f(p),X‚ü©$ . is used evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= DecreasingStepsize (; length=2.0, shift=2) : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (500) | StopWhenGradientNormLess (1.0e-6) ): a functor indicating that the stopping criterion is fulfilled sub_cost= FrankWolfeCost (p, X) : the cost of the Frank-Wolfe sub problem. This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_grad= FrankWolfeGradient (p, X) : the gradient of the Frank-Wolfe sub problem. This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. sub_objective= ManifoldGradientObjective (sub_cost, sub_gradient) : the objective for the Frank-Wolfe sub problem. This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_problem= DefaultManoptProblem (M, sub_objective) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= GradientDescentState (M, copy(M,p)) :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing the gradient at the current iterate sub_stopping_criterion= [ StopAfterIteration ](@ref) (300) [  |  ](@ref StopWhenAny)[ StopWhenStepsizeLess ](@ref) (1e-8) : a functor indicating that the stopping criterion is fulfilled This is used to define the sub state= keyword and has hence no effect, if you set sub state` directly. X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing the gradient at the current iterate All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. If you provide a  ManifoldFirstOrderObjective  directly, the  evaluation=  keyword is ignored. The decorations are still applied to the objective. Output the obtained (approximate) minimizer  $p^*$ , see  get_solver_return  for details source"},{"id":2856,"pagetitle":"Frank-Wolfe","title":"Manopt.Frank_Wolfe_method!","ref":"/manopt/stable/solvers/FrankWolfe/#Manopt.Frank_Wolfe_method!","content":" Manopt.Frank_Wolfe_method!  ‚Äî  Function Frank_Wolfe_method(M, f, grad_f, p=rand(M))\nFrank_Wolfe_method(M, gradient_objective, p=rand(M); kwargs...)\nFrank_Wolfe_method!(M, f, grad_f, p; kwargs...)\nFrank_Wolfe_method!(M, gradient_objective, p; kwargs...) Perform the Frank-Wolfe algorithm to compute for  $\\mathcal C ‚äÇ \\mathcal M$  the constrained problem \\[    \\operatorname*{arg\\,min}_{p‚àà\\mathcal C} f(p),\\] where the main step is a constrained optimisation is within the algorithm, that is the sub problem (Oracle) \\[   \\operatorname*{arg\\,min}_{q ‚àà C} ‚ü®\\operatorname{grad} f(p_k), \\log_{p_k}q‚ü©.\\] for every iterate  $p_k$  together with a stepsize  $s_k‚â§1$ . The algorhtm can be performed in-place of  p . This algorithm is inspired by but slightly more general than [ WS22 ]. The next iterate is then given by  $p_{k+1} = Œ≥_{p_k,q_k}(s_k)$ , where by default  $Œ≥$  is the shortest geodesic between the two points but can also be changed to use a retraction and its inverse. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place p : a point on the manifold  $\\mathcal M$ Alternatively to  f  and  grad_f  you can provide the corresponding  AbstractManifoldFirstOrderObjective gradient_objective  directly. Keyword arguments differential= nothing : specify a specific function to evaluate the differential. By default,  $Df(p)[X] = ‚ü®\\operatorname{grad}f(p),X‚ü©$ . is used evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= DecreasingStepsize (; length=2.0, shift=2) : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (500) | StopWhenGradientNormLess (1.0e-6) ): a functor indicating that the stopping criterion is fulfilled sub_cost= FrankWolfeCost (p, X) : the cost of the Frank-Wolfe sub problem. This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_grad= FrankWolfeGradient (p, X) : the gradient of the Frank-Wolfe sub problem. This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. sub_objective= ManifoldGradientObjective (sub_cost, sub_gradient) : the objective for the Frank-Wolfe sub problem. This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_problem= DefaultManoptProblem (M, sub_objective) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= GradientDescentState (M, copy(M,p)) :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing the gradient at the current iterate sub_stopping_criterion= [ StopAfterIteration ](@ref) (300) [  |  ](@ref StopWhenAny)[ StopWhenStepsizeLess ](@ref) (1e-8) : a functor indicating that the stopping criterion is fulfilled This is used to define the sub state= keyword and has hence no effect, if you set sub state` directly. X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing the gradient at the current iterate All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. If you provide a  ManifoldFirstOrderObjective  directly, the  evaluation=  keyword is ignored. The decorations are still applied to the objective. Output the obtained (approximate) minimizer  $p^*$ , see  get_solver_return  for details source"},{"id":2857,"pagetitle":"Frank-Wolfe","title":"State","ref":"/manopt/stable/solvers/FrankWolfe/#State","content":" State"},{"id":2858,"pagetitle":"Frank-Wolfe","title":"Manopt.FrankWolfeState","ref":"/manopt/stable/solvers/FrankWolfe/#Manopt.FrankWolfeState","content":" Manopt.FrankWolfeState  ‚Äî  Type FrankWolfeState <: AbstractManoptSolverState A struct to store the current state of the  Frank_Wolfe_method It comes in two forms, depending on the realisation of the  subproblem . Fields p::P : a point on the manifold  $\\mathcal M$  storing the current iterate X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing the gradient at the current iterate inverse_retraction_method::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports sub_problem::Union{AbstractManoptProblem, F} :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state::Union{AbstractManoptProblem, F} :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled stepsize::Stepsize : a functor inheriting from  Stepsize  to determine a step size retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions The sub task requires a method to solve \\[   \\operatorname*{arg\\,min}_{q ‚àà C} ‚ü®\\operatorname{grad} f(p_k), \\log_{p_k}q‚ü©.\\] Constructor FrankWolfeState(M, sub_problem, sub_state; kwargs...) Initialise the Frank Wolfe method state. FrankWolfeState(M, sub_problem; evaluation=AllocatingEvaluation(), kwargs...) Initialise the Frank Wolfe method state, where  sub_problem  is a closed form solution with  evaluation  as type of evaluation. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ sub_problem :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. Keyword arguments p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopAfterIteration (200) | StopWhenGradientNormLess (1e-6) : a functor indicating that the stopping criterion is fulfilled stepsize= default_stepsize (M, FrankWolfeState) : a functor inheriting from  Stepsize  to determine a step size X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ to specify the representation of a tangent vector where the remaining fields from before are keyword arguments. source"},{"id":2859,"pagetitle":"Frank-Wolfe","title":"Helpers","ref":"/manopt/stable/solvers/FrankWolfe/#Helpers","content":" Helpers For the inner sub-problem you can easily create the corresponding cost and gradient using"},{"id":2860,"pagetitle":"Frank-Wolfe","title":"Manopt.FrankWolfeCost","ref":"/manopt/stable/solvers/FrankWolfe/#Manopt.FrankWolfeCost","content":" Manopt.FrankWolfeCost  ‚Äî  Type FrankWolfeCost{P,T} A structure to represent the oracle sub problem in the  Frank_Wolfe_method . The cost function reads \\[F(q) = ‚ü®X, \\log_p q‚ü©\\] The values  p  and  X  are stored within this functor and should be references to the iterate and gradient from within  FrankWolfeState . source"},{"id":2861,"pagetitle":"Frank-Wolfe","title":"Manopt.FrankWolfeGradient","ref":"/manopt/stable/solvers/FrankWolfe/#Manopt.FrankWolfeGradient","content":" Manopt.FrankWolfeGradient  ‚Äî  Type FrankWolfeGradient{P,T} A structure to represent the gradient of the oracle sub problem in the  Frank_Wolfe_method , that is for a given point  p  and a tangent vector  X  the function reads \\[F(q) = ‚ü®X, \\log_p q‚ü©\\] Its gradient can be computed easily using  adjoint_differential_log_argument . The values  p  and  X  are stored within this functor and should be references to the iterate and gradient from within  FrankWolfeState . source [WS22] M.¬†Weber and S.¬†Sra.  Riemannian Optimization via Frank-Wolfe Methods .  Mathematical¬†Programming  199 , 525‚Äì556  (2022)."},{"id":2864,"pagetitle":"Levenberg‚ÄìMarquardt","title":"Levenberg-Marquardt","ref":"/manopt/stable/solvers/LevenbergMarquardt/#Levenberg-Marquardt","content":" Levenberg-Marquardt"},{"id":2865,"pagetitle":"Levenberg‚ÄìMarquardt","title":"Manopt.LevenbergMarquardt","ref":"/manopt/stable/solvers/LevenbergMarquardt/#Manopt.LevenbergMarquardt","content":" Manopt.LevenbergMarquardt  ‚Äî  Function LevenbergMarquardt(M, f, jacobian_f, p, num_components=-1; kwargs...)\nLevenbergMarquardt(M, vgf, p; kwargs...)\nLevenbergMarquardt(M, nlso, p; kwargs...)\nLevenbergMarquardt!(M, f, jacobian_f, p, num_components=-1; kwargs...)\nLevenbergMarquardt!(M, vgf, p, num_components=-1; kwargs...)\nLevenbergMarquardt!(M, nlso, p, num_components=-1; kwargs...) compute the the Riemannian Levenberg-Marquardt algorithm [ Pee93 ,  AOT22 ] to solve \\[\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} \\frac{1}{2} \\sum_{}^{}_{i=1}^m \\lvert f_i(p) \\rvert^2\\] where  $f: \\mathcal M ‚Üí ‚Ñù^m$  is written with component functions  $f_i: \\mathcal M ‚Üí ‚Ñù$ ,  $i=1,‚Ä¶,m$ , and each component function is continuously differentiable. The second block of signatures perform the optimization in-place of  p . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí‚Ñù^m$ . The cost function can be provided in two different ways as a single function returning a vector  $f(p) ‚àà ‚Ñù^m$ as a vector of functions, where each single function returns a scalar  $f_i(p) ‚àà ‚Ñù$ The type is determined by the  function_type=  keyword argument. jacobian_f :   the Jacobian of  $f$ . The Jacobian can be provided in three different ways as a single function returning a vector of gradient vectors  $\\bigl(\\operatorname{grad} f_i(p)\\bigr)_{i=1}^m$ as a vector of functions, where each single function returns a gradient vector  $\\operatorname{grad} f_i(p)$ ,  $i=1,‚Ä¶,m$ as a single function returning a (coefficient) matrix  $J ‚àà ‚Ñù^{m√ód}$ , where  $d$  is the dimension of the manifold. These coefficients are given with respect to an  AbstractBasis  of the tangent space at  p . The type is determined by the  jacobian_type=  keyword argument. p : a point on the manifold  $\\mathcal M$ num_components : length  $m$  of the vector returned by the cost function. By default its value is -1 which means that it is determined automatically by calling  f  one additional time. This is only possible when  evaluation  is  AllocatingEvaluation , for mutating evaluation this value must be explicitly specified. You can also provide the cost and its Jacobian already as a VectorGradientFunction vgf , Alternatively, passing a  NonlinearLeastSquaresObjective nlso . Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. Œ∑=0.2 :                   scaling factor for the sufficient cost decrease threshold required to accept new proposal points. Allowed range:  0 < Œ∑ < 1 . expect_zero_residual=false : whether or not the algorithm might expect that the value of residual (objective) at minimum is equal to 0. damping_term_min=0.1 :      initial (and also minimal) value of the damping term Œ≤=5.0 :                     parameter by which the damping term is multiplied when the current new point is rejected function_type= FunctionVectorialType : an  AbstractVectorialType  specifying the type of cost function provided. initial_jacobian_f :      the initial Jacobian of the cost function  f . By default this is a matrix of size  num_components  times the manifold dimension of similar type as  p . initial_residual_values : the initial residual vector of the cost function  f . By default this is a vector of length  num_components  of similar type as  p . jacobian_type= FunctionVectorialType : an  AbstractVectorialType  specifying the type of Jacobian provided. linear_subsolver! :    a function with three arguments  sk, JJ, grad_f_c that solves the linear subproblem sk .= JJ \\ grad f c , where JJ is (up to numerical issues) a symmetric positive definite matrix. Default value is [ default lm lin_solve!`](@ref). retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2866,"pagetitle":"Levenberg‚ÄìMarquardt","title":"Manopt.LevenbergMarquardt!","ref":"/manopt/stable/solvers/LevenbergMarquardt/#Manopt.LevenbergMarquardt!","content":" Manopt.LevenbergMarquardt!  ‚Äî  Function LevenbergMarquardt(M, f, jacobian_f, p, num_components=-1; kwargs...)\nLevenbergMarquardt(M, vgf, p; kwargs...)\nLevenbergMarquardt(M, nlso, p; kwargs...)\nLevenbergMarquardt!(M, f, jacobian_f, p, num_components=-1; kwargs...)\nLevenbergMarquardt!(M, vgf, p, num_components=-1; kwargs...)\nLevenbergMarquardt!(M, nlso, p, num_components=-1; kwargs...) compute the the Riemannian Levenberg-Marquardt algorithm [ Pee93 ,  AOT22 ] to solve \\[\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} \\frac{1}{2} \\sum_{}^{}_{i=1}^m \\lvert f_i(p) \\rvert^2\\] where  $f: \\mathcal M ‚Üí ‚Ñù^m$  is written with component functions  $f_i: \\mathcal M ‚Üí ‚Ñù$ ,  $i=1,‚Ä¶,m$ , and each component function is continuously differentiable. The second block of signatures perform the optimization in-place of  p . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí‚Ñù^m$ . The cost function can be provided in two different ways as a single function returning a vector  $f(p) ‚àà ‚Ñù^m$ as a vector of functions, where each single function returns a scalar  $f_i(p) ‚àà ‚Ñù$ The type is determined by the  function_type=  keyword argument. jacobian_f :   the Jacobian of  $f$ . The Jacobian can be provided in three different ways as a single function returning a vector of gradient vectors  $\\bigl(\\operatorname{grad} f_i(p)\\bigr)_{i=1}^m$ as a vector of functions, where each single function returns a gradient vector  $\\operatorname{grad} f_i(p)$ ,  $i=1,‚Ä¶,m$ as a single function returning a (coefficient) matrix  $J ‚àà ‚Ñù^{m√ód}$ , where  $d$  is the dimension of the manifold. These coefficients are given with respect to an  AbstractBasis  of the tangent space at  p . The type is determined by the  jacobian_type=  keyword argument. p : a point on the manifold  $\\mathcal M$ num_components : length  $m$  of the vector returned by the cost function. By default its value is -1 which means that it is determined automatically by calling  f  one additional time. This is only possible when  evaluation  is  AllocatingEvaluation , for mutating evaluation this value must be explicitly specified. You can also provide the cost and its Jacobian already as a VectorGradientFunction vgf , Alternatively, passing a  NonlinearLeastSquaresObjective nlso . Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. Œ∑=0.2 :                   scaling factor for the sufficient cost decrease threshold required to accept new proposal points. Allowed range:  0 < Œ∑ < 1 . expect_zero_residual=false : whether or not the algorithm might expect that the value of residual (objective) at minimum is equal to 0. damping_term_min=0.1 :      initial (and also minimal) value of the damping term Œ≤=5.0 :                     parameter by which the damping term is multiplied when the current new point is rejected function_type= FunctionVectorialType : an  AbstractVectorialType  specifying the type of cost function provided. initial_jacobian_f :      the initial Jacobian of the cost function  f . By default this is a matrix of size  num_components  times the manifold dimension of similar type as  p . initial_residual_values : the initial residual vector of the cost function  f . By default this is a vector of length  num_components  of similar type as  p . jacobian_type= FunctionVectorialType : an  AbstractVectorialType  specifying the type of Jacobian provided. linear_subsolver! :    a function with three arguments  sk, JJ, grad_f_c that solves the linear subproblem sk .= JJ \\ grad f c , where JJ is (up to numerical issues) a symmetric positive definite matrix. Default value is [ default lm lin_solve!`](@ref). retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2867,"pagetitle":"Levenberg‚ÄìMarquardt","title":"Options","ref":"/manopt/stable/solvers/LevenbergMarquardt/#Options","content":" Options"},{"id":2868,"pagetitle":"Levenberg‚ÄìMarquardt","title":"Manopt.LevenbergMarquardtState","ref":"/manopt/stable/solvers/LevenbergMarquardt/#Manopt.LevenbergMarquardtState","content":" Manopt.LevenbergMarquardtState  ‚Äî  Type LevenbergMarquardtState{P,T} <: AbstractGradientSolverState Describes a Gradient based descent algorithm, with Fields A default value is given in brackets if a parameter can be left out in initialization. p::P : a point on the manifold  $\\mathcal M$  storing the current iterate retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions residual_values :      value of  $F$  calculated in the solver setup or the previous iteration residual_values_temp : value of  $F$  for the current proposal point stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled jacobian :                 the current Jacobian of  $F$ gradient :             the current gradient of  $F$ step_vector :          the tangent vector at  x  that is used to move to the next point last_stepsize :        length of  step_vector Œ∑ :                    Scaling factor for the sufficient cost decrease threshold required to accept new proposal points. Allowed range:  0 < Œ∑ < 1 . damping_term :         current value of the damping term damping_term_min :     initial (and also minimal) value of the damping term Œ≤ :                    parameter by which the damping term is multiplied when the current new point is rejected expect_zero_residual : if true, the algorithm expects that the value of the residual (objective) at minimum is equal to 0. linear_subsolver! :    a function with three arguments  sk, JJ, grad_f_c that solves the linear subproblem sk .= JJ \\ grad f c , where JJ is (up to numerical issues) a symmetric positive definite matrix. Default value is [ default lm lin_solve!`](@ref). Constructor LevenbergMarquardtState(M, initial_residual_values, initial_jacobian; kwargs...) Generate the Levenberg-Marquardt solver state. Keyword arguments The following fields are keyword arguments Œ≤=5.0 damping_term_min=0.1 Œ∑=0.2 , expect_zero_residual=false initial_gradient= zero_vector (M, p) retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopAfterIteration (200) | StopWhenGradientNormLess (1e-12) | StopWhenStepsizeLess (1e-12) : a functor indicating that the stopping criterion is fulfilled See also gradient_descent ,  LevenbergMarquardt source"},{"id":2869,"pagetitle":"Levenberg‚ÄìMarquardt","title":"Technical details","ref":"/manopt/stable/solvers/LevenbergMarquardt/#sec-lm-technical-details","content":" Technical details The  LevenbergMarquardt  solver requires the following functions of a manifold to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified. the  norm  as well, to stop when the norm of the gradient is small, but if you implemented  inner , the norm is provided already. A  copyto! (M, q, p)  and  copy (M,p)  for points."},{"id":2870,"pagetitle":"Levenberg‚ÄìMarquardt","title":"Internals","ref":"/manopt/stable/solvers/LevenbergMarquardt/#Internals","content":" Internals"},{"id":2871,"pagetitle":"Levenberg‚ÄìMarquardt","title":"Manopt.default_lm_lin_solve!","ref":"/manopt/stable/solvers/LevenbergMarquardt/#Manopt.default_lm_lin_solve!","content":" Manopt.default_lm_lin_solve!  ‚Äî  Function default_lm_lin_solve!(sk, JJ, grad_f_c) Solve the system  JJ \\ grad_f_c  where JJ is (mathematically) a symmetric positive definite matrix and save the result to  sk . In case of numerical errors the  PosDefException  is caught and the default symmetric solver  (Symmetric(JJ) \\ grad_f_c)  is used. The function is intended to be used with  LevenbergMarquardt . source"},{"id":2872,"pagetitle":"Levenberg‚ÄìMarquardt","title":"Literature","ref":"/manopt/stable/solvers/LevenbergMarquardt/#Literature","content":" Literature [AOT22] S.¬†Adachi, T.¬†Okuno and A.¬†Takeda.  Riemannian Levenberg-Marquardt Method with Global and Local Convergence Properties . ArXiv¬†Preprint (2022). [Pee93] R.¬†Peeters.  On a Riemannian version of the Levenberg-Marquardt algorithm . Serie Research Memoranda¬†0011 (VU University Amsterdam, Faculty of Economics, Business Administration and Econometrics, 1993)."},{"id":2875,"pagetitle":"Nelder‚ÄìMead","title":"Nelder Mead method","ref":"/manopt/stable/solvers/NelderMead/#sec-nelder-meadSolver","content":" Nelder Mead method"},{"id":2876,"pagetitle":"Nelder‚ÄìMead","title":"Manopt.NelderMead","ref":"/manopt/stable/solvers/NelderMead/#Manopt.NelderMead","content":" Manopt.NelderMead  ‚Äî  Function NelderMead(M::AbstractManifold, f, population=NelderMeadSimplex(M))\nNelderMead(M::AbstractManifold, mco::AbstractManifoldCostObjective, population=NelderMeadSimplex(M))\nNelderMead!(M::AbstractManifold, f, population)\nNelderMead!(M::AbstractManifold, mco::AbstractManifoldCostObjective, population) Solve a Nelder-Mead minimization problem for the cost function  $f: \\mathcal M ‚Üí ‚Ñù$  on the manifold  M . If the initial  NelderMeadSimplex  is not provided, a random set of points is chosen. The compuation can be performed in-place of the  population . The algorithm consists of the following steps. Let  $d$  denote the dimension of the manifold  $\\mathcal M$ . Order the simplex vertices  $p_i, i=1,‚Ä¶,d+1$  by increasing cost, such that we have  $f(p_1) ‚â§ f(p_2) ‚â§ ‚Ä¶ ‚â§ f(p_{d+1})$ . Compute the Riemannian center of mass [ Kar77 ], cf.  mean ,  $p_{\\text{m}}$   of the simplex vertices  $p_1,‚Ä¶,p_{d+1}$ . Reflect the point with the worst point at the mean  $p_{\\text{r}} = \\operatorname{retr}_{p_{\\text{m}}}\\bigl( - Œ±\\operatorname{retr}^{-1}_{p_{\\text{m}}} (p_{d+1}) \\bigr)$   If  $f(p_1) ‚â§ f(p_{\\text{r}}) ‚â§ f(p_{d})$  then set  $p_{d+1} = p_{\\text{r}}$  and go to step 1. Expand the simplex if  $f(p_{\\text{r}}) < f(p_1)$  by computing the expantion point  $p_{\\text{e}} = \\operatorname{retr}_{p_{\\text{m}}}\\bigl( - Œ≥Œ±\\operatorname{retr}^{-1}_{p_{\\text{m}}} (p_{d+1}) \\bigr)$ ,  which in this formulation allows to reuse the tangent vector from the inverse retraction from before.  If  $f(p_{\\text{e}}) < f(p_{\\text{r}})$  then set  $p_{d+1} = p_{\\text{e}}$  otherwise set set  $p_{d+1} = p_{\\text{r}}$ . Then go to Step 1. Contract the simplex if  $f(p_{\\text{r}}) ‚â• f(p_d)$ . If  $f(p_{\\text{r}}) < f(p_{d+1})$  set the step  $s = -œÅ$ otherwise set  $s=œÅ$ . Compute the contraction point  $p_{\\text{c}} = \\operatorname{retr}_{p_{\\text{m}}}\\bigl(s\\operatorname{retr}^{-1}_{p_{\\text{m}}} p_{d+1} \\bigr)$ . in this case if  $f(p_{\\text{c}}) < f(p_{\\text{r}})$  set  $p_{d+1} = p_{\\text{c}}$  and go to step 1 in this case if  $f(p_{\\text{c}}) < f(p_{d+1})$  set  $p_{d+1} = p_{\\text{c}}$  and go to step 1 Shrink all points (closer to  $p_1$ ). For all  $i=2,...,d+1$  set   $p_{i} = \\operatorname{retr}_{p_{1}}\\bigl( œÉ\\operatorname{retr}^{-1}_{p_{1}} p_{i} \\bigr).$ For more details, see The Euclidean variant in the Wikipedia  https://en.wikipedia.org/wiki/Nelder-Mead_method  or Algorithm 4.1 in  http://www.optimization-online.org/DB_FILE/2007/08/1742.pdf . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v population:: NelderMeadSimplex = NelderMeadSimplex (M) : an initial simplex of  $d+1$  points, where  $d$  is the  manifold_dimension  of  M . Keyword arguments stopping_criterion= StopAfterIteration (2000) | StopWhenPopulationConcentrated () ): a functor indicating that the stopping criterion is fulfilled a  StoppingCriterion Œ±=1.0 : reflection parameter  $Œ± > 0$ : Œ≥=2.0  expansion parameter  $Œ≥$ : œÅ=1/2 : contraction parameter,  $0 < œÅ ‚â§ \\frac{1}{2}$ , œÉ=1/2 : shrink coefficient,  $0 < œÉ ‚â§ 1$ inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2877,"pagetitle":"Nelder‚ÄìMead","title":"Manopt.NelderMead!","ref":"/manopt/stable/solvers/NelderMead/#Manopt.NelderMead!","content":" Manopt.NelderMead!  ‚Äî  Function NelderMead(M::AbstractManifold, f, population=NelderMeadSimplex(M))\nNelderMead(M::AbstractManifold, mco::AbstractManifoldCostObjective, population=NelderMeadSimplex(M))\nNelderMead!(M::AbstractManifold, f, population)\nNelderMead!(M::AbstractManifold, mco::AbstractManifoldCostObjective, population) Solve a Nelder-Mead minimization problem for the cost function  $f: \\mathcal M ‚Üí ‚Ñù$  on the manifold  M . If the initial  NelderMeadSimplex  is not provided, a random set of points is chosen. The compuation can be performed in-place of the  population . The algorithm consists of the following steps. Let  $d$  denote the dimension of the manifold  $\\mathcal M$ . Order the simplex vertices  $p_i, i=1,‚Ä¶,d+1$  by increasing cost, such that we have  $f(p_1) ‚â§ f(p_2) ‚â§ ‚Ä¶ ‚â§ f(p_{d+1})$ . Compute the Riemannian center of mass [ Kar77 ], cf.  mean ,  $p_{\\text{m}}$   of the simplex vertices  $p_1,‚Ä¶,p_{d+1}$ . Reflect the point with the worst point at the mean  $p_{\\text{r}} = \\operatorname{retr}_{p_{\\text{m}}}\\bigl( - Œ±\\operatorname{retr}^{-1}_{p_{\\text{m}}} (p_{d+1}) \\bigr)$   If  $f(p_1) ‚â§ f(p_{\\text{r}}) ‚â§ f(p_{d})$  then set  $p_{d+1} = p_{\\text{r}}$  and go to step 1. Expand the simplex if  $f(p_{\\text{r}}) < f(p_1)$  by computing the expantion point  $p_{\\text{e}} = \\operatorname{retr}_{p_{\\text{m}}}\\bigl( - Œ≥Œ±\\operatorname{retr}^{-1}_{p_{\\text{m}}} (p_{d+1}) \\bigr)$ ,  which in this formulation allows to reuse the tangent vector from the inverse retraction from before.  If  $f(p_{\\text{e}}) < f(p_{\\text{r}})$  then set  $p_{d+1} = p_{\\text{e}}$  otherwise set set  $p_{d+1} = p_{\\text{r}}$ . Then go to Step 1. Contract the simplex if  $f(p_{\\text{r}}) ‚â• f(p_d)$ . If  $f(p_{\\text{r}}) < f(p_{d+1})$  set the step  $s = -œÅ$ otherwise set  $s=œÅ$ . Compute the contraction point  $p_{\\text{c}} = \\operatorname{retr}_{p_{\\text{m}}}\\bigl(s\\operatorname{retr}^{-1}_{p_{\\text{m}}} p_{d+1} \\bigr)$ . in this case if  $f(p_{\\text{c}}) < f(p_{\\text{r}})$  set  $p_{d+1} = p_{\\text{c}}$  and go to step 1 in this case if  $f(p_{\\text{c}}) < f(p_{d+1})$  set  $p_{d+1} = p_{\\text{c}}$  and go to step 1 Shrink all points (closer to  $p_1$ ). For all  $i=2,...,d+1$  set   $p_{i} = \\operatorname{retr}_{p_{1}}\\bigl( œÉ\\operatorname{retr}^{-1}_{p_{1}} p_{i} \\bigr).$ For more details, see The Euclidean variant in the Wikipedia  https://en.wikipedia.org/wiki/Nelder-Mead_method  or Algorithm 4.1 in  http://www.optimization-online.org/DB_FILE/2007/08/1742.pdf . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v population:: NelderMeadSimplex = NelderMeadSimplex (M) : an initial simplex of  $d+1$  points, where  $d$  is the  manifold_dimension  of  M . Keyword arguments stopping_criterion= StopAfterIteration (2000) | StopWhenPopulationConcentrated () ): a functor indicating that the stopping criterion is fulfilled a  StoppingCriterion Œ±=1.0 : reflection parameter  $Œ± > 0$ : Œ≥=2.0  expansion parameter  $Œ≥$ : œÅ=1/2 : contraction parameter,  $0 < œÅ ‚â§ \\frac{1}{2}$ , œÉ=1/2 : shrink coefficient,  $0 < œÉ ‚â§ 1$ inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2878,"pagetitle":"Nelder‚ÄìMead","title":"State","ref":"/manopt/stable/solvers/NelderMead/#State","content":" State"},{"id":2879,"pagetitle":"Nelder‚ÄìMead","title":"Manopt.NelderMeadState","ref":"/manopt/stable/solvers/NelderMead/#Manopt.NelderMeadState","content":" Manopt.NelderMeadState  ‚Äî  Type NelderMeadState <: AbstractManoptSolverState Describes all parameters and the state of a Nelder-Mead heuristic based optimization algorithm. Fields The naming of these parameters follows the  Wikipedia article  of the Euclidean case. The default is given in brackets, the required value range after the description population:: NelderMeadSimplex : a population (set) of  $d+1$  points  $x_i$ ,  $i=1,‚Ä¶,n+1$ , where  $d$  is the  manifold_dimension  of  M . stepsize::Stepsize : a functor inheriting from  Stepsize  to determine a step size Œ± : the reflection parameter  $Œ± > 0$ : Œ≥  the expansion parameter  $Œ≥ > 0$ : œÅ : the contraction parameter,  $0 < œÅ ‚â§ \\frac{1}{2}$ , œÉ : the shrinkage coefficient,  $0 < œÉ ‚â§ 1$ p::P : a point on the manifold  $\\mathcal M$  storing the current best point inverse_retraction_method::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions Constructors NelderMeadState(M::AbstractManifold; kwargs...) Construct a Nelder-Mead Option with a default population (if not provided) of set of  dimension(M)+1  random points stored in  NelderMeadSimplex . Keyword arguments population= NelderMeadSimplex (M) stopping_criterion= StopAfterIteration (2000) | StopWhenPopulationConcentrated () ): a functor indicating that the stopping criterion is fulfilled a  StoppingCriterion Œ±=1.0 : reflection parameter  $Œ± > 0$ : Œ≥=2.0  expansion parameter  $Œ≥$ : œÅ=1/2 : contraction parameter,  $0 < œÅ ‚â§ \\frac{1}{2}$ , œÉ=1/2 : shrink coefficient,  $0 < œÉ ‚â§ 1$ inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions p=copy(M, population.pts[1]) : initialise the storage for the best point (iterate)¬® source"},{"id":2880,"pagetitle":"Nelder‚ÄìMead","title":"Simplex","ref":"/manopt/stable/solvers/NelderMead/#Simplex","content":" Simplex"},{"id":2881,"pagetitle":"Nelder‚ÄìMead","title":"Manopt.NelderMeadSimplex","ref":"/manopt/stable/solvers/NelderMead/#Manopt.NelderMeadSimplex","content":" Manopt.NelderMeadSimplex  ‚Äî  Type NelderMeadSimplex A simplex for the Nelder-Mead algorithm. Constructors NelderMeadSimplex(M::AbstractManifold) Construct a  simplex using  $d+1$  random points from manifold  M , where  $d$  is the  manifold_dimension  of  M . NelderMeadSimplex(\n    M::AbstractManifold,\n    p,\n    B::AbstractBasis=default_basis(M, typeof(p));\n    a::Real=0.025,\n    retraction_method::AbstractRetractionMethod=default_retraction_method(M, typeof(p)),\n) Construct a simplex from a basis  B  with one point being  p  and other points constructed by moving by  a  in each principal direction defined by basis  B  of the tangent space at point  p  using retraction  retraction_method . This works similarly to how the initial simplex is constructed in the Euclidean Nelder-Mead algorithm, just in the tangent space at point  p . source"},{"id":2882,"pagetitle":"Nelder‚ÄìMead","title":"Additional stopping criteria","ref":"/manopt/stable/solvers/NelderMead/#Additional-stopping-criteria","content":" Additional stopping criteria"},{"id":2883,"pagetitle":"Nelder‚ÄìMead","title":"Manopt.StopWhenPopulationConcentrated","ref":"/manopt/stable/solvers/NelderMead/#Manopt.StopWhenPopulationConcentrated","content":" Manopt.StopWhenPopulationConcentrated  ‚Äî  Type StopWhenPopulationConcentrated <: StoppingCriterion A stopping criterion for  NelderMead  to indicate to stop when both the maximal distance of the first to the remaining the cost values and the maximal distance of the first to the remaining the population points drops below a certain tolerance  tol_f  and  tol_p , respectively. Constructor StopWhenPopulationConcentrated(tol_f::Real=1e-8, tol_x::Real=1e-8) source"},{"id":2884,"pagetitle":"Nelder‚ÄìMead","title":"Technical details","ref":"/manopt/stable/solvers/NelderMead/#Technical-details","content":" Technical details The  NelderMead  solver requires the following functions of a manifold to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified. An  inverse_retract! (M, X, p, q) ; it is recommended to set the  default_inverse_retraction_method  to a favourite retraction. If this default is set, a  inverse_retraction_method=  does not have to be specified. The  distance (M, p, q)  when using the default stopping criterion, which includes  StopWhenPopulationConcentrated . Within the default initialization  rand (M)  is used to generate the initial population A  mean (M, population)  has to be available, for example by loading  Manifolds.jl  and its  statistics  tools"},{"id":2887,"pagetitle":"Adaptive Regularization with Cubics","title":"Adaptive regularization with cubics","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#Adaptive-regularization-with-cubics","content":" Adaptive regularization with cubics"},{"id":2888,"pagetitle":"Adaptive Regularization with Cubics","title":"Manopt.adaptive_regularization_with_cubics","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#Manopt.adaptive_regularization_with_cubics","content":" Manopt.adaptive_regularization_with_cubics  ‚Äî  Function adaptive_regularization_with_cubics(M, f, grad_f, Hess_f, p=rand(M); kwargs...)\nadaptive_regularization_with_cubics(M, f, grad_f, p=rand(M); kwargs...)\nadaptive_regularization_with_cubics(M, mho, p=rand(M); kwargs...)\nadaptive_regularization_with_cubics!(M, f, grad_f, Hess_f, p; kwargs...)\nadaptive_regularization_with_cubics!(M, f, grad_f, p; kwargs...)\nadaptive_regularization_with_cubics!(M, mho, p; kwargs...) Solve an optimization problem on the manifold  M  by iteratively minimizing \\[m_k(X) = f(p_k) + ‚ü®X, \\operatorname{grad} f(p^{(k)})‚ü© + \\frac{1}{2}‚ü®X, \\operatorname{Hess} f(p^{(k)})[X]‚ü© + \\frac{œÉ_k}{3}\\lVert X \\rVert^3\\] on the tangent space at the current iterate  $p_k$ , where  $X ‚àà T_{p_k}\\mathcal M$  and  $œÉ_k > 0$  is a regularization parameter. Let  $Xp^{(k)}$  denote the minimizer of the model  $m_k$  and use the model improvement \\[  œÅ_k = \\frac{f(p_k) - f(\\operatorname{retr}_{p_k}(X_k))}{m_k(0) - m_k(X_k) + \\frac{œÉ_k}{3}\\lVert X_k\\rVert^3}.\\] With two thresholds  $Œ∑_2 ‚â• Œ∑_1 > 0$  set  $p_{k+1} = \\operatorname{retr}_{p_k}(X_k)$  if  $œÅ ‚â• Œ∑_1$  and reject the candidate otherwise, that is, set  $p_{k+1} = p_k$ . Further update the regularization parameter using factors  $0 < Œ≥_1 < 1 < Œ≥_2$  reads \\[œÉ_{k+1} =\n\\begin{cases}\n    \\max\\{œÉ_{\\min}, Œ≥_1œÉ_k\\} & \\text{ if } œÅ \\geq Œ∑_2 &\\text{   (the model was very successful)},\\\\\n    œÉ_k & \\text{ if } œÅ ‚àà [Œ∑_1, Œ∑_2)&\\text{   (the model was successful)},\\\\\n    Œ≥_2œÉ_k & \\text{ if } œÅ < Œ∑_1&\\text{   (the model was unsuccessful)}.\n\\end{cases}\\] For more details see [ ABBC20 ]. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place Hess_f : the (Riemannian) Hessian  $\\operatorname{Hess}f: T_{p}\\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p, X) -> Y  or a function  (M, Y, p, X) -> Y  computing  Y  in-place p : a point on the manifold  $\\mathcal M$ the cost  f  and its gradient and Hessian might also be provided as a  ManifoldHessianObjective Keyword arguments œÉ=100.0 / sqrt(manifold_dimension(M) : initial regularization parameter œÉmin=1e-10 : minimal regularization value  $œÉ_{\\min}$ Œ∑1=0.1 : lower model success threshold Œ∑2=0.9 : upper model success threshold Œ≥1=0.1 : regularization reduction factor (for the success case) Œ≥2=2.0 : regularization increment factor (for the non-success case) evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. initial_tangent_vector=zero_vector(M, p) : initialize any tangent vector data, maxIterLanczos=200 : a shortcut to set the stopping criterion in the sub solver, œÅ_regularization=1e3 : a regularization to avoid dividing by zero for small values of cost and model retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions : stopping_criterion= StopAfterIteration (40) | StopWhenGradientNormLess (1e-9) | StopWhenAllLanczosVectorsUsed (maxIterLanczos) : a functor indicating that the stopping criterion is fulfilled sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. sub_objective=nothing : a shortcut to modify the objective of the subproblem used within in the  sub_problem=  keyword By default, this is initialized as a  AdaptiveRagularizationWithCubicsModelObjective , which can further be decorated by using the  sub_kwargs=  keyword. sub_state= LanczosState (M, copy(M,p)) :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. sub_problem= DefaultManoptProblem (M, sub_objective) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. If you provide the  ManifoldFirstOrderObjective  directly, the  evaluation=  keyword is ignored. The decorations are still applied to the objective. If you activate tutorial mode (cf.  is_tutorial_mode ), this solver provides additional debug warnings. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2889,"pagetitle":"Adaptive Regularization with Cubics","title":"Manopt.adaptive_regularization_with_cubics!","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#Manopt.adaptive_regularization_with_cubics!","content":" Manopt.adaptive_regularization_with_cubics!  ‚Äî  Function adaptive_regularization_with_cubics(M, f, grad_f, Hess_f, p=rand(M); kwargs...)\nadaptive_regularization_with_cubics(M, f, grad_f, p=rand(M); kwargs...)\nadaptive_regularization_with_cubics(M, mho, p=rand(M); kwargs...)\nadaptive_regularization_with_cubics!(M, f, grad_f, Hess_f, p; kwargs...)\nadaptive_regularization_with_cubics!(M, f, grad_f, p; kwargs...)\nadaptive_regularization_with_cubics!(M, mho, p; kwargs...) Solve an optimization problem on the manifold  M  by iteratively minimizing \\[m_k(X) = f(p_k) + ‚ü®X, \\operatorname{grad} f(p^{(k)})‚ü© + \\frac{1}{2}‚ü®X, \\operatorname{Hess} f(p^{(k)})[X]‚ü© + \\frac{œÉ_k}{3}\\lVert X \\rVert^3\\] on the tangent space at the current iterate  $p_k$ , where  $X ‚àà T_{p_k}\\mathcal M$  and  $œÉ_k > 0$  is a regularization parameter. Let  $Xp^{(k)}$  denote the minimizer of the model  $m_k$  and use the model improvement \\[  œÅ_k = \\frac{f(p_k) - f(\\operatorname{retr}_{p_k}(X_k))}{m_k(0) - m_k(X_k) + \\frac{œÉ_k}{3}\\lVert X_k\\rVert^3}.\\] With two thresholds  $Œ∑_2 ‚â• Œ∑_1 > 0$  set  $p_{k+1} = \\operatorname{retr}_{p_k}(X_k)$  if  $œÅ ‚â• Œ∑_1$  and reject the candidate otherwise, that is, set  $p_{k+1} = p_k$ . Further update the regularization parameter using factors  $0 < Œ≥_1 < 1 < Œ≥_2$  reads \\[œÉ_{k+1} =\n\\begin{cases}\n    \\max\\{œÉ_{\\min}, Œ≥_1œÉ_k\\} & \\text{ if } œÅ \\geq Œ∑_2 &\\text{   (the model was very successful)},\\\\\n    œÉ_k & \\text{ if } œÅ ‚àà [Œ∑_1, Œ∑_2)&\\text{   (the model was successful)},\\\\\n    Œ≥_2œÉ_k & \\text{ if } œÅ < Œ∑_1&\\text{   (the model was unsuccessful)}.\n\\end{cases}\\] For more details see [ ABBC20 ]. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place Hess_f : the (Riemannian) Hessian  $\\operatorname{Hess}f: T_{p}\\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p, X) -> Y  or a function  (M, Y, p, X) -> Y  computing  Y  in-place p : a point on the manifold  $\\mathcal M$ the cost  f  and its gradient and Hessian might also be provided as a  ManifoldHessianObjective Keyword arguments œÉ=100.0 / sqrt(manifold_dimension(M) : initial regularization parameter œÉmin=1e-10 : minimal regularization value  $œÉ_{\\min}$ Œ∑1=0.1 : lower model success threshold Œ∑2=0.9 : upper model success threshold Œ≥1=0.1 : regularization reduction factor (for the success case) Œ≥2=2.0 : regularization increment factor (for the non-success case) evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. initial_tangent_vector=zero_vector(M, p) : initialize any tangent vector data, maxIterLanczos=200 : a shortcut to set the stopping criterion in the sub solver, œÅ_regularization=1e3 : a regularization to avoid dividing by zero for small values of cost and model retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions : stopping_criterion= StopAfterIteration (40) | StopWhenGradientNormLess (1e-9) | StopWhenAllLanczosVectorsUsed (maxIterLanczos) : a functor indicating that the stopping criterion is fulfilled sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. sub_objective=nothing : a shortcut to modify the objective of the subproblem used within in the  sub_problem=  keyword By default, this is initialized as a  AdaptiveRagularizationWithCubicsModelObjective , which can further be decorated by using the  sub_kwargs=  keyword. sub_state= LanczosState (M, copy(M,p)) :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. sub_problem= DefaultManoptProblem (M, sub_objective) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. If you provide the  ManifoldFirstOrderObjective  directly, the  evaluation=  keyword is ignored. The decorations are still applied to the objective. If you activate tutorial mode (cf.  is_tutorial_mode ), this solver provides additional debug warnings. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2890,"pagetitle":"Adaptive Regularization with Cubics","title":"State","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#State","content":" State"},{"id":2891,"pagetitle":"Adaptive Regularization with Cubics","title":"Manopt.AdaptiveRegularizationState","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#Manopt.AdaptiveRegularizationState","content":" Manopt.AdaptiveRegularizationState  ‚Äî  Type AdaptiveRegularizationState{P,T} <: AbstractHessianSolverState A state for the  adaptive_regularization_with_cubics  solver. Fields Œ∑1 ,  Œ∑1 : bounds for evaluating the regularization parameter Œ≥1 ,  Œ≥2 :  shrinking and expansion factors for regularization parameter  œÉ H : the current Hessian evaluation s : the current solution from the subsolver p::P : a point on the manifold  $\\mathcal M$  storing the current iterate q : a point for the candidates to evaluate model and œÅ X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing the gradient at the current iterate s : the tangent vector step resulting from minimizing the model problem in the tangent space  $T_{p}\\mathcal M$ œÉ : the current cubic regularization parameter œÉmin : lower bound for the cubic regularization parameter œÅ_regularization : regularization parameter for computing œÅ. When approaching convergence œÅ may be difficult to compute with numerator and denominator approaching zero. Regularizing the ratio lets œÅ go to 1 near convergence. œÅ : the current regularized ratio of actual improvement and model improvement. œÅ_denominator : a value to store the denominator from the computation of œÅ to allow for a warning or error when this value is non-positive. retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled sub_problem::Union{AbstractManoptProblem, F} :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state::Union{AbstractManoptProblem, F} :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. Furthermore the following integral fields are defined Constructor AdaptiveRegularizationState(M, sub_problem, sub_state; kwargs...) Construct the solver state with all fields stated as keyword arguments and the following defaults Keyword arguments Œ∑1=0.1 Œ∑2=0.9 Œ≥1=0.1 Œ≥2=2.0 œÉ=100/manifold_dimension(M) `œÉmin=1e-7 œÅ_regularization=1e3 evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. p= rand (M) : a point on the manifold  $\\mathcal M$ retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopAfterIteration (100) : a functor indicating that the stopping criterion is fulfilled X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ source"},{"id":2892,"pagetitle":"Adaptive Regularization with Cubics","title":"Sub solvers","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#Sub-solvers","content":" Sub solvers There are several ways to approach the subsolver. The default is the first one."},{"id":2893,"pagetitle":"Adaptive Regularization with Cubics","title":"Lanczos iteration","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#arc-Lanczos","content":" Lanczos iteration"},{"id":2894,"pagetitle":"Adaptive Regularization with Cubics","title":"Manopt.LanczosState","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#Manopt.LanczosState","content":" Manopt.LanczosState  ‚Äî  Type LanczosState{P,T,SC,B,I,R,TM,V,Y} <: AbstractManoptSolverState Solve the adaptive regularized subproblem with a Lanczos iteration Fields stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled stop_newton::StoppingCriterion : a functor indicating that the stopping criterion is fulfilledused for the inner Newton iteration œÉ :               the current regularization parameter X :               the Iterate Lanczos_vectors : the obtained Lanczos vectors tridig_matrix :   the tridiagonal coefficient matrix T coefficients :    the coefficients  $y_1,...y_k$  that determine the solution Hp :              a temporary tangent vector containing the evaluation of the Hessian Hp_residual :     a temporary tangent vector containing the residual to the Hessian S :               the current obtained / approximated solution Constructor LanczosState(TpM::TangentSpace; kwargs...) Keyword arguments X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ as the iterate maxIterLanzcos=200 : shortcut to set the maximal number of iterations in the  stopping_crtierion= Œ∏=0.5 : set the parameter in the  StopWhenFirstOrderProgress  within the default  stopping_criterion= . stopping_criterion= StopAfterIteration (maxIterLanczos) | StopWhenFirstOrderProgress (Œ∏) : a functor indicating that the stopping criterion is fulfilled stopping_criterion_newton= StopAfterIteration (200) : a functor indicating that the stopping criterion is fulfilled used for the inner Newton iteration œÉ=10.0 : specify the regularization parameter source"},{"id":2895,"pagetitle":"Adaptive Regularization with Cubics","title":"(Conjugate) gradient descent","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#(Conjugate)-gradient-descent","content":" (Conjugate) gradient descent There is a generic objective, that implements the sub problem"},{"id":2896,"pagetitle":"Adaptive Regularization with Cubics","title":"Manopt.AdaptiveRagularizationWithCubicsModelObjective","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#Manopt.AdaptiveRagularizationWithCubicsModelObjective","content":" Manopt.AdaptiveRagularizationWithCubicsModelObjective  ‚Äî  Type AdaptiveRagularizationWithCubicsModelObjective A model for the adaptive regularization with Cubics \\[m(X) = f(p) + ‚ü®\\operatorname{grad} f(p), X ‚ü©_p + \\frac{1}{2} ‚ü®\\operatorname{Hess} f(p)[X], X‚ü©_p\n       +  \\frac{œÉ}{3} \\lVert X \\rVert^3,\\] cf. Eq. (33) in [ ABBC20 ] Fields objective : an  AbstractManifoldHessianObjective  proving  $f$ , its gradient and Hessian œÉ :         the current (cubic) regularization parameter Constructors AdaptiveRagularizationWithCubicsModelObjective(mho, œÉ=1.0) with either an  AbstractManifoldHessianObjective objective  or an decorator containing such an objective. source Since the sub problem is given on the tangent space, you have to provide arc_obj = AdaptiveRagularizationWithCubicsModelObjective(mho, œÉ)\nsub_problem = DefaultProblem(TangentSpaceAt(M,p), arc_obj) where  mho  is the Hessian objective of  f  to solve. Then use this for the  sub_problem  keyword and use your favourite gradient based solver for the  sub_state  keyword, for example a  ConjugateGradientDescentState"},{"id":2897,"pagetitle":"Adaptive Regularization with Cubics","title":"Additional stopping criteria","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#Additional-stopping-criteria","content":" Additional stopping criteria"},{"id":2898,"pagetitle":"Adaptive Regularization with Cubics","title":"Manopt.StopWhenAllLanczosVectorsUsed","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#Manopt.StopWhenAllLanczosVectorsUsed","content":" Manopt.StopWhenAllLanczosVectorsUsed  ‚Äî  Type StopWhenAllLanczosVectorsUsed <: StoppingCriterion When an inner iteration has used up all Lanczos vectors, then this stopping criterion is a fallback / security stopping criterion to not access a non-existing field in the array allocated for vectors. Note that this stopping criterion (for now) is only implemented for the case that an  AdaptiveRegularizationState  when using a  LanczosState  subsolver Fields maxLanczosVectors : maximal number of Lanczos vectors at_iteration  indicates at which iteration (including  i=0 ) the stopping criterion was fulfilled and is  -1  while it is not fulfilled. Constructor StopWhenAllLanczosVectorsUsed(maxLancosVectors::Int) source"},{"id":2899,"pagetitle":"Adaptive Regularization with Cubics","title":"Manopt.StopWhenFirstOrderProgress","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#Manopt.StopWhenFirstOrderProgress","content":" Manopt.StopWhenFirstOrderProgress  ‚Äî  Type StopWhenFirstOrderProgress <: StoppingCriterion A stopping criterion related to the Riemannian adaptive regularization with cubics (ARC) solver indicating that the model function at the current (outer) iterate, \\[m_k(X) = f(p_k) + ‚ü®X, \\operatorname{grad} f(p^{(k)})‚ü© + \\frac{1}{2}‚ü®X, \\operatorname{Hess} f(p^{(k)})[X]‚ü© + \\frac{œÉ_k}{3}\\lVert X \\rVert^3\\] defined on the tangent space  $T_{p}\\mathcal M$  fulfills at the current iterate  $X_k$  that \\[m(X_k) \\leq m(0)\n\\quad\\text{ and }\\quad\n\\lVert \\operatorname{grad} m(X_k) \\rVert ‚â§ Œ∏ \\lVert X_k \\rVert^2\\] Fields Œ∏ :      the factor  $Œ∏$  in the second condition at_iteration::Int : an integer indicating at which the stopping criterion last indicted to stop, which might also be before the solver started ( 0 ). Any negative value indicates that this was not yet the case; Constructor StopWhenAllLanczosVectorsUsed(Œ∏) source"},{"id":2900,"pagetitle":"Adaptive Regularization with Cubics","title":"Technical details","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#sec-arc-technical-details","content":" Technical details The  adaptive_regularization_with_cubics  requires the following functions of a manifolds to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified. if you do not provide an initial regularization parameter  œÉ , a  manifold_dimension  is required. By default the tangent vector storing the gradient is initialized calling  zero_vector (M,p) . inner (M, p, X, Y)  is used within the algorithm step Furthermore, within the Lanczos subsolver, generating a random vector (at  p ) using  rand! (M, X; vector_at=p)  in place of  X  is required"},{"id":2901,"pagetitle":"Adaptive Regularization with Cubics","title":"Literature","ref":"/manopt/stable/solvers/adaptive-regularization-with-cubics/#Literature","content":" Literature [ABBC20] N.¬†Agarwal, N.¬†Boumal, B.¬†Bullins and C.¬†Cartis.  Adaptive regularization with cubics on manifolds .  Mathematical¬†Programming  (2020)."},{"id":2904,"pagetitle":"Alternating Gradient Descent","title":"Alternating gradient descent","ref":"/manopt/stable/solvers/alternating_gradient_descent/#solver-alternating-gradient-descent","content":" Alternating gradient descent"},{"id":2905,"pagetitle":"Alternating Gradient Descent","title":"Manopt.alternating_gradient_descent","ref":"/manopt/stable/solvers/alternating_gradient_descent/#Manopt.alternating_gradient_descent","content":" Manopt.alternating_gradient_descent  ‚Äî  Function alternating_gradient_descent(M::ProductManifold, f, grad_f, p=rand(M))\nalternating_gradient_descent(M::ProductManifold, ago::ManifoldAlternatingGradientObjective, p)\nalternating_gradient_descent!(M::ProductManifold, f, grad_f, p)\nalternating_gradient_descent!(M::ProductManifold, ago::ManifoldAlternatingGradientObjective, p) perform an alternating gradient descent. This can be done in-place of the start point  p Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : a gradient, that can be of two cases is a single function returning an  ArrayPartition  from  RecursiveArrayTools.jl  or is a vector functions each returning a component part of the whole gradient p : a point on the manifold  $\\mathcal M$ Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. evaluation_order=:Linear : whether to use a randomly permuted sequence ( :FixedRandom ), a per cycle permuted sequence ( :Random ) or the default  :Linear  one. inner_iterations=5 :  how many gradient steps to take in a component before alternating to the next stopping_criterion= StopAfterIteration (1000) ): a functor indicating that the stopping criterion is fulfilled stepsize= ArmijoLinesearch () : a functor inheriting from  Stepsize  to determine a step size order=[1:n] :         the initial permutation, where  n  is the number of gradients in  gradF . retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions Output usually the obtained (approximate) minimizer, see  get_solver_return  for details Note The input of each of the (component) gradients is still the whole vector  X , just that all other then the  i th input component are assumed to be fixed and just the  i th components gradient is computed / returned. source"},{"id":2906,"pagetitle":"Alternating Gradient Descent","title":"Manopt.alternating_gradient_descent!","ref":"/manopt/stable/solvers/alternating_gradient_descent/#Manopt.alternating_gradient_descent!","content":" Manopt.alternating_gradient_descent!  ‚Äî  Function alternating_gradient_descent(M::ProductManifold, f, grad_f, p=rand(M))\nalternating_gradient_descent(M::ProductManifold, ago::ManifoldAlternatingGradientObjective, p)\nalternating_gradient_descent!(M::ProductManifold, f, grad_f, p)\nalternating_gradient_descent!(M::ProductManifold, ago::ManifoldAlternatingGradientObjective, p) perform an alternating gradient descent. This can be done in-place of the start point  p Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : a gradient, that can be of two cases is a single function returning an  ArrayPartition  from  RecursiveArrayTools.jl  or is a vector functions each returning a component part of the whole gradient p : a point on the manifold  $\\mathcal M$ Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. evaluation_order=:Linear : whether to use a randomly permuted sequence ( :FixedRandom ), a per cycle permuted sequence ( :Random ) or the default  :Linear  one. inner_iterations=5 :  how many gradient steps to take in a component before alternating to the next stopping_criterion= StopAfterIteration (1000) ): a functor indicating that the stopping criterion is fulfilled stepsize= ArmijoLinesearch () : a functor inheriting from  Stepsize  to determine a step size order=[1:n] :         the initial permutation, where  n  is the number of gradients in  gradF . retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions Output usually the obtained (approximate) minimizer, see  get_solver_return  for details Note The input of each of the (component) gradients is still the whole vector  X , just that all other then the  i th input component are assumed to be fixed and just the  i th components gradient is computed / returned. source"},{"id":2907,"pagetitle":"Alternating Gradient Descent","title":"State","ref":"/manopt/stable/solvers/alternating_gradient_descent/#State","content":" State"},{"id":2908,"pagetitle":"Alternating Gradient Descent","title":"Manopt.AlternatingGradientDescentState","ref":"/manopt/stable/solvers/alternating_gradient_descent/#Manopt.AlternatingGradientDescentState","content":" Manopt.AlternatingGradientDescentState  ‚Äî  Type AlternatingGradientDescentState <: AbstractGradientDescentSolverState Store the fields for an alternating gradient descent algorithm, see also  alternating_gradient_descent . Fields direction:: DirectionUpdateRule evaluation_order::Symbol : whether to use a randomly permuted sequence ( :FixedRandom ), a per cycle newly permuted sequence ( :Random ) or the default  :Linear  evaluation order. inner_iterations : how many gradient steps to take in a component before alternating to the next order : the current permutation retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize::Stepsize : a functor inheriting from  Stepsize  to determine a step size stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled p::P : a point on the manifold  $\\mathcal M$  storing the current iterate X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing the gradient at the current iterate k , √¨`:              internal counters for the outer and inner iterations, respectively. Constructors AlternatingGradientDescentState(M::AbstractManifold; kwargs...) Keyword arguments inner_iterations=5 p= rand (M) : a point on the manifold  $\\mathcal M$ order_type::Symbol=:Linear order::Vector{<:Int}=Int[] stopping_criterion= StopAfterIteration (1000) : a functor indicating that the stopping criterion is fulfilled stepsize= default_stepsize (M, AlternatingGradientDescentState) : a functor inheriting from  Stepsize  to determine a step size X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ Generate the options for point  p  and where  inner_iterations ,  order_type ,  order ,  retraction_method ,  stopping_criterion , and  stepsize ` are keyword arguments source Additionally, the options share a  DirectionUpdateRule , which chooses the current component, so they can be decorated further; The most inner one should always be the following one though."},{"id":2909,"pagetitle":"Alternating Gradient Descent","title":"Manopt.AlternatingGradient","ref":"/manopt/stable/solvers/alternating_gradient_descent/#Manopt.AlternatingGradient","content":" Manopt.AlternatingGradient  ‚Äî  Function AlternatingGradient(; kwargs...)\nAlternatingGradient(M::AbstractManifold; kwargs...) Specify that a gradient based method should only update parts of the gradient in order to do a alternating gradient descent. Keyword arguments initial_gradient= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value Info This function generates a  ManifoldDefaultsFactory  for  AlternatingGradientRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2910,"pagetitle":"Alternating Gradient Descent","title":"Manopt.AlternatingGradientRule","ref":"/manopt/stable/solvers/alternating_gradient_descent/#Manopt.AlternatingGradientRule","content":" Manopt.AlternatingGradientRule  ‚Äî  Type AlternatingGradientRule <: AbstractGradientGroupDirectionRule Create a functor  (problem, state k) -> (s,X)  to evaluate the alternating gradient, that is alternating between the components of the gradient and has an field for partial evaluation of the gradient in-place. Fields X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ Constructor AlternatingGradientRule(M::AbstractManifold; p=rand(M), X=zero_vector(M, p)) Initialize the alternating gradient processor with tangent vector type of  X , where both  M  and  p  are just help variables. See also alternating_gradient_descent , [ AlternatingGradient ])@ref) source which internally uses"},{"id":2911,"pagetitle":"Alternating Gradient Descent","title":"Technical details","ref":"/manopt/stable/solvers/alternating_gradient_descent/#sec-agd-technical-details","content":" Technical details The  alternating_gradient_descent  solver requires the following functions of a manifold to be available The problem has to be phrased on a  ProductManifold , to be able to alternate between parts of the input. A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified. By default alternating gradient descent uses  ArmijoLinesearch  which requires  max_stepsize (M)  to be set and an implementation of  inner (M, p, X) . By default the tangent vector storing the gradient is initialized calling  zero_vector (M,p) ."},{"id":2914,"pagetitle":"Augmented Lagrangian Method","title":"Augmented Lagrangian method","ref":"/manopt/stable/solvers/augmented_Lagrangian_method/#Augmented-Lagrangian-method","content":" Augmented Lagrangian method"},{"id":2915,"pagetitle":"Augmented Lagrangian Method","title":"Manopt.augmented_Lagrangian_method","ref":"/manopt/stable/solvers/augmented_Lagrangian_method/#Manopt.augmented_Lagrangian_method","content":" Manopt.augmented_Lagrangian_method  ‚Äî  Function augmented_Lagrangian_method(M, f, grad_f, p=rand(M); kwargs...)\naugmented_Lagrangian_method(M, cmo::ConstrainedManifoldObjective, p=rand(M); kwargs...)\naugmented_Lagrangian_method!(M, f, grad_f, p; kwargs...)\naugmented_Lagrangian_method!(M, cmo::ConstrainedManifoldObjective, p; kwargs...) perform the augmented Lagrangian method (ALM) [ LB19 ]. This method can work in-place of  p . The aim of the ALM is to find the solution of the constrained optimisation task \\[\\begin{aligned}\n\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} & f(p)\\\\\n\\text{subject to}\\quad&g_i(p) ‚â§ 0 \\quad \\text{ for } i= 1, ‚Ä¶, m,\\\\\n\\quad & h_j(p)=0 \\quad \\text{ for } j=1,‚Ä¶,n,\n\\end{aligned}\\] where  M  is a Riemannian manifold, and  $f$ ,  $\\{g_i\\}_{i=1}^{n}$  and  $\\{h_j\\}_{j=1}^{m}$  are twice continuously differentiable functions from  M  to ‚Ñù. In every step  $k$  of the algorithm, the  AugmentedLagrangianCost $\\mathcal L_{œÅ^{(k)}}(p, Œº^{(k)}, Œª^{(k)})$  is minimized on \\mathcal M,   where  $Œº^{(k)} ‚àà ‚Ñù^n$  and  $Œª^{(k)} ‚àà ‚Ñù^m$  are the current iterates of the Lagrange multipliers and  $œÅ^{(k)}$  is the current penalty parameter. The Lagrange multipliers are then updated by \\[Œª_j^{(k+1)} =\\operatorname{clip}_{[Œª_{\\min},Œª_{\\max}]} (Œª_j^{(k)} + œÅ^{(k)} h_j(p^{(k+1)})) \\text{for all} j=1,‚Ä¶,p,\\] and \\[Œº_i^{(k+1)} =\\operatorname{clip}_{[0,Œº_{\\max}]} (Œº_i^{(k)} + œÅ^{(k)} g_i(p^{(k+1)})) \\text{ for all } i=1,‚Ä¶,m,\\] where  $Œª_{\\text{min}} ‚â§ Œª_{\\text{max}}$  and  $Œº_{\\text{max}}$  are the multiplier boundaries. Next, the accuracy tolerance  $œµ$  is updated as \\[œµ^{(k)}=\\max\\{œµ_{\\min}, Œ∏_œµ œµ^{(k-1)}\\},\\] where  $œµ_{\\text{min}}$  is the lowest value  $œµ$  is allowed to become and  $Œ∏_œµ ‚àà (0,1)$  is constant scaling factor. Last, the penalty parameter  $œÅ$  is updated as follows: with \\[œÉ^{(k)}=\\max_{j=1,‚Ä¶,p, i=1,‚Ä¶,m} \\{\\|h_j(p^{(k)})\\|, \\|\\max_{i=1,‚Ä¶,m}\\{g_i(p^{(k)}), -\\frac{Œº_i^{(k-1)}}{œÅ^{(k-1)}} \\}\\| \\}.\\] œÅ  is updated as \\[œÅ^{(k)} = \\begin{cases}\nœÅ^{(k-1)}/Œ∏_œÅ,  & \\text{if } œÉ^{(k)}\\leq Œ∏_œÅ œÉ^{(k-1)} ,\\\\\nœÅ^{(k-1)}, & \\text{else,}\n\\end{cases}\\] where  $Œ∏_œÅ ‚àà (0,1)$  is a constant scaling factor. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place Optional (if not called with the  ConstrainedManifoldObjective cmo ) g=nothing : the inequality constraints h=nothing : the equality constraints grad_g=nothing : the gradient of the inequality constraints grad_h=nothing : the gradient of the equality constraints Note that one of the pairs ( g ,  grad_g ) or ( h ,  grad_h ) has to be provided. Otherwise the problem is not constrained and a better solver would be for example  quasi_Newton . Keyword Arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. œµ=1e-3 :           the accuracy tolerance œµ_min=1e-6 :       the lower bound for the accuracy tolerance œµ_exponent=1/100 : exponent of the œµ update factor; also 1/number of iterations until maximal accuracy is needed to end algorithm naturally equality_constraints=nothing : the number  $n$  of equality constraints. If not provided, a call to the gradient of  g  is performed to estimate these. gradient_range=nothing : specify how both gradients of the constraints are represented gradient_equality_range=gradient_range :  specify how gradients of the equality constraints are represented, see  VectorGradientFunction . gradient_inequality_range=gradient_range :  specify how gradients of the inequality constraints are represented, see  VectorGradientFunction . inequality_constraints=nothing : the number  $m$  of inequality constraints.  If not provided, a call to the gradient of  g  is performed to estimate these. Œª=ones(size(h(M,x),1)) : the Lagrange multiplier with respect to the equality constraints Œª_max=20.0 :       an upper bound for the Lagrange multiplier belonging to the equality constraints Œª_min=- Œª_max :    a lower bound for the Lagrange multiplier belonging to the equality constraints Œº=ones(size(h(M,x),1)) : the Lagrange multiplier with respect to the inequality constraints Œº_max=20.0 : an upper bound for the Lagrange multiplier belonging to the inequality constraints œÅ=1.0 :            the penalty parameter œÑ=0.8 :            factor for the improvement of the evaluation of the penalty parameter Œ∏_œÅ=0.3 :          the scaling factor of the penalty parameter Œ∏_œµ=(œµ_min / œµ)^(œµ_exponent) : the scaling factor of the exactness sub_cost=[ AugmentedLagrangianCost¬± (@ref) (cmo, œÅ, Œº, Œª):  use augmented Lagrangian cost, based on the  ConstrainedManifoldObjective  build from the functions provided.  This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_grad=[ AugmentedLagrangianGrad ](@ref) (cmo, œÅ, Œº, Œª) : use augmented Lagrangian gradient, based on the [ ConstrainedManifoldObjective ](@ref) build from the functions provided. This is used to define the sub problem= keyword and has hence no effect, if you set sub problem` directly. sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. stopping_criterion= StopAfterIteration (300) | ( StopWhenSmallerOrEqual (:œµ, œµ_min) & StopWhenChangeLess (1e-10) ) | StopWhenChangeLess `: a functor indicating that the stopping criterion is fulfilled sub_problem= DefaultManoptProblem (M, sub_objective) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= QuasiNewtonState :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function.as the quasi newton method, the  QuasiNewtonLimitedMemoryDirectionUpdate  with  InverseBFGS  is used. sub_stopping_criterion::StoppingCriterion= StopAfterIteration (300) | StopWhenGradientNormLess (œµ) | StopWhenStepsizeLess (1e-8) , For the  range s of the constraints' gradient, other power manifold tangent space representations, mainly the  ArrayPowerRepresentation  can be used if the gradients can be computed more efficiently in that representation. All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2916,"pagetitle":"Augmented Lagrangian Method","title":"Manopt.augmented_Lagrangian_method!","ref":"/manopt/stable/solvers/augmented_Lagrangian_method/#Manopt.augmented_Lagrangian_method!","content":" Manopt.augmented_Lagrangian_method!  ‚Äî  Function augmented_Lagrangian_method(M, f, grad_f, p=rand(M); kwargs...)\naugmented_Lagrangian_method(M, cmo::ConstrainedManifoldObjective, p=rand(M); kwargs...)\naugmented_Lagrangian_method!(M, f, grad_f, p; kwargs...)\naugmented_Lagrangian_method!(M, cmo::ConstrainedManifoldObjective, p; kwargs...) perform the augmented Lagrangian method (ALM) [ LB19 ]. This method can work in-place of  p . The aim of the ALM is to find the solution of the constrained optimisation task \\[\\begin{aligned}\n\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} & f(p)\\\\\n\\text{subject to}\\quad&g_i(p) ‚â§ 0 \\quad \\text{ for } i= 1, ‚Ä¶, m,\\\\\n\\quad & h_j(p)=0 \\quad \\text{ for } j=1,‚Ä¶,n,\n\\end{aligned}\\] where  M  is a Riemannian manifold, and  $f$ ,  $\\{g_i\\}_{i=1}^{n}$  and  $\\{h_j\\}_{j=1}^{m}$  are twice continuously differentiable functions from  M  to ‚Ñù. In every step  $k$  of the algorithm, the  AugmentedLagrangianCost $\\mathcal L_{œÅ^{(k)}}(p, Œº^{(k)}, Œª^{(k)})$  is minimized on \\mathcal M,   where  $Œº^{(k)} ‚àà ‚Ñù^n$  and  $Œª^{(k)} ‚àà ‚Ñù^m$  are the current iterates of the Lagrange multipliers and  $œÅ^{(k)}$  is the current penalty parameter. The Lagrange multipliers are then updated by \\[Œª_j^{(k+1)} =\\operatorname{clip}_{[Œª_{\\min},Œª_{\\max}]} (Œª_j^{(k)} + œÅ^{(k)} h_j(p^{(k+1)})) \\text{for all} j=1,‚Ä¶,p,\\] and \\[Œº_i^{(k+1)} =\\operatorname{clip}_{[0,Œº_{\\max}]} (Œº_i^{(k)} + œÅ^{(k)} g_i(p^{(k+1)})) \\text{ for all } i=1,‚Ä¶,m,\\] where  $Œª_{\\text{min}} ‚â§ Œª_{\\text{max}}$  and  $Œº_{\\text{max}}$  are the multiplier boundaries. Next, the accuracy tolerance  $œµ$  is updated as \\[œµ^{(k)}=\\max\\{œµ_{\\min}, Œ∏_œµ œµ^{(k-1)}\\},\\] where  $œµ_{\\text{min}}$  is the lowest value  $œµ$  is allowed to become and  $Œ∏_œµ ‚àà (0,1)$  is constant scaling factor. Last, the penalty parameter  $œÅ$  is updated as follows: with \\[œÉ^{(k)}=\\max_{j=1,‚Ä¶,p, i=1,‚Ä¶,m} \\{\\|h_j(p^{(k)})\\|, \\|\\max_{i=1,‚Ä¶,m}\\{g_i(p^{(k)}), -\\frac{Œº_i^{(k-1)}}{œÅ^{(k-1)}} \\}\\| \\}.\\] œÅ  is updated as \\[œÅ^{(k)} = \\begin{cases}\nœÅ^{(k-1)}/Œ∏_œÅ,  & \\text{if } œÉ^{(k)}\\leq Œ∏_œÅ œÉ^{(k-1)} ,\\\\\nœÅ^{(k-1)}, & \\text{else,}\n\\end{cases}\\] where  $Œ∏_œÅ ‚àà (0,1)$  is a constant scaling factor. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place Optional (if not called with the  ConstrainedManifoldObjective cmo ) g=nothing : the inequality constraints h=nothing : the equality constraints grad_g=nothing : the gradient of the inequality constraints grad_h=nothing : the gradient of the equality constraints Note that one of the pairs ( g ,  grad_g ) or ( h ,  grad_h ) has to be provided. Otherwise the problem is not constrained and a better solver would be for example  quasi_Newton . Keyword Arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. œµ=1e-3 :           the accuracy tolerance œµ_min=1e-6 :       the lower bound for the accuracy tolerance œµ_exponent=1/100 : exponent of the œµ update factor; also 1/number of iterations until maximal accuracy is needed to end algorithm naturally equality_constraints=nothing : the number  $n$  of equality constraints. If not provided, a call to the gradient of  g  is performed to estimate these. gradient_range=nothing : specify how both gradients of the constraints are represented gradient_equality_range=gradient_range :  specify how gradients of the equality constraints are represented, see  VectorGradientFunction . gradient_inequality_range=gradient_range :  specify how gradients of the inequality constraints are represented, see  VectorGradientFunction . inequality_constraints=nothing : the number  $m$  of inequality constraints.  If not provided, a call to the gradient of  g  is performed to estimate these. Œª=ones(size(h(M,x),1)) : the Lagrange multiplier with respect to the equality constraints Œª_max=20.0 :       an upper bound for the Lagrange multiplier belonging to the equality constraints Œª_min=- Œª_max :    a lower bound for the Lagrange multiplier belonging to the equality constraints Œº=ones(size(h(M,x),1)) : the Lagrange multiplier with respect to the inequality constraints Œº_max=20.0 : an upper bound for the Lagrange multiplier belonging to the inequality constraints œÅ=1.0 :            the penalty parameter œÑ=0.8 :            factor for the improvement of the evaluation of the penalty parameter Œ∏_œÅ=0.3 :          the scaling factor of the penalty parameter Œ∏_œµ=(œµ_min / œµ)^(œµ_exponent) : the scaling factor of the exactness sub_cost=[ AugmentedLagrangianCost¬± (@ref) (cmo, œÅ, Œº, Œª):  use augmented Lagrangian cost, based on the  ConstrainedManifoldObjective  build from the functions provided.  This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_grad=[ AugmentedLagrangianGrad ](@ref) (cmo, œÅ, Œº, Œª) : use augmented Lagrangian gradient, based on the [ ConstrainedManifoldObjective ](@ref) build from the functions provided. This is used to define the sub problem= keyword and has hence no effect, if you set sub problem` directly. sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. stopping_criterion= StopAfterIteration (300) | ( StopWhenSmallerOrEqual (:œµ, œµ_min) & StopWhenChangeLess (1e-10) ) | StopWhenChangeLess `: a functor indicating that the stopping criterion is fulfilled sub_problem= DefaultManoptProblem (M, sub_objective) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= QuasiNewtonState :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function.as the quasi newton method, the  QuasiNewtonLimitedMemoryDirectionUpdate  with  InverseBFGS  is used. sub_stopping_criterion::StoppingCriterion= StopAfterIteration (300) | StopWhenGradientNormLess (œµ) | StopWhenStepsizeLess (1e-8) , For the  range s of the constraints' gradient, other power manifold tangent space representations, mainly the  ArrayPowerRepresentation  can be used if the gradients can be computed more efficiently in that representation. All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2917,"pagetitle":"Augmented Lagrangian Method","title":"State","ref":"/manopt/stable/solvers/augmented_Lagrangian_method/#State","content":" State"},{"id":2918,"pagetitle":"Augmented Lagrangian Method","title":"Manopt.AugmentedLagrangianMethodState","ref":"/manopt/stable/solvers/augmented_Lagrangian_method/#Manopt.AugmentedLagrangianMethodState","content":" Manopt.AugmentedLagrangianMethodState  ‚Äî  Type AugmentedLagrangianMethodState{P,T} <: AbstractManoptSolverState Describes the augmented Lagrangian method, with Fields a default value is given in brackets if a parameter can be left out in initialization. œµ :     the accuracy tolerance œµ_min : the lower bound for the accuracy tolerance Œª :     the Lagrange multiplier with respect to the equality constraints Œª_max : an upper bound for the Lagrange multiplier belonging to the equality constraints Œª_min : a lower bound for the Lagrange multiplier belonging to the equality constraints p::P : a point on the manifold  $\\mathcal M$  storing the current iterate penalty : evaluation of the current penalty term, initialized to  Inf . Œº :     the Lagrange multiplier with respect to the inequality constraints Œº_max : an upper bound for the Lagrange multiplier belonging to the inequality constraints œÅ :     the penalty parameter sub_problem::Union{AbstractManoptProblem, F} :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state::Union{AbstractManoptProblem, F} :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. œÑ :     factor for the improvement of the evaluation of the penalty parameter Œ∏_œÅ :   the scaling factor of the penalty parameter Œ∏_œµ :   the scaling factor of the accuracy tolerance stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled Constructor AugmentedLagrangianMethodState(M::AbstractManifold, co::ConstrainedManifoldObjective,\n    sub_problem, sub_state; kwargs...\n) construct an augmented Lagrangian method options, where the manifold  M  and the  ConstrainedManifoldObjective co  are used for manifold- or objective specific defaults. AugmentedLagrangianMethodState(M::AbstractManifold, co::ConstrainedManifoldObjective,\n    sub_problem; evaluation=AllocatingEvaluation(), kwargs...\n) construct an augmented Lagrangian method options, where the manifold  M  and the  ConstrainedManifoldObjective co  are used for manifold- or objective specific defaults, and  sub_problem  is a closed form solution with  evaluation  as type of evaluation. Keyword arguments the following keyword arguments are available to initialise the corresponding fields œµ=1e‚Äì3 œµ_min=1e-6 Œª=ones(n) :  n  is the number of equality constraints in the  ConstrainedManifoldObjective co . Œª_max=20.0 Œª_min=- Œª_max Œº=ones(m) :  m  is the number of inequality constraints in the  ConstrainedManifoldObjective co . Œº_max=20.0 p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value œÅ=1.0 œÑ=0.8 Œ∏_œÅ=0.3 Œ∏_œµ=(œµ_min/œµ)^(œµ_exponent) stopping criterion= StopAfterIteration (300) | ( StopWhenSmallerOrEqual `(:œµ, œµ min) [  &  ](@ref StopWhenAll)[ StopWhenChangeLess ](@ref) (1e-10) ) [  |  ](@ref StopWhenAny)[ StopWhenChangeLess ](@ref) . See also augmented_Lagrangian_method source"},{"id":2919,"pagetitle":"Augmented Lagrangian Method","title":"Helping functions","ref":"/manopt/stable/solvers/augmented_Lagrangian_method/#Helping-functions","content":" Helping functions"},{"id":2920,"pagetitle":"Augmented Lagrangian Method","title":"Manopt.AugmentedLagrangianCost","ref":"/manopt/stable/solvers/augmented_Lagrangian_method/#Manopt.AugmentedLagrangianCost","content":" Manopt.AugmentedLagrangianCost  ‚Äî  Type AugmentedLagrangianCost{CO,R,T} Stores the parameters  $œÅ ‚àà ‚Ñù$ ,  $Œº ‚àà ‚Ñù^m$ ,  $Œª ‚àà ‚Ñù^n$  of the augmented Lagrangian associated to the  ConstrainedManifoldObjective co . This struct is also a functor  (M,p) -> v  that can be used as a cost function within a solver, based on the internal  ConstrainedManifoldObjective  it computes \\[\\mathcal L_\\rho(p, Œº, Œª)\n= f(x) + \\frac{œÅ}{2} \\biggl(\n    \\sum_{j=1}^n \\Bigl( h_j(p) + \\frac{Œª_j}{œÅ} \\Bigr)^2\n    +\n    \\sum_{i=1}^m \\max\\Bigl\\{ 0, \\frac{Œº_i}{œÅ} + g_i(p) \\Bigr\\}^2\n\\Bigr)\\] Fields co::CO ,  œÅ::R ,  Œº::T ,  Œª::T  as mentioned in the formula, where  $R$  should be the number type used and  $T$  the vector type. Constructor AugmentedLagrangianCost(co, œÅ, Œº, Œª) source"},{"id":2921,"pagetitle":"Augmented Lagrangian Method","title":"Manopt.AugmentedLagrangianGrad","ref":"/manopt/stable/solvers/augmented_Lagrangian_method/#Manopt.AugmentedLagrangianGrad","content":" Manopt.AugmentedLagrangianGrad  ‚Äî  Type AugmentedLagrangianGrad{CO,R,T} <: AbstractConstrainedFunctor{T} Stores the parameters  $œÅ ‚àà ‚Ñù$ ,  $Œº ‚àà ‚Ñù^m$ ,  $Œª ‚àà ‚Ñù^n$  of the augmented Lagrangian associated to the  ConstrainedManifoldObjective co . This struct is also a functor in both formats (M, p) -> X  to compute the gradient in allocating fashion. (M, X, p)  to compute the gradient in in-place fashion. additionally this gradient does accept a positional last argument to specify the  range  for the internal gradient call of the constrained objective. based on the internal  ConstrainedManifoldObjective  and computes the gradient  $(_tex(:grad))$(_tex(:Cal, \"L\"))_{œÅ}(p, Œº, Œª) , see also [ AugmentedLagrangianCost`](@ref). Fields co::CO ,  œÅ::R ,  Œº::T ,  Œª::T  as mentioned in the formula, where  $R$  should be the number type used and  $T$  the vector type. Constructor AugmentedLagrangianGrad(co, œÅ, Œº, Œª) source"},{"id":2922,"pagetitle":"Augmented Lagrangian Method","title":"Technical details","ref":"/manopt/stable/solvers/augmented_Lagrangian_method/#sec-agd-technical-details","content":" Technical details The  augmented_Lagrangian_method  solver requires the following functions of a manifold to be available A  copyto! (M, q, p)  and  copy (M,p)  for points. Everything the subsolver requires, which by default is the  quasi_Newton  method A  zero_vector (M,p) ."},{"id":2923,"pagetitle":"Augmented Lagrangian Method","title":"Literature","ref":"/manopt/stable/solvers/augmented_Lagrangian_method/#Literature","content":" Literature [LB19] C.¬†Liu and N.¬†Boumal.  Simple algorithms for optimization on Riemannian manifolds with constraints .  Applied¬†Mathematics¬†&¬†Optimization  (2019),  arXiv:1091.10000 ."},{"id":2926,"pagetitle":"CMA-ES","title":"Covariance matrix adaptation evolutionary strategy","ref":"/manopt/stable/solvers/cma_es/#Covariance-matrix-adaptation-evolutionary-strategy","content":" Covariance matrix adaptation evolutionary strategy The CMA-ES algorithm has been implemented based on [ Han23 ] with basic Riemannian adaptations, related to transport of covariance matrix and its update vectors. Other attempts at adapting CMA-ES to Riemannian optimization include [ CFFS10 ]. The algorithm is suitable for global optimization. Covariance matrix transport between consecutive mean points is handled by  eigenvector_transport!  function which is based on the idea of transport of matrix eigenvectors."},{"id":2927,"pagetitle":"CMA-ES","title":"Manopt.cma_es","ref":"/manopt/stable/solvers/cma_es/#Manopt.cma_es","content":" Manopt.cma_es  ‚Äî  Function cma_es(M, f, p_m=rand(M); œÉ::Real=1.0, kwargs...) Perform covariance matrix adaptation evolutionary strategy search for global gradient-free randomized optimization. It is suitable for complicated non-convex functions. It can be reasonably expected to find global minimum within 3œÉ distance from  p_m . Implementation is based on [ Han23 ] with basic adaptations to the Riemannian setting. Input M :      a manifold  $\\mathcal M$ f :      a cost function  $f: \\mathcal M‚Üí‚Ñù$  to find a minimizer  $p^*$  for Keyword arguments p_m= rand (M) : an initial point  p œÉ=1.0 : initial standard deviation Œª :                  ( 4 + Int(floor(3 * log(manifold_dimension(M)))) population size (can be increased for a more thorough global search but decreasing is not recommended) tol_fun=1e-12 : tolerance for the  StopWhenPopulationCostConcentrated , similar to absolute difference between function values at subsequent points tol_x=1e-12 : tolerance for the  StopWhenPopulationStronglyConcentrated , similar to absolute difference between subsequent point but actually computed from distribution parameters. stopping_criterion= default_cma_es_stopping_criterion(M, Œª; tol_fun=tol_fun, tol_x=tol_x) : a functor indicating that the stopping criterion is fulfilled retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports basis                ( DefaultOrthonormalBasis() ) basis used to represent covariance in rng=default_rng() : random number generator for generating new points on  M All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2928,"pagetitle":"CMA-ES","title":"State","ref":"/manopt/stable/solvers/cma_es/#State","content":" State"},{"id":2929,"pagetitle":"CMA-ES","title":"Manopt.CMAESState","ref":"/manopt/stable/solvers/cma_es/#Manopt.CMAESState","content":" Manopt.CMAESState  ‚Äî  Type CMAESState{P,T} <: AbstractManoptSolverState State of covariance matrix adaptation evolution strategy. Fields p::P : a point on the manifold  $\\mathcal M$  storing the best point found so far p_obj                        objective value at  p Œº                            parent number Œª                            population size Œº_eff                        variance effective selection mass for the mean c_1                          learning rate for the rank-one update c_c                          decay rate for cumulation path for the rank-one update c_Œº                          learning rate for the rank-Œº update c_œÉ                          decay rate for the cumulation path for the step-size control c_m                          learning rate for the mean d_œÉ                          damping parameter for step-size update population                   population of the current generation ys_c                         coordinates of random vectors for the current generation covariance_matrix            coordinates of the covariance matrix covariance_matrix_eigen      eigen decomposition of  covariance_matrix covariance_matrix_cond       condition number of  covariance_matrix , updated after eigen decomposition best_fitness_current_gen     best fitness value of individuals in the current generation median_fitness_current_gen   median fitness value of individuals in the current generation worst_fitness_current_gen    worst fitness value of individuals in the current generation p_m                          point around which the search for new candidates is done œÉ                            step size p_œÉ                          coordinates of a vector in  $T_{p_m}\\mathcal M$ p_c                          coordinates of a vector in  $T_{p_m}\\mathcal M$ deviations                   standard deviations of coordinate RNG buffer                       buffer for random number generation and  wmean_y_c  of length  n_coords e_mv_norm                    expected value of norm of the  n_coords -variable standard normal distribution recombination_weights        recombination weights used for updating covariance matrix retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports basis                        a real coefficient basis for covariance matrix rng                          RNG for generating new points Constructor CMAESState(\n    M::AbstractManifold,\n    p_m::P,\n    Œº::Int,\n    Œª::Int,\n    Œº_eff::TParams,\n    c_1::TParams,\n    c_c::TParams,\n    c_Œº::TParams,\n    c_œÉ::TParams,\n    c_m::TParams,\n    d_œÉ::TParams,\n    stop::TStopping,\n    covariance_matrix::Matrix{TParams},\n    œÉ::TParams,\n    recombination_weights::Vector{TParams};\n    retraction_method::TRetraction=default_retraction_method(M, typeof(p_m)),\n    vector_transport_method::TVTM=default_vector_transport_method(M, typeof(p_m)),\n    basis::TB=default_basis(M, typeof(p_m)),\n    rng::TRng=default_rng(),\n) where {\n    P,\n    TParams<:Real,\n    TStopping<:StoppingCriterion,\n    TRetraction<:AbstractRetractionMethod,\n    TVTM<:AbstractVectorTransportMethod,\n    TB<:AbstractBasis,\n    TRng<:AbstractRNG,\n} See also cma_es source"},{"id":2930,"pagetitle":"CMA-ES","title":"Stopping criteria","ref":"/manopt/stable/solvers/cma_es/#Stopping-criteria","content":" Stopping criteria"},{"id":2931,"pagetitle":"CMA-ES","title":"Manopt.StopWhenBestCostInGenerationConstant","ref":"/manopt/stable/solvers/cma_es/#Manopt.StopWhenBestCostInGenerationConstant","content":" Manopt.StopWhenBestCostInGenerationConstant  ‚Äî  Type StopWhenBestCostInGenerationConstant <: StoppingCriterion Stop if the range of the best objective function values of the last  iteration_range  generations is zero. This corresponds to  EqualFUnValues  condition from [ Han23 ]. See also  StopWhenPopulationCostConcentrated . source"},{"id":2932,"pagetitle":"CMA-ES","title":"Manopt.StopWhenCovarianceIllConditioned","ref":"/manopt/stable/solvers/cma_es/#Manopt.StopWhenCovarianceIllConditioned","content":" Manopt.StopWhenCovarianceIllConditioned  ‚Äî  Type StopWhenCovarianceIllConditioned <: StoppingCriterion Stop CMA-ES if condition number of covariance matrix exceeds  threshold . This corresponds to  ConditionCov  condition from [ Han23 ]. source"},{"id":2933,"pagetitle":"CMA-ES","title":"Manopt.StopWhenEvolutionStagnates","ref":"/manopt/stable/solvers/cma_es/#Manopt.StopWhenEvolutionStagnates","content":" Manopt.StopWhenEvolutionStagnates  ‚Äî  Type StopWhenEvolutionStagnates{TParam<:Real} <: StoppingCriterion The best and median fitness in each iteration is tracked over the last 20% but at least  min_size  and no more than  max_size  iterations. Solver is stopped if in both histories the median of the most recent  fraction  of values is not better than the median of the oldest  fraction . source"},{"id":2934,"pagetitle":"CMA-ES","title":"Manopt.StopWhenPopulationCostConcentrated","ref":"/manopt/stable/solvers/cma_es/#Manopt.StopWhenPopulationCostConcentrated","content":" Manopt.StopWhenPopulationCostConcentrated  ‚Äî  Type StopWhenPopulationCostConcentrated{TParam<:Real} <: StoppingCriterion Stop if the range of the best objective function value in the last  max_size  generations and all function values in the current generation is below  tol . This corresponds to  TolFun  condition from [ Han23 ]. Constructor StopWhenPopulationCostConcentrated(tol::Real, max_size::Int) source"},{"id":2935,"pagetitle":"CMA-ES","title":"Manopt.StopWhenPopulationDiverges","ref":"/manopt/stable/solvers/cma_es/#Manopt.StopWhenPopulationDiverges","content":" Manopt.StopWhenPopulationDiverges  ‚Äî  Type StopWhenPopulationDiverges{TParam<:Real} <: StoppingCriterion Stop if  œÉ  times maximum deviation increased by more than  tol . This usually indicates a far too small  œÉ , or divergent behavior. This corresponds to  TolXUp  condition from [ Han23 ]. source"},{"id":2936,"pagetitle":"CMA-ES","title":"Manopt.StopWhenPopulationStronglyConcentrated","ref":"/manopt/stable/solvers/cma_es/#Manopt.StopWhenPopulationStronglyConcentrated","content":" Manopt.StopWhenPopulationStronglyConcentrated  ‚Äî  Type StopWhenPopulationStronglyConcentrated{TParam<:Real} <: StoppingCriterion Stop if the standard deviation in all coordinates is smaller than  tol  and norm of  œÉ * p_c  is smaller than  tol . This corresponds to  TolX  condition from [ Han23 ]. Fields tol  the tolerance to verify against at_iteration  an internal field to indicate at with iteration  $i \\geq 0$  the tolerance was met. Constructor StopWhenPopulationStronglyConcentrated(tol::Real) source"},{"id":2937,"pagetitle":"CMA-ES","title":"Technical details","ref":"/manopt/stable/solvers/cma_es/#sec-cma-es-technical-details","content":" Technical details The  cma_es  solver requires the following functions of a manifold to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified. A  vector_transport_to! M, Y, p, X, q) ; it is recommended to set the  default_vector_transport_method  to a favourite retraction. If this default is set, a  vector_transport_method=  does not have to be specified. A  copyto! (M, q, p)  and  copy (M,p)  for points and similarly  copy(M, p, X)  for tangent vectors. get_coordinates! (M, Y, p, X, b)  and  get_vector! (M, X, p, c, b)  with respect to the  AbstractBasis b  provided, which is  DefaultOrthonormalBasis  by default from the  basis=  keyword. An  is_flat (M) ."},{"id":2938,"pagetitle":"CMA-ES","title":"Internal helpers","ref":"/manopt/stable/solvers/cma_es/#Internal-helpers","content":" Internal helpers You may add new methods to  eigenvector_transport!  if you know a more optimized implementation for your manifold."},{"id":2939,"pagetitle":"CMA-ES","title":"Manopt.eigenvector_transport!","ref":"/manopt/stable/solvers/cma_es/#Manopt.eigenvector_transport!","content":" Manopt.eigenvector_transport!  ‚Äî  Function eigenvector_transport!(\n    M::AbstractManifold,\n    matrix_eigen::Eigen,\n    p,\n    q,\n    basis::AbstractBasis,\n    vtm::AbstractVectorTransportMethod,\n) Transport the matrix with  matrix_eig  eigen decomposition when expanded in  basis  from point  p  to point  q  on  M . Update  matrix_eigen  in-place. (p, matrix_eig)  belongs to the fiber bundle of  $B = \\mathcal M √ó SPD(n)$ , where  n  is the (real) dimension of  M . The function corresponds to the Ehresmann connection defined by vector transport  vtm  of eigenvectors of  matrix_eigen . source"},{"id":2940,"pagetitle":"CMA-ES","title":"Literature","ref":"/manopt/stable/solvers/cma_es/#Literature","content":" Literature [CFFS10] S.¬†Colutto, F.¬†Fruhauf, M.¬†Fuchs and O.¬†Scherzer.  The CMA-ES on Riemannian Manifolds to Reconstruct Shapes in 3-D Voxel Images .  IEEE¬†Transactions¬†on¬†Evolutionary¬†Computation  14 , 227‚Äì245  (2010). [Han23] N.¬†Hansen.  The CMA Evolution Strategy: A Tutorial . ArXiv¬†Preprint (2023)."},{"id":2943,"pagetitle":"Conjugate gradient descent","title":"Conjugate gradient descent","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Conjugate-gradient-descent","content":" Conjugate gradient descent"},{"id":2944,"pagetitle":"Conjugate gradient descent","title":"Manopt.conjugate_gradient_descent","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.conjugate_gradient_descent","content":" Manopt.conjugate_gradient_descent  ‚Äî  Function conjugate_gradient_descent(M, f, grad_f, p=rand(M))\nconjugate_gradient_descent!(M, f, grad_f, p)\nconjugate_gradient_descent(M, gradient_objective, p)\nconjugate_gradient_descent!(M, gradient_objective, p; kwargs...) perform a conjugate gradient based descent- \\[p_{k+1} = \\operatorname{retr}_{p_k} \\bigl( s_kŒ¥_k \\bigr),\\] where  $\\operatorname{retr}$  denotes a retraction on the  Manifold M  and one can employ different rules to update the descent direction  $Œ¥_k$  based on the last direction  $Œ¥_{k-1}$  and both gradients  $\\operatorname{grad}f(x_k)$ , $\\operatorname{grad} f(x_{k-1})$ . The  Stepsize $s_k$  may be determined by a  Linesearch . Alternatively to  f  and  grad_f  you can provide the  AbstractManifoldFirstOrderObjective gradient_objective  directly. Available update rules are  SteepestDescentCoefficientRule , which yields a  gradient_descent ,  ConjugateDescentCoefficient  (the default),  DaiYuanCoefficientRule ,  FletcherReevesCoefficient ,  HagerZhangCoefficient ,  HestenesStiefelCoefficient ,  LiuStoreyCoefficient , and  PolakRibiereCoefficient . These can all be combined with a  ConjugateGradientBealeRestartRule  rule. They all compute  $Œ≤_k$  such that this algorithm updates the search direction as \\[Œ¥_k=\\operatorname{grad}f(p_k) + Œ≤_k \\delta_{k-1}\\] Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place p : a point on the manifold  $\\mathcal M$ Keyword arguments coefficient::DirectionUpdateRule= ConjugateDescentCoefficient () : rule to compute the descent direction update coefficient  $Œ≤_k$ , as a functor, where the resulting function maps are  (amp, cgs, k) -> Œ≤  with  amp  an  AbstractManoptProblem ,  cgs  is the  ConjugateGradientDescentState , and  k  is the current iterate. differential= nothing : specify a specific function to evaluate the differential. By default,  $Df(p)[X] = ‚ü®\\operatorname{grad}f(p),X‚ü©$ . is used evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= ArmijoLinesearch () : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (500) | StopWhenGradientNormLess (1e-8) : a functor indicating that the stopping criterion is fulfilled vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports If you provide the  ManifoldFirstOrderObjective  directly, the  evaluation=  keyword is ignored. The decorations are still applied to the objective. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2945,"pagetitle":"Conjugate gradient descent","title":"Manopt.conjugate_gradient_descent!","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.conjugate_gradient_descent!","content":" Manopt.conjugate_gradient_descent!  ‚Äî  Function conjugate_gradient_descent(M, f, grad_f, p=rand(M))\nconjugate_gradient_descent!(M, f, grad_f, p)\nconjugate_gradient_descent(M, gradient_objective, p)\nconjugate_gradient_descent!(M, gradient_objective, p; kwargs...) perform a conjugate gradient based descent- \\[p_{k+1} = \\operatorname{retr}_{p_k} \\bigl( s_kŒ¥_k \\bigr),\\] where  $\\operatorname{retr}$  denotes a retraction on the  Manifold M  and one can employ different rules to update the descent direction  $Œ¥_k$  based on the last direction  $Œ¥_{k-1}$  and both gradients  $\\operatorname{grad}f(x_k)$ , $\\operatorname{grad} f(x_{k-1})$ . The  Stepsize $s_k$  may be determined by a  Linesearch . Alternatively to  f  and  grad_f  you can provide the  AbstractManifoldFirstOrderObjective gradient_objective  directly. Available update rules are  SteepestDescentCoefficientRule , which yields a  gradient_descent ,  ConjugateDescentCoefficient  (the default),  DaiYuanCoefficientRule ,  FletcherReevesCoefficient ,  HagerZhangCoefficient ,  HestenesStiefelCoefficient ,  LiuStoreyCoefficient , and  PolakRibiereCoefficient . These can all be combined with a  ConjugateGradientBealeRestartRule  rule. They all compute  $Œ≤_k$  such that this algorithm updates the search direction as \\[Œ¥_k=\\operatorname{grad}f(p_k) + Œ≤_k \\delta_{k-1}\\] Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place p : a point on the manifold  $\\mathcal M$ Keyword arguments coefficient::DirectionUpdateRule= ConjugateDescentCoefficient () : rule to compute the descent direction update coefficient  $Œ≤_k$ , as a functor, where the resulting function maps are  (amp, cgs, k) -> Œ≤  with  amp  an  AbstractManoptProblem ,  cgs  is the  ConjugateGradientDescentState , and  k  is the current iterate. differential= nothing : specify a specific function to evaluate the differential. By default,  $Df(p)[X] = ‚ü®\\operatorname{grad}f(p),X‚ü©$ . is used evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= ArmijoLinesearch () : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (500) | StopWhenGradientNormLess (1e-8) : a functor indicating that the stopping criterion is fulfilled vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports If you provide the  ManifoldFirstOrderObjective  directly, the  evaluation=  keyword is ignored. The decorations are still applied to the objective. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2946,"pagetitle":"Conjugate gradient descent","title":"State","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#State","content":" State"},{"id":2947,"pagetitle":"Conjugate gradient descent","title":"Manopt.ConjugateGradientDescentState","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.ConjugateGradientDescentState","content":" Manopt.ConjugateGradientDescentState  ‚Äî  Type ConjugateGradientState <: AbstractGradientSolverState specify options for a conjugate gradient descent algorithm, that solves a [ DefaultManoptProblem ]. Fields p::P : a point on the manifold  $\\mathcal M$  storing the current iterate X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ Œ¥ :                       the current descent direction, also a tangent vector Œ≤ :                       the current update coefficient rule, see . coefficient :             function to determine the new  Œ≤ stepsize::Stepsize : a functor inheriting from  Stepsize  to determine a step size stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Constructor ConjugateGradientState(M::AbstractManifold; kwargs...) where the last five fields can be set by their names as keyword and the  X  can be set to a tangent vector type using the keyword  initial_gradient  which defaults to  zero_vector(M,p) , and  Œ¥  is initialized to a copy of this vector. Keyword arguments The following fields from above <re keyword arguments initial_gradient= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value coefficient=[ ConjugateDescentCoefficient ](@ref) () : specify a CG coefficient, see also the [ ManifoldDefaultsFactory`](@ref). stepsize= default_stepsize (M, ConjugateGradientDescentState; retraction_method=retraction_method) : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (500) | StopWhenGradientNormLess (1e-8) ): a functor indicating that the stopping criterion is fulfilled retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports See also conjugate_gradient_descent ,  DefaultManoptProblem ,  ArmijoLinesearch source"},{"id":2948,"pagetitle":"Conjugate gradient descent","title":"Available coefficients","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#cg-coeffs","content":" Available coefficients The update rules act as  DirectionUpdateRule , which internally always first evaluate the gradient itself."},{"id":2949,"pagetitle":"Conjugate gradient descent","title":"Manopt.ConjugateDescentCoefficient","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.ConjugateDescentCoefficient","content":" Manopt.ConjugateDescentCoefficient  ‚Äî  Function ConjugateDescentCoefficient()\nConjugateDescentCoefficient(M::AbstractManifold) Compute the (classical) conjugate gradient coefficient based on [ Fle87 ] adapted to manifolds Denote the last iterate and gradient by  $p_k,X_k$ , the current iterate and gradient by  $p_{k+1}, X_{k+1}$ , respectively, as well as the last update direction by  $Œ¥_k$ . Then the coefficient reads \\[Œ≤_k = \\frac{\\mathrm{D}_{}f(p_{k+1})[X_{k+1}]}{\\mathrm{D}_{}f(p_k)[-Œ¥_k]}\n = \\frac{\\lVert X_{k+1} \\rVert_{p_{k+1}}^2}{‚ü®-Œ¥_k,X_k‚ü©_{p_k}}\\] The second one it the one usually stated, while the first one avoids to use the metric  inner . The first one is implemented here, but falls back to calling  inner  if there is no dedicated differential available. Info This function generates a  ManifoldDefaultsFactory  for  ConjugateDescentCoefficientRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2950,"pagetitle":"Conjugate gradient descent","title":"Manopt.ConjugateGradientBealeRestart","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.ConjugateGradientBealeRestart","content":" Manopt.ConjugateGradientBealeRestart  ‚Äî  Function ConjugateGradientBealeRestart(direction_update::Union{DirectionUpdateRule,ManifoldDefaultsFactory}; kwargs...)\nConjugateGradientBealeRestart(M::AbstractManifold, direction_update::Union{DirectionUpdateRule,ManifoldDefaultsFactory}; kwargs...) Compute a conjugate gradient coefficient with a potential restart, when two directions are nearly orthogonal. See [ HZ06 , page 12] (in the preprint, page 46 in Journal page numbers). This method is named after E. Beale from his proceedings paper in 1972 [ Bea72 ]. This method acts as a  decorator  to any existing  DirectionUpdateRule direction_update . Denote the last iterate and gradient by  $p_k,X_k$ , the current iterate and gradient by  $p_{k+1}, X_{k+1}$ , respectively, as well as the last update direction by  $Œ¥_k$ . Then a restart is performed, hence  $Œ≤_k = 0$  returned if \\[  \\frac{‚ü®X_{k+1}, \\mathcal T_{p_{k+1}‚Üêp_k}X_k‚ü©}{\\lVert X_k \\rVert_{p_k}} > Œµ,\\] where  $Œµ$  is the  threshold , which is set by default to  0.2 , see [ Pow77 ] Input direction_update : a  DirectionUpdateRule  or a corresponding  ManifoldDefaultsFactory  to produce such a rule. Keyword arguments vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports threshold=0.2 Info This function generates a  ManifoldDefaultsFactory  for  ConjugateGradientBealeRestartRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2951,"pagetitle":"Conjugate gradient descent","title":"Manopt.DaiYuanCoefficient","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.DaiYuanCoefficient","content":" Manopt.DaiYuanCoefficient  ‚Äî  Function DaiYuanCoefficient(; kwargs...)\nDaiYuanCoefficient(M::AbstractManifold; kwargs...) Computes an update coefficient for the  conjugate_gradient_descent  algorithm based on [ DY99 ] adapted to Riemannian manifolds. Denote the last iterate and gradient by  $p_k,X_k$ , the current iterate and gradient by  $p_{k+1}, X_{k+1}$ , respectively, as well as the last update direction by  $Œ¥_k$ . Let  $ŒΩ_k = X_{k+1} - \\mathcal T_{p_{k+1}‚Üêp_k}X_k$ , where  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  denotes a vector transport. Then the coefficient reads \\[Œ≤_k =\n=\n\\frac{\\mathrm{D}_{}f(p_{k+1})[X_{k+1}]}{‚ü®Œ¥_k,ŒΩ_k‚ü©_{p_{k+1}}}\n=\n\\frac{\\lVert X_{k+1} \\rVert_{p_{k+1}}^2}{‚ü®\\mathcal T_{p_{k+1}‚Üêp_k}Œ¥_k, ŒΩ_k‚ü©_{p_{k+1}}}\\] The second one it the one usually stated, while the first one avoids to use the metric  inner . The first one is implemented here, but falls back to calling  inner  if there is no dedicated differential available. Keyword arguments vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Info This function generates a  ManifoldDefaultsFactory  for  DaiYuanCoefficientRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2952,"pagetitle":"Conjugate gradient descent","title":"Manopt.FletcherReevesCoefficient","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.FletcherReevesCoefficient","content":" Manopt.FletcherReevesCoefficient  ‚Äî  Function FletcherReevesCoefficient()\nFletcherReevesCoefficient(M::AbstractManifold) Computes an update coefficient for the  conjugate_gradient_descent  algorithm based on [ FR64 ] adapted to manifolds Denote the last iterate and gradient by  $p_k,X_k$ , the current iterate and gradient by  $p_{k+1}, X_{k+1}$ , respectively, as well as the last update direction by  $Œ¥_k$ . Then the coefficient reads \\[Œ≤_k = \\frac{\\mathrm{D}_{}f(p_{k+1})[X_{k+1}]}{\\mathrm{D}_{}f(p_k)[X_k]}\n = \\frac{\\lVert X_{k+1} \\rVert_{p_{k+1}}^2}{\\lVert X_k \\rVert_{p_k}^2}\\] The second one it the one usually stated, while the first one avoids to use the metric  inner . The first one is implemented here, but falls back to calling  inner  if there is no dedicated differential available. Info This function generates a  ManifoldDefaultsFactory  for  FletcherReevesCoefficientRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2953,"pagetitle":"Conjugate gradient descent","title":"Manopt.HagerZhangCoefficient","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.HagerZhangCoefficient","content":" Manopt.HagerZhangCoefficient  ‚Äî  Function HagerZhangCoefficient(; kwargs...)\nHagerZhangCoefficient(M::AbstractManifold; kwargs...) Computes an update coefficient for the  conjugate_gradient_descent  algorithm based on [ FR64 ] adapted to manifolds Denote the last iterate and gradient by  $p_k,X_k$ , the current iterate and gradient by  $p_{k+1}, X_{k+1}$ , respectively, as well as the last update direction by  $Œ¥_k$ . Let  $ŒΩ_k = X_{k+1} - \\mathcal T_{p_{k+1}‚Üêp_k}X_k$ , where  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  denotes a vector transport. Then the coefficient reads \\[Œ≤_k = \\Bigl‚ü®ŒΩ_k - \\frac{2\\lVert ŒΩ_k \\rVert_{p_{k+1}}^2}{‚ü®\\mathcal T_{p_{k+1}‚Üêp_k}Œ¥_k, ŒΩ_k‚ü©_{p_{k+1}}}\n  \\mathcal T_{p_{k+1}‚Üêp_k}Œ¥_k,\n  \\frac{X_{k+1}}{‚ü®\\mathcal T_{p_{k+1}‚Üêp_k}Œ¥_k, ŒΩ_k‚ü©_{p_{k+1}}}\n\\Bigr‚ü©_{p_{k+1}}.\\] This method includes a numerical stability proposed by those authors. Keyword arguments vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Info This function generates a  ManifoldDefaultsFactory  for  HagerZhangCoefficientRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2954,"pagetitle":"Conjugate gradient descent","title":"Manopt.HestenesStiefelCoefficient","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.HestenesStiefelCoefficient","content":" Manopt.HestenesStiefelCoefficient  ‚Äî  Function HestenesStiefelCoefficient(; kwargs...)\nHestenesStiefelCoefficient(M::AbstractManifold; kwargs...) Computes an update coefficient for the  conjugate_gradient_descent  algorithm based on [ HS52 ] adapted to manifolds Denote the last iterate and gradient by  $p_k,X_k$ , the current iterate and gradient by  $p_{k+1}, X_{k+1}$ , respectively, as well as the last update direction by  $Œ¥_k$ . Let  $ŒΩ_k = X_{k+1} - \\mathcal T_{p_{k+1}‚Üêp_k}X_k$ , where  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  denotes a vector transport. Then the coefficient reads \\[\\begin{aligned}\nŒ≤_k\n&= \\frac{\\mathrm{D}_{}f(p_{k+1})[ŒΩ_k]}{\\mathrm{D}_{}f(p_{k+1})[\\mathcal T_{p_{k+1}‚Üêp_k}Œ¥_k] - \\mathrm{D}_{}f(p_k)[Œ¥_k]}\n\\\\&= \\frac{‚ü®X_{k+1},ŒΩ_k‚ü©_{p_{k+1}}}{‚ü®\\mathcal T_{p_{k+1}‚Üêp_k}Œ¥_k,X_{k+1}‚ü©_{p_{k+1}} - ‚ü®Œ¥_k,X_k‚ü©_{p_{k}}}\n\\\\&= \\frac{‚ü®X_{k+1},ŒΩ_k‚ü©_{p_{k+1}}}{‚ü®\\mathcal T_{p_{k+1}‚Üêp_k}Œ¥_k,ŒΩ_k‚ü©_{p_{k+1}}},\n\\end{aligned}\\] The third one is the one usually stated, while the first one avoids to use the metric  inner . The first one is implemented here, but falls back to calling  inner  if there is no dedicated differential available. Keyword arguments vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Info This function generates a  ManifoldDefaultsFactory  for  HestenesStiefelCoefficientRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2955,"pagetitle":"Conjugate gradient descent","title":"Manopt.LiuStoreyCoefficient","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.LiuStoreyCoefficient","content":" Manopt.LiuStoreyCoefficient  ‚Äî  Function LiuStoreyCoefficient(; kwargs...)\nLiuStoreyCoefficient(M::AbstractManifold; kwargs...) Computes an update coefficient for the  conjugate_gradient_descent  algorithm based on [ LS91 ] adapted to manifolds Denote the last iterate and gradient by  $p_k,X_k$ , the current iterate and gradient by  $p_{k+1}, X_{k+1}$ , respectively, as well as the last update direction by  $Œ¥_k$ . Let  $ŒΩ_k = X_{k+1} - \\mathcal T_{p_{k+1}‚Üêp_k}X_k$ , where  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  denotes a vector transport. Then the coefficient reads \\[Œ≤_k\n= - \\frac{\\mathrm{D}_{}f(p_{k+1})[ŒΩ_k]}{\\mathrm{D}_{}f(p_k)[Œ¥_k]}\n= - \\frac{‚ü®X_{k+1},ŒΩ_k‚ü©_{p_{k+1}}}{‚ü®Œ¥_k,X_k‚ü©_{p_k}}.\\] The second one it the one usually stated, while the first one avoids to use the metric  inner . The first one is implemented here, but falls back to calling  inner  if there is no dedicated differential available. Keyword arguments vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Info This function generates a  ManifoldDefaultsFactory  for  LiuStoreyCoefficientRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2956,"pagetitle":"Conjugate gradient descent","title":"Manopt.PolakRibiereCoefficient","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.PolakRibiereCoefficient","content":" Manopt.PolakRibiereCoefficient  ‚Äî  Function PolakRibiereCoefficient(; kwargs...)\nPolakRibiereCoefficient(M::AbstractManifold; kwargs...) Computes an update coefficient for the  conjugate_gradient_descent  algorithm based on [ PR69 ] adapted to Riemannian manifolds. Denote the last iterate and gradient by  $p_k,X_k$ , the current iterate and gradient by  $p_{k+1}, X_{k+1}$ , respectively, as well as the last update direction by  $Œ¥_k$ . Let  $ŒΩ_k = X_{k+1} - \\mathcal T_{p_{k+1}‚Üêp_k}X_k$ , where  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  denotes a vector transport. Then the coefficient reads \\[Œ≤_k\n= \\frac{\\mathrm{D}_{}f(p_{k+1})[ŒΩ_k]}{\\mathrm{D}_{}f(p_k)[X_k]}\n= \\frac{‚ü®X_{k+1},ŒΩ_k‚ü©_{p_{k+1}}}{\\lVert X_k \\rVert_{{p_k}}^2}.\\] The second one is the one usually stated, while the first one avoids to use the metric  inner . The first one is implemented here, but falls back to calling  inner  if there is no dedicated differential available. Keyword arguments vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Info This function generates a  ManifoldDefaultsFactory  for  PolakRibiereCoefficientRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2957,"pagetitle":"Conjugate gradient descent","title":"Manopt.SteepestDescentCoefficient","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.SteepestDescentCoefficient","content":" Manopt.SteepestDescentCoefficient  ‚Äî  Function SteepestDescentCoefficient()\nSteepestDescentCoefficient(M::AbstractManifold) Computes an update coefficient for the  conjugate_gradient_descent  algorithm so that is falls back to a  gradient_descent  method, that is \\[Œ≤_k = 0\\] Info This function generates a  ManifoldDefaultsFactory  for  SteepestDescentCoefficient . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2958,"pagetitle":"Conjugate gradient descent","title":"Internal rules for coefficients","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Internal-rules-for-coefficients","content":" Internal rules for coefficients"},{"id":2959,"pagetitle":"Conjugate gradient descent","title":"Manopt.ConjugateGradientBealeRestartRule","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.ConjugateGradientBealeRestartRule","content":" Manopt.ConjugateGradientBealeRestartRule  ‚Äî  Type ConjugateGradientBealeRestartRule <: DirectionUpdateRule A functor  (problem, state, k) -> Œ≤_k  to compute the conjugate gradient update coefficient based on a restart idea of [ Bea72 ], following [ HZ06 , page 12] adapted to manifolds. Fields direction_update::DirectionUpdateRule : the actual rule, that is restarted threshold::Real : a threshold for the restart check. vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Constructor ConjugateGradientBealeRestartRule(\n    direction_update::Union{DirectionUpdateRule,ManifoldDefaultsFactory};\n    kwargs...\n)\nConjugateGradientBealeRestartRule(\n    M::AbstractManifold=DefaultManifold(),\n    direction_update::Union{DirectionUpdateRule,ManifoldDefaultsFactory};\n    kwargs...\n) Construct the Beale restart coefficient update rule adapted to manifolds. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$  If this is not provided, the  DefaultManifold()  from  ManifoldsBase.jl  is used. direction_update : a  DirectionUpdateRule  or a corresponding  ManifoldDefaultsFactory  to produce such a rule. Keyword arguments vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports threshold=0.2 See also ConjugateGradientBealeRestart ,  conjugate_gradient_descent source"},{"id":2960,"pagetitle":"Conjugate gradient descent","title":"Manopt.DaiYuanCoefficientRule","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.DaiYuanCoefficientRule","content":" Manopt.DaiYuanCoefficientRule  ‚Äî  Type DaiYuanCoefficientRule <: DirectionUpdateRule A functor  (problem, state, k) -> Œ≤_k  to compute the conjugate gradient update coefficient based on [ DY99 ] adapted to manifolds Fields vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Constructor DaiYuanCoefficientRule(M::AbstractManifold; kwargs...) Construct the Dai‚ÄîYuan coefficient update rule. Keyword arguments vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports See also DaiYuanCoefficient ,  conjugate_gradient_descent source"},{"id":2961,"pagetitle":"Conjugate gradient descent","title":"Manopt.FletcherReevesCoefficientRule","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.FletcherReevesCoefficientRule","content":" Manopt.FletcherReevesCoefficientRule  ‚Äî  Type FletcherReevesCoefficientRule <: DirectionUpdateRule A functor  (problem, state, k) -> Œ≤_k  to compute the conjugate gradient update coefficient based on [ FR64 ] adapted to manifolds Constructor FletcherReevesCoefficientRule() Construct the Fletcher‚ÄîReeves coefficient update rule. See also FletcherReevesCoefficient ,  conjugate_gradient_descent source"},{"id":2962,"pagetitle":"Conjugate gradient descent","title":"Manopt.HagerZhangCoefficientRule","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.HagerZhangCoefficientRule","content":" Manopt.HagerZhangCoefficientRule  ‚Äî  Type HagerZhangCoefficientRule <: DirectionUpdateRule A functor  (problem, state, k) -> Œ≤_k  to compute the conjugate gradient update coefficient based on [ HZ05 ] adapted to manifolds Fields vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Constructor HagerZhangCoefficientRule(M::AbstractManifold; kwargs...) Construct the Hager-Zhang coefficient update rule based on [ HZ05 ] adapted to manifolds. Keyword arguments vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports See also HagerZhangCoefficient ,  conjugate_gradient_descent source"},{"id":2963,"pagetitle":"Conjugate gradient descent","title":"Manopt.HestenesStiefelCoefficientRule","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.HestenesStiefelCoefficientRule","content":" Manopt.HestenesStiefelCoefficientRule  ‚Äî  Type HestenesStiefelCoefficientRuleRule <: DirectionUpdateRule A functor  (problem, state, k) -> Œ≤_k  to compute the conjugate gradient update coefficient based on [ HS52 ] adapted to manifolds Fields vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Constructor HestenesStiefelCoefficientRuleRule(M::AbstractManifold; kwargs...) Construct the Hestenes-Stiefel coefficient update rule based on [ HS52 ] adapted to manifolds. Keyword arguments vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports See also HestenesStiefelCoefficient ,  conjugate_gradient_descent source"},{"id":2964,"pagetitle":"Conjugate gradient descent","title":"Manopt.LiuStoreyCoefficientRule","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.LiuStoreyCoefficientRule","content":" Manopt.LiuStoreyCoefficientRule  ‚Äî  Type LiuStoreyCoefficientRule <: DirectionUpdateRule A functor  (problem, state, k) -> Œ≤_k  to compute the conjugate gradient update coefficient based on [ LS91 ] adapted to manifolds Fields vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Constructor LiuStoreyCoefficientRule(M::AbstractManifold; kwargs...) Construct the Lui-Storey coefficient update rule based on [ LS91 ] adapted to manifolds. Keyword arguments vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports See also LiuStoreyCoefficient ,  conjugate_gradient_descent source"},{"id":2965,"pagetitle":"Conjugate gradient descent","title":"Manopt.PolakRibiereCoefficientRule","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.PolakRibiereCoefficientRule","content":" Manopt.PolakRibiereCoefficientRule  ‚Äî  Type PolakRibiereCoefficientRule <: DirectionUpdateRule A functor  (problem, state, k) -> Œ≤_k  to compute the conjugate gradient update coefficient based on [ PR69 ] adapted to manifolds Fields vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Constructor PolakRibiereCoefficientRule(M::AbstractManifold; kwargs...) Construct the Dai‚ÄîYuan coefficient update rule. Keyword arguments vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports See also PolakRibiereCoefficient ,  conjugate_gradient_descent source"},{"id":2966,"pagetitle":"Conjugate gradient descent","title":"Manopt.SteepestDescentCoefficientRule","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Manopt.SteepestDescentCoefficientRule","content":" Manopt.SteepestDescentCoefficientRule  ‚Äî  Type SteepestDescentCoefficientRule <: DirectionUpdateRule A functor  (problem, state, k) -> Œ≤_k  to compute the conjugate gradient update coefficient to obtain the steepest direction, that is  $Œ≤_k=0$ . Constructor SteepestDescentCoefficientRule() Construct the steepest descent coefficient update rule. See also SteepestDescentCoefficient ,  conjugate_gradient_descent source"},{"id":2967,"pagetitle":"Conjugate gradient descent","title":"Technical details","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#sec-cgd-technical-details","content":" Technical details The  conjugate_gradient_descent  solver requires the following functions of a manifold to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified. A  vector_transport_to! M, Y, p, X, q) ; it is recommended to set the  default_vector_transport_method  to a favourite retraction. If this default is set, a  vector_transport_method=  or  vector_transport_method_dual=  (for  $\\mathcal N$ ) does not have to be specified. By default gradient descent uses  ArmijoLinesearch  which requires  max_stepsize (M)  to be set and an implementation of  inner (M, p, X) . By default the stopping criterion uses the  norm  as well, to stop when the norm of the gradient is small, but if you implemented  inner , the norm is provided already. By default the tangent vector storing the gradient is initialized calling  zero_vector (M,p) ."},{"id":2968,"pagetitle":"Conjugate gradient descent","title":"Literature","ref":"/manopt/stable/solvers/conjugate_gradient_descent/#Literature","content":" Literature [Bea72] E.¬†M.¬†Beale.  A derivation of conjugate gradients . In:  Numerical methods for nonlinear optimization , edited by F.¬†A.¬†Lootsma (Academic Press, London, London, 1972); pp.¬†39‚Äì43. [DY99] Y.¬†H.¬†Dai and Y.¬†Yuan.  A Nonlinear Conjugate Gradient Method with a Strong Global Convergence Property .  SIAM¬†Journal¬†on¬†Optimization  10 , 177‚Äì182  (1999). [Fle87] R.¬†Fletcher.  Practical Methods of Optimization . 2¬†Edition,  A Wiley-Interscience Publication  (John Wiley & Sons Ltd., 1987). [FR64] R.¬†Fletcher and C.¬†M.¬†Reeves.  Function minimization by conjugate gradients .  The¬†Computer¬†Journal  7 , 149‚Äì154  (1964). [HZ06] W.¬†W.¬†Hager and H.¬†Zhang.  A survey of nonlinear conjugate gradient methods . Pacific¬†Journal¬†of¬†Optimization  2 , 35‚Äì58 (2006). [HZ05] W.¬†W.¬†Hager and H.¬†Zhang.  A New Conjugate Gradient Method with Guaranteed Descent and an Efficient Line Search .  SIAM¬†Journal¬†on¬†Optimization  16 , 170‚Äì192  (2005). [HS52] M.¬†Hestenes and E.¬†Stiefel.  Methods of conjugate gradients for solving linear systems .  Journal¬†of¬†Research¬†of¬†the¬†National¬†Bureau¬†of¬†Standards  49 , 409  (1952). [LS91] Y.¬†Liu and C.¬†Storey.  Efficient generalized conjugate gradient algorithms,  part 1: Theory .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  69 , 129‚Äì137  (1991). [PR69] E.¬†Polak and G.¬†Ribi√®re.  Note sur la convergence de m√©thodes de directions conjugu√©es .  Revue¬†fran√ßaise¬†d‚Äôinformatique¬†et¬†de¬†recherche¬†op√©rationnelle  3 , 35‚Äì43  (1969). [Pow77] M.¬†J.¬†Powell.  Restart procedures for the conjugate gradient method .  Mathematical¬†Programming  12 , 241‚Äì254  (1977)."},{"id":2971,"pagetitle":"Conjugate Residual","title":"Conjugate residual solver in a Tangent space","ref":"/manopt/stable/solvers/conjugate_residual/#Conjugate-residual-solver-in-a-Tangent-space","content":" Conjugate residual solver in a Tangent space"},{"id":2972,"pagetitle":"Conjugate Residual","title":"Manopt.conjugate_residual","ref":"/manopt/stable/solvers/conjugate_residual/#Manopt.conjugate_residual","content":" Manopt.conjugate_residual  ‚Äî  Function conjugate_residual(TpM::TangentSpace, A, b, X=zero_vector(TpM))\nconjugate_residual(TpM::TangentSpace, slso::SymmetricLinearSystemObjective, X=zero_vector(TpM))\nconjugate_residual!(TpM::TangentSpace, A, b, X)\nconjugate_residual!(TpM::TangentSpace, slso::SymmetricLinearSystemObjective, X) Compute the solution of  $\\mathcal A(p)[X] + b(p) = 0_p$ , where $\\mathcal A$  is a linear, symmetric operator on  $T_{p}\\mathcal M$ $b$  is a vector field on the manifold $X ‚àà T_{p}\\mathcal M$  is a tangent vector $0_p$  is the zero vector  $T_{p}\\mathcal M$ . This implementation follows Algorithm 3 in [ LY24 ] and is initalised with  $X^{(0)}$  as the zero vector and the initial residual  $r^{(0)} = -b(p) - \\mathcal A(p)[X^{(0)}]$ the initial conjugate direction  $d^{(0)} = r^{(0)}$ initialize  $Y^{(0)} = \\mathcal A(p)[X^{(0)}]$ performed the following steps at iteration  $k=0,‚Ä¶$  until the  stopping_criterion  is fulfilled. compute a step size  $Œ±_k = \\displaystyle\\frac{‚ü® r^{(k)}, \\mathcal A(p)[r^{(k)}] ‚ü©_p}{‚ü® \\mathcal A(p)[d^{(k)}], \\mathcal A(p)[d^{(k)}] ‚ü©_p}$ do a step  $X^{(k+1)} = X^{(k)} + Œ±_kd^{(k)}$ update the residual  $r^{(k+1)} = r^{(k)} + Œ±_k Y^{(k)}$ compute  $Z = \\mathcal A(p)[r^{(k+1)}]$ Update the conjugate coefficient  $Œ≤_k = \\displaystyle\\frac{‚ü® r^{(k+1)}, \\mathcal A(p)[r^{(k+1)}] ‚ü©_p}{‚ü® r^{(k)}, \\mathcal A(p)[r^{(k)}] ‚ü©_p}$ Update the conjugate direction  $d^{(k+1)} = r^{(k+1)} + Œ≤_kd^{(k)}$ Update   $Y^{(k+1)} = -Z + Œ≤_k Y^{(k)}$ Note that the right hand side of Step 7 is the same as evaluating  $\\mathcal A[d^{(k+1)}]$ , but avoids the actual evaluation Input TpM  the  TangentSpace  as the domain A  a symmetric linear operator on the tangent space  (M, p, X) -> Y b  a vector field on the tangent space  (M, p) -> X X  the initial tangent vector Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. stopping_criterion= StopAfterIteration ( manifold_dimension (M) | StopWhenRelativeResidualLess (c,1e-8) ,  where  c  is  $\\lVert b \\rVert_{}$ : a functor indicating that the stopping criterion is fulfilled Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2973,"pagetitle":"Conjugate Residual","title":"Manopt.conjugate_residual!","ref":"/manopt/stable/solvers/conjugate_residual/#Manopt.conjugate_residual!","content":" Manopt.conjugate_residual!  ‚Äî  Function conjugate_residual(TpM::TangentSpace, A, b, X=zero_vector(TpM))\nconjugate_residual(TpM::TangentSpace, slso::SymmetricLinearSystemObjective, X=zero_vector(TpM))\nconjugate_residual!(TpM::TangentSpace, A, b, X)\nconjugate_residual!(TpM::TangentSpace, slso::SymmetricLinearSystemObjective, X) Compute the solution of  $\\mathcal A(p)[X] + b(p) = 0_p$ , where $\\mathcal A$  is a linear, symmetric operator on  $T_{p}\\mathcal M$ $b$  is a vector field on the manifold $X ‚àà T_{p}\\mathcal M$  is a tangent vector $0_p$  is the zero vector  $T_{p}\\mathcal M$ . This implementation follows Algorithm 3 in [ LY24 ] and is initalised with  $X^{(0)}$  as the zero vector and the initial residual  $r^{(0)} = -b(p) - \\mathcal A(p)[X^{(0)}]$ the initial conjugate direction  $d^{(0)} = r^{(0)}$ initialize  $Y^{(0)} = \\mathcal A(p)[X^{(0)}]$ performed the following steps at iteration  $k=0,‚Ä¶$  until the  stopping_criterion  is fulfilled. compute a step size  $Œ±_k = \\displaystyle\\frac{‚ü® r^{(k)}, \\mathcal A(p)[r^{(k)}] ‚ü©_p}{‚ü® \\mathcal A(p)[d^{(k)}], \\mathcal A(p)[d^{(k)}] ‚ü©_p}$ do a step  $X^{(k+1)} = X^{(k)} + Œ±_kd^{(k)}$ update the residual  $r^{(k+1)} = r^{(k)} + Œ±_k Y^{(k)}$ compute  $Z = \\mathcal A(p)[r^{(k+1)}]$ Update the conjugate coefficient  $Œ≤_k = \\displaystyle\\frac{‚ü® r^{(k+1)}, \\mathcal A(p)[r^{(k+1)}] ‚ü©_p}{‚ü® r^{(k)}, \\mathcal A(p)[r^{(k)}] ‚ü©_p}$ Update the conjugate direction  $d^{(k+1)} = r^{(k+1)} + Œ≤_kd^{(k)}$ Update   $Y^{(k+1)} = -Z + Œ≤_k Y^{(k)}$ Note that the right hand side of Step 7 is the same as evaluating  $\\mathcal A[d^{(k+1)}]$ , but avoids the actual evaluation Input TpM  the  TangentSpace  as the domain A  a symmetric linear operator on the tangent space  (M, p, X) -> Y b  a vector field on the tangent space  (M, p) -> X X  the initial tangent vector Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. stopping_criterion= StopAfterIteration ( manifold_dimension (M) | StopWhenRelativeResidualLess (c,1e-8) ,  where  c  is  $\\lVert b \\rVert_{}$ : a functor indicating that the stopping criterion is fulfilled Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2974,"pagetitle":"Conjugate Residual","title":"State","ref":"/manopt/stable/solvers/conjugate_residual/#State","content":" State"},{"id":2975,"pagetitle":"Conjugate Residual","title":"Manopt.ConjugateResidualState","ref":"/manopt/stable/solvers/conjugate_residual/#Manopt.ConjugateResidualState","content":" Manopt.ConjugateResidualState  ‚Äî  Type ConjugateResidualState{T,R,TStop<:StoppingCriterion} <: AbstractManoptSolverState A state for the  conjugate_residual  solver. Fields X::T : the iterate r::T : the residual  $r = -b(p) - \\mathcal A(p)[X]$ d::T : the conjugate direction Ar::T ,  Ad::T : storages for  $\\mathcal A(p)[d]$ ,  $\\mathcal A(p)[r]$ rAr::R : internal field for storing  $‚ü® r, \\mathcal A(p)[r] ‚ü©$ Œ±::R : a step length Œ≤::R : the conjugate coefficient stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled Constructor ConjugateResidualState(TpM::TangentSpace,slso::SymmetricLinearSystemObjective; kwargs...) Initialise the state with default values. Keyword arguments r=- get_gradient (TpM, slso, X) d=copy(TpM, r) Ar= get_hessian (TpM, slso, X, r) Ad=copy(TpM, Ar) Œ±::R=0.0 Œ≤::R=0.0 stopping_criterion= StopAfterIteration ( manifold_dimension (M) ) | StopWhenGradientNormLess (1e-8) : a functor indicating that the stopping criterion is fulfilled X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ See also conjugate_residual source"},{"id":2976,"pagetitle":"Conjugate Residual","title":"Objective","ref":"/manopt/stable/solvers/conjugate_residual/#Objective","content":" Objective"},{"id":2977,"pagetitle":"Conjugate Residual","title":"Manopt.SymmetricLinearSystemObjective","ref":"/manopt/stable/solvers/conjugate_residual/#Manopt.SymmetricLinearSystemObjective","content":" Manopt.SymmetricLinearSystemObjective  ‚Äî  Type SymmetricLinearSystemObjective{E<:AbstractEvaluationType,TA,T} <: AbstractManifoldObjective{E} Model the objective \\[f(X) = \\frac{1}{2} \\lVert \\mathcal A[X] + b \\rVert_{p}^2,\\qquad X ‚àà T_{p}\\mathcal M,\\] defined on the tangent space  $T_{p}\\mathcal M$  at  $p$  on the manifold  $\\mathcal M$ . In other words this is an objective to solve  $\\mathcal A = -b(p)$  for some linear symmetric operator and a vector function. Note the minus on the right hand side, which makes this objective especially tailored for (iteratively) solving Newton-like equations. Fields A!! : a symmetric, linear operator on the tangent space b!! : a gradient function where  A!!  can work as an allocating operator  (M, p, X) -> Y  or an in-place one  (M, Y, p, X) -> Y , and similarly  b!!  can either be a function  (M, p) -> X  or  (M, X, p) -> X . The first variants allocate for the result, the second variants work in-place. Constructor SymmetricLinearSystemObjective(A, b; evaluation=AllocatingEvaluation()) Generate the objective specifying whether the two parts work allocating or in-place. source"},{"id":2978,"pagetitle":"Conjugate Residual","title":"Additional stopping criterion","ref":"/manopt/stable/solvers/conjugate_residual/#Additional-stopping-criterion","content":" Additional stopping criterion"},{"id":2979,"pagetitle":"Conjugate Residual","title":"Manopt.StopWhenRelativeResidualLess","ref":"/manopt/stable/solvers/conjugate_residual/#Manopt.StopWhenRelativeResidualLess","content":" Manopt.StopWhenRelativeResidualLess  ‚Äî  Type StopWhenRelativeResidualLess <: StoppingCriterion Stop when re relative residual in the  conjugate_residual  is below a certain threshold, i.e. \\[\\displaystyle\\frac{\\lVert r^{(k) \\rVert_{}}{c} ‚â§ Œµ,\\] where  $c = \\lVert b \\rVert_{}$  of the initial vector from the vector field in  $\\mathcal A(p)[X] + b(p) = 0_p$ , from the  conjugate_residual Fields at_iteration::Int : an integer indicating at which the stopping criterion last indicted to stop, which might also be before the solver started ( 0 ). Any negative value indicates that this was not yet the case; c : the initial norm Œµ : the threshold norm_rk : the last computed norm of the residual Constructor StopWhenRelativeResidualLess(c, Œµ; norm_r = 2*c*Œµ) Initialise the stopping criterion. Note The initial norm of the vector field  $c = \\lVert b \\rVert_{}$  that is stored internally is updated on initialisation, that is, if this stopping criterion is called with  k<=0 . source"},{"id":2980,"pagetitle":"Conjugate Residual","title":"Internal functions","ref":"/manopt/stable/solvers/conjugate_residual/#Internal-functions","content":" Internal functions"},{"id":2981,"pagetitle":"Conjugate Residual","title":"Manopt.get_b","ref":"/manopt/stable/solvers/conjugate_residual/#Manopt.get_b","content":" Manopt.get_b  ‚Äî  Function get_b(TpM::TangentSpace, slso::SymmetricLinearSystemObjective) evaluate the stored value for computing the right hand side  $b$  in  $\\mathcal A=-b$ . source"},{"id":2982,"pagetitle":"Conjugate Residual","title":"Literature","ref":"/manopt/stable/solvers/conjugate_residual/#Literature","content":" Literature [LY24] Z.¬†Lai and A.¬†Yoshise.  Riemannian Interior Point Methods for Constrained Optimization on Manifolds .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  201 , 433‚Äì469  (2024),  arXiv:2203.09762 ."},{"id":2985,"pagetitle":"Convex bundle method","title":"Convex bundle method","ref":"/manopt/stable/solvers/convex_bundle_method/#Convex-bundle-method","content":" Convex bundle method"},{"id":2986,"pagetitle":"Convex bundle method","title":"Manopt.convex_bundle_method","ref":"/manopt/stable/solvers/convex_bundle_method/#Manopt.convex_bundle_method","content":" Manopt.convex_bundle_method  ‚Äî  Function convex_bundle_method(M, f, ‚àÇf, p)\nconvex_bundle_method!(M, f, ‚àÇf, p) perform a convex bundle method  $p^{(k+1)} = \\operatorname{retr}_{p^{(k)}}(-g_k)$  where \\[g_k = \\sum_{j\\in J_k} Œª_j^k \\mathrm{P}_{p_k‚Üêq_j}X_{q_j},\\] and  $p_k$  is the last serious iterate,  $X_{q_j} ‚àà ‚àÇf(q_j)$ , and the  $Œª_j^k$  are solutions to the quadratic subproblem provided by the  convex_bundle_method_subsolver . Though the subdifferential might be set valued, the argument  ‚àÇf  should always return one element from the subdifferential, but not necessarily deterministic. For more details, see [ BHJ24 ]. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v ‚àÇf :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. p : a point on the manifold  $\\mathcal M$ Keyword arguments atol_Œª=eps()  : tolerance parameter for the convex coefficients in  $Œª$ . atol_errors=eps() : : tolerance parameter for the linearization errors. bundle_cap=25 ` m=1e-3 : : the parameter to test the decrease of the cost:  $f(q_{k+1}) ‚â§ f(p_k) + m Œæ$ . diameter=50.0 : estimate for the diameter of the level set of the objective function at the starting point. domain=(M, p) -> isfinite(f(M, p)) : a function to that evaluates to true when the current candidate is in the domain of the objective  f , and false otherwise. evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. k_max=0 : upper bound on the sectional curvature of the manifold. stepsize= default_stepsize (M, ConvexBundleMethodState) : a functor inheriting from  Stepsize  to determine a step size inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses *  inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses stopping_criterion= StopWhenLagrangeMultiplierLess (1e-8) | StopAfterIteration (5000) : a functor indicating that the stopping criterion is fulfilled vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports sub_state= convex_bundle_method_subsolver `:  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. sub_problem= AllocatingEvaluation :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2987,"pagetitle":"Convex bundle method","title":"Manopt.convex_bundle_method!","ref":"/manopt/stable/solvers/convex_bundle_method/#Manopt.convex_bundle_method!","content":" Manopt.convex_bundle_method!  ‚Äî  Function convex_bundle_method(M, f, ‚àÇf, p)\nconvex_bundle_method!(M, f, ‚àÇf, p) perform a convex bundle method  $p^{(k+1)} = \\operatorname{retr}_{p^{(k)}}(-g_k)$  where \\[g_k = \\sum_{j\\in J_k} Œª_j^k \\mathrm{P}_{p_k‚Üêq_j}X_{q_j},\\] and  $p_k$  is the last serious iterate,  $X_{q_j} ‚àà ‚àÇf(q_j)$ , and the  $Œª_j^k$  are solutions to the quadratic subproblem provided by the  convex_bundle_method_subsolver . Though the subdifferential might be set valued, the argument  ‚àÇf  should always return one element from the subdifferential, but not necessarily deterministic. For more details, see [ BHJ24 ]. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v ‚àÇf :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. p : a point on the manifold  $\\mathcal M$ Keyword arguments atol_Œª=eps()  : tolerance parameter for the convex coefficients in  $Œª$ . atol_errors=eps() : : tolerance parameter for the linearization errors. bundle_cap=25 ` m=1e-3 : : the parameter to test the decrease of the cost:  $f(q_{k+1}) ‚â§ f(p_k) + m Œæ$ . diameter=50.0 : estimate for the diameter of the level set of the objective function at the starting point. domain=(M, p) -> isfinite(f(M, p)) : a function to that evaluates to true when the current candidate is in the domain of the objective  f , and false otherwise. evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. k_max=0 : upper bound on the sectional curvature of the manifold. stepsize= default_stepsize (M, ConvexBundleMethodState) : a functor inheriting from  Stepsize  to determine a step size inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses *  inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses stopping_criterion= StopWhenLagrangeMultiplierLess (1e-8) | StopAfterIteration (5000) : a functor indicating that the stopping criterion is fulfilled vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports sub_state= convex_bundle_method_subsolver `:  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. sub_problem= AllocatingEvaluation :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":2988,"pagetitle":"Convex bundle method","title":"State","ref":"/manopt/stable/solvers/convex_bundle_method/#State","content":" State"},{"id":2989,"pagetitle":"Convex bundle method","title":"Manopt.ConvexBundleMethodState","ref":"/manopt/stable/solvers/convex_bundle_method/#Manopt.ConvexBundleMethodState","content":" Manopt.ConvexBundleMethodState  ‚Äî  Type ConvexBundleMethodState <: AbstractManoptSolverState Stores option values for a  convex_bundle_method  solver. Fields THe following fields require a (real) number type  R , as well as point type  P  and a tangent vector type  T ` atol_Œª::R :                 tolerance parameter for the convex coefficients in Œª `atol_errors::R:             tolerance parameter for the linearization errors bundle<:AbstractVector{Tuple{<:P,<:T}} : bundle that collects each iterate with the computed subgradient at the iterate bundle_cap::Int : the maximal number of elements the bundle is allowed to remember diameter::R : estimate for the diameter of the level set of the objective function at the starting point domain: the domain of f as a function (M,p) -> b that evaluates to true when the current candidate is in the domain of f`, and false otherwise, g::T :                      descent direction inverse_retraction_method::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses k_max::R :                  upper bound on the sectional curvature of the manifold linearization_errors<:AbstractVector{<:R} : linearization errors at the last serious step m::R :                      the parameter to test the decrease of the cost:  $f(q_{k+1}) ‚â§ f(p_k) + m Œæ$ . p::P : a point on the manifold  $\\mathcal M$  storing the current iterate p_last_serious::P :         last serious iterate retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled transported_subgradients :  subgradients of the bundle that are transported to  p_last_serious vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing a subgradient at the current iterate stepsize::Stepsize : a functor inheriting from  Stepsize  to determine a step size Œµ::R :                      convex combination of the linearization errors Œª:::AbstractVector{<:R} :   convex coefficients from the slution of the subproblem Œæ :                         the stopping parameter given by  $Œæ = -\\lVert g\\rvert^2 ‚Äì Œµ$ sub_problem::Union{AbstractManoptProblem, F} :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state::Union{AbstractManoptProblem, F} :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. Constructor ConvexBundleMethodState(M::AbstractManifold, sub_problem, sub_state; kwargs...)\nConvexBundleMethodState(M::AbstractManifold, sub_problem=convex_bundle_method_subsolver; evaluation=AllocatingEvaluation(), kwargs...) Generate the state for the  convex_bundle_method  on the manifold  M Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ sub_problem :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. Keyword arguments Most of the following keyword arguments set default values for the fields mentioned before. atol_Œª=eps() atol_errors=eps() bundle_cap=25 ` m=1e-2 diameter=50.0 domain=(M, p) -> isfinite(f(M, p)) k_max=0 k_min=0 p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value stepsize= default_stepsize (M, ConvexBundleMethodState) : a functor inheriting from  Stepsize  to determine a step size inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopWhenLagrangeMultiplierLess (1e-8) | StopAfterIteration (5000) : a functor indicating that the stopping criterion is fulfilled X= zero_vector (M, p)  specify the type of tangent vector to use. vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports source"},{"id":2990,"pagetitle":"Convex bundle method","title":"Stopping criteria","ref":"/manopt/stable/solvers/convex_bundle_method/#Stopping-criteria","content":" Stopping criteria"},{"id":2991,"pagetitle":"Convex bundle method","title":"Manopt.StopWhenLagrangeMultiplierLess","ref":"/manopt/stable/solvers/convex_bundle_method/#Manopt.StopWhenLagrangeMultiplierLess","content":" Manopt.StopWhenLagrangeMultiplierLess  ‚Äî  Type StopWhenLagrangeMultiplierLess <: StoppingCriterion Stopping Criteria for Lagrange multipliers. Currently these are meant for the  convex_bundle_method  and  proximal_bundle_method , where based on the Lagrange multipliers an approximate (sub)gradient  $g$  and an error estimate  $Œµ$  is computed. The  mode=:both  requires that both  $Œµ$  and  $\\lvert g \\rvert$  are smaller than their  tolerance s for the  convex_bundle_method , and that  $c$  and  $\\lvert d \\rvert$  are smaller than their  tolerance s for the  proximal_bundle_method . The  mode=:estimate  requires that, for the  convex_bundle_method $-Œæ = \\lvert g \\rvert^2 + Œµ$  is less than a given  tolerance . For the  proximal_bundle_method , the equation reads  $-ŒΩ = Œº \\lvert d \\rvert^2 + c$ . Constructors StopWhenLagrangeMultiplierLess(tolerance=1e-6; mode::Symbol=:estimate, names=nothing) Create the stopping criterion for one of the  mode s mentioned. Note that tolerance can be a single number for the  :estimate  case, but a vector of two values is required for the  :both  mode. Here the first entry specifies the tolerance for  $Œµ$  ( $c$ ), the second the tolerance for  $\\lvert g \\rvert$  ( $\\lvert d \\rvert$ ), respectively. source"},{"id":2992,"pagetitle":"Convex bundle method","title":"Debug functions","ref":"/manopt/stable/solvers/convex_bundle_method/#Debug-functions","content":" Debug functions"},{"id":2993,"pagetitle":"Convex bundle method","title":"Manopt.DebugWarnIfLagrangeMultiplierIncreases","ref":"/manopt/stable/solvers/convex_bundle_method/#Manopt.DebugWarnIfLagrangeMultiplierIncreases","content":" Manopt.DebugWarnIfLagrangeMultiplierIncreases  ‚Äî  Type DebugWarnIfLagrangeMultiplierIncreases <: DebugAction print a warning if the Lagrange parameter based value  $-Œæ$  of the bundle method increases. Constructor DebugWarnIfLagrangeMultiplierIncreases(warn=:Once; tol=1e2) Initialize the warning to warning level ( :Once ) and introduce a tolerance for the test of  1e2 . The  warn  level can be set to  :Once  to only warn the first time the cost increases, to  :Always  to report an increase every time it happens, and it can be set to  :No  to deactivate the warning, then this  DebugAction  is inactive. All other symbols are handled as if they were  :Always . source"},{"id":2994,"pagetitle":"Convex bundle method","title":"Helpers and internal functions","ref":"/manopt/stable/solvers/convex_bundle_method/#Helpers-and-internal-functions","content":" Helpers and internal functions"},{"id":2995,"pagetitle":"Convex bundle method","title":"Manopt.convex_bundle_method_subsolver","ref":"/manopt/stable/solvers/convex_bundle_method/#Manopt.convex_bundle_method_subsolver","content":" Manopt.convex_bundle_method_subsolver  ‚Äî  Function Œª = convex_bundle_method_subsolver(M, p_last_serious, linearization_errors, transported_subgradients)\nconvex_bundle_method_subsolver!(M, Œª, p_last_serious, linearization_errors, transported_subgradients) solver for the subproblem of the convex bundle method at the last serious iterate  $p_k$  given the current linearization errors  $c_j^k$ , and transported subgradients  $\\mathrm{P}_{p_k‚Üêq_j} X_{q_j}$ . The computation can also be done in-place of  Œª . The subproblem for the convex bundle method is \\[\\begin{align*}\n    \\operatorname*{arg\\,min}_{Œª ‚àà ‚Ñù^{\\lvert J_k\\rvert}}&\n    \\frac{1}{2} \\Bigl\\lVert \\sum_{j ‚àà J_k} Œª_j \\mathrm{P}_{p_k‚Üêq_j} X_{q_j} \\Bigr\\rVert^2\n    + \\sum_{j ‚àà J_k} Œª_j \\, c_j^k\n    \\\\\n    \\text{s. t.}\\quad &\n    \\sum_{j ‚àà J_k} Œª_j = 1,\n    \\quad Œª_j ‚â• 0\n    \\quad \\text{for all }\n    j ‚àà J_k,\n\\end{align*}\\] where  $J_k = \\{j ‚àà J_{k-1} \\ | \\ Œª_j > 0\\} \\cup \\{k\\}$ . See [ BHJ24 ] for more details Tip A default subsolver based on  RipQP .jl  and  QuadraticModels  is available if these two packages are loaded. source"},{"id":2996,"pagetitle":"Convex bundle method","title":"Manopt.DomainBackTrackingStepsize","ref":"/manopt/stable/solvers/convex_bundle_method/#Manopt.DomainBackTrackingStepsize","content":" Manopt.DomainBackTrackingStepsize  ‚Äî  Type DomainBackTrackingStepsize <: Stepsize Implement a backtrack as long as we are  $q = \\operatorname{retr}_p(X)$  yields a point closer to  $p$  than  $\\lVert X \\rVert_p$  or  $q$  is not on the domain. For the domain this step size requires a  ConvexBundleMethodState . source"},{"id":2997,"pagetitle":"Convex bundle method","title":"Manopt.DomainBackTracking","ref":"/manopt/stable/solvers/convex_bundle_method/#Manopt.DomainBackTracking","content":" Manopt.DomainBackTracking  ‚Äî  Function DomainBackTracking(; kwargs...)\nDomainBackTracking(M::AbstractManifold; kwargs...) Specify a step size that performs a backtracking to the interior of the domain of the objective function. Keyword arguments candidate_point=allocate_result(M, rand) : speciy a point to be used as memory for the candidate points. contraction_factor : how to update  $s$  in the decrease step initial_stepsize `: specify an initial step size retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions Info This function generates a  ManifoldDefaultsFactory  for  DomainBackTrackingStepsize . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":2998,"pagetitle":"Convex bundle method","title":"Manopt.NullStepBackTrackingStepsize","ref":"/manopt/stable/solvers/convex_bundle_method/#Manopt.NullStepBackTrackingStepsize","content":" Manopt.NullStepBackTrackingStepsize  ‚Äî  Type NullStepBackTrackingStepsize <: Stepsize Implement a backtracking with a geometric condition in the case of a null step. For the domain this step size requires a  ConvexBundleMethodState . source"},{"id":2999,"pagetitle":"Convex bundle method","title":"Manopt.estimate_sectional_curvature","ref":"/manopt/stable/solvers/convex_bundle_method/#Manopt.estimate_sectional_curvature","content":" Manopt.estimate_sectional_curvature  ‚Äî  Function estimate_sectional_curvature(M::AbstractManifold, p) Estimate the sectional curvature of a manifold  $\\mathcal M$  at a point  $p \\in \\mathcal M$  on two random tangent vectors at  $p$  that are orthogonal to each other. See also sectional_curvature source"},{"id":3000,"pagetitle":"Convex bundle method","title":"Manopt.Œ∂_1","ref":"/manopt/stable/solvers/convex_bundle_method/#Manopt.Œ∂_1","content":" Manopt.Œ∂_1  ‚Äî  Function Œ∂_1(œâ, Œ¥) compute a curvature-dependent bound. The formula reads \\[\\zeta_{1, œâ}(Œ¥)\n\\coloneqq\n\\begin{cases}\n    1 & \\text{if } œâ ‚â• 0, \\\\\n    \\sqrt{-œâ} \\, Œ¥ \\cot(\\sqrt{-œâ} \\, Œ¥) & \\text{if } œâ < 0,\n\\end{cases}\\] where  $œâ ‚â§ Œ∫_p$  for all  $p ‚àà \\mathcal U$  is a lower bound to the sectional curvature in a (strongly geodesically convex) bounded subset  $\\mathcal U ‚äÜ \\mathcal M$  with diameter  $Œ¥$ . source"},{"id":3001,"pagetitle":"Convex bundle method","title":"Manopt.Œ∂_2","ref":"/manopt/stable/solvers/convex_bundle_method/#Manopt.Œ∂_2","content":" Manopt.Œ∂_2  ‚Äî  Function Œ∂_2(Œ©, Œ¥) compute a curvature-dependent bound. The formula reads \\[\\zeta_{2, Œ©}(Œ¥) \\coloneqq\n\\begin{cases}\n    1 & \\text{if } Œ© ‚â§ 0,\\\\\n    \\sqrt{Œ©} \\, Œ¥ \\cot(\\sqrt{Œ©} \\, Œ¥) & \\text{if } Œ© > 0,\n\\end{cases}\\] where  $Œ© ‚â• Œ∫_p$  for all  $p ‚àà \\mathcal U$  is an upper bound to the sectional curvature in a (strongly geodesically convex) bounded subset  $\\mathcal U ‚äÜ \\mathcal M$  with diameter  $Œ¥$ . source"},{"id":3002,"pagetitle":"Convex bundle method","title":"Manopt.close_point","ref":"/manopt/stable/solvers/convex_bundle_method/#Manopt.close_point","content":" Manopt.close_point  ‚Äî  Function close_point(M, p, tol; retraction_method=default_retraction_method(M, typeof(p))) sample a random point close to  $p ‚àà \\mathcal M$  within a tolerance  tol  and a  retraction . source"},{"id":3003,"pagetitle":"Convex bundle method","title":"Literature","ref":"/manopt/stable/solvers/convex_bundle_method/#Literature","content":" Literature [BHJ24] R.¬†Bergmann, R.¬†Herzog and H.¬†Jasa.  The Riemannian Convex Bundle Method , preprint (2024),  arXiv:2402.13670 ."},{"id":3006,"pagetitle":"Cyclic Proximal Point","title":"Cyclic proximal point","ref":"/manopt/stable/solvers/cyclic_proximal_point/#Cyclic-proximal-point","content":" Cyclic proximal point The Cyclic Proximal Point (CPP) algorithm aims to minimize \\[F(x) = \\sum_{i=1}^c f_i(x)\\] assuming that the proximal maps  $\\operatorname{prox}_{Œª f_i}(x)$  are given in closed form or can be computed efficiently (at least approximately). The algorithm then cycles through these proximal maps, where the type of cycle might differ and the proximal parameter  $Œª_k$  changes after each cycle  $k$ . For a convergence result on  Hadamard manifolds  see  Baƒç√°k [Bac14] ."},{"id":3007,"pagetitle":"Cyclic Proximal Point","title":"Manopt.cyclic_proximal_point","ref":"/manopt/stable/solvers/cyclic_proximal_point/#Manopt.cyclic_proximal_point","content":" Manopt.cyclic_proximal_point  ‚Äî  Function cyclic_proximal_point(M, f, proxes_f, p; kwargs...)\ncyclic_proximal_point(M, mpo, p; kwargs...)\ncyclic_proximal_point!(M, f, proxes_f; kwargs...)\ncyclic_proximal_point!(M, mpo; kwargs...) perform a cyclic proximal point algorithm. This can be done in-place of  p . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f :        a cost function  $f: \\mathcal M‚Üí‚Ñù$  to minimize proxes_f : an Array of proximal maps ( Function s)  (M,Œª,p) -> q  or  (M, q, Œª, p) -> q  for the summands of  $f$  (see  evaluation ) where  f  and the proximal maps  proxes_f  can also be given directly as a  ManifoldProximalMapObjective mpo Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. evaluation_order=:Linear : whether to use a randomly permuted sequence ( :FixedRandom :, a per cycle permuted sequence ( :Random ) or the default linear one. Œª=iter -> 1/iter :         a function returning the (square summable but not summable) sequence of  $Œª_i$ stopping_criterion= StopAfterIteration (5000) | StopWhenChangeLess (1e-12) ): a functor indicating that the stopping criterion is fulfilled All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3008,"pagetitle":"Cyclic Proximal Point","title":"Manopt.cyclic_proximal_point!","ref":"/manopt/stable/solvers/cyclic_proximal_point/#Manopt.cyclic_proximal_point!","content":" Manopt.cyclic_proximal_point!  ‚Äî  Function cyclic_proximal_point(M, f, proxes_f, p; kwargs...)\ncyclic_proximal_point(M, mpo, p; kwargs...)\ncyclic_proximal_point!(M, f, proxes_f; kwargs...)\ncyclic_proximal_point!(M, mpo; kwargs...) perform a cyclic proximal point algorithm. This can be done in-place of  p . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f :        a cost function  $f: \\mathcal M‚Üí‚Ñù$  to minimize proxes_f : an Array of proximal maps ( Function s)  (M,Œª,p) -> q  or  (M, q, Œª, p) -> q  for the summands of  $f$  (see  evaluation ) where  f  and the proximal maps  proxes_f  can also be given directly as a  ManifoldProximalMapObjective mpo Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. evaluation_order=:Linear : whether to use a randomly permuted sequence ( :FixedRandom :, a per cycle permuted sequence ( :Random ) or the default linear one. Œª=iter -> 1/iter :         a function returning the (square summable but not summable) sequence of  $Œª_i$ stopping_criterion= StopAfterIteration (5000) | StopWhenChangeLess (1e-12) ): a functor indicating that the stopping criterion is fulfilled All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3009,"pagetitle":"Cyclic Proximal Point","title":"Technical details","ref":"/manopt/stable/solvers/cyclic_proximal_point/#sec-cppa-technical-details","content":" Technical details The  cyclic_proximal_point  solver requires no additional functions to be available for your manifold, besides the ones you use in the proximal maps. By default, one of the stopping criteria is  StopWhenChangeLess , which either requires An  inverse_retract! (M, X, p, q) ; it is recommended to set the  default_inverse_retraction_method  to a favourite retraction. If this default is set, a  inverse_retraction_method=  or  inverse_retraction_method_dual=  (for  $\\mathcal N$ ) does not have to be specified or the  distance (M, p, q)  for said default inverse retraction."},{"id":3010,"pagetitle":"Cyclic Proximal Point","title":"State","ref":"/manopt/stable/solvers/cyclic_proximal_point/#State","content":" State"},{"id":3011,"pagetitle":"Cyclic Proximal Point","title":"Manopt.CyclicProximalPointState","ref":"/manopt/stable/solvers/cyclic_proximal_point/#Manopt.CyclicProximalPointState","content":" Manopt.CyclicProximalPointState  ‚Äî  Type CyclicProximalPointState <: AbstractManoptSolverState stores options for the  cyclic_proximal_point  algorithm. These are the Fields p::P : a point on the manifold  $\\mathcal M$  storing the current iterate stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled Œª :         a function for the values of  $Œª_k$  per iteration(cycle  $k$ oder_type : whether to use a randomly permuted sequence ( :FixedRandomOrder ), a per cycle permuted sequence ( :RandomOrder ) or the default linear one. Constructor CyclicProximalPointState(M::AbstractManifold; kwargs...) Generate the options Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ Keyword arguments evaluation_order=:LinearOrder : soecify the  order_type Œª=i -> 1.0 / i  a function to compute the  $Œª_k, k ‚àà \\mathcal N$ , p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value stopping_criterion= StopAfterIteration (2000) : a functor indicating that the stopping criterion is fulfilled See also cyclic_proximal_point source"},{"id":3012,"pagetitle":"Cyclic Proximal Point","title":"Debug functions","ref":"/manopt/stable/solvers/cyclic_proximal_point/#Debug-functions","content":" Debug functions"},{"id":3013,"pagetitle":"Cyclic Proximal Point","title":"Manopt.DebugProximalParameter","ref":"/manopt/stable/solvers/cyclic_proximal_point/#Manopt.DebugProximalParameter","content":" Manopt.DebugProximalParameter  ‚Äî  Type DebugProximalParameter <: DebugAction print the current iterates proximal point algorithm parameter given by  AbstractManoptSolverState s  o.Œª . source"},{"id":3014,"pagetitle":"Cyclic Proximal Point","title":"Record functions","ref":"/manopt/stable/solvers/cyclic_proximal_point/#Record-functions","content":" Record functions"},{"id":3015,"pagetitle":"Cyclic Proximal Point","title":"Manopt.RecordProximalParameter","ref":"/manopt/stable/solvers/cyclic_proximal_point/#Manopt.RecordProximalParameter","content":" Manopt.RecordProximalParameter  ‚Äî  Type RecordProximalParameter <: RecordAction record the current iterates proximal point algorithm parameter given by in  AbstractManoptSolverState s  o.Œª . source"},{"id":3016,"pagetitle":"Cyclic Proximal Point","title":"Literature","ref":"/manopt/stable/solvers/cyclic_proximal_point/#Literature","content":" Literature [Bac14] M.¬†Baƒç√°k.  Computing medians and means in Hadamard spaces .  SIAM¬†Journal¬†on¬†Optimization  24 , 1542‚Äì1566  (2014),  arXiv:1210.2145 ."},{"id":3019,"pagetitle":"Difference of Convex","title":"Difference of convex","ref":"/manopt/stable/solvers/difference_of_convex/#Difference-of-convex","content":" Difference of convex"},{"id":3020,"pagetitle":"Difference of Convex","title":"Difference of convex algorithm","ref":"/manopt/stable/solvers/difference_of_convex/#solver-difference-of-convex","content":" Difference of convex algorithm"},{"id":3021,"pagetitle":"Difference of Convex","title":"Manopt.difference_of_convex_algorithm","ref":"/manopt/stable/solvers/difference_of_convex/#Manopt.difference_of_convex_algorithm","content":" Manopt.difference_of_convex_algorithm  ‚Äî  Function difference_of_convex_algorithm(M, f, g, ‚àÇh, p=rand(M); kwargs...)\ndifference_of_convex_algorithm(M, mdco, p; kwargs...)\ndifference_of_convex_algorithm!(M, f, g, ‚àÇh, p; kwargs...)\ndifference_of_convex_algorithm!(M, mdco, p; kwargs...) Compute the difference of convex algorithm [ BFSS24 ] to minimize \\[    \\operatorname*{arg\\,min}_{p‚àà\\mathcal M}\\ g(p) - h(p)\\] where you need to provide  $f(p) = g(p) - h(p)$ ,  $g$  and the subdifferential  $‚àÇh$  of  $h$ . This algorithm performs the following steps given a start point  p =  $p^{(0)}$ . Then repeat for  $k=0,1,‚Ä¶$ Take  $X^{(k)}  ‚àà ‚àÇh(p^{(k)})$ Set the next iterate to the solution of the subproblem \\[  p^{(k+1)} ‚àà \\operatorname*{arg\\,min}_{q ‚àà \\mathcal M} g(q) - ‚ü®X^{(k)}, \\log_{p^{(k)}}q‚ü©\\] until the stopping criterion (see the  stopping_criterion  keyword is fulfilled. Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. gradient=nothing :        specify  $\\operatorname{grad} f$ , for debug / analysis or enhancing the  stopping_criterion= grad_g=nothing :          specify the gradient of  g . If specified, a subsolver is automatically set up. stopping_criterion= StopAfterIteration (200) | StopWhenChangeLess (1e-8) : a functor indicating that the stopping criterion is fulfilled g=nothing :               specify the function  g  If specified, a subsolver is automatically set up. sub_cost= LinearizedDCCost (g, p, initial_vector) : a cost to be used within the default  sub_problem . This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_grad= LinearizedDCGrad (grad_g, p, initial_vector; evaluation=evaluation) : gradient to be used within the default  sub_problem . This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_hess :              (a finite difference approximation using  sub_grad  by default):  specify a Hessian of the  sub_cost , which the default solver, see  sub_state=  needs. This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. sub_objective :         a gradient or Hessian objective based on  sub_cost= ,  sub_grad= , and  sub_hess if provided  the objective used within  sub_problem . This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_state= ( GradientDescentState  or  TrustRegionsState  if  sub_hessian  is provided):  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. sub_problem= DefaultManoptProblem (M, sub_objective) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_stopping_criterion= StopAfterIteration (300) | StopWhenStepsizeLess (1e-9) | StopWhenGradientNormLess (1e-9) : a stopping criterion used withing the default  sub_state=  This is used to define the  sub_state=  keyword and has hence no effect, if you set  sub_state  directly. sub_stepsize= ArmijoLinesearch (M) ) specify a step size used within the  sub_state . This is used to define the  sub_state=  keyword and has hence no effect, if you set  sub_state  directly. X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ to specify the representation of a tangent vector All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3022,"pagetitle":"Difference of Convex","title":"Manopt.difference_of_convex_algorithm!","ref":"/manopt/stable/solvers/difference_of_convex/#Manopt.difference_of_convex_algorithm!","content":" Manopt.difference_of_convex_algorithm!  ‚Äî  Function difference_of_convex_algorithm(M, f, g, ‚àÇh, p=rand(M); kwargs...)\ndifference_of_convex_algorithm(M, mdco, p; kwargs...)\ndifference_of_convex_algorithm!(M, f, g, ‚àÇh, p; kwargs...)\ndifference_of_convex_algorithm!(M, mdco, p; kwargs...) Compute the difference of convex algorithm [ BFSS24 ] to minimize \\[    \\operatorname*{arg\\,min}_{p‚àà\\mathcal M}\\ g(p) - h(p)\\] where you need to provide  $f(p) = g(p) - h(p)$ ,  $g$  and the subdifferential  $‚àÇh$  of  $h$ . This algorithm performs the following steps given a start point  p =  $p^{(0)}$ . Then repeat for  $k=0,1,‚Ä¶$ Take  $X^{(k)}  ‚àà ‚àÇh(p^{(k)})$ Set the next iterate to the solution of the subproblem \\[  p^{(k+1)} ‚àà \\operatorname*{arg\\,min}_{q ‚àà \\mathcal M} g(q) - ‚ü®X^{(k)}, \\log_{p^{(k)}}q‚ü©\\] until the stopping criterion (see the  stopping_criterion  keyword is fulfilled. Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. gradient=nothing :        specify  $\\operatorname{grad} f$ , for debug / analysis or enhancing the  stopping_criterion= grad_g=nothing :          specify the gradient of  g . If specified, a subsolver is automatically set up. stopping_criterion= StopAfterIteration (200) | StopWhenChangeLess (1e-8) : a functor indicating that the stopping criterion is fulfilled g=nothing :               specify the function  g  If specified, a subsolver is automatically set up. sub_cost= LinearizedDCCost (g, p, initial_vector) : a cost to be used within the default  sub_problem . This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_grad= LinearizedDCGrad (grad_g, p, initial_vector; evaluation=evaluation) : gradient to be used within the default  sub_problem . This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_hess :              (a finite difference approximation using  sub_grad  by default):  specify a Hessian of the  sub_cost , which the default solver, see  sub_state=  needs. This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. sub_objective :         a gradient or Hessian objective based on  sub_cost= ,  sub_grad= , and  sub_hess if provided  the objective used within  sub_problem . This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_state= ( GradientDescentState  or  TrustRegionsState  if  sub_hessian  is provided):  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. sub_problem= DefaultManoptProblem (M, sub_objective) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_stopping_criterion= StopAfterIteration (300) | StopWhenStepsizeLess (1e-9) | StopWhenGradientNormLess (1e-9) : a stopping criterion used withing the default  sub_state=  This is used to define the  sub_state=  keyword and has hence no effect, if you set  sub_state  directly. sub_stepsize= ArmijoLinesearch (M) ) specify a step size used within the  sub_state . This is used to define the  sub_state=  keyword and has hence no effect, if you set  sub_state  directly. X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ to specify the representation of a tangent vector All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3023,"pagetitle":"Difference of Convex","title":"Difference of convex proximal point","ref":"/manopt/stable/solvers/difference_of_convex/#solver-difference-of-convex-proximal-point","content":" Difference of convex proximal point"},{"id":3024,"pagetitle":"Difference of Convex","title":"Manopt.difference_of_convex_proximal_point","ref":"/manopt/stable/solvers/difference_of_convex/#Manopt.difference_of_convex_proximal_point","content":" Manopt.difference_of_convex_proximal_point  ‚Äî  Function difference_of_convex_proximal_point(M, grad_h, p=rand(M); kwargs...)\ndifference_of_convex_proximal_point(M, mdcpo, p=rand(M); kwargs...)\ndifference_of_convex_proximal_point!(M, grad_h, p; kwargs...)\ndifference_of_convex_proximal_point!(M, mdcpo, p; kwargs...) Compute the difference of convex proximal point algorithm [ SO15 ] to minimize \\[    \\operatorname*{arg\\,min}_{p‚àà\\mathcal M} g(p) - h(p)\\] where you have to provide the subgradient  $‚àÇh$  of  $h$  and either the proximal map  $\\operatorname{prox}_{Œªg}$  of  g  as a function  prox_g(M, Œª, p)  or   prox_g(M, q, Œª, p) the functions  g  and  grad_g  to compute the proximal map using a sub solver your own sub-solver, specified by  sub_problem= and  sub_state= This algorithm performs the following steps given a start point  p =  $p^{(0)}$ . Then repeat for  $k=0,1,‚Ä¶$ $X^{(k)}  ‚àà \\operatorname{grad} h(p^{(k)})$ $q^{(k)} = \\operatorname{retr}_{p^{(k)}}(Œª_kX^{(k)})$ $r^{(k)} = \\operatorname{prox}_{Œª_kg}(q^{(k)})$ $X^{(k)} = \\operatorname{retr}^{-1}_{p^{(k)}}(r^{(k)})$ Compute a stepsize  $s_k$  and set  $p^{(k+1)} = \\operatorname{retr}_{p^{(k)}}(s_kX^{(k)})$ . until the  stopping_criterion  is fulfilled. See [ ACOO20 ] for more details on the modified variant, where steps 4-6 are slightly changed, since here the classical proximal point method for DC functions is obtained for  $s_k = 1$  and one can hence employ usual line search method. Keyword arguments Œª :                          (  k -> 1/2  ) a function returning the sequence of prox parameters  $Œª_k$ cost=nothing : provide the cost  f , for debug reasons / analysis evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. gradient=nothing : specify  $\\operatorname{grad} f$ , for debug / analysis  or enhancing the  stopping_criterion prox_g=nothing : specify a proximal map for the sub problem  or  both of the following g=nothing : specify the function  g . grad_g=nothing : specify the gradient of  g . If both  g and  grad_g  are specified, a subsolver is automatically set up. inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= ConstantLength () : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (200) | StopWhenChangeLess (1e-8) ): a functor indicating that the stopping criterion is fulfilled A  StopWhenGradientNormLess (1e-8)  is added with  | , when a  gradient  is provided. sub_cost= ProximalDCCost (g, copy(M, p), Œª(1)) ): cost to be used within the default  sub_problem  that is initialized as soon as  g  is provided. This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_grad= ProximalDCGrad (grad_g, copy(M, p), Œª(1); evaluation=evaluation) : gradient to be used within the default  sub_problem , that is initialized as soon as  grad_g  is provided. This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_hess :              (a finite difference approximation using  sub_grad  by default):  specify a Hessian of the  sub_cost , which the default solver, see  sub_state=  needs. sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. sub_objective :         a gradient or Hessian objective based on  sub_cost= ,  sub_grad= , and  sub_hess if provided  the objective used within  sub_problem . This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_problem= DefaultManoptProblem (M, sub_objective) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= ( GradientDescentState  or  TrustRegionsState  if  sub_hessian  is provided):  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. sub_stopping_criterion= ( StopAfterIteration (300) | [ StopWhenGradientNormLess ](@ref) (1e-8) : a functor indicating that the stopping criterion is fulfilled This is used to define the sub state= keyword and has hence no effect, if you set sub state` directly. All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3025,"pagetitle":"Difference of Convex","title":"Manopt.difference_of_convex_proximal_point!","ref":"/manopt/stable/solvers/difference_of_convex/#Manopt.difference_of_convex_proximal_point!","content":" Manopt.difference_of_convex_proximal_point!  ‚Äî  Function difference_of_convex_proximal_point(M, grad_h, p=rand(M); kwargs...)\ndifference_of_convex_proximal_point(M, mdcpo, p=rand(M); kwargs...)\ndifference_of_convex_proximal_point!(M, grad_h, p; kwargs...)\ndifference_of_convex_proximal_point!(M, mdcpo, p; kwargs...) Compute the difference of convex proximal point algorithm [ SO15 ] to minimize \\[    \\operatorname*{arg\\,min}_{p‚àà\\mathcal M} g(p) - h(p)\\] where you have to provide the subgradient  $‚àÇh$  of  $h$  and either the proximal map  $\\operatorname{prox}_{Œªg}$  of  g  as a function  prox_g(M, Œª, p)  or   prox_g(M, q, Œª, p) the functions  g  and  grad_g  to compute the proximal map using a sub solver your own sub-solver, specified by  sub_problem= and  sub_state= This algorithm performs the following steps given a start point  p =  $p^{(0)}$ . Then repeat for  $k=0,1,‚Ä¶$ $X^{(k)}  ‚àà \\operatorname{grad} h(p^{(k)})$ $q^{(k)} = \\operatorname{retr}_{p^{(k)}}(Œª_kX^{(k)})$ $r^{(k)} = \\operatorname{prox}_{Œª_kg}(q^{(k)})$ $X^{(k)} = \\operatorname{retr}^{-1}_{p^{(k)}}(r^{(k)})$ Compute a stepsize  $s_k$  and set  $p^{(k+1)} = \\operatorname{retr}_{p^{(k)}}(s_kX^{(k)})$ . until the  stopping_criterion  is fulfilled. See [ ACOO20 ] for more details on the modified variant, where steps 4-6 are slightly changed, since here the classical proximal point method for DC functions is obtained for  $s_k = 1$  and one can hence employ usual line search method. Keyword arguments Œª :                          (  k -> 1/2  ) a function returning the sequence of prox parameters  $Œª_k$ cost=nothing : provide the cost  f , for debug reasons / analysis evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. gradient=nothing : specify  $\\operatorname{grad} f$ , for debug / analysis  or enhancing the  stopping_criterion prox_g=nothing : specify a proximal map for the sub problem  or  both of the following g=nothing : specify the function  g . grad_g=nothing : specify the gradient of  g . If both  g and  grad_g  are specified, a subsolver is automatically set up. inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= ConstantLength () : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (200) | StopWhenChangeLess (1e-8) ): a functor indicating that the stopping criterion is fulfilled A  StopWhenGradientNormLess (1e-8)  is added with  | , when a  gradient  is provided. sub_cost= ProximalDCCost (g, copy(M, p), Œª(1)) ): cost to be used within the default  sub_problem  that is initialized as soon as  g  is provided. This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_grad= ProximalDCGrad (grad_g, copy(M, p), Œª(1); evaluation=evaluation) : gradient to be used within the default  sub_problem , that is initialized as soon as  grad_g  is provided. This is used to define the  sub_objective=  keyword and has hence no effect, if you set  sub_objective  directly. sub_hess :              (a finite difference approximation using  sub_grad  by default):  specify a Hessian of the  sub_cost , which the default solver, see  sub_state=  needs. sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. sub_objective :         a gradient or Hessian objective based on  sub_cost= ,  sub_grad= , and  sub_hess if provided  the objective used within  sub_problem . This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_problem= DefaultManoptProblem (M, sub_objective) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= ( GradientDescentState  or  TrustRegionsState  if  sub_hessian  is provided):  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. sub_stopping_criterion= ( StopAfterIteration (300) | [ StopWhenGradientNormLess ](@ref) (1e-8) : a functor indicating that the stopping criterion is fulfilled This is used to define the sub state= keyword and has hence no effect, if you set sub state` directly. All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3026,"pagetitle":"Difference of Convex","title":"Solver states","ref":"/manopt/stable/solvers/difference_of_convex/#Solver-states","content":" Solver states"},{"id":3027,"pagetitle":"Difference of Convex","title":"Manopt.DifferenceOfConvexState","ref":"/manopt/stable/solvers/difference_of_convex/#Manopt.DifferenceOfConvexState","content":" Manopt.DifferenceOfConvexState  ‚Äî  Type DifferenceOfConvexState{Pr,St,P,T,SC<:StoppingCriterion} <:\n           AbstractManoptSolverState A struct to store the current state of the [ difference_of_convex_algorithm ])(@ref). It comes in two forms, depending on the realisation of the  subproblem . Fields p::P : a point on the manifold  $\\mathcal M$  storing the current iterate X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing a subgradient at the current iterate sub_problem::Union{AbstractManoptProblem, F} :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state::Union{AbstractManoptProblem, F} :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled The sub task consists of a method to solve \\[    \\operatorname*{arg\\,min}_{q‚àà\\mathcal M}\\ g(p) - ‚ü®X, \\log_p q‚ü©\\] is needed. Besides a problem and a state, one can also provide a function and an  AbstractEvaluationType , respectively, to indicate a closed form solution for the sub task. Constructors DifferenceOfConvexState(M, sub_problem, sub_state; kwargs...)\nDifferenceOfConvexState(M, sub_solver; evaluation=InplaceEvaluation(), kwargs...) Generate the state either using a solver from Manopt, given by an  AbstractManoptProblem sub_problem  and an  AbstractManoptSolverState sub_state , or a closed form solution  sub_solver  for the sub-problem the function expected to be of the form  (M, p, X) -> q  or  (M, q, p, X) -> q , where by default its  AbstractEvaluationType evaluation  is in-place of  q . Here the elements passed are the current iterate  p  and the subgradient  X  of  h  can be passed to that function. further keyword arguments p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value stopping_criterion= StopAfterIteration (200) : a functor indicating that the stopping criterion is fulfilled X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ to specify the representation of a tangent vector source"},{"id":3028,"pagetitle":"Difference of Convex","title":"Manopt.DifferenceOfConvexProximalState","ref":"/manopt/stable/solvers/difference_of_convex/#Manopt.DifferenceOfConvexProximalState","content":" Manopt.DifferenceOfConvexProximalState  ‚Äî  Type DifferenceOfConvexProximalState{P, T, Pr, St, S<:Stepsize, SC<:StoppingCriterion, RTR<:AbstractRetractionMethod, ITR<:AbstractInverseRetractionMethod}\n    <: AbstractSubProblemSolverState A struct to store the current state of the algorithm as well as the form. It comes in two forms, depending on the realisation of the  subproblem . Fields inverse_retraction_method::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses p::P : a point on the manifold  $\\mathcal M$  storing the current iterate q::P : a point on the manifold  $\\mathcal M$  storing the gradient step r::P : a point on the manifold  $\\mathcal M$  storing the result of the proximal map retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize::Stepsize : a functor inheriting from  Stepsize  to determine a step size stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled X ,  Y : the current gradient and descent direction, respectively their common type is set by the keyword  X sub_problem::Union{AbstractManoptProblem, F} :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state::Union{AbstractManoptProblem, F} :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. Constructor DifferenceOfConvexProximalState(M::AbstractManifold, sub_problem, sub_state; kwargs...) construct an difference of convex proximal point state DifferenceOfConvexProximalState(M::AbstractManifold, sub_problem;\n    evaluation=AllocatingEvaluation(), kwargs... ) construct an difference of convex proximal point state, where  sub_problem  is a closed form solution with  evaluation  as type of evaluation. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ sub_problem :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. Keyword arguments inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= ConstantLength () : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopWhenChangeLess` (1e-8) : a functor indicating that the stopping criterion is fulfilled X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ to specify the representation of a tangent vector source"},{"id":3029,"pagetitle":"Difference of Convex","title":"The difference of convex objective","ref":"/manopt/stable/solvers/difference_of_convex/#The-difference-of-convex-objective","content":" The difference of convex objective"},{"id":3030,"pagetitle":"Difference of Convex","title":"Manopt.ManifoldDifferenceOfConvexObjective","ref":"/manopt/stable/solvers/difference_of_convex/#Manopt.ManifoldDifferenceOfConvexObjective","content":" Manopt.ManifoldDifferenceOfConvexObjective  ‚Äî  Type ManifoldDifferenceOfConvexObjective{E} <: AbstractManifoldCostObjective{E} Specify an objective for a  difference_of_convex_algorithm . The objective  $f: \\mathcal M ‚Üí ‚Ñù$  is given as \\[    f(p) = g(p) - h(p)\\] where both  $g$  and  $h$  are convex, lower semicontinuous and proper. Furthermore the subdifferential  $‚àÇh$  of  $h$  is required. Fields cost : an implementation of  $f(p) = g(p)-h(p)$  as a function  f(M,p) . ‚àÇh!! : a deterministic version of  $‚àÇh: \\mathcal M ‚Üí T\\mathcal M$ , in the sense that calling  ‚àÇh(M, p)  returns a subgradient of  $h$  at  p  and if there is more than one, it returns a deterministic choice. Note that the subdifferential might be given in two possible signatures ‚àÇh(M,p)  which does an  AllocatingEvaluation ‚àÇh!(M, X, p)  which does an  InplaceEvaluation  in place of  X . source as well as for the corresponding sub problem"},{"id":3031,"pagetitle":"Difference of Convex","title":"Manopt.LinearizedDCCost","ref":"/manopt/stable/solvers/difference_of_convex/#Manopt.LinearizedDCCost","content":" Manopt.LinearizedDCCost  ‚Äî  Type LinearizedDCCost A functor  (M,q) ‚Üí ‚Ñù  to represent the inner problem of a  ManifoldDifferenceOfConvexObjective . This is a cost function of the form \\[    F_{p_k,X_k}(p) = g(p) - ‚ü®X_k, \\log_{p_k}p‚ü©\\] for a point  p_k  and a tangent vector  X_k  at  p_k  (for example outer iterates) that are stored within this functor as well. Fields g  a function pk  a point on a manifold Xk  a tangent vector at  pk Both interim values can be set using  set_parameter!(::LinearizedDCCost, ::Val{:p}, p)  and  set_parameter!(::LinearizedDCCost, ::Val{:X}, X) , respectively. Constructor LinearizedDCCost(g, p, X) source"},{"id":3032,"pagetitle":"Difference of Convex","title":"Manopt.LinearizedDCGrad","ref":"/manopt/stable/solvers/difference_of_convex/#Manopt.LinearizedDCGrad","content":" Manopt.LinearizedDCGrad  ‚Äî  Type LinearizedDCGrad A functor  (M,X,p) ‚Üí ‚Ñù  to represent the gradient of the inner problem of a  ManifoldDifferenceOfConvexObjective . This is a gradient function of the form \\[    F_{p_k,X_k}(p) = g(p) - ‚ü®X_k, \\log_{p_k}p‚ü©\\] its gradient is given by using  $F=F_1(F_2(p))$ , where  $F_1(X) = ‚ü®X_k,X‚ü©$  and  $F_2(p) = \\log_{p_k}p$  and the chain rule as well as the adjoint differential of the logarithmic map with respect to its argument for  $D^*F_2(p)$ \\[    \\operatorname{grad} F(q) = \\operatorname{grad} f(q) - DF_2^*(q)[X]\\] for a point  pk  and a tangent vector  Xk  at  pk  (the outer iterates) that are stored within this functor as well Fields grad_g!!  the gradient of  $g$  (see also  LinearizedDCCost ) pk  a point on a manifold Xk  a tangent vector at  pk Both interim values can be set using  set_parameter!(::LinearizedDCGrad, ::Val{:p}, p)  and  set_parameter!(::LinearizedDCGrad, ::Val{:X}, X) , respectively. Constructor LinearizedDCGrad(grad_g, p, X; evaluation=AllocatingEvaluation()) Where you specify whether  grad_g  is  AllocatingEvaluation  or  InplaceEvaluation , while this function still provides  both  signatures. source"},{"id":3033,"pagetitle":"Difference of Convex","title":"Manopt.ManifoldDifferenceOfConvexProximalObjective","ref":"/manopt/stable/solvers/difference_of_convex/#Manopt.ManifoldDifferenceOfConvexProximalObjective","content":" Manopt.ManifoldDifferenceOfConvexProximalObjective  ‚Äî  Type ManifoldDifferenceOfConvexProximalObjective{E} <: Problem Specify an objective  difference_of_convex_proximal_point  algorithm. The problem is of the form \\[    \\operatorname*{argmin}_{p‚àà\\mathcal M} g(p) - h(p)\\] where both  $g$  and  $h$  are convex, lower semicontinuous and proper. Fields cost :     implementation of  $f(p) = g(p)-h(p)$ gradient : the gradient of the cost grad_h!! : a function  $\\operatorname{grad}h: \\mathcal M ‚Üí T\\mathcal M$ , Note that both the gradients might be given in two possible signatures as allocating or in-place. Constructor ManifoldDifferenceOfConvexProximalObjective(gradh; cost=nothing, gradient=nothing) an note that neither cost nor gradient are required for the algorithm, just for eventual debug or stopping criteria. source as well as for the corresponding sub problems"},{"id":3034,"pagetitle":"Difference of Convex","title":"Manopt.ProximalDCCost","ref":"/manopt/stable/solvers/difference_of_convex/#Manopt.ProximalDCCost","content":" Manopt.ProximalDCCost  ‚Äî  Type ProximalDCCost A functor  (M, p) ‚Üí ‚Ñù  to represent the inner cost function of a  ManifoldDifferenceOfConvexProximalObjective . This is the cost function of the proximal map of  g . \\[    F_{p_k}(p) = \\frac{1}{2Œª}d_{\\mathcal M}(p_k,p)^2 + g(p)\\] for a point  pk  and a proximal parameter  $Œª$ . Fields g   - a function pk  - a point on a manifold Œª   - the prox parameter Both interim values can be set using  set_parameter!(::ProximalDCCost, ::Val{:p}, p)  and  set_parameter!(::ProximalDCCost, ::Val{:Œª}, Œª) , respectively. Constructor ProximalDCCost(g, p, Œª) source"},{"id":3035,"pagetitle":"Difference of Convex","title":"Manopt.ProximalDCGrad","ref":"/manopt/stable/solvers/difference_of_convex/#Manopt.ProximalDCGrad","content":" Manopt.ProximalDCGrad  ‚Äî  Type ProximalDCGrad A functor  (M,X,p) ‚Üí ‚Ñù  to represent the gradient of the inner cost function of a  ManifoldDifferenceOfConvexProximalObjective . This is the gradient function of the proximal map cost function of  g . Based on \\[    F_{p_k}(p) = \\frac{1}{2Œª}d_{\\mathcal M}(p_k,p)^2 + g(p)\\] it reads \\[    \\operatorname{grad} F_{p_k}(p) = \\operatorname{grad} g(p) - \\frac{1}{Œª}\\log_p p_k\\] for a point  pk  and a proximal parameter  Œª . Fields grad_g   - a gradient function pk  - a point on a manifold Œª   - the prox parameter Both interim values can be set using  set_parameter!(::ProximalDCGrad, ::Val{:p}, p)  and  set_parameter!(::ProximalDCGrad, ::Val{:Œª}, Œª) , respectively. Constructor ProximalDCGrad(grad_g, pk, Œª; evaluation=AllocatingEvaluation()) Where you specify whether  grad_g  is  AllocatingEvaluation  or  InplaceEvaluation , while this function still always provides  both  signatures. source"},{"id":3036,"pagetitle":"Difference of Convex","title":"Helper functions","ref":"/manopt/stable/solvers/difference_of_convex/#Helper-functions","content":" Helper functions"},{"id":3037,"pagetitle":"Difference of Convex","title":"Manopt.get_subtrahend_gradient","ref":"/manopt/stable/solvers/difference_of_convex/#Manopt.get_subtrahend_gradient","content":" Manopt.get_subtrahend_gradient  ‚Äî  Function X = get_subtrahend_gradient(amp, q)\nget_subtrahend_gradient!(amp, X, q) Evaluate the (sub)gradient of the subtrahend  h  from within a  ManifoldDifferenceOfConvexObjective amp  at the point  q  (in place of  X ). The evaluation is done in place of  X  for the  ! -variant. The  T= AllocatingEvaluation  problem might still allocate memory within. When the non-mutating variant is called with a  T= InplaceEvaluation  memory for the result is allocated. source X = get_subtrahend_gradient(M::AbstractManifold, dcpo::ManifoldDifferenceOfConvexProximalObjective, p)\nget_subtrahend_gradient!(M::AbstractManifold, X, dcpo::ManifoldDifferenceOfConvexProximalObjective, p) Evaluate the gradient of the subtrahend  $h$  from within a  ManifoldDifferenceOfConvexProximalObjective P at the point p` (in place of X). source"},{"id":3038,"pagetitle":"Difference of Convex","title":"Technical details","ref":"/manopt/stable/solvers/difference_of_convex/#sec-cp-technical-details","content":" Technical details The  difference_of_convex_algorithm  and  difference_of_convex_proximal_point  solver requires the following functions of a manifold to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  or  retraction_method_dual=  (for  $\\mathcal N$ ) does not have to be specified. An  inverse_retract! (M, X, p, q) ; it is recommended to set the  default_inverse_retraction_method  to a favourite retraction. If this default is set, a  inverse_retraction_method=  or  inverse_retraction_method_dual=  (for  $\\mathcal N$ ) does not have to be specified. By default, one of the stopping criteria is  StopWhenChangeLess , which either requires A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  or  retraction_method_dual=  (for  $\\mathcal N$ ) does not have to be specified. An  inverse_retract! (M, X, p, q) ; it is recommended to set the  default_inverse_retraction_method  to a favourite retraction. If this default is set, a  inverse_retraction_method=  or  inverse_retraction_method_dual=  (for  $\\mathcal N$ ) does not have to be specified or the  distance (M, p, q)  for said default inverse retraction. A  copyto! (M, q, p)  and  copy (M,p)  for points. By default the tangent vector storing the gradient is initialized calling  zero_vector (M,p) . everything the subsolver requires, which by default is the  trust_regions  or if you do not provide a Hessian  gradient_descent ."},{"id":3039,"pagetitle":"Difference of Convex","title":"Literature","ref":"/manopt/stable/solvers/difference_of_convex/#Literature","content":" Literature [ACOO20] Y.¬†T.¬†Almeida, J.¬†X.¬†Cruz Neto, P.¬†R.¬†Oliveira and J.¬†C.¬†Oliveira Souza.  A modified proximal point method for DC functions on Hadamard manifolds .  Computational¬†Optimization¬†and¬†Applications  76 , 649‚Äì673  (2020). [BFSS24] R.¬†Bergmann, O.¬†P.¬†Ferreira, E.¬†M.¬†Santos and J.¬†C.¬†Souza.  The difference of convex algorithm on Hadamard manifolds .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  (2024). [SO15] J.¬†C.¬†Souza and P.¬†R.¬†Oliveira.  A proximal point algorithm for DC fuctions on Hadamard manifolds .  Journal¬†of¬†Global¬†Optimization  63 , 797‚Äì810  (2015)."},{"id":3042,"pagetitle":"Exact Penalty Method","title":"Exact penalty method","ref":"/manopt/stable/solvers/exact_penalty_method/#Exact-penalty-method","content":" Exact penalty method"},{"id":3043,"pagetitle":"Exact Penalty Method","title":"Manopt.exact_penalty_method","ref":"/manopt/stable/solvers/exact_penalty_method/#Manopt.exact_penalty_method","content":" Manopt.exact_penalty_method  ‚Äî  Function exact_penalty_method(M, f, grad_f, p=rand(M); kwargs...)\nexact_penalty_method(M, cmo::ConstrainedManifoldObjective, p=rand(M); kwargs...)\nexact_penalty_method!(M, f, grad_f, p; kwargs...)\nexact_penalty_method!(M, cmo::ConstrainedManifoldObjective, p; kwargs...) perform the exact penalty method (EPM) [ LB19 ] The aim of the EPM is to find a solution of the constrained optimisation task \\[\\begin{aligned}\n\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} & f(p)\\\\\n\\text{subject to}\\quad&g_i(p) ‚â§ 0 \\quad \\text{ for } i= 1, ‚Ä¶, m,\\\\\n\\quad & h_j(p)=0 \\quad \\text{ for } j=1,‚Ä¶,n,\n\\end{aligned}\\] where  M  is a Riemannian manifold, and  $f$ ,  $\\{g_i\\}_{i=1}^{n}$  and  $\\{h_j\\}_{j=1}^{m}$  are twice continuously differentiable functions from  M  to ‚Ñù. For that a weighted  $L_1$ -penalty term for the violation of the constraints is added to the objective \\[f(x) + œÅ\\biggl( \\sum_{i=1}^m \\max\\bigl\\{0, g_i(x)\\bigr\\} + \\sum_{j=1}^n \\vert h_j(x)\\vert\\biggr),\\] where  $œÅ>0$  is the penalty parameter. Since this is non-smooth, a  SmoothingTechnique  with parameter  u  is applied, see the  ExactPenaltyCost . In every step  $k$  of the exact penalty method, the smoothed objective is then minimized over all  $p ‚àà\\mathcal M$ . Then, the accuracy tolerance  $œµ$  and the smoothing parameter  $u$  are updated by setting \\[œµ^{(k)}=\\max\\{œµ_{\\min}, Œ∏_œµ œµ^{(k-1)}\\},\\] where  $œµ_{\\min}$  is the lowest value  $œµ$  is allowed to become and  $Œ∏_œµ ‚àà (0,1)$  is constant scaling factor, and \\[u^{(k)} = \\max \\{u_{\\min}, \\theta_u u^{(k-1)} \\},\\] where  $u_{\\min}$  is the lowest value  $u$  is allowed to become and  $Œ∏_u ‚àà (0,1)$  is constant scaling factor. Finally, the penalty parameter  $œÅ$  is updated as \\[œÅ^{(k)} = \\begin{cases}\nœÅ^{(k-1)}/Œ∏_œÅ,  & \\text{if } \\displaystyle \\max_{j ‚àà \\mathcal{E},i ‚àà \\mathcal{I}} \\Bigl\\{ \\vert h_j(x^{(k)}) \\vert, g_i(x^{(k)})\\Bigr\\} \\geq u^{(k-1)} \\Bigr) ,\\\\\nœÅ^{(k-1)}, & \\text{else,}\n\\end{cases}\\] where  $Œ∏_œÅ ‚àà (0,1)$  is a constant scaling factor. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place p : a point on the manifold  $\\mathcal M$ Keyword arguments if not called with the  ConstrainedManifoldObjective cmo g=nothing : the inequality constraints h=nothing : the equality constraints grad_g=nothing : the gradient of the inequality constraints grad_h=nothing : the gradient of the equality constraints Note that one of the pairs ( g ,  grad_g ) or ( h ,  grad_h ) has to be provided. Otherwise the problem is not constrained and a better solver would be for example  quasi_Newton . Further keyword arguments œµ=1e‚Äì3 : the accuracy tolerance œµ_exponent=1/100 : exponent of the œµ update factor; œµ_min=1e-6 : the lower bound for the accuracy tolerance u=1e‚Äì1 : the smoothing parameter and threshold for violation of the constraints u_exponent=1/100 : exponent of the u update factor; u_min=1e-6 : the lower bound for the smoothing parameter and threshold for violation of the constraints œÅ=1.0 : the penalty parameter equality_constraints=nothing : the number  $n$  of equality constraints. If not provided, a call to the gradient of  g  is performed to estimate these. gradient_range=nothing : specify how both gradients of the constraints are represented gradient_equality_range=gradient_range :  specify how gradients of the equality constraints are represented, see  VectorGradientFunction . gradient_inequality_range=gradient_range :  specify how gradients of the inequality constraints are represented, see  VectorGradientFunction . inequality_constraints=nothing : the number  $m$  of inequality constraints.  If not provided, a call to the gradient of  g  is performed to estimate these. min_stepsize=1e-10 : the minimal step size smoothing= LogarithmicSumOfExponentials : a  SmoothingTechnique  to use sub_cost= ExactPenaltyCost (problem, œÅ, u; smoothing=smoothing) : cost to use in the sub solver This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_grad= ExactPenaltyGrad (problem, œÅ, u; smoothing=smoothing) : gradient to use in the sub solver This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. sub_stopping_criterion= StopAfterIteration (200) | StopWhenGradientNormLess (œµ) | StopWhenStepsizeLess (1e-10) : a stopping cirterion for the sub solver This is used to define the  sub_state=  keyword and has hence no effect, if you set  sub_state  directly. sub_state= DefaultManoptProblem (M, ManifoldGradientObjective `(sub cost, sub grad; evaluation=evaluation):  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. sub_state= QuasiNewtonState :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. where  QuasiNewtonLimitedMemoryDirectionUpdate  with  InverseBFGS  is used stopping_criterion= StopAfterIteration (300) | ( StopWhenSmallerOrEqual (œµ, œµ_min) & StopWhenChangeLess (1e-10) ) : a functor indicating that the stopping criterion is fulfilled For the  range s of the constraints' gradient, other power manifold tangent space representations, mainly the  ArrayPowerRepresentation  can be used if the gradients can be computed more efficiently in that representation. All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3044,"pagetitle":"Exact Penalty Method","title":"Manopt.exact_penalty_method!","ref":"/manopt/stable/solvers/exact_penalty_method/#Manopt.exact_penalty_method!","content":" Manopt.exact_penalty_method!  ‚Äî  Function exact_penalty_method(M, f, grad_f, p=rand(M); kwargs...)\nexact_penalty_method(M, cmo::ConstrainedManifoldObjective, p=rand(M); kwargs...)\nexact_penalty_method!(M, f, grad_f, p; kwargs...)\nexact_penalty_method!(M, cmo::ConstrainedManifoldObjective, p; kwargs...) perform the exact penalty method (EPM) [ LB19 ] The aim of the EPM is to find a solution of the constrained optimisation task \\[\\begin{aligned}\n\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} & f(p)\\\\\n\\text{subject to}\\quad&g_i(p) ‚â§ 0 \\quad \\text{ for } i= 1, ‚Ä¶, m,\\\\\n\\quad & h_j(p)=0 \\quad \\text{ for } j=1,‚Ä¶,n,\n\\end{aligned}\\] where  M  is a Riemannian manifold, and  $f$ ,  $\\{g_i\\}_{i=1}^{n}$  and  $\\{h_j\\}_{j=1}^{m}$  are twice continuously differentiable functions from  M  to ‚Ñù. For that a weighted  $L_1$ -penalty term for the violation of the constraints is added to the objective \\[f(x) + œÅ\\biggl( \\sum_{i=1}^m \\max\\bigl\\{0, g_i(x)\\bigr\\} + \\sum_{j=1}^n \\vert h_j(x)\\vert\\biggr),\\] where  $œÅ>0$  is the penalty parameter. Since this is non-smooth, a  SmoothingTechnique  with parameter  u  is applied, see the  ExactPenaltyCost . In every step  $k$  of the exact penalty method, the smoothed objective is then minimized over all  $p ‚àà\\mathcal M$ . Then, the accuracy tolerance  $œµ$  and the smoothing parameter  $u$  are updated by setting \\[œµ^{(k)}=\\max\\{œµ_{\\min}, Œ∏_œµ œµ^{(k-1)}\\},\\] where  $œµ_{\\min}$  is the lowest value  $œµ$  is allowed to become and  $Œ∏_œµ ‚àà (0,1)$  is constant scaling factor, and \\[u^{(k)} = \\max \\{u_{\\min}, \\theta_u u^{(k-1)} \\},\\] where  $u_{\\min}$  is the lowest value  $u$  is allowed to become and  $Œ∏_u ‚àà (0,1)$  is constant scaling factor. Finally, the penalty parameter  $œÅ$  is updated as \\[œÅ^{(k)} = \\begin{cases}\nœÅ^{(k-1)}/Œ∏_œÅ,  & \\text{if } \\displaystyle \\max_{j ‚àà \\mathcal{E},i ‚àà \\mathcal{I}} \\Bigl\\{ \\vert h_j(x^{(k)}) \\vert, g_i(x^{(k)})\\Bigr\\} \\geq u^{(k-1)} \\Bigr) ,\\\\\nœÅ^{(k-1)}, & \\text{else,}\n\\end{cases}\\] where  $Œ∏_œÅ ‚àà (0,1)$  is a constant scaling factor. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place p : a point on the manifold  $\\mathcal M$ Keyword arguments if not called with the  ConstrainedManifoldObjective cmo g=nothing : the inequality constraints h=nothing : the equality constraints grad_g=nothing : the gradient of the inequality constraints grad_h=nothing : the gradient of the equality constraints Note that one of the pairs ( g ,  grad_g ) or ( h ,  grad_h ) has to be provided. Otherwise the problem is not constrained and a better solver would be for example  quasi_Newton . Further keyword arguments œµ=1e‚Äì3 : the accuracy tolerance œµ_exponent=1/100 : exponent of the œµ update factor; œµ_min=1e-6 : the lower bound for the accuracy tolerance u=1e‚Äì1 : the smoothing parameter and threshold for violation of the constraints u_exponent=1/100 : exponent of the u update factor; u_min=1e-6 : the lower bound for the smoothing parameter and threshold for violation of the constraints œÅ=1.0 : the penalty parameter equality_constraints=nothing : the number  $n$  of equality constraints. If not provided, a call to the gradient of  g  is performed to estimate these. gradient_range=nothing : specify how both gradients of the constraints are represented gradient_equality_range=gradient_range :  specify how gradients of the equality constraints are represented, see  VectorGradientFunction . gradient_inequality_range=gradient_range :  specify how gradients of the inequality constraints are represented, see  VectorGradientFunction . inequality_constraints=nothing : the number  $m$  of inequality constraints.  If not provided, a call to the gradient of  g  is performed to estimate these. min_stepsize=1e-10 : the minimal step size smoothing= LogarithmicSumOfExponentials : a  SmoothingTechnique  to use sub_cost= ExactPenaltyCost (problem, œÅ, u; smoothing=smoothing) : cost to use in the sub solver This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_grad= ExactPenaltyGrad (problem, œÅ, u; smoothing=smoothing) : gradient to use in the sub solver This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. sub_stopping_criterion= StopAfterIteration (200) | StopWhenGradientNormLess (œµ) | StopWhenStepsizeLess (1e-10) : a stopping cirterion for the sub solver This is used to define the  sub_state=  keyword and has hence no effect, if you set  sub_state  directly. sub_state= DefaultManoptProblem (M, ManifoldGradientObjective `(sub cost, sub grad; evaluation=evaluation):  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. sub_state= QuasiNewtonState :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. where  QuasiNewtonLimitedMemoryDirectionUpdate  with  InverseBFGS  is used stopping_criterion= StopAfterIteration (300) | ( StopWhenSmallerOrEqual (œµ, œµ_min) & StopWhenChangeLess (1e-10) ) : a functor indicating that the stopping criterion is fulfilled For the  range s of the constraints' gradient, other power manifold tangent space representations, mainly the  ArrayPowerRepresentation  can be used if the gradients can be computed more efficiently in that representation. All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3045,"pagetitle":"Exact Penalty Method","title":"State","ref":"/manopt/stable/solvers/exact_penalty_method/#State","content":" State"},{"id":3046,"pagetitle":"Exact Penalty Method","title":"Manopt.ExactPenaltyMethodState","ref":"/manopt/stable/solvers/exact_penalty_method/#Manopt.ExactPenaltyMethodState","content":" Manopt.ExactPenaltyMethodState  ‚Äî  Type ExactPenaltyMethodState{P,T} <: AbstractManoptSolverState Describes the exact penalty method, with Fields œµ : the accuracy tolerance œµ_min : the lower bound for the accuracy tolerance p::P : a point on the manifold  $\\mathcal M$  storing the current iterate œÅ : the penalty parameter sub_problem::Union{AbstractManoptProblem, F} :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state::Union{AbstractManoptProblem, F} :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled u : the smoothing parameter and threshold for violation of the constraints u_min : the lower bound for the smoothing parameter and threshold for violation of the constraints Œ∏_œµ : the scaling factor of the tolerance parameter Œ∏_œÅ : the scaling factor of the penalty parameter Œ∏_u : the scaling factor of the smoothing parameter Constructor ExactPenaltyMethodState(M::AbstractManifold, sub_problem, sub_state; kwargs...) construct the exact penalty state. ExactPenaltyMethodState(M::AbstractManifold, sub_problem;\n    evaluation=AllocatingEvaluation(), kwargs... ) construct the exact penalty state, where  sub_problem  is a closed form solution with  evaluation  as type of evaluation. Keyword arguments œµ=1e-3 œµ_min=1e-6 œµ_exponent=1 / 100 : a shortcut for the scaling factor  $Œ∏_œµ$ Œ∏_œµ=(œµ_min / œµ)^(œµ_exponent) u=1e-1 u_min=1e-6 u_exponent=1 / 100 :  a shortcut for the scaling factor  $Œ∏_u$ . Œ∏_u=(u_min / u)^(u_exponent) p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value œÅ=1.0 Œ∏_œÅ=0.3 stopping_criterion= StopAfterIteration (300) | ( : a functor indicating that the stopping criterion is fulfilled  StopWhenSmallerOrEqual (:œµ, œµ_min) | StopWhenChangeLess (1e-10) ) See also exact_penalty_method source"},{"id":3047,"pagetitle":"Exact Penalty Method","title":"Helping functions","ref":"/manopt/stable/solvers/exact_penalty_method/#Helping-functions","content":" Helping functions"},{"id":3048,"pagetitle":"Exact Penalty Method","title":"Manopt.ExactPenaltyCost","ref":"/manopt/stable/solvers/exact_penalty_method/#Manopt.ExactPenaltyCost","content":" Manopt.ExactPenaltyCost  ‚Äî  Type ExactPenaltyCost{S, Pr, R} Represent the cost of the exact penalty method based on a  ConstrainedManifoldObjective P  and a parameter  $œÅ$  given by \\[f(p) + œÅ\\Bigl(\n    \\sum_{i=0}^m \\max\\{0,g_i(p)\\} + \\sum_{j=0}^n \\lvert h_j(p)\\rvert\n\\Bigr),\\] where an additional parameter  $u$  is used as well as a smoothing technique, for example  LogarithmicSumOfExponentials  or  LinearQuadraticHuber  to obtain a smooth cost function. This struct is also a functor  (M,p) -> v  of the cost  $v$ . Fields œÅ ,  u : as described in the mathematical formula, . co :     the original cost Constructor ExactPenaltyCost(co::ConstrainedManifoldObjective, œÅ, u; smoothing=LinearQuadraticHuber()) source"},{"id":3049,"pagetitle":"Exact Penalty Method","title":"Manopt.ExactPenaltyGrad","ref":"/manopt/stable/solvers/exact_penalty_method/#Manopt.ExactPenaltyGrad","content":" Manopt.ExactPenaltyGrad  ‚Äî  Type ExactPenaltyGrad{S, CO, R} Represent the gradient of the  ExactPenaltyCost  based on a  ConstrainedManifoldObjective co  and a parameter  $œÅ$  and a smoothing technique, which uses an additional parameter  $u$ . This struct is also a functor in both formats (M, p) -> X  to compute the gradient in allocating fashion. (M, X, p)  to compute the gradient in in-place fashion. Fields œÅ ,  u  as stated before co  the nonsmooth objective Constructor ExactPenaltyGradient(co::ConstrainedManifoldObjective, œÅ, u; smoothing=LinearQuadraticHuber()) source"},{"id":3050,"pagetitle":"Exact Penalty Method","title":"Manopt.SmoothingTechnique","ref":"/manopt/stable/solvers/exact_penalty_method/#Manopt.SmoothingTechnique","content":" Manopt.SmoothingTechnique  ‚Äî  Type abstract type SmoothingTechnique Specify a smoothing technique, see for example  ExactPenaltyCost  and  ExactPenaltyGrad . source"},{"id":3051,"pagetitle":"Exact Penalty Method","title":"Manopt.LinearQuadraticHuber","ref":"/manopt/stable/solvers/exact_penalty_method/#Manopt.LinearQuadraticHuber","content":" Manopt.LinearQuadraticHuber  ‚Äî  Type LinearQuadraticHuber <: SmoothingTechnique Specify a smoothing based on  $\\max\\{0,x\\} ‚âà \\mathcal P(x,u)$  for some  $u$ , where \\[\\mathcal P(x, u) = \\begin{cases}\n  0 & \\text{ if } x \\leq 0,\\\\\n  \\frac{x^2}{2u} & \\text{ if } 0 \\leq x \\leq u,\\\\\n  x-\\frac{u}{2} & \\text{ if } x \\geq u.\n\\end{cases}\\] source"},{"id":3052,"pagetitle":"Exact Penalty Method","title":"Manopt.LogarithmicSumOfExponentials","ref":"/manopt/stable/solvers/exact_penalty_method/#Manopt.LogarithmicSumOfExponentials","content":" Manopt.LogarithmicSumOfExponentials  ‚Äî  Type LogarithmicSumOfExponentials <: SmoothingTechnique Specify a smoothing based on  $\\max\\{a,b\\} ‚âà u \\log(\\mathrm{e}^{\\frac{a}{u}}+\\mathrm{e}^{\\frac{b}{u}})$  for some  $u$ . source"},{"id":3053,"pagetitle":"Exact Penalty Method","title":"Technical details","ref":"/manopt/stable/solvers/exact_penalty_method/#sec-dr-technical-details","content":" Technical details The  exact_penalty_method  solver requires the following functions of a manifold to be available A  copyto! (M, q, p)  and  copy (M,p)  for points. Everything the subsolver requires, which by default is the  quasi_Newton  method A  zero_vector (M,p) . The stopping criteria involves  StopWhenChangeLess  and  StopWhenGradientNormLess  which require An  inverse_retract! (M, X, p, q) ; it is recommended to set the  default_inverse_retraction_method  to a favourite retraction. If this default is set, a  inverse_retraction_method=  or  inverse_retraction_method_dual=  (for  $\\mathcal N$ ) does not have to be specified or the  distance (M, p, q)  for said default inverse retraction. the  norm  as well, to stop when the norm of the gradient is small, but if you implemented  inner , the norm is provided already."},{"id":3054,"pagetitle":"Exact Penalty Method","title":"Literature","ref":"/manopt/stable/solvers/exact_penalty_method/#Literature","content":" Literature [LB19] C.¬†Liu and N.¬†Boumal.  Simple algorithms for optimization on Riemannian manifolds with constraints .  Applied¬†Mathematics¬†&¬†Optimization  (2019),  arXiv:1091.10000 ."},{"id":3057,"pagetitle":"Gradient Descent","title":"Gradient descent","ref":"/manopt/stable/solvers/gradient_descent/#Gradient-descent","content":" Gradient descent"},{"id":3058,"pagetitle":"Gradient Descent","title":"Manopt.gradient_descent","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.gradient_descent","content":" Manopt.gradient_descent  ‚Äî  Function gradient_descent(M, f, grad_f, p=rand(M); kwargs...)\ngradient_descent(M, gradient_objective, p=rand(M); kwargs...)\ngradient_descent!(M, f, grad_f, p; kwargs...)\ngradient_descent!(M, gradient_objective, p; kwargs...) perform the gradient descent algorithm \\[p_{k+1} = \\operatorname{retr}_{p_k}\\bigl( s_k\\operatorname{grad}f(p_k) \\bigr),\n\\qquad k=0,1,‚Ä¶\\] where  $s_k > 0$  denotes a step size. The algorithm can be performed in-place of  p . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place p : a point on the manifold  $\\mathcal M$ Alternatively to  f  and  grad_f  you can provide the corresponding  AbstractManifoldFirstOrderObjective gradient_objective  directly. Keyword arguments differential= nothing : specify a specific function to evaluate the differential. By default,  $Df(p)[X] = ‚ü®\\operatorname{grad}f(p),X‚ü©$ . is used direction= IdentityUpdateRule () : specify to perform a certain processing of the direction, for example  Nesterov ,  MomentumGradient  or  AverageGradient . evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second.For example  grad_f(M,p)  allocates, but  grad_f!(M, X, p)  computes the result in-place of  X . retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= default_stepsize (M, GradientDescentState) : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (200) | StopWhenGradientNormLess (1e-8) : a functor indicating that the stopping criterion is fulfilled X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing the gradient at the current iterate All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. If you provide the  ManifoldFirstOrderObjective  directly, the  evaluation=  keyword is ignored. The decorations are still applied to the objective. If you activate tutorial mode (cf.  is_tutorial_mode ), this solver provides additional debug warnings. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3059,"pagetitle":"Gradient Descent","title":"Manopt.gradient_descent!","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.gradient_descent!","content":" Manopt.gradient_descent!  ‚Äî  Function gradient_descent(M, f, grad_f, p=rand(M); kwargs...)\ngradient_descent(M, gradient_objective, p=rand(M); kwargs...)\ngradient_descent!(M, f, grad_f, p; kwargs...)\ngradient_descent!(M, gradient_objective, p; kwargs...) perform the gradient descent algorithm \\[p_{k+1} = \\operatorname{retr}_{p_k}\\bigl( s_k\\operatorname{grad}f(p_k) \\bigr),\n\\qquad k=0,1,‚Ä¶\\] where  $s_k > 0$  denotes a step size. The algorithm can be performed in-place of  p . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place p : a point on the manifold  $\\mathcal M$ Alternatively to  f  and  grad_f  you can provide the corresponding  AbstractManifoldFirstOrderObjective gradient_objective  directly. Keyword arguments differential= nothing : specify a specific function to evaluate the differential. By default,  $Df(p)[X] = ‚ü®\\operatorname{grad}f(p),X‚ü©$ . is used direction= IdentityUpdateRule () : specify to perform a certain processing of the direction, for example  Nesterov ,  MomentumGradient  or  AverageGradient . evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second.For example  grad_f(M,p)  allocates, but  grad_f!(M, X, p)  computes the result in-place of  X . retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= default_stepsize (M, GradientDescentState) : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (200) | StopWhenGradientNormLess (1e-8) : a functor indicating that the stopping criterion is fulfilled X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing the gradient at the current iterate All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. If you provide the  ManifoldFirstOrderObjective  directly, the  evaluation=  keyword is ignored. The decorations are still applied to the objective. If you activate tutorial mode (cf.  is_tutorial_mode ), this solver provides additional debug warnings. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3060,"pagetitle":"Gradient Descent","title":"State","ref":"/manopt/stable/solvers/gradient_descent/#State","content":" State"},{"id":3061,"pagetitle":"Gradient Descent","title":"Manopt.GradientDescentState","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.GradientDescentState","content":" Manopt.GradientDescentState  ‚Äî  Type GradientDescentState{P,T} <: AbstractGradientSolverState Describes the state of a gradient based descent algorithm. Fields p::P : a point on the manifold  $\\mathcal M$  storing the current iterate X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing the gradient at the current iterate stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled stepsize::Stepsize : a functor inheriting from  Stepsize  to determine a step size direction:: DirectionUpdateRule  : a processor to handle the obtained gradient and compute a direction to ‚Äúwalk into‚Äù. retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions Constructor GradientDescentState(M::AbstractManifold; kwargs...) Initialize the gradient descent solver state, where Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ Keyword arguments direction= IdentityUpdateRule () p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value stopping_criterion= StopAfterIteration (100) : a functor indicating that the stopping criterion is fulfilled stepsize= default_stepsize (M, GradientDescentState; retraction_method=retraction_method) : a functor inheriting from  Stepsize  to determine a step size retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ to specify the representation of a tangent vector See also gradient_descent source"},{"id":3062,"pagetitle":"Gradient Descent","title":"Direction update rules","ref":"/manopt/stable/solvers/gradient_descent/#Direction-update-rules","content":" Direction update rules A field of the options is the  direction , a  DirectionUpdateRule , which by default  IdentityUpdateRule  just evaluates the gradient but can be enhanced for example to"},{"id":3063,"pagetitle":"Gradient Descent","title":"Manopt.AverageGradient","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.AverageGradient","content":" Manopt.AverageGradient  ‚Äî  Function AverageGradient(; kwargs...)\nAverageGradient(M::AbstractManifold; kwargs...) Add an average of gradients to a gradient processor. A set of previous directions (from the inner processor) and the last iterate are stored, average is taken after vector transporting them to the current iterates tangent space. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$  (optional) Keyword arguments p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value direction= IdentityUpdateRule  preprocess the actual gradient before adding momentum gradients=[zero_vector(M, p) for _ in 1:n]  how to initialise the internal storage n=10  number of gradient evaluations to take the mean over X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Info This function generates a  ManifoldDefaultsFactory  for  AverageGradientRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":3064,"pagetitle":"Gradient Descent","title":"Manopt.DirectionUpdateRule","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.DirectionUpdateRule","content":" Manopt.DirectionUpdateRule  ‚Äî  Type DirectionUpdateRule A general functor, that handles direction update rules. It's fields are usually only a  StoreStateAction  by default initialized to the fields required for the specific coefficient, but can also be replaced by a (common, global) individual one that provides these values. source"},{"id":3065,"pagetitle":"Gradient Descent","title":"Manopt.IdentityUpdateRule","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.IdentityUpdateRule","content":" Manopt.IdentityUpdateRule  ‚Äî  Type IdentityUpdateRule <: DirectionUpdateRule The default gradient direction update is the identity, usually it just evaluates the gradient. You can also use  Gradient()  to create the corresponding factory, though this only delays this parameter-free instantiation to later. source"},{"id":3066,"pagetitle":"Gradient Descent","title":"Manopt.MomentumGradient","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.MomentumGradient","content":" Manopt.MomentumGradient  ‚Äî  Function MomentumGradient() Append a momentum to a gradient processor, where the last direction and last iterate are stored and the new is composed as  $Œ∑_i = m*Œ∑_{i-1}' - s d_i$ , where  $sd_i$  is the current (inner) direction and  $Œ∑_{i-1}'$  is the vector transported last direction multiplied by momentum  $m$ . Input M  (optional) Keyword arguments p= rand (M) : a point on the manifold  $\\mathcal M$ direction= IdentityUpdateRule  preprocess the actual gradient before adding momentum X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ momentum=0.2  amount of momentum to use vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Info This function generates a  ManifoldDefaultsFactory  for  MomentumGradientRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":3067,"pagetitle":"Gradient Descent","title":"Manopt.Nesterov","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.Nesterov","content":" Manopt.Nesterov  ‚Äî  Function Nesterov(; kwargs...)\nNesterov(M::AbstractManifold; kwargs...) Assume  $f$  is  $L$ -Lipschitz and  $Œº$ -strongly convex. Given a step size  $h_k<\\frac{1}{L}$  (from the  GradientDescentState a  shrinkage  parameter  $Œ≤_k$ and a current iterate  $p_k$ as well as the interim values  $Œ≥_k$  and  $v_k$  from the previous iterate. This compute a Nesterov type update using the following steps, see [ ZS18 ] Compute the positive root  $Œ±_k‚àà(0,1)$  of  $Œ±^2 = h_k\\bigl((1-Œ±_k)Œ≥_k+Œ±_k Œº\\bigr)$ . Set  $\\barŒ≥_k+1 = (1-Œ±_k)Œ≥_k + Œ±_kŒº$ $y_k = \\operatorname{retr}_{p_k}\\Bigl(\\frac{Œ±_kŒ≥_k}{Œ≥_k + Œ±_kŒº}\\operatorname{retr}^{-1}_{p_k}v_k \\Bigr)$ $x_{k+1} = \\operatorname{retr}_{y_k}(-h_k \\operatorname{grad}f(y_k))$ $v_{k+1} = \\operatorname{retr}_{y_k}\\Bigl(\\frac{(1-Œ±_k)Œ≥_k}{\\barŒ≥_k}\\operatorname{retr}_{y_k}^{-1}(v_k) - \\frac{Œ±_k}{\\barŒ≥_{k+1}}\\operatorname{grad}f(y_k) \\Bigr)$ $Œ≥_{k+1} = \\frac{1}{1+Œ≤_k}\\barŒ≥_{k+1}$ Then the direction from  $p_k$  to  $p_k+1$  by  $d = \\operatorname{retr}^{-1}_{p_k}p_{k+1}$  is returned. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$  (optional) Keyword arguments p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value Œ≥=0.001 Œº=0.9 shrinkage = k -> 0.8 inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses Info This function generates a  ManifoldDefaultsFactory  for  NesterovRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":3068,"pagetitle":"Gradient Descent","title":"Manopt.PreconditionedDirection","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.PreconditionedDirection","content":" Manopt.PreconditionedDirection  ‚Äî  Function PreconditionedDirection(preconditioner; kwargs...)\nPreconditionedDirection(M::AbstractManifold, preconditioner; kwargs...) Add a preconditioner to a gradient processor following the  motivation for optimization , as a linear invertible map  $P: T_{p}\\mathcal M ‚Üí T_{p}\\mathcal M$  that usually should be symmetric:  $‚ü®X, P(Y)‚ü© = ‚ü®P(X), Y‚ü©$ positive definite  $‚ü®X, P(X)‚ü© > 0$  for  $X$  not the zero-vector The gradient is then preconditioned as  $P(X)$ , where  $X$  is either the gradient of the objective or the result of a previous (internally stored) gradient processor. For example if you provide as the preconditioner the inverse of the Hessian  $\\operatorname{Hess}^{-1} f$ , you turn a gradient descent into a Newton method. Arguments M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$  (optional) preconditioner :   preconditioner function, either as a  (M, p, X) -> Y  allocating or  (M, Y, p, X) -> Y  mutating function Keyword arguments direction= IdentityUpdateRule  internal  DirectionUpdateRule  to determine the gradients to store or a  ManifoldDefaultsFactory  generating one evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. Info This function generates a  ManifoldDefaultsFactory  for  PreconditionedDirectionRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source which internally use the  ManifoldDefaultsFactory  and produce the internal elements"},{"id":3069,"pagetitle":"Gradient Descent","title":"Manopt.AverageGradientRule","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.AverageGradientRule","content":" Manopt.AverageGradientRule  ‚Äî  Type AverageGradientRule <: DirectionUpdateRule Add an average of gradients to a gradient processor. A set of previous directions (from the inner processor) and the last iterate are stored. The average is taken after vector transporting them to the current iterates tangent space. Fields gradients :               the last  n  gradient/direction updates last_iterate :            last iterate (needed to transport the gradients) direction :               internal  DirectionUpdateRule  to determine directions to apply the averaging to vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Constructors AverageGradientRule(\n    M::AbstractManifold;\n    p::P=rand(M);\n    n::Int=10\n    direction::Union{<:DirectionUpdateRule,ManifoldDefaultsFactory}=IdentityUpdateRule(),\n    gradients = fill(zero_vector(p.M, o.x),n),\n    last_iterate = deepcopy(x0),\n    vector_transport_method = default_vector_transport_method(M, typeof(p))\n) Add average to a gradient problem, where n :                       determines the size of averaging direction :               is the internal  DirectionUpdateRule  to determine the gradients to store gradients :               can be pre-filled with some history last_iterate :            stores the last iterate vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports source"},{"id":3070,"pagetitle":"Gradient Descent","title":"Manopt.ConjugateDescentCoefficientRule","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.ConjugateDescentCoefficientRule","content":" Manopt.ConjugateDescentCoefficientRule  ‚Äî  Type ConjugateDescentCoefficientRule <: DirectionUpdateRule A functor  (problem, state, k) -> Œ≤_k  to compute the conjugate gradient update coefficient adapted to manifolds See also  conjugate_gradient_descent Constructor ConjugateDescentCoefficientRule() Construct the conjugate descent coefficient update rule, a new storage is created by default. See also ConjugateDescentCoefficient ,  conjugate_gradient_descent source"},{"id":3071,"pagetitle":"Gradient Descent","title":"Manopt.MomentumGradientRule","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.MomentumGradientRule","content":" Manopt.MomentumGradientRule  ‚Äî  Type MomentumGradientRule <: DirectionUpdateRule Store the necessary information to compute the  MomentumGradient  direction update. Fields p_old::P : a point on the manifold  $\\mathcal M$ momentum::Real : factor for the momentum direction : internal  DirectionUpdateRule  to determine directions to add the momentum to. vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports X_old::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ Constructors MomentumGradientRule(M::AbstractManifold; kwargs...) Initialize a momentum gradient rule to  s , where  p  and  X  are memory for interim values. Keyword arguments p= rand (M) : a point on the manifold  $\\mathcal M$ s= IdentityUpdateRule () momentum=0.2 vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ See also MomentumGradient source"},{"id":3072,"pagetitle":"Gradient Descent","title":"Manopt.NesterovRule","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.NesterovRule","content":" Manopt.NesterovRule  ‚Äî  Type NesterovRule <: DirectionUpdateRule Compute a Nesterov inspired direction update rule. See  Nesterov  for details Fields Œ≥::Real ,  Œº::Real : coefficients from the last iterate v::P :      an interim point to compute the next gradient evaluation point  y_k shrinkage : a function  k -> ...  to compute the shrinkage  $Œ≤_k$  per iterate  k `. inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses Constructor NesterovRule(M::AbstractManifold; kwargs...) Keyword arguments p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value Œ≥=0.001 ` Œº=0.9 ` shrinkage = k -> 0.8 inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses See also Nesterov source"},{"id":3073,"pagetitle":"Gradient Descent","title":"Manopt.PreconditionedDirectionRule","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.PreconditionedDirectionRule","content":" Manopt.PreconditionedDirectionRule  ‚Äî  Type PreconditionedDirectionRule{E<:AbstractEvaluationType} <: DirectionUpdateRule Add a preconditioning as gradient processor, see  PreconditionedDirection  for more mathematical background. Fields direction :      internal  DirectionUpdateRule  to determine directions to apply the preconditioning to preconditioner : the preconditioner function Constructors PreconditionedDirectionRule(\n    M::AbstractManifold,\n    preconditioner;\n    direction::Union{<:DirectionUpdateRule,ManifoldDefaultsFactory}=IdentityUpdateRule(),\n    evaluation::AbstractEvaluationType=AllocatingEvaluation()\n) Add preconditioning to a gradient problem. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ preconditioner :   preconditioner function, either as a  (M, p, X)  -> Y allocating or (M, Y, p, X) -> Y` mutating function Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. direction= IdentityUpdateRule  internal  DirectionUpdateRule  to determine the gradients to store or a  ManifoldDefaultsFactory  generating one source"},{"id":3074,"pagetitle":"Gradient Descent","title":"Debug actions","ref":"/manopt/stable/solvers/gradient_descent/#Debug-actions","content":" Debug actions"},{"id":3075,"pagetitle":"Gradient Descent","title":"Manopt.DebugGradient","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.DebugGradient","content":" Manopt.DebugGradient  ‚Äî  Type DebugGradient <: DebugAction debug for the gradient evaluated at the current iterate Constructors DebugGradient(; long=false, prefix= , format= \"$prefix%s\", io=stdout) display the short ( false ) or long ( true ) default text for the gradient, or set the  prefix  manually. Alternatively the complete format can be set. source"},{"id":3076,"pagetitle":"Gradient Descent","title":"Manopt.DebugGradientNorm","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.DebugGradientNorm","content":" Manopt.DebugGradientNorm  ‚Äî  Type DebugGradientNorm <: DebugAction debug for gradient evaluated at the current iterate. Constructors DebugGradientNorm([long=false,p=print]) display the short ( false ) or long ( true ) default text for the gradient norm. DebugGradientNorm(prefix[, p=print]) display the a  prefix  in front of the gradient norm. source"},{"id":3077,"pagetitle":"Gradient Descent","title":"Manopt.DebugStepsize","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.DebugStepsize","content":" Manopt.DebugStepsize  ‚Äî  Type DebugStepsize <: DebugAction debug for the current step size. Constructors DebugStepsize(;long=false,prefix=\"step size:\", format=\"$prefix%s\", io=stdout) display the a  prefix  in front of the step size. source"},{"id":3078,"pagetitle":"Gradient Descent","title":"Record actions","ref":"/manopt/stable/solvers/gradient_descent/#Record-actions","content":" Record actions"},{"id":3079,"pagetitle":"Gradient Descent","title":"Manopt.RecordGradient","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.RecordGradient","content":" Manopt.RecordGradient  ‚Äî  Type RecordGradient <: RecordAction record the gradient evaluated at the current iterate Constructors RecordGradient(Œæ) initialize the  RecordAction  to the corresponding type of the tangent vector. source"},{"id":3080,"pagetitle":"Gradient Descent","title":"Manopt.RecordGradientNorm","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.RecordGradientNorm","content":" Manopt.RecordGradientNorm  ‚Äî  Type RecordGradientNorm <: RecordAction record the norm of the current gradient source"},{"id":3081,"pagetitle":"Gradient Descent","title":"Manopt.RecordStepsize","ref":"/manopt/stable/solvers/gradient_descent/#Manopt.RecordStepsize","content":" Manopt.RecordStepsize  ‚Äî  Type RecordStepsize <: RecordAction record the step size source"},{"id":3082,"pagetitle":"Gradient Descent","title":"Technical details","ref":"/manopt/stable/solvers/gradient_descent/#sec-gradient-descent-technical-details","content":" Technical details The  gradient_descent  solver requires the following functions of a manifold to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified. By default gradient descent uses  ArmijoLinesearch  which requires  max_stepsize (M)  to be set and an implementation of  inner (M, p, X) . By default the stopping criterion uses the  norm  as well, to stop when the norm of the gradient is small, but if you implemented  inner , the norm is provided already. By default the tangent vector storing the gradient is initialized calling  zero_vector (M,p) ."},{"id":3083,"pagetitle":"Gradient Descent","title":"Literature","ref":"/manopt/stable/solvers/gradient_descent/#Literature","content":" Literature [Lue72] D.¬†G.¬†Luenberger.  The gradient projection method along geodesics . Management¬†Science  18 , 620‚Äì631 (1972). [ZS18] H.¬†Zhang and S.¬†Sra.  Towards Riemannian accelerated gradient methods , arXiv¬†Preprint,¬†1806.02812 (2018)."},{"id":3086,"pagetitle":"Interior Point Newton","title":"Interior point Newton method","ref":"/manopt/stable/solvers/interior_point_Newton/#Interior-point-Newton-method","content":" Interior point Newton method"},{"id":3087,"pagetitle":"Interior Point Newton","title":"Manopt.interior_point_Newton","ref":"/manopt/stable/solvers/interior_point_Newton/#Manopt.interior_point_Newton","content":" Manopt.interior_point_Newton  ‚Äî  Function interior_point_Newton(M, f, grad_f, Hess_f, p=rand(M); kwargs...)\ninterior_point_Newton(M, cmo::ConstrainedManifoldObjective, p=rand(M); kwargs...)\ninterior_point_Newton!(M, f, grad]_f, Hess_f, p; kwargs...)\ninterior_point_Newton(M, ConstrainedManifoldObjective, p; kwargs...) perform the interior point Newton method following [ LY24 ]. In order to solve the constrained problem \\[\\begin{aligned}\n\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} & f(p)\\\\\n\\text{subject to}\\quad&g_i(p) ‚â§ 0 \\quad \\text{ for } i= 1, ‚Ä¶, m,\\\\\n\\quad & h_j(p)=0 \\quad \\text{ for } j=1,‚Ä¶,n,\n\\end{aligned}\\] This algorithms iteratively solves the linear system based on extending the KKT system by a slack variable  s . \\[\\operatorname{J} F(p, Œº, Œª, s)[X, Y, Z, W] = -F(p, Œº, Œª, s),\n\\text{ where }\nX ‚àà T_{p}\\mathcal M, Y,W ‚àà ‚Ñù^m, Z ‚àà ‚Ñù^n,\\] see  CondensedKKTVectorFieldJacobian  and  CondensedKKTVectorField , respectively, for the reduced form, this is usually solved in. From the resulting  X  and  Z  in the reeuced form, the other two,  $Y$ ,  $W$ , are then computed. From the gradient  $(X,Y,Z,W)$  at the current iterate  $(p, Œº, Œª, s)$ , a line search is performed using the  KKTVectorFieldNormSq  norm of the KKT vector field (squared) and its gradient  KKTVectorFieldNormSqGradient  together with the  InteriorPointCentralityCondition . Note that since the vector field  $F$  includes the gradients of the constraint functions  $g, h$ , its gradient or Jacobian requires the Hessians of the constraints. For that seach direction a line search is performed, that additionally ensures that the constraints are further fulfilled. Input M : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place Hess_f : the (Riemannian) Hessian  $\\operatorname{Hess}f: T_{p}\\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p, X) -> Y  or a function  (M, Y, p, X) -> Y  computing  Y  in-place p : a point on the manifold  $\\mathcal M$ or a  ConstrainedManifoldObjective cmo  containing  f ,  grad_f ,  Hess_f , and the constraints Keyword arguments The keyword arguments related to the constraints (the first eleven) are ignored if you pass a  ConstrainedManifoldObjective cmo centrality_condition=missing ; an additional condition when to accept a step size. This can be used to ensure that the resulting iterate is still an interior point if you provide a check  (N,q) -> true/false , where  N  is the manifold of the  step_problem . equality_constraints=nothing : the number  $n$  of equality constraints. evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. g=nothing : the inequality constraints grad_g=nothing : the gradient of the inequality constraints grad_h=nothing : the gradient of the equality constraints gradient_range=nothing : specify how gradients are represented, where  nothing  is equivalent to  NestedPowerRepresentation gradient_equality_range=gradient_range : specify how the gradients of the equality constraints are represented gradient_inequality_range=gradient_range : specify how the gradients of the inequality constraints are represented h=nothing : the equality constraints Hess_g=nothing : the Hessian of the inequality constraints Hess_h=nothing : the Hessian of the equality constraints inequality_constraints=nothing : the number  $m$  of inequality constraints. Œª=ones(length(h(M, p))) : the Lagrange multiplier with respect to the equality constraints  $h$ Œº=ones(length(g(M, p))) : the Lagrange multiplier with respect to the inequality constraints  $g$ retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions œÅ=Œº's / length(Œº) :  store the orthogonality  Œº's/m  to compute the barrier parameter  Œ≤  in the sub problem. s=copy(Œº) : initial value for the slack variables œÉ= calculate_œÉ (M, cmo, p, Œº, Œª, s) :  scaling factor for the barrier parameter  Œ≤  in the sub problem, which is updated during the iterations step_objective : a  ManifoldGradientObjective  of the norm of the KKT vector field  KKTVectorFieldNormSq  and its gradient  KKTVectorFieldNormSqGradient step_problem : the manifold  $\\mathcal M √ó ‚Ñù^m √ó ‚Ñù^n √ó ‚Ñù^m$  together with the  step_objective  as the problem the linesearch  stepsize=  employs for determining a step size step_state : the  StepsizeState  with point and search direction stepsize= ArmijoLinesearch () : a functor inheriting from  Stepsize  to determine a step size with the  centrality_condtion  keyword as additional criterion to accept a step, if this is provided stopping_criterion= StopAfterIteration (200) | StopWhenKKTResidualLess (1e-8) : a functor indicating that the stopping criterion is fulfilled a stopping criterion, by default depending on the residual of the KKT vector field or a maximal number of steps, which ever hits first. sub_kwargs=(;) : keyword arguments to decorate the sub options, for example debug, that automatically respects the main solvers debug options (like sub-sampling) as well sub_objective : The  SymmetricLinearSystemObjective  modelling the system of equations to use in the sub solver, includes the  CondensedKKTVectorFieldJacobian $\\mathcal A(X)$  and the  CondensedKKTVectorField $b$  in  $\\mathcal A(X) + b = 0$  we aim to solve. This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_stopping_criterion= StopAfterIteration (manifold_dimension(M)) | StopWhenRelativeResidualLess (c,1e-8) , where  $c = \\lVert b \\rVert_{}$  from the system to solve. This is used to define the  sub_state=  keyword and has hence no effect, if you set  sub_state  directly. sub_problem= DefaultManoptProblem (M, sub_objective) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= ConjugateResidualState :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. vector_space= Rn  a function that, given an integer, returns the manifold to be used for the vector space components  $‚Ñù^m,‚Ñù^n$ X= zero_vector (M,p) : th initial gradient with respect to  p . Y=zero(Œº) :  the initial gradient with respct to  Œº Z=zero(Œª) :  the initial gradient with respct to  Œª W=zero(s) :  the initial gradient with respct to  s As well as internal keywords used to set up these given keywords like  _step_M ,  _step_p ,  _sub_M ,  _sub_p , and  _sub_X , that should not be changed. All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective, respectively. Note The  centrality_condition=mising  disables to check centrality during the line search, but you can pass  InteriorPointCentralityCondition (cmo, Œ≥) , where  Œ≥  is a constant, to activate this check. Output The obtained approximate constrained minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3088,"pagetitle":"Interior Point Newton","title":"Manopt.interior_point_Newton!","ref":"/manopt/stable/solvers/interior_point_Newton/#Manopt.interior_point_Newton!","content":" Manopt.interior_point_Newton!  ‚Äî  Function interior_point_Newton(M, f, grad_f, Hess_f, p=rand(M); kwargs...)\ninterior_point_Newton(M, cmo::ConstrainedManifoldObjective, p=rand(M); kwargs...)\ninterior_point_Newton!(M, f, grad]_f, Hess_f, p; kwargs...)\ninterior_point_Newton(M, ConstrainedManifoldObjective, p; kwargs...) perform the interior point Newton method following [ LY24 ]. In order to solve the constrained problem \\[\\begin{aligned}\n\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} & f(p)\\\\\n\\text{subject to}\\quad&g_i(p) ‚â§ 0 \\quad \\text{ for } i= 1, ‚Ä¶, m,\\\\\n\\quad & h_j(p)=0 \\quad \\text{ for } j=1,‚Ä¶,n,\n\\end{aligned}\\] This algorithms iteratively solves the linear system based on extending the KKT system by a slack variable  s . \\[\\operatorname{J} F(p, Œº, Œª, s)[X, Y, Z, W] = -F(p, Œº, Œª, s),\n\\text{ where }\nX ‚àà T_{p}\\mathcal M, Y,W ‚àà ‚Ñù^m, Z ‚àà ‚Ñù^n,\\] see  CondensedKKTVectorFieldJacobian  and  CondensedKKTVectorField , respectively, for the reduced form, this is usually solved in. From the resulting  X  and  Z  in the reeuced form, the other two,  $Y$ ,  $W$ , are then computed. From the gradient  $(X,Y,Z,W)$  at the current iterate  $(p, Œº, Œª, s)$ , a line search is performed using the  KKTVectorFieldNormSq  norm of the KKT vector field (squared) and its gradient  KKTVectorFieldNormSqGradient  together with the  InteriorPointCentralityCondition . Note that since the vector field  $F$  includes the gradients of the constraint functions  $g, h$ , its gradient or Jacobian requires the Hessians of the constraints. For that seach direction a line search is performed, that additionally ensures that the constraints are further fulfilled. Input M : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place Hess_f : the (Riemannian) Hessian  $\\operatorname{Hess}f: T_{p}\\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p, X) -> Y  or a function  (M, Y, p, X) -> Y  computing  Y  in-place p : a point on the manifold  $\\mathcal M$ or a  ConstrainedManifoldObjective cmo  containing  f ,  grad_f ,  Hess_f , and the constraints Keyword arguments The keyword arguments related to the constraints (the first eleven) are ignored if you pass a  ConstrainedManifoldObjective cmo centrality_condition=missing ; an additional condition when to accept a step size. This can be used to ensure that the resulting iterate is still an interior point if you provide a check  (N,q) -> true/false , where  N  is the manifold of the  step_problem . equality_constraints=nothing : the number  $n$  of equality constraints. evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. g=nothing : the inequality constraints grad_g=nothing : the gradient of the inequality constraints grad_h=nothing : the gradient of the equality constraints gradient_range=nothing : specify how gradients are represented, where  nothing  is equivalent to  NestedPowerRepresentation gradient_equality_range=gradient_range : specify how the gradients of the equality constraints are represented gradient_inequality_range=gradient_range : specify how the gradients of the inequality constraints are represented h=nothing : the equality constraints Hess_g=nothing : the Hessian of the inequality constraints Hess_h=nothing : the Hessian of the equality constraints inequality_constraints=nothing : the number  $m$  of inequality constraints. Œª=ones(length(h(M, p))) : the Lagrange multiplier with respect to the equality constraints  $h$ Œº=ones(length(g(M, p))) : the Lagrange multiplier with respect to the inequality constraints  $g$ retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions œÅ=Œº's / length(Œº) :  store the orthogonality  Œº's/m  to compute the barrier parameter  Œ≤  in the sub problem. s=copy(Œº) : initial value for the slack variables œÉ= calculate_œÉ (M, cmo, p, Œº, Œª, s) :  scaling factor for the barrier parameter  Œ≤  in the sub problem, which is updated during the iterations step_objective : a  ManifoldGradientObjective  of the norm of the KKT vector field  KKTVectorFieldNormSq  and its gradient  KKTVectorFieldNormSqGradient step_problem : the manifold  $\\mathcal M √ó ‚Ñù^m √ó ‚Ñù^n √ó ‚Ñù^m$  together with the  step_objective  as the problem the linesearch  stepsize=  employs for determining a step size step_state : the  StepsizeState  with point and search direction stepsize= ArmijoLinesearch () : a functor inheriting from  Stepsize  to determine a step size with the  centrality_condtion  keyword as additional criterion to accept a step, if this is provided stopping_criterion= StopAfterIteration (200) | StopWhenKKTResidualLess (1e-8) : a functor indicating that the stopping criterion is fulfilled a stopping criterion, by default depending on the residual of the KKT vector field or a maximal number of steps, which ever hits first. sub_kwargs=(;) : keyword arguments to decorate the sub options, for example debug, that automatically respects the main solvers debug options (like sub-sampling) as well sub_objective : The  SymmetricLinearSystemObjective  modelling the system of equations to use in the sub solver, includes the  CondensedKKTVectorFieldJacobian $\\mathcal A(X)$  and the  CondensedKKTVectorField $b$  in  $\\mathcal A(X) + b = 0$  we aim to solve. This is used to define the  sub_problem=  keyword and has hence no effect, if you set  sub_problem  directly. sub_stopping_criterion= StopAfterIteration (manifold_dimension(M)) | StopWhenRelativeResidualLess (c,1e-8) , where  $c = \\lVert b \\rVert_{}$  from the system to solve. This is used to define the  sub_state=  keyword and has hence no effect, if you set  sub_state  directly. sub_problem= DefaultManoptProblem (M, sub_objective) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= ConjugateResidualState :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. vector_space= Rn  a function that, given an integer, returns the manifold to be used for the vector space components  $‚Ñù^m,‚Ñù^n$ X= zero_vector (M,p) : th initial gradient with respect to  p . Y=zero(Œº) :  the initial gradient with respct to  Œº Z=zero(Œª) :  the initial gradient with respct to  Œª W=zero(s) :  the initial gradient with respct to  s As well as internal keywords used to set up these given keywords like  _step_M ,  _step_p ,  _sub_M ,  _sub_p , and  _sub_X , that should not be changed. All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective, respectively. Note The  centrality_condition=mising  disables to check centrality during the line search, but you can pass  InteriorPointCentralityCondition (cmo, Œ≥) , where  Œ≥  is a constant, to activate this check. Output The obtained approximate constrained minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3089,"pagetitle":"Interior Point Newton","title":"State","ref":"/manopt/stable/solvers/interior_point_Newton/#State","content":" State"},{"id":3090,"pagetitle":"Interior Point Newton","title":"Manopt.InteriorPointNewtonState","ref":"/manopt/stable/solvers/interior_point_Newton/#Manopt.InteriorPointNewtonState","content":" Manopt.InteriorPointNewtonState  ‚Äî  Type InteriorPointNewtonState{P,T} <: AbstractHessianSolverState Fields Œª :           the Lagrange multiplier with respect to the equality constraints Œº :           the Lagrange multiplier with respect to the inequality constraints p::P : a point on the manifold  $\\mathcal M$  storing the current iterate s :           the current slack variable sub_problem::Union{AbstractManoptProblem, F} :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state::Union{AbstractManoptProblem, F} :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. X :           the current gradient with respect to  p Y :           the current gradient with respect to  Œº Z :           the current gradient with respect to  Œª W :           the current gradient with respect to  s œÅ :           store the orthogonality  Œº's/m  to compute the barrier parameter  Œ≤  in the sub problem œÉ :           scaling factor for the barrier parameter  Œ≤  in the sub problem stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize::Stepsize : a functor inheriting from  Stepsize  to determine a step size step_problem : an  AbstractManoptProblem  storing the manifold and objective for the line search step_state : storing iterate and search direction in a state for the line search, see  StepsizeState Constructor InteriorPointNewtonState(\n    M::AbstractManifold,\n    cmo::ConstrainedManifoldObjective,\n    sub_problem::Pr,\n    sub_state::St;\n    kwargs...\n) Initialize the state, where both the  AbstractManifold  and the  ConstrainedManifoldObjective  are used to fill in reasonable defaults for the keywords. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ cmo :         a  ConstrainedManifoldObjective sub_problem :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. Keyword arguments Let  m  and  n  denote the number of inequality and equality constraints, respectively p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value Œº=ones(m) X= zero_vector (M,p) Y=zero(Œº) Œª=zeros(n) Z=zero(Œª) s=ones(m) W=zero(s) œÅ=Œº's/m œÉ= calculate_œÉ (M, cmo, p, Œº, Œª, s) stopping_criterion= StopAfterIteration (200) | StopWhenChangeLess (1e-8) : a functor indicating that the stopping criterion is fulfilled retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions step_objective= ManifoldGradientObjective ( KKTVectorFieldNormSq (cmo) ,  KKTVectorFieldNormSqGradient (cmo) ; evaluation= InplaceEvaluation ()) vector_space= Rn : a function that, given an integer, returns the manifold to be used for the vector space components  $‚Ñù^m,‚Ñù^n$ step_problem : wrap the manifold  $\\mathcal M √ó ‚Ñù^m √ó ‚Ñù^n √ó ‚Ñù^m$ step_state : the  StepsizeState  with point and search direction stepsize= ArmijoLinesearch () : a functor inheriting from  Stepsize  to determine a step size with the  InteriorPointCentralityCondition  as additional condition to accept a step and internally  _step_M  and  _step_p  for the manifold and point in the stepsize. source"},{"id":3091,"pagetitle":"Interior Point Newton","title":"Subproblem functions","ref":"/manopt/stable/solvers/interior_point_Newton/#Subproblem-functions","content":" Subproblem functions"},{"id":3092,"pagetitle":"Interior Point Newton","title":"Manopt.CondensedKKTVectorField","ref":"/manopt/stable/solvers/interior_point_Newton/#Manopt.CondensedKKTVectorField","content":" Manopt.CondensedKKTVectorField  ‚Äî  Type CondensedKKTVectorField{O<:ConstrainedManifoldObjective,T,R} <: AbstractConstrainedSlackFunctor{T,R} Given the constrained optimization problem \\[\\begin{aligned}\n\\min_{p ‚àà\\mathcal{M}} &f(p)\\\\\n\\text{subject to } &g_i(p)\\leq 0 \\quad \\text{ for } i= 1, ‚Ä¶, m,\\\\\n\\quad &h_j(p)=0 \\quad \\text{ for } j=1,‚Ä¶,n,\n\\end{aligned}\\] Then reformulating the KKT conditions of the Lagrangian from the optimality conditions of the Lagrangian \\[\\mathcal L(p, Œº, Œª) = f(p) + \\sum_{j=1}^n Œª_jh_j(p) + \\sum_{i=1}^m Œº_ig_i(p)\\] in a perturbed / barrier method in a condensed form using a slack variable  $s ‚àà ‚Ñù^m$  and a barrier parameter  $Œ≤$  and the Riemannian gradient of the Lagrangian with respect to the first parameter  $\\operatorname{grad}_p L(p, Œº, Œª)$ . Let  $\\mathcal N = \\mathcal M √ó ‚Ñù^n$ . We obtain the linear system \\[\\mathcal A(p,Œª)[X,Y] = -b(p,Œª),\\qquad \\text{where } (X,Y) ‚àà T_{(p,Œª)}\\mathcal N\\] where  $\\mathcal A: T_{(p,Œª)}\\mathcal N ‚Üí T_{(p,Œª)}\\mathcal N$  is a linear operator and this struct models the right hand side  $b(p,Œª) ‚àà T_{(p,Œª)}\\mathcal M$  given by \\[b(p,Œª) = \\begin{pmatrix}\n\\operatorname{grad} f(p)\n+ \\displaystyle\\sum_{j=1}^n Œª_j \\operatorname{grad} h_j(p)\n+ \\displaystyle\\sum_{i=1}^m Œº_i \\operatorname{grad} g_i(p)\n+ \\displaystyle\\sum_{i=1}^m \\frac{Œº_i}{s_i}\\bigl(\n  Œº_i(g_i(p)+s_i) + Œ≤ - Œº_is_i\n\\bigr)\\operatorname{grad} g_i(p)\\\\\nh(p)\n\\end{pmatrix}\\] Fields cmo  the  ConstrainedManifoldObjective Œº::T  the vector in  $‚Ñù^m$  of coefficients for the inequality constraints s::T  the vector in  $‚Ñù^m$  of sclack variables Œ≤::R  the barrier parameter  $Œ≤‚àà‚Ñù$ Constructor CondensedKKTVectorField(cmo, Œº, s, Œ≤) source"},{"id":3093,"pagetitle":"Interior Point Newton","title":"Manopt.CondensedKKTVectorFieldJacobian","ref":"/manopt/stable/solvers/interior_point_Newton/#Manopt.CondensedKKTVectorFieldJacobian","content":" Manopt.CondensedKKTVectorFieldJacobian  ‚Äî  Type CondensedKKTVectorFieldJacobian{O<:ConstrainedManifoldObjective,T,R}  <: AbstractConstrainedSlackFunctor{T,R} Given the constrained optimization problem \\[\\begin{aligned}\n\\min_{p ‚àà\\mathcal{M}} &f(p)\\\\\n\\text{subject to } &g_i(p)\\leq 0 \\quad \\text{ for } i= 1, ‚Ä¶, m,\\\\\n\\quad &h_j(p)=0 \\quad \\text{ for } j=1,‚Ä¶,n,\n\\end{aligned}\\] we reformulate the KKT conditions of the Lagrangian from the optimality conditions of the Lagrangian \\[\\mathcal L(p, Œº, Œª) = f(p) + \\sum_{j=1}^n Œª_jh_j(p) + \\sum_{i=1}^m Œº_ig_i(p)\\] in a perturbed / barrier method enhanced as well as condensed form as using  $\\operatorname{grad}_o L(p, Œº, Œª)$  the Riemannian gradient of the Lagrangian with respect to the first parameter. Let  $\\mathcal N = \\mathcal M √ó ‚Ñù^n$ . We obtain the linear system \\[\\mathcal A(p,Œª)[X,Y] = -b(p,Œª),\\qquad \\text{where } X ‚àà T_p\\mathcal M, Y ‚àà ‚Ñù^n\\] where  $\\mathcal A: T_{(p,Œª)}\\mathcal N ‚Üí T_{(p,Œª)}\\mathcal N$  is a linear operator on  $T_{(p,Œª)}\\mathcal N = T_p\\mathcal M √ó ‚Ñù^n$  given by \\[\\mathcal A(p,Œª)[X,Y] = \\begin{pmatrix}\n\\operatorname{Hess}_p\\mathcal L(p, Œº, Œª)[X]\n+ \\displaystyle\\sum_{i=1}^m \\frac{Œº_i}{s_i}‚ü®\\operatorname{grad} g_i(p), X‚ü©\\operatorname{grad} g_i(p)\n+ \\displaystyle\\sum_{j=1}^n Y_j \\operatorname{grad} h_j(p)\n\\\\\n\\Bigl( ‚ü®\\operatorname{grad} h_j(p), X‚ü© \\Bigr)_{j=1}^n\n\\end{pmatrix}\\] Fields cmo  the  ConstrainedManifoldObjective Œº::V  the vector in  $‚Ñù^m$  of coefficients for the inequality constraints s::V  the vector in  $‚Ñù^m$  of slack variables Œ≤::R  the barrier parameter  $Œ≤‚àà‚Ñù$ Constructor CondensedKKTVectorFieldJacobian(cmo, Œº, s, Œ≤) source"},{"id":3094,"pagetitle":"Interior Point Newton","title":"Manopt.KKTVectorField","ref":"/manopt/stable/solvers/interior_point_Newton/#Manopt.KKTVectorField","content":" Manopt.KKTVectorField  ‚Äî  Type KKTVectorField{O<:ConstrainedManifoldObjective} Implement the vectorfield  $F$  KKT-conditions, inlcuding a slack variable for the inequality constraints. Given the  LagrangianCost \\[\\mathcal L(p; Œº, Œª) = f(p) + \\sum_{i=1}^m Œº_ig_i(p) + \\sum_{j=1}^n Œª_jh_j(p)\\] the  LagrangianGradient \\[\\operatorname{grad}\\mathcal L(p, Œº, Œª) = \\operatorname{grad}f(p) + \\sum_{j=1}^n Œª_j \\operatorname{grad} h_j(p) + \\sum_{i=1}^m Œº_i \\operatorname{grad} g_i(p),\\] and introducing the slack variables  $s=-g(p) ‚àà ‚Ñù^m$  the vector field is given by \\[F(p, Œº, Œª, s) = \\begin{pmatrix}\n\\operatorname{grad}_p \\mathcal L(p, Œº, Œª)\\\\\ng(p) + s\\\\\nh(p)\\\\\nŒº ‚äô s\n\\end{pmatrix}, \\text{ where } p \\in \\mathcal M, Œº, s \\in ‚Ñù^m\\text{ and } Œª \\in ‚Ñù^n,\\] where  $‚äô$  denotes the Hadamard (or elementwise) product Fields cmo  the  ConstrainedManifoldObjective While the point  p  is arbitrary and usually not needed, it serves as internal memory in the computations. Furthermore Both fields together also calrify the product manifold structure to use. Constructor KKTVectorField(cmo::ConstrainedManifoldObjective) Example Define  F = KKTVectorField(cmo)  for some  ConstrainedManifoldObjective cmo  and let  N  be the product manifold of  $\\mathcal M√ó‚Ñù^m√ó‚Ñù^n√ó‚Ñù^m$ . Then, you can call this cost as  F(N, q)  or as the in-place variant  F(N, Y, q) , where  q  is a point on  N  and  Y  is a tangent vector at  q  for the result. source"},{"id":3095,"pagetitle":"Interior Point Newton","title":"Manopt.KKTVectorFieldJacobian","ref":"/manopt/stable/solvers/interior_point_Newton/#Manopt.KKTVectorFieldJacobian","content":" Manopt.KKTVectorFieldJacobian  ‚Äî  Type KKTVectorFieldJacobian{O<:ConstrainedManifoldObjective} Implement the Jacobian of the vector field  $F$  of the KKT-conditions, inlcuding a slack variable for the inequality constraints, see  KKTVectorField  and  KKTVectorFieldAdjointJacobian .. \\[\\operatorname{J} F(p, Œº, Œª, s)[X, Y, Z, W] = \\begin{pmatrix}\n    \\operatorname{Hess}_p \\mathcal L(p, Œº, Œª)[X] + \\displaystyle\\sum_{i=1}^m Y_i \\operatorname{grad} g_i(p) + \\displaystyle\\sum_{j=1}^n Z_j \\operatorname{grad} h_j(p)\\\\\n    \\Bigl( ‚ü®\\operatorname{grad} g_i(p), X‚ü© + W_i\\Bigr)_{i=1}^m\\\\\n    \\Bigl( ‚ü®\\operatorname{grad} h_j(p), X‚ü© \\Bigr)_{j=1}^n\\\\\n    Œº ‚äô W + s ‚äô Y\n\\end{pmatrix},\\] where  $‚äô$  denotes the Hadamard (or elementwise) product See also the  LagrangianHessian $\\operatorname{Hess}_p \\mathcal L(p, Œº, Œª)[X]$ . Fields cmo  the  ConstrainedManifoldObjective Constructor KKTVectorFieldJacobian(cmo::ConstrainedManifoldObjective) Generate the Jacobian of the KKT vector field related to some  ConstrainedManifoldObjective cmo . Example Define  JF = KKTVectorFieldJacobian(cmo)  for some  ConstrainedManifoldObjective cmo  and let  N  be the product manifold of  $\\mathcal M√ó‚Ñù^m√ó‚Ñù^n√ó‚Ñù^m$ . Then, you can call this cost as  JF(N, q, Y)  or as the in-place variant  JF(N, Z, q, Y) , where  q  is a point on  N  and  Y  and  Z  are a tangent vector at  q . source"},{"id":3096,"pagetitle":"Interior Point Newton","title":"Manopt.KKTVectorFieldAdjointJacobian","ref":"/manopt/stable/solvers/interior_point_Newton/#Manopt.KKTVectorFieldAdjointJacobian","content":" Manopt.KKTVectorFieldAdjointJacobian  ‚Äî  Type KKTVectorFieldAdjointJacobian{O<:ConstrainedManifoldObjective} Implement the Adjoint of the Jacobian of the vector field  $F$  of the KKT-conditions, inlcuding a slack variable for the inequality constraints, see  KKTVectorField  and  KKTVectorFieldJacobian . \\[\\operatorname{J}^* F(p, Œº, Œª, s)[X, Y, Z, W] = \\begin{pmatrix}\n    \\operatorname{Hess}_p \\mathcal L(p, Œº, Œª)[X] + \\displaystyle\\sum_{i=1}^m Y_i \\operatorname{grad} g_i(p) + \\displaystyle\\sum_{j=1}^n Z_j \\operatorname{grad} h_j(p)\\\\\n    \\Bigl( ‚ü®\\operatorname{grad} g_i(p), X‚ü© + s_iW_i\\Bigr)_{i=1}^m\\\\\n    \\Bigl( ‚ü®\\operatorname{grad} h_j(p), X‚ü© \\Bigr)_{j=1}^n\\\\\n    Œº ‚äô W + Y\n\\end{pmatrix},\\] where  $‚äô$  denotes the Hadamard (or elementwise) product See also the  LagrangianHessian $\\operatorname{Hess}_p \\mathcal L(p, Œº, Œª)[X]$ . Fields cmo  the  ConstrainedManifoldObjective Constructor KKTVectorFieldAdjointJacobian(cmo::ConstrainedManifoldObjective) Generate the Adjoint Jacobian of the KKT vector field related to some  ConstrainedManifoldObjective cmo . Example Define  AdJF = KKTVectorFieldAdjointJacobian(cmo)  for some  ConstrainedManifoldObjective cmo  and let  N  be the product manifold of  $\\mathcal M√ó‚Ñù^m√ó‚Ñù^n√ó‚Ñù^m$ . Then, you can call this cost as  AdJF(N, q, Y)  or as the in-place variant  AdJF(N, Z, q, Y) , where  q  is a point on  N  and  Y  and  Z  are a tangent vector at  q . source"},{"id":3097,"pagetitle":"Interior Point Newton","title":"Manopt.KKTVectorFieldNormSq","ref":"/manopt/stable/solvers/interior_point_Newton/#Manopt.KKTVectorFieldNormSq","content":" Manopt.KKTVectorFieldNormSq  ‚Äî  Type KKTVectorFieldNormSq{O<:ConstrainedManifoldObjective} Implement the square of the norm of the vectorfield  $F$  of the KKT-conditions, inlcuding a slack variable for the inequality constraints, see  KKTVectorField , where this functor applies the norm to. In [ LY24 ] this is called the merit function. Fields cmo  the  ConstrainedManifoldObjective Constructor KKTVectorFieldNormSq(cmo::ConstrainedManifoldObjective) Example Define  f = KKTVectorFieldNormSq(cmo)  for some  ConstrainedManifoldObjective cmo  and let  N  be the product manifold of  $\\mathcal M√ó‚Ñù^m√ó‚Ñù^n√ó‚Ñù^m$ . Then, you can call this cost as  f(N, q) , where  q  is a point on  N . source"},{"id":3098,"pagetitle":"Interior Point Newton","title":"Manopt.KKTVectorFieldNormSqGradient","ref":"/manopt/stable/solvers/interior_point_Newton/#Manopt.KKTVectorFieldNormSqGradient","content":" Manopt.KKTVectorFieldNormSqGradient  ‚Äî  Type KKTVectorFieldNormSqGradient{O<:ConstrainedManifoldObjective} Compute the gradient of the  KKTVectorFieldNormSq $œÜ(p,Œº,Œª,s) = \\lVert F(p,Œº,Œª,s)\\rVert^2$ , that is of the norm squared of the  KKTVectorField $F$ . This is given in [ LY24 ] as the gradient of their merit function, which we can write with the adjoint  $J^*$  of the Jacobian \\[\\operatorname{grad} œÜ = 2\\operatorname{J}^* F(p, Œº, Œª, s)[F(p, Œº, Œª, s)],\\] and hence is computed with  KKTVectorFieldAdjointJacobian  and  KKTVectorField . For completeness, the gradient reads, using the  LagrangianGradient $L = \\operatorname{grad}_p \\mathcal L(p,Œº,Œª) ‚àà T_p\\mathcal M$ , for a shorthand of the first component of  $F$ , as \\[\\operatorname{grad} œÜ\n=\n2 \\begin{pmatrix}\n\\operatorname{grad}_p \\mathcal L(p,Œº,Œª)[L] + (g_i(p) + s_i)\\operatorname{grad} g_i(p) + h_j(p)\\operatorname{grad} h_j(p)\\\\\n  \\Bigl( ‚ü®\\operatorname{grad} g_i(p), L‚ü© + s_i\\Bigr)_{i=1}^m + Œº ‚äô s ‚äô s\\\\\n  \\Bigl( ‚ü®\\operatorname{grad} h_j(p), L‚ü© \\Bigr)_{j=1}^n\\\\\n  g + s + Œº ‚äô Œº ‚äô s\n\\end{pmatrix},\\] where  $‚äô$  denotes the Hadamard (or elementwise) product. Fields cmo  the  ConstrainedManifoldObjective Constructor KKTVectorFieldNormSqGradient(cmo::ConstrainedManifoldObjective) Example Define  grad_f = KKTVectorFieldNormSqGradient(cmo)  for some  ConstrainedManifoldObjective cmo  and let  N  be the product manifold of  $\\mathcal M√ó‚Ñù^m√ó‚Ñù^n√ó‚Ñù^m$ . Then, you can call this cost as  grad_f(N, q)  or as the in-place variant  grad_f(N, Y, q) , where  q  is a point on  N  and  Y  is a tangent vector at  q  returning the resulting gradient at. source"},{"id":3099,"pagetitle":"Interior Point Newton","title":"Helpers","ref":"/manopt/stable/solvers/interior_point_Newton/#Helpers","content":" Helpers"},{"id":3100,"pagetitle":"Interior Point Newton","title":"Manopt.InteriorPointCentralityCondition","ref":"/manopt/stable/solvers/interior_point_Newton/#Manopt.InteriorPointCentralityCondition","content":" Manopt.InteriorPointCentralityCondition  ‚Äî  Type InteriorPointCentralityCondition{CO,R} A functor to check the centrality condition. In order to obtain a step in the linesearch performed within the  interior_point_Newton , Section 6 of [ LY24 ] propose the following additional conditions to hold inspired by the Euclidean case described in Section 6 [ ETTZ96 ]: For a given  ConstrainedManifoldObjective  assume consider the  KKTVectorField $F$ , that is we are at a point  $q = (p, Œª, Œº, s)$   on  $\\mathcal M √ó ‚Ñù^m √ó ‚Ñù^n √ó ‚Ñù^m$ and a search direction  $V = (X, Y, Z, W)$ . Then, let \\[œÑ_1 = \\frac{m‚ãÖ\\min\\{ Œº ‚äô s\\}}{Œº^{\\mathrm{T}}s}\n\\quad\\text{ and }\\quad\nœÑ_2 = \\frac{Œº^{\\mathrm{T}}s}{\\lVert F(q) \\rVert}\\] where  $‚äô$  denotes the Hadamard (or elementwise) product. For a new candidate  $q(Œ±) = \\bigl(p(Œ±), Œª(Œ±), Œº(Œ±), s(Œ±)\\bigr) := (\\operatorname{retr}_p(Œ±X), Œª+Œ±Y, Œº+Œ±Z, s+Œ±W)$ , we then define two functions \\[c_1(Œ±) = \\min\\{ Œº(Œ±) ‚äô s(Œ±) \\} - \\frac{Œ≥œÑ_1 Œº(Œ±)^{\\mathrm{T}}s(Œ±)}{m}\n\\quad\\text{ and }\\quad\nc_2(Œ±) = Œº(Œ±)^{\\mathrm{T}}s(Œ±) ‚Äì Œ≥œÑ_2 \\lVert F(q(Œ±)) \\rVert.\\] While the paper now states that the (Armijo) linesearch starts at a point  $\\tilde Œ±$ , it is easier to include the condition that  $c_1(Œ±) ‚â• 0$  and  $c_2(Œ±) ‚â• 0$  into the linesearch as well. The functor  InteriorPointCentralityCondition(cmo, Œ≥, Œº, s, normKKT)(N,qŒ±)  defined here evaluates this condition and returns true if both  $c_1$  and  $c_2$  are nonnegative. Fields cmo : a  ConstrainedManifoldObjective Œ≥ : a constant œÑ1 ,  œÑ2 : the constants given in the formula. Constructor InteriorPointCentralityCondition(cmo, Œ≥)\nInteriorPointCentralityCondition(cmo, Œ≥, œÑ1, œÑ2) Initialise the centrality conditions. The parameters  œÑ1 ,  œÑ2  are initialise to zero if not provided. Note Besides  get_parameter  for all three constants, and  set_parameter!  for  $Œ≥$ , to update  $œÑ_1$  and  $œÑ_2$ , call  set_parameter(ipcc, :œÑ, N, q)  to update both  $œÑ_1$  and  $œÑ_2$  according to the formulae above. source"},{"id":3101,"pagetitle":"Interior Point Newton","title":"Manopt.calculate_œÉ","ref":"/manopt/stable/solvers/interior_point_Newton/#Manopt.calculate_œÉ","content":" Manopt.calculate_œÉ  ‚Äî  Function calculate_œÉ(M, cmo, p, Œº, Œª, s; kwargs...) Compute the new  $œÉ$  factor for the barrier parameter in  interior_point_Newton  as \\[\\min\\{\\frac{1}{2}, \\lVert F(p; Œº, Œª, s)\\rVert^{\\frac{1}{2}} \\},\\] where  $F$  is the KKT vector field, hence the  KKTVectorFieldNormSq  is used. Keyword arguments vector_space= Rn  a function that, given an integer, returns the manifold to be used for the vector space components  $‚Ñù^m,‚Ñù^n$ N  the manifold  $\\mathcal M √ó ‚Ñù^m √ó ‚Ñù^n √ó ‚Ñù^m$  the vector field lives on (generated using  vector_space ) q  provide memory on  N  for interims evaluation of the vector field source"},{"id":3102,"pagetitle":"Interior Point Newton","title":"Additional stopping criteria","ref":"/manopt/stable/solvers/interior_point_Newton/#Additional-stopping-criteria","content":" Additional stopping criteria"},{"id":3103,"pagetitle":"Interior Point Newton","title":"Manopt.StopWhenKKTResidualLess","ref":"/manopt/stable/solvers/interior_point_Newton/#Manopt.StopWhenKKTResidualLess","content":" Manopt.StopWhenKKTResidualLess  ‚Äî  Type StopWhenKKTResidualLess <: StoppingCriterion Stop when the KKT residual r^2\n= \\lVert \\operatorname{grad}_p \\mathcal L(p, Œº, Œª) \\rVert^2\n+ \\sum_{i=1}^m [Œº_i]_{-}^2 + [g_i(p)]_+^2 + \\lvert \\mu_ig_i(p)^2\n+ \\sum_{j=1}^n \\lvert h_i(p)\\rvert^2. is less than a given threshold  $r < Œµ$ . We use  $[v]_+ = \\max\\{0,v\\}$  and  $[v]_- = \\min\\{0,t\\}$  for the positive and negative part of  $v$ , respectively Fields Œµ : a threshold residual : store the last residual if the stopping criterion is hit. at_iteration : source"},{"id":3104,"pagetitle":"Interior Point Newton","title":"References","ref":"/manopt/stable/solvers/interior_point_Newton/#References","content":" References [ETTZ96] A.¬†S.¬†El-Bakry, R.¬†A.¬†Tapia, T.¬†Tsuchiya and Y.¬†Zhang.  On the formulation and theory of the Newton interior-point method for nonlinear programming .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  89 , 507‚Äì541  (1996). [LY24] Z.¬†Lai and A.¬†Yoshise.  Riemannian Interior Point Methods for Constrained Optimization on Manifolds .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  201 , 433‚Äì469  (2024),  arXiv:2203.09762 ."},{"id":3107,"pagetitle":"Mesh Adaptive Direct Search","title":"Mesh adaptive direct search (MADS)","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Mesh-adaptive-direct-search-(MADS)","content":" Mesh adaptive direct search (MADS)"},{"id":3108,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.mesh_adaptive_direct_search","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.mesh_adaptive_direct_search","content":" Manopt.mesh_adaptive_direct_search  ‚Äî  Function mesh_adaptive_direct_search(M, f, p=rand(M); kwargs...)\nmesh_adaptive_direct_search(M, mco::AbstractManifoldCostObjective, p=rand(M); kwargs..)\nmesh_adaptive_direct_search!(M, f, p; kwargs...)\nmesh_adaptive_direct_search!(M, mco::AbstractManifoldCostObjective, p; kwargs..) The Mesh Adaptive Direct Search (MADS) algorithm minimizes an objective function  $f: \\mathcal M ‚Üí ‚Ñù$  on the manifold  M . The algorithm constructs an implicit mesh in the tangent space  $T_{p}\\mathcal M$  at the current candidate  $p$ . Each iteration consists of a search step and a poll step. The search step selects points from the implicit mesh and attempts to find an improved candidate solution that reduces the value of  $f$ . If the search step fails to generate an improved candidate solution, the poll step is performed. It consists of a local exploration on the current implicit mesh in the neighbourhood of the current iterate. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v p : a point on the manifold  $\\mathcal M$ Keyword arguments mesh_basis= DefaultOrthonormalBasis : a basis to generate the mesh in. The mesh is generated in coordinates of this basis in every tangent space max_stepsize= injectivity_radius (M) : a maximum step size to take. any vector generated on the mesh is shortened to this length to avoid leaving the injectivity radius, poll:: AbstractMeshPollFunction = LowerTriangularAdaptivePoll (M, copy(M,p)) : the poll function to use. The  mesh_basis  (as  basis ),  retraction_method , and  vector_transport_method  are passed to this default as well. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions scale_mesh= injectivity_radius (M) / 4 : initial scaling of the mesh search:: AbstractMeshSearchFunction = DefaultMeshAdaptiveDirectSearch (M, copy(M,p)) : the search function to use. The  retraction_method  is passed to this default as well. stopping_criterion= StopAfterIteration (500) | StopWhenPollSizeLess (1e-10) : a functor indicating that the stopping criterion is fulfilled vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3109,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.mesh_adaptive_direct_search!","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.mesh_adaptive_direct_search!","content":" Manopt.mesh_adaptive_direct_search!  ‚Äî  Function mesh_adaptive_direct_search(M, f, p=rand(M); kwargs...)\nmesh_adaptive_direct_search(M, mco::AbstractManifoldCostObjective, p=rand(M); kwargs..)\nmesh_adaptive_direct_search!(M, f, p; kwargs...)\nmesh_adaptive_direct_search!(M, mco::AbstractManifoldCostObjective, p; kwargs..) The Mesh Adaptive Direct Search (MADS) algorithm minimizes an objective function  $f: \\mathcal M ‚Üí ‚Ñù$  on the manifold  M . The algorithm constructs an implicit mesh in the tangent space  $T_{p}\\mathcal M$  at the current candidate  $p$ . Each iteration consists of a search step and a poll step. The search step selects points from the implicit mesh and attempts to find an improved candidate solution that reduces the value of  $f$ . If the search step fails to generate an improved candidate solution, the poll step is performed. It consists of a local exploration on the current implicit mesh in the neighbourhood of the current iterate. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v p : a point on the manifold  $\\mathcal M$ Keyword arguments mesh_basis= DefaultOrthonormalBasis : a basis to generate the mesh in. The mesh is generated in coordinates of this basis in every tangent space max_stepsize= injectivity_radius (M) : a maximum step size to take. any vector generated on the mesh is shortened to this length to avoid leaving the injectivity radius, poll:: AbstractMeshPollFunction = LowerTriangularAdaptivePoll (M, copy(M,p)) : the poll function to use. The  mesh_basis  (as  basis ),  retraction_method , and  vector_transport_method  are passed to this default as well. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions scale_mesh= injectivity_radius (M) / 4 : initial scaling of the mesh search:: AbstractMeshSearchFunction = DefaultMeshAdaptiveDirectSearch (M, copy(M,p)) : the search function to use. The  retraction_method  is passed to this default as well. stopping_criterion= StopAfterIteration (500) | StopWhenPollSizeLess (1e-10) : a functor indicating that the stopping criterion is fulfilled vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3110,"pagetitle":"Mesh Adaptive Direct Search","title":"State","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#State","content":" State"},{"id":3111,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.MeshAdaptiveDirectSearchState","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.MeshAdaptiveDirectSearchState","content":" Manopt.MeshAdaptiveDirectSearchState  ‚Äî  Type MeshAdaptiveDirectSearchState <: AbstractManoptSolverState Fields p::P : a point on the manifold  $\\mathcal M$  storing the current iterate mesh_size : the current (internal) mesh size scale_mesh : the current scaling of the internal mesh size, yields the actual mesh size used max_stepsize : an upper bound for the longest step taken in looking for a candidate in either poll or search poll_size stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled poll:: [ AbstractMeshPollFunction ]: a poll step (functor) to perform search:: [ AbstractMeshSearchFunction }(@ref) a search step (functor) to perform source"},{"id":3112,"pagetitle":"Mesh Adaptive Direct Search","title":"Poll","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Poll","content":" Poll"},{"id":3113,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.AbstractMeshPollFunction","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.AbstractMeshPollFunction","content":" Manopt.AbstractMeshPollFunction  ‚Äî  Type AbstractMeshPollFunction An abstract type for common ‚Äúpoll‚Äù strategies in the  mesh_adaptive_direct_search  solver. A subtype of this The functor has to fulfil be callable as  poll!(problem, mesh_size; kwargs...)  and modify the state as well as to provide functions is_successful(poll!)  that indicates whether the last poll was successful in finding a new candidate get_basepoint(poll!)  that returns the base point at which the mesh is build get_candidate(poll!)  that returns the last found candidate if the poll was successful. Otherwise the base point is returned get_descent_direction(poll!)  the the vector that points from the base point to the candidate. If the last poll was not successful, the zero vector is returned update_basepoint!(M, poll!, p)  that updates the base point to  p  and all necessary internal data to a new point to build a mesh at The  kwargs...  could include scale_mesh=1.0 : to rescale the mesh globally max_stepsize=Inf : avoid exceeding a step size beyond this value, e.g. injectivity radius. any vector longer than this should be shortened to the provided maximum step size. source"},{"id":3114,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.LowerTriangularAdaptivePoll","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.LowerTriangularAdaptivePoll","content":" Manopt.LowerTriangularAdaptivePoll  ‚Äî  Type LowerTriangularAdaptivePoll <: AbstractMeshPollFunction Generate a mesh (poll step) based on Section 6 and 7 of [ Dre07 ], with two small modifications: The mesh can be scaled globally so instead of  $Œî_0^m=1$  a certain different scale is used Any poll direction can be rescaled if it is too long. This is to not exceed the injectivity radius for example. Functor (p::LowerTriangularAdaptivePoll)(problem, mesh_size; scale_mesh=1.0, max_stepsize=inf) Fields base_point::P : a point on the manifold, where the mesh is build in the tangent space basis : a basis of the current tangent space with respect to which the mesh is stored candidate::P : a memory for a new point/candidate mesh : a vector of tangent vectors storing the mesh. random_vector : a  $d$ -dimensional random vector  $b_l$ ` random_index : a random index  $Œπ$ retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports X::T  the last successful poll direction stored as a tangent vector. initialised to the zero vector and reset to the zero vector after moving to a new tangent space. Constructor LowerTriangularAdaptivePoll(M, p=rand(M); kwargs...) Keyword arguments basis= DefaultOrthonormalBasis retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ source as well as the internal functions"},{"id":3115,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.get_descent_direction","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.get_descent_direction-Tuple{LowerTriangularAdaptivePoll}","content":" Manopt.get_descent_direction  ‚Äî  Method get_descent_direction(ltap::LowerTriangularAdaptivePoll) Return the direction of the last  LowerTriangularAdaptivePoll  that yields a descent of the cost. If the poll was not successful, the zero vector is returned source"},{"id":3116,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.is_successful","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.is_successful-Tuple{LowerTriangularAdaptivePoll}","content":" Manopt.is_successful  ‚Äî  Method is_successful(ltap::LowerTriangularAdaptivePoll) Return whether the last  LowerTriangularAdaptivePoll  step was successful source"},{"id":3117,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.get_candidate","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.get_candidate-Tuple{LowerTriangularAdaptivePoll}","content":" Manopt.get_candidate  ‚Äî  Method get_candidate(ltap::LowerTriangularAdaptivePoll) Return the candidate of the last successful  LowerTriangularAdaptivePoll . If the poll was unsuccessful, the base point is returned. source"},{"id":3118,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.get_basepoint","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.get_basepoint-Tuple{LowerTriangularAdaptivePoll}","content":" Manopt.get_basepoint  ‚Äî  Method get_basepoint(ltap::LowerTriangularAdaptivePoll) Return the base point of the tangent space, where the mash for the  LowerTriangularAdaptivePoll  is build in. source"},{"id":3119,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.update_basepoint!","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.update_basepoint!-Union{Tuple{P}, Tuple{Any, LowerTriangularAdaptivePoll{P, T, F} where {T, F<:Real}, P}} where P","content":" Manopt.update_basepoint!  ‚Äî  Method update_basepoint!(M, ltap::LowerTriangularAdaptivePoll, p) Update the base point of the  LowerTriangularAdaptivePoll . This especially also updates the basis, that is used to build a (new) mesh. source"},{"id":3120,"pagetitle":"Mesh Adaptive Direct Search","title":"Search","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Search","content":" Search"},{"id":3121,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.AbstractMeshSearchFunction","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.AbstractMeshSearchFunction","content":" Manopt.AbstractMeshSearchFunction  ‚Äî  Type AbstractMeshSearchFunction Should be callable as  search!(problem, mesh_size, p, X; kwargs...) where  X  is the last successful poll direction from the tangent space at  p  if that exists and the zero vector otherwise. Besides that the following functions should be implemented is_successful(search!)  that indicates whether the last search was successful in finding a new candidate get_candidate(search!)  that returns the last found candidate source"},{"id":3122,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.DefaultMeshAdaptiveDirectSearch","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.DefaultMeshAdaptiveDirectSearch","content":" Manopt.DefaultMeshAdaptiveDirectSearch  ‚Äî  Type DefaultMeshAdaptiveDirectSearch <: AbstractMeshSearchFunction Functor (s::DefaultMeshAdaptiveDirectSearch)(problem, mesh_size::Real, X; scale_mesh::Real=1.0, max_stepsize::Real=inf) Fields q : a temporary memory for a point on the manifold X : information to perform the search, e.g. the last direction found by poll. last_search_improved::Bool  indicate whether the last search was successful, i.e. improved the cost. retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions Constructor DefaultMeshAdaptiveDirectSearch(M::AbstractManifold, p=rand(M); kwargs...) Keyword arguments `X::T=zero_vector(M, p) retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions source as well as the internal functions"},{"id":3123,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.is_successful","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.is_successful-Tuple{DefaultMeshAdaptiveDirectSearch}","content":" Manopt.is_successful  ‚Äî  Method is_successful(dmads::DefaultMeshAdaptiveDirectSearch) Return whether the last  DefaultMeshAdaptiveDirectSearch  was successful. source"},{"id":3124,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.get_candidate","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.get_candidate-Tuple{DefaultMeshAdaptiveDirectSearch}","content":" Manopt.get_candidate  ‚Äî  Method get_candidate(dmads::DefaultMeshAdaptiveDirectSearch) Return the last candidate a  DefaultMeshAdaptiveDirectSearch  found source"},{"id":3125,"pagetitle":"Mesh Adaptive Direct Search","title":"Additional stopping criteria","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Additional-stopping-criteria","content":" Additional stopping criteria"},{"id":3126,"pagetitle":"Mesh Adaptive Direct Search","title":"Manopt.StopWhenPollSizeLess","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Manopt.StopWhenPollSizeLess","content":" Manopt.StopWhenPollSizeLess  ‚Äî  Type StopWhenPollSizeLess <: StoppingCriterion stores a threshold when to stop looking at the poll mesh size of an  MeshAdaptiveDirectSearchState . Constructor StopWhenPollSizeLess(Œµ) initialize the stopping criterion to a threshold  Œµ . source"},{"id":3127,"pagetitle":"Mesh Adaptive Direct Search","title":"Technical details","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Technical-details","content":" Technical details The  mesh_adaptive_direct_search  solver requires the following functions of a manifold to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified. Within the default initialization  rand (M)  is used to generate the initial population A  vector_transport_to! M, Y, p, X, q) ; it is recommended to set the  default_vector_transport_method  to a favourite retraction. If this default is set, a  vector_transport_method=  does not have to be specified."},{"id":3128,"pagetitle":"Mesh Adaptive Direct Search","title":"Literature","ref":"/manopt/stable/solvers/mesh_adaptive_direct_search/#Literature","content":" Literature [Dre07] D.¬†W.¬†Dreisigmeyer.  Direct Search Alogirthms over Riemannian Manifolds  (Optimization Online, 2007)."},{"id":3131,"pagetitle":"Particle Swarm Optimization","title":"Particle swarm optimization","ref":"/manopt/stable/solvers/particle_swarm/#Particle-swarm-optimization","content":" Particle swarm optimization"},{"id":3132,"pagetitle":"Particle Swarm Optimization","title":"Manopt.particle_swarm","ref":"/manopt/stable/solvers/particle_swarm/#Manopt.particle_swarm","content":" Manopt.particle_swarm  ‚Äî  Function patricle_swarm(M, f; kwargs...)\npatricle_swarm(M, f, swarm; kwargs...)\npatricle_swarm(M, mco::AbstractManifoldCostObjective; kwargs..)\npatricle_swarm(M, mco::AbstractManifoldCostObjective, swarm; kwargs..)\nparticle_swarm!(M, f, swarm; kwargs...)\nparticle_swarm!(M, mco::AbstractManifoldCostObjective, swarm; kwargs..) perform the particle swarm optimization algorithm (PSO) to solve \\[\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} f(p)\\] PSO starts with an initial  swarm  [ BIA10 ] of points on the manifold. If no  swarm  is provided, the  swarm_size  keyword is used to generate random points. The computation can be perfomed in-place of  swarm . To this end, a swarm  $S = \\{s_1, \\ldots, s_n\\}$  of particles is moved around the manifold  M  in the following manner. For every particle  $s_k^{(i)}$  the new particle velocities  $X_k^{(i)}$  are computed in every step  $i$  of the algorithm by \\[X_k^{(i)} = œâ \\mathcal T_{s_k^{(i)‚Üês_k^{(i-1)}} X_k^{(i-1)} + c r_1  \\operatorname{retr}^{-1}_{s_k^{(i)}}(p_k^{(i)}) + s r_2 \\operatorname{retr}^{-1}_{s_k^{(i)}}(p),\\] where $s_k^{(i)}$  is the current particle position, $œâ$  denotes the inertia, $c$  and  $s$  are a cognitive and a social weight, respectively, $r_j$ ,  $j=1,2$  are random factors which are computed new for each particle and step \\mathcal T_{‚ãÖ‚Üê‚ãÖ} is a vector transport, and \\operatorname{retr}^{-1} is an inverse retraction Then the position of the particle is updated as \\[s_k^{(i+1)} = \\operatorname{retr}_{s_k^{(i)}}(X_k^{(i)}),\\] Then the single particles best entries  $p_k^{(i)}$  are updated as \\[p_k^{(i+1)} = \\begin{cases}\ns_k^{(i+1)},  & \\text{if } F(s_k^{(i+1)})<F(p_{k}^{(i)}),\\\\\np_{k}^{(i)}, & \\text{else,}\n\\end{cases}\\] and the global best position \\[g^{(i+1)} = \\begin{cases}\np_k^{(i+1)},  & \\text{if } F(p_k^{(i+1)})<F(g_{k}^{(i)}),\\\\\ng_{k}^{(i)}, & \\text{else,}\n\\end{cases}\\] Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v swarm = [rand(M) for _ in 1:swarm_size] : an initial swarm of points. Instead of a cost function  f  you can also provide an  AbstractManifoldCostObjective mco . Keyword Arguments cognitive_weight=1.4 : a cognitive weight factor inertia=0.65 : the inertia of the particles inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions social_weight=1.4 : a social weight factor swarm_size=100 : swarm size, if it should be generated randomly stopping_criterion= StopAfterIteration (500) | StopWhenChangeLess (1e-4) : a functor indicating that the stopping criterion is fulfilled vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports velocity :                  a set of tangent vectors (of type  AbstractVector{T} ) representing the velocities of the particles, per default a random tangent vector per initial position All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. If you provide the objective directly, these decorations can still be specified Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3133,"pagetitle":"Particle Swarm Optimization","title":"Manopt.particle_swarm!","ref":"/manopt/stable/solvers/particle_swarm/#Manopt.particle_swarm!","content":" Manopt.particle_swarm!  ‚Äî  Function patricle_swarm(M, f; kwargs...)\npatricle_swarm(M, f, swarm; kwargs...)\npatricle_swarm(M, mco::AbstractManifoldCostObjective; kwargs..)\npatricle_swarm(M, mco::AbstractManifoldCostObjective, swarm; kwargs..)\nparticle_swarm!(M, f, swarm; kwargs...)\nparticle_swarm!(M, mco::AbstractManifoldCostObjective, swarm; kwargs..) perform the particle swarm optimization algorithm (PSO) to solve \\[\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} f(p)\\] PSO starts with an initial  swarm  [ BIA10 ] of points on the manifold. If no  swarm  is provided, the  swarm_size  keyword is used to generate random points. The computation can be perfomed in-place of  swarm . To this end, a swarm  $S = \\{s_1, \\ldots, s_n\\}$  of particles is moved around the manifold  M  in the following manner. For every particle  $s_k^{(i)}$  the new particle velocities  $X_k^{(i)}$  are computed in every step  $i$  of the algorithm by \\[X_k^{(i)} = œâ \\mathcal T_{s_k^{(i)‚Üês_k^{(i-1)}} X_k^{(i-1)} + c r_1  \\operatorname{retr}^{-1}_{s_k^{(i)}}(p_k^{(i)}) + s r_2 \\operatorname{retr}^{-1}_{s_k^{(i)}}(p),\\] where $s_k^{(i)}$  is the current particle position, $œâ$  denotes the inertia, $c$  and  $s$  are a cognitive and a social weight, respectively, $r_j$ ,  $j=1,2$  are random factors which are computed new for each particle and step \\mathcal T_{‚ãÖ‚Üê‚ãÖ} is a vector transport, and \\operatorname{retr}^{-1} is an inverse retraction Then the position of the particle is updated as \\[s_k^{(i+1)} = \\operatorname{retr}_{s_k^{(i)}}(X_k^{(i)}),\\] Then the single particles best entries  $p_k^{(i)}$  are updated as \\[p_k^{(i+1)} = \\begin{cases}\ns_k^{(i+1)},  & \\text{if } F(s_k^{(i+1)})<F(p_{k}^{(i)}),\\\\\np_{k}^{(i)}, & \\text{else,}\n\\end{cases}\\] and the global best position \\[g^{(i+1)} = \\begin{cases}\np_k^{(i+1)},  & \\text{if } F(p_k^{(i+1)})<F(g_{k}^{(i)}),\\\\\ng_{k}^{(i)}, & \\text{else,}\n\\end{cases}\\] Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v swarm = [rand(M) for _ in 1:swarm_size] : an initial swarm of points. Instead of a cost function  f  you can also provide an  AbstractManifoldCostObjective mco . Keyword Arguments cognitive_weight=1.4 : a cognitive weight factor inertia=0.65 : the inertia of the particles inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions social_weight=1.4 : a social weight factor swarm_size=100 : swarm size, if it should be generated randomly stopping_criterion= StopAfterIteration (500) | StopWhenChangeLess (1e-4) : a functor indicating that the stopping criterion is fulfilled vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports velocity :                  a set of tangent vectors (of type  AbstractVector{T} ) representing the velocities of the particles, per default a random tangent vector per initial position All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. If you provide the objective directly, these decorations can still be specified Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3134,"pagetitle":"Particle Swarm Optimization","title":"State","ref":"/manopt/stable/solvers/particle_swarm/#State","content":" State"},{"id":3135,"pagetitle":"Particle Swarm Optimization","title":"Manopt.ParticleSwarmState","ref":"/manopt/stable/solvers/particle_swarm/#Manopt.ParticleSwarmState","content":" Manopt.ParticleSwarmState  ‚Äî  Type ParticleSwarmState{P,T} <: AbstractManoptSolverState Describes a particle swarm optimizing algorithm, with Fields cognitive_weight : a cognitive weight factor inertia :          the inertia of the particles inverse_retraction_method::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions social_weight :    a social weight factor stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports velocity :         a set of tangent vectors (of type  AbstractVector{T} ) representing the velocities of the particles Internal and temporary fields cognitive_vector : temporary storage for a tangent vector related to  cognitive_weight p::P : a point on the manifold  $\\mathcal M$  storing the best point visited by all particles positional_best :  storing the best position  $p_i$  every single swarm participant visited q::P : a point on the manifold  $\\mathcal M$  serving as temporary storage for interims results; avoids allocations social_vec :       temporary storage for a tangent vector related to  social_weight swarm :            a set of points (of type  AbstractVector{P} ) on a manifold  $\\{a_i\\}_{i=1}^{N}$ Constructor ParticleSwarmState(M, initial_swarm, velocity; kawrgs...) construct a particle swarm solver state for the manifold  M  starting with the initial population  initial_swarm  with  velocities . The  p  used in the following defaults is the type of one point from the swarm. Keyword arguments cognitive_weight=1.4 inertia=0.65 inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions social_weight=1.4 stopping_criterion= StopAfterIteration (500) | StopWhenChangeLess (1e-4) : a functor indicating that the stopping criterion is fulfilled vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports See also particle_swarm source"},{"id":3136,"pagetitle":"Particle Swarm Optimization","title":"Stopping criteria","ref":"/manopt/stable/solvers/particle_swarm/#Stopping-criteria","content":" Stopping criteria"},{"id":3137,"pagetitle":"Particle Swarm Optimization","title":"Manopt.StopWhenSwarmVelocityLess","ref":"/manopt/stable/solvers/particle_swarm/#Manopt.StopWhenSwarmVelocityLess","content":" Manopt.StopWhenSwarmVelocityLess  ‚Äî  Type StopWhenSwarmVelocityLess <: StoppingCriterion Stopping criterion for  particle_swarm , when the velocity of the swarm is less than a threshold. Fields threshold :      the threshold at_iteration :   store the iteration the stopping criterion was (last) fulfilled reason :         store the reason why the stopping criterion was fulfilled, see  get_reason velocity_norms : interim vector to store the norms of the velocities before computing its norm Constructor StopWhenSwarmVelocityLess(tolerance::Float64) initialize the stopping criterion to a certain  tolerance . source"},{"id":3138,"pagetitle":"Particle Swarm Optimization","title":"Technical details","ref":"/manopt/stable/solvers/particle_swarm/#sec-arc-technical-details","content":" Technical details The  particle_swarm  solver requires the following functions of a manifold to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified. An  inverse_retract! (M, X, p, q) ; it is recommended to set the  default_inverse_retraction_method  to a favourite retraction. If this default is set, a  inverse_retraction_method=  does not have to be specified. A  vector_transport_to! M, Y, p, X, q) ; it is recommended to set the  default_vector_transport_method  to a favourite retraction. If this default is set, a  vector_transport_method=  does not have to be specified. By default the stopping criterion uses the  norm  as well, to stop when the norm of the gradient is small, but if you implemented  inner , the norm is provided already. Tangent vectors storing the social and cognitive vectors are initialized calling  zero_vector (M,p) . A  copyto! (M, q, p)  and  copy (M,p)  for points. The  distance (M, p, q)  when using the default stopping criterion, which uses  StopWhenChangeLess ."},{"id":3139,"pagetitle":"Particle Swarm Optimization","title":"Literature","ref":"/manopt/stable/solvers/particle_swarm/#Literature","content":" Literature [BIA10] P.¬†B.¬†Borckmans, M.¬†Ishteva and P.-A.¬†Absil.  A Modified Particle Swarm Optimization Algorithm for the Best Low Multilinear Rank Approximation of Higher-Order Tensors . In:  7th International Conference on Swarm INtelligence  (Springer Berlin Heidelberg, 2010); pp.¬†13‚Äì23."},{"id":3142,"pagetitle":"Primal-dual Riemannian semismooth Newton","title":"Primal-dual Riemannian semismooth Newton algorithm","ref":"/manopt/stable/solvers/primal_dual_semismooth_Newton/#solver-pdrssn","content":" Primal-dual Riemannian semismooth Newton algorithm The Primal-dual Riemannian semismooth Newton Algorithm is a second-order method derived from the  ChambollePock . The aim is to solve an optimization problem on a manifold with a cost function of the form \\[F(p) + G(Œõ(p)),\\] where  $F:\\mathcal M ‚Üí \\overline{‚Ñù}$ ,  $G:\\mathcal N ‚Üí \\overline{‚Ñù}$ , and  $Œõ:\\mathcal M ‚Üí\\mathcal N$ . If the manifolds  $\\mathcal M$  or  $\\mathcal N$  are not Hadamard, it has to be considered locally only, that is on geodesically convex sets  $\\mathcal C \\subset \\mathcal M$  and  $\\mathcal D \\subset\\mathcal N$  such that  $Œõ(\\mathcal C) \\subset \\mathcal D$ . The algorithm comes down to applying the Riemannian semismooth Newton method to the rewritten primal-dual optimality conditions. Define the vector field  $X: \\mathcal{M} \\times \\mathcal{T}_{n}^{*} \\mathcal{N} \\rightarrow \\mathcal{T} \\mathcal{M} \\times \\mathcal{T}_{n}^{*} \\mathcal{N}$  as \\[X\\left(p, \\xi_{n}\\right):=\\left(\\begin{array}{c}\n-\\log _{p} \\operatorname{prox}_{\\sigma F}\\left(\\exp _{p}\\left(\\mathcal{P}_{p \\leftarrow m}\\left(-\\sigma\\left(D_{m} \\Lambda\\right)^{*}\\left[\\mathcal{P}_{\\Lambda(m) \\leftarrow n} \\xi_{n}\\right]\\right)^{\\sharp}\\right)\\right) \\\\\n\\xi_{n}-\\operatorname{prox}_{\\tau G_{n}^{*}}\\left(\\xi_{n}+\\tau\\left(\\mathcal{P}_{n \\leftarrow \\Lambda(m)} D_{m} \\Lambda\\left[\\log _{m} p\\right]\\right)^{\\flat}\\right)\n\\end{array}\\right)\\] and solve for  $X(p,Œæ_{n})=0$ . Given base points  $m‚àà\\mathcal C$ ,  $n=Œõ(m)‚àà\\mathcal D$ , initial primal and dual values  $p^{(0)} ‚àà\\mathcal C$ ,  $Œæ_{n}^{(0)} ‚àà \\mathcal T_{n}^{*}\\mathcal N$ , and primal and dual step sizes  $\\sigma$ ,  $\\tau$ . The algorithms performs the steps  $k=1,‚Ä¶,$  (until a  StoppingCriterion  is reached) Choose any element \\[V^{(k)} ‚àà ‚àÇ_C X(p^{(k)},Œæ_n^{(k)})\\] of the Clarke generalized covariant derivative Solve \\[V^{(k)} [(d_p^{(k)}, d_n^{(k)})] = - X(p^{(k)},Œæ_n^{(k)})\\] in the vector space  $\\mathcal{T}_{p^{(k)}} \\mathcal{M} \\times \\mathcal{T}_{n}^{*} \\mathcal{N}$ Update \\[p^{(k+1)} := \\exp_{p^{(k)}}(d_p^{(k)})\\] and \\[Œæ_n^{(k+1)} := Œæ_n^{(k)} + d_n^{(k)}\\] Furthermore you can exchange the exponential map, the logarithmic map, and the parallel transport by a retraction, an inverse retraction and a vector transport. Finally you can also update the base points  $m$  and  $n$  during the iterations. This introduces a few additional vector transports. The same holds for the case that  $Œõ(m^{(k)})\\neq n^{(k)}$  at some point. All these cases are covered in the algorithm."},{"id":3143,"pagetitle":"Primal-dual Riemannian semismooth Newton","title":"Manopt.primal_dual_semismooth_Newton","ref":"/manopt/stable/solvers/primal_dual_semismooth_Newton/#Manopt.primal_dual_semismooth_Newton","content":" Manopt.primal_dual_semismooth_Newton  ‚Äî  Function primal_dual_semismooth_Newton(M, N, cost, p, X, m, n, prox_F, diff_prox_F, prox_G_dual, diff_prox_dual_G, linearized_operator, adjoint_linearized_operator) Perform the Primal-Dual Riemannian semismooth Newton algorithm. Given a  cost  function  $\\mathcal E: \\mathcal M ‚Üí \\overline{‚Ñù}$  of the form \\[\\mathcal E(p) = F(p) + G( Œõ(p) ),\\] where  $F: \\mathcal M ‚Üí \\overline{‚Ñù}$ ,  $G: \\mathcal N ‚Üí \\overline{‚Ñù}$ , and  $Œõ: \\mathcal M ‚Üí \\mathcal N$ . The remaining input parameters are p, X :                          primal and dual start points  $p‚àà\\mathcal M$  and  $X ‚àà T_n\\mathcal N$ m,n :                           base points on  $\\mathcal M$  and ` \\mathcal N , respectively. linearized_forward_operator :   the linearization  $DŒõ(‚ãÖ)[‚ãÖ]$  of the operator  $Œõ(‚ãÖ)$ . adjoint_linearized_operator :   the adjoint  $DŒõ^*$  of the linearized operator  $DŒõ(m):  T_{m}\\mathcal M ‚Üí T_{Œõ(m)}\\mathcal N$ prox_F, prox_G_Dual :           the proximal maps of  $F$  and  $G^\\ast_n$ diff_prox_F, diff_prox_dual_G : the (Clarke Generalized) differentials of the proximal maps of  $F$  and  $G^\\ast_n$ For more details on the algorithm, see [ DL21 ]. Keyword arguments dual_stepsize=1/sqrt(8) : proximal parameter of the dual prox evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses Œõ=missing : the exact operator, that is required if  Œõ(m)=n  does not hold;  missing  indicates, that the forward operator is exact. primal_stepsize=1/sqrt(8) : proximal parameter of the primal prox reg_param=1e-5 : regularisation parameter for the Newton matrix Note that this changes the arguments the  forward_operator  is called. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopAfterIteration (50) : a functor indicating that the stopping criterion is fulfilled update_primal_base=missing : function to update  m  (identity by default/missing) update_dual_base=missing : function to update  n  (identity by default/missing) vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3144,"pagetitle":"Primal-dual Riemannian semismooth Newton","title":"Manopt.primal_dual_semismooth_Newton!","ref":"/manopt/stable/solvers/primal_dual_semismooth_Newton/#Manopt.primal_dual_semismooth_Newton!","content":" Manopt.primal_dual_semismooth_Newton!  ‚Äî  Function primal_dual_semismooth_Newton(M, N, cost, p, X, m, n, prox_F, diff_prox_F, prox_G_dual, diff_prox_dual_G, linearized_operator, adjoint_linearized_operator) Perform the Primal-Dual Riemannian semismooth Newton algorithm. Given a  cost  function  $\\mathcal E: \\mathcal M ‚Üí \\overline{‚Ñù}$  of the form \\[\\mathcal E(p) = F(p) + G( Œõ(p) ),\\] where  $F: \\mathcal M ‚Üí \\overline{‚Ñù}$ ,  $G: \\mathcal N ‚Üí \\overline{‚Ñù}$ , and  $Œõ: \\mathcal M ‚Üí \\mathcal N$ . The remaining input parameters are p, X :                          primal and dual start points  $p‚àà\\mathcal M$  and  $X ‚àà T_n\\mathcal N$ m,n :                           base points on  $\\mathcal M$  and ` \\mathcal N , respectively. linearized_forward_operator :   the linearization  $DŒõ(‚ãÖ)[‚ãÖ]$  of the operator  $Œõ(‚ãÖ)$ . adjoint_linearized_operator :   the adjoint  $DŒõ^*$  of the linearized operator  $DŒõ(m):  T_{m}\\mathcal M ‚Üí T_{Œõ(m)}\\mathcal N$ prox_F, prox_G_Dual :           the proximal maps of  $F$  and  $G^\\ast_n$ diff_prox_F, diff_prox_dual_G : the (Clarke Generalized) differentials of the proximal maps of  $F$  and  $G^\\ast_n$ For more details on the algorithm, see [ DL21 ]. Keyword arguments dual_stepsize=1/sqrt(8) : proximal parameter of the dual prox evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses Œõ=missing : the exact operator, that is required if  Œõ(m)=n  does not hold;  missing  indicates, that the forward operator is exact. primal_stepsize=1/sqrt(8) : proximal parameter of the primal prox reg_param=1e-5 : regularisation parameter for the Newton matrix Note that this changes the arguments the  forward_operator  is called. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopAfterIteration (50) : a functor indicating that the stopping criterion is fulfilled update_primal_base=missing : function to update  m  (identity by default/missing) update_dual_base=missing : function to update  n  (identity by default/missing) vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3145,"pagetitle":"Primal-dual Riemannian semismooth Newton","title":"State","ref":"/manopt/stable/solvers/primal_dual_semismooth_Newton/#State","content":" State"},{"id":3146,"pagetitle":"Primal-dual Riemannian semismooth Newton","title":"Manopt.PrimalDualSemismoothNewtonState","ref":"/manopt/stable/solvers/primal_dual_semismooth_Newton/#Manopt.PrimalDualSemismoothNewtonState","content":" Manopt.PrimalDualSemismoothNewtonState  ‚Äî  Type PrimalDualSemismoothNewtonState <: AbstractPrimalDualSolverState Fields m::P : a point on the manifold  $\\mathcal M$ n::Q : a point on the manifold  $\\mathcal N$ p::P : a point on the manifold  $\\mathcal M$  storing the current iterate X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ primal_stepsize::Float64 :  proximal parameter of the primal prox dual_stepsize::Float64 :    proximal parameter of the dual prox reg_param::Float64 :        regularisation parameter for the Newton matrix stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled update_primal_base :        function to update the primal base update_dual_base :          function to update the dual base inverse_retraction_method::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports where for the update functions a  AbstractManoptProblem amp ,  AbstractManoptSolverState ams  and the current iterate  i  are the arguments. If you activate these to be different from the default identity, you have to provide  p.Œõ  for the algorithm to work (which might be  missing ). Constructor PrimalDualSemismoothNewtonState(M::AbstractManifold; kwargs...) Generate a state for the  primal_dual_semismooth_Newton . Keyword arguments m= rand (M) n= rand (N) p= rand (M) X= zero_vector (M, p) primal_stepsize=1/sqrt(8) dual_stepsize=1/sqrt(8) reg_param=1e-5 update_primal_base=(amp, ams, k) -> o.m update_dual_base=(amp, ams, k) -> o.n retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses stopping_criterion= [ StopAfterIteration ](@ref) (50)`: a functor indicating that the stopping criterion is fulfilled vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports source"},{"id":3147,"pagetitle":"Primal-dual Riemannian semismooth Newton","title":"Technical details","ref":"/manopt/stable/solvers/primal_dual_semismooth_Newton/#sec-ssn-technical-details","content":" Technical details The  primal_dual_semismooth_Newton  solver requires the following functions of a manifold to be available for both the manifold  $\\mathcal M$ and  $\\mathcal N$ A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified. An  inverse_retract! (M, X, p, q) ; it is recommended to set the  default_inverse_retraction_method  to a favourite retraction. If this default is set, a  inverse_retraction_method=  does not have to be specified. A  vector_transport_to! M, Y, p, X, q) ; it is recommended to set the  default_vector_transport_method  to a favourite retraction. If this default is set, a  vector_transport_method=  does not have to be specified. A  copyto! (M, q, p)  and  copy (M,p)  for points. A  get_basis  for the  DefaultOrthonormalBasis  on  $\\mathcal M$ exp  and  log  (on  $\\mathcal M$ ) A  DiagonalizingOrthonormalBasis  to compute the differentials of the exponential and logarithmic map Tangent vectors storing the social and cognitive vectors are initialized calling  zero_vector (M,p) ."},{"id":3148,"pagetitle":"Primal-dual Riemannian semismooth Newton","title":"Literature","ref":"/manopt/stable/solvers/primal_dual_semismooth_Newton/#Literature","content":" Literature [DL21] W.¬†Diepeveen and J.¬†Lellmann.  An Inexact Semismooth Newton Method on Riemannian Manifolds with Application to Duality-Based Total Variation Denoising .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  14 , 1565‚Äì1600  (2021),  arXiv:2102.10309 ."},{"id":3151,"pagetitle":"Projected Gradient Method","title":"Projected gradient method","ref":"/manopt/stable/solvers/projected_gradient_method/#Projected-gradient-method","content":" Projected gradient method"},{"id":3152,"pagetitle":"Projected Gradient Method","title":"Manopt.projected_gradient_method","ref":"/manopt/stable/solvers/projected_gradient_method/#Manopt.projected_gradient_method","content":" Manopt.projected_gradient_method  ‚Äî  Function projected_gradient_method(M, f, grad_f, proj, p=rand(M); kwargs...)\nprojected_gradient_method(M, obj::ManifoldConstrainedSetObjective, p=rand(M); kwargs...)\nprojected_gradient_method!(M, f, grad_f, proj, p; kwargs...)\nprojected_gradient_method!(M, obj::ManifoldConstrainedSetObjective, p; kwargs...) Compute the projected gradient method for the constrained problem \\[\\begin{aligned}\n\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} & f(p)\\\\\n\\text{subject to}\\quad& p ‚àà \\mathcal C ‚äÇ \\mathcal M\n\\end{aligned}\\] by performing the following steps Using the  stepsize $Œ±_k$  compute a candidate  $q_k = \\operatorname{proj}_{\\mathcal C}\\Bigl(\\operatorname{retr}_{p_k}\\bigl(-Œ±_k \\operatorname{grad} f(p_k)\\bigr)\\Bigr)$ Compute a backtracking stepsize  $Œ≤_k ‚â§ 1$  along  $Y_k = \\operatorname{retr}_{p_k}^{-1}q_k$ Compute the new iterate  $p_{k+1} = \\operatorname{retr}_{p_k}( Œ≤_k \\operatorname{retr}_{p_k}^{-1}q_k )$ until the  stopping_criterion=  is fulfilled. For more information see [ BFNZ25 ]. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place proj  the function that projects onto the set  $\\mathcal C$  as a function  (M, p) -> q  or a function  (M, q, p) -> q  computing the projection in-place of  q . p : a point on the manifold  $\\mathcal M$ Keyword arguments backtrack= ArmijoLinesearchStepsize (M; stop_increasing_at_step=0) : a functor inheriting from  Stepsize  to determine a step size to perform the backtracking to determine the  $Œ≤_k$ . Note that the method requires  $Œ≤_k ‚â§ 1$ , otherwise the projection step no longer provides points within the constraints evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= ConstantStepsize (injectivity_radius(M)/2) : a functor inheriting from  Stepsize  to determine a step size to perform the candidate projected step. stopping_criterion= StopAfterIteration (500) | StopWhenGradientNormLess (1.0e-6) ): a functor indicating that the stopping criterion is fulfilled All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3153,"pagetitle":"Projected Gradient Method","title":"Manopt.projected_gradient_method!","ref":"/manopt/stable/solvers/projected_gradient_method/#Manopt.projected_gradient_method!","content":" Manopt.projected_gradient_method!  ‚Äî  Function projected_gradient_method(M, f, grad_f, proj, p=rand(M); kwargs...)\nprojected_gradient_method(M, obj::ManifoldConstrainedSetObjective, p=rand(M); kwargs...)\nprojected_gradient_method!(M, f, grad_f, proj, p; kwargs...)\nprojected_gradient_method!(M, obj::ManifoldConstrainedSetObjective, p; kwargs...) Compute the projected gradient method for the constrained problem \\[\\begin{aligned}\n\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} & f(p)\\\\\n\\text{subject to}\\quad& p ‚àà \\mathcal C ‚äÇ \\mathcal M\n\\end{aligned}\\] by performing the following steps Using the  stepsize $Œ±_k$  compute a candidate  $q_k = \\operatorname{proj}_{\\mathcal C}\\Bigl(\\operatorname{retr}_{p_k}\\bigl(-Œ±_k \\operatorname{grad} f(p_k)\\bigr)\\Bigr)$ Compute a backtracking stepsize  $Œ≤_k ‚â§ 1$  along  $Y_k = \\operatorname{retr}_{p_k}^{-1}q_k$ Compute the new iterate  $p_{k+1} = \\operatorname{retr}_{p_k}( Œ≤_k \\operatorname{retr}_{p_k}^{-1}q_k )$ until the  stopping_criterion=  is fulfilled. For more information see [ BFNZ25 ]. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place proj  the function that projects onto the set  $\\mathcal C$  as a function  (M, p) -> q  or a function  (M, q, p) -> q  computing the projection in-place of  q . p : a point on the manifold  $\\mathcal M$ Keyword arguments backtrack= ArmijoLinesearchStepsize (M; stop_increasing_at_step=0) : a functor inheriting from  Stepsize  to determine a step size to perform the backtracking to determine the  $Œ≤_k$ . Note that the method requires  $Œ≤_k ‚â§ 1$ , otherwise the projection step no longer provides points within the constraints evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= ConstantStepsize (injectivity_radius(M)/2) : a functor inheriting from  Stepsize  to determine a step size to perform the candidate projected step. stopping_criterion= StopAfterIteration (500) | StopWhenGradientNormLess (1.0e-6) ): a functor indicating that the stopping criterion is fulfilled All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3154,"pagetitle":"Projected Gradient Method","title":"State","ref":"/manopt/stable/solvers/projected_gradient_method/#State","content":" State"},{"id":3155,"pagetitle":"Projected Gradient Method","title":"Manopt.ProjectedGradientMethodState","ref":"/manopt/stable/solvers/projected_gradient_method/#Manopt.ProjectedGradientMethodState","content":" Manopt.ProjectedGradientMethodState  ‚Äî  Type ProjectedGradientMethodState <: AbstractManoptSolverState Fields backtracking::Stepsize : a functor inheriting from  Stepsize  to determine a step size to determine the step size  $Œ≤_k$  step size from  $p_k$  to the candidate  $q_k$ inverse_retraction_method::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses p::P : a point on the manifold  $\\mathcal M$  storing the current iterate q  an interims point for the projected gradient step stepsize::Stepsize : a functor inheriting from  Stepsize  to determine a step size  $Œ±_k$  to determine the  $q_k$  candidate stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ Y::T  a temporary memory for a tangent vector to store the no. Used within the backtracking Constructor ProjectedGradientMethodState(M, p=rand(M); kwargs...) Keyword arguments backtracking= ArmijoLinesearchStepsize (M) : a functor inheriting from  Stepsize  to determine a step size  $p_k$  to the candidate  $q_k$ inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses stepsize= ConstantStepsize (M) : a functor inheriting from  Stepsize  to determine a step size  $Œ±_k$  to determine the  $q_k$  candidate stop= StopAfterIteration (300) : a functor indicating that the stopping criterion is fulfilled retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ source"},{"id":3156,"pagetitle":"Projected Gradient Method","title":"Stopping criteria","ref":"/manopt/stable/solvers/projected_gradient_method/#Stopping-criteria","content":" Stopping criteria"},{"id":3157,"pagetitle":"Projected Gradient Method","title":"Manopt.StopWhenProjectedGradientStationary","ref":"/manopt/stable/solvers/projected_gradient_method/#Manopt.StopWhenProjectedGradientStationary","content":" Manopt.StopWhenProjectedGradientStationary  ‚Äî  Type StopWhenProjectedGradientStationary <: StoppingCriterion Stop when the step taken by the projection is  (before linesearch) exactly the opposite of the source"},{"id":3158,"pagetitle":"Projected Gradient Method","title":"Literature","ref":"/manopt/stable/solvers/projected_gradient_method/#Literature","content":" Literature [BFNZ25] R.¬†Bergmann, O.¬†P.¬†Ferreira, S.¬†Z.¬†N√©meth and J.¬†Zhu.  On projection mappings and the gradient projection method on hyperbolic space forms . Preprint,¬†in¬†preparation (2025)."},{"id":3161,"pagetitle":"Proximal bundle method","title":"Proximal bundle method","ref":"/manopt/stable/solvers/proximal_bundle_method/#Proximal-bundle-method","content":" Proximal bundle method"},{"id":3162,"pagetitle":"Proximal bundle method","title":"Manopt.proximal_bundle_method","ref":"/manopt/stable/solvers/proximal_bundle_method/#Manopt.proximal_bundle_method","content":" Manopt.proximal_bundle_method  ‚Äî  Function proximal_bundle_method(M, f, ‚àÇf, p=rand(M), kwargs...)\nproximal_bundle_method!(M, f, ‚àÇf, p, kwargs...) perform a proximal bundle method  $p^{(k+1)} = \\operatorname{retr}_{p^{(k)}}(-d_k)$ , where  $\\operatorname{retr}$  is a retraction and \\[d_k = \\frac{1}{\\mu_k} \\sum_{j\\in J_k} Œª_j^k \\mathrm{P}_{p_k‚Üêq_j}X_{q_j},\\] with  $X_{q_j} ‚àà ‚àÇf(q_j)$ ,  $p_k$  the last serious iterate,  $\\mu_k$  a proximal parameter, and the  $Œª_j^k$  as solutions to the quadratic subproblem provided by the sub solver, see for example the  proximal_bundle_method_subsolver . Though the subdifferential might be set valued, the argument  ‚àÇf  should always return  one  element from the subdifferential, but not necessarily deterministic. For more details see [ HNP23 ]. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v ‚àÇf :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. p : a point on the manifold  $\\mathcal M$ Keyword arguments Œ±‚ÇÄ=1.2 :          initialization value for  Œ± , used to update  Œ∑ bundle_size=50 :  the maximal size of the bundle Œ¥=1.0 :           parameter for updating  Œº : if  $Œ¥ < 0$  then  $Œº = \\log(i + 1)$ , else  $Œº += Œ¥ Œº$ Œµ=1e-2 :          stepsize-like parameter related to the injectivity radius of the manifold evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses m=0.0125 :        a real number that controls the decrease of the cost function Œº=0.5 :           initial proximal parameter for the subproblem retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopWhenLagrangeMultiplierLess (1e-8) | StopAfterIteration (5000) : a functor indicating that the stopping criterion is fulfilled sub_problem= proximal_bundle_method_subsolver `:  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= AllocatingEvaluation :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3163,"pagetitle":"Proximal bundle method","title":"Manopt.proximal_bundle_method!","ref":"/manopt/stable/solvers/proximal_bundle_method/#Manopt.proximal_bundle_method!","content":" Manopt.proximal_bundle_method!  ‚Äî  Function proximal_bundle_method(M, f, ‚àÇf, p=rand(M), kwargs...)\nproximal_bundle_method!(M, f, ‚àÇf, p, kwargs...) perform a proximal bundle method  $p^{(k+1)} = \\operatorname{retr}_{p^{(k)}}(-d_k)$ , where  $\\operatorname{retr}$  is a retraction and \\[d_k = \\frac{1}{\\mu_k} \\sum_{j\\in J_k} Œª_j^k \\mathrm{P}_{p_k‚Üêq_j}X_{q_j},\\] with  $X_{q_j} ‚àà ‚àÇf(q_j)$ ,  $p_k$  the last serious iterate,  $\\mu_k$  a proximal parameter, and the  $Œª_j^k$  as solutions to the quadratic subproblem provided by the sub solver, see for example the  proximal_bundle_method_subsolver . Though the subdifferential might be set valued, the argument  ‚àÇf  should always return  one  element from the subdifferential, but not necessarily deterministic. For more details see [ HNP23 ]. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v ‚àÇf :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. p : a point on the manifold  $\\mathcal M$ Keyword arguments Œ±‚ÇÄ=1.2 :          initialization value for  Œ± , used to update  Œ∑ bundle_size=50 :  the maximal size of the bundle Œ¥=1.0 :           parameter for updating  Œº : if  $Œ¥ < 0$  then  $Œº = \\log(i + 1)$ , else  $Œº += Œ¥ Œº$ Œµ=1e-2 :          stepsize-like parameter related to the injectivity radius of the manifold evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses m=0.0125 :        a real number that controls the decrease of the cost function Œº=0.5 :           initial proximal parameter for the subproblem retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopWhenLagrangeMultiplierLess (1e-8) | StopAfterIteration (5000) : a functor indicating that the stopping criterion is fulfilled sub_problem= proximal_bundle_method_subsolver `:  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= AllocatingEvaluation :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3164,"pagetitle":"Proximal bundle method","title":"State","ref":"/manopt/stable/solvers/proximal_bundle_method/#State","content":" State"},{"id":3165,"pagetitle":"Proximal bundle method","title":"Manopt.ProximalBundleMethodState","ref":"/manopt/stable/solvers/proximal_bundle_method/#Manopt.ProximalBundleMethodState","content":" Manopt.ProximalBundleMethodState  ‚Äî  Type ProximalBundleMethodState <: AbstractManoptSolverState stores option values for a  proximal_bundle_method  solver. Fields Œ± :                        curvature-dependent parameter used to update  Œ∑ Œ±‚ÇÄ :                       initialization value for  Œ± , used to update  Œ∑ approx_errors :            approximation of the linearization errors at the last serious step bundle :                   bundle that collects each iterate with the computed subgradient at the iterate bundle_size :              the maximal size of the bundle c :                        convex combination of the approximation errors d :                        descent direction Œ¥ :                        parameter for updating  Œº : if  $Œ¥ < 0$  then  $Œº = \\log(i + 1)$ , else  $Œº += Œ¥ Œº$ Œµ :                        stepsize-like parameter related to the injectivity radius of the manifold Œ∑ :                        curvature-dependent term for updating the approximation errors inverse_retraction_method::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses Œª :                        convex coefficients that solve the subproblem m :                        the parameter to test the decrease of the cost Œº :                        (initial) proximal parameter for the subproblem ŒΩ :                        the stopping parameter given by  $ŒΩ = - Œº |d|^2 - c$ p::P : a point on the manifold  $\\mathcal M$  storing the current iterate p_last_serious :           last serious iterate retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled transported_subgradients : subgradients of the bundle that are transported to  p_last_serious vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing a subgradient at the current iterate sub_problem::Union{AbstractManoptProblem, F} :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state::Union{AbstractManoptProblem, F} :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. Constructor ProximalBundleMethodState(M::AbstractManifold, sub_problem, sub_state; kwargs...)\nProximalBundleMethodState(M::AbstractManifold, sub_problem=proximal_bundle_method_subsolver; evaluation=AllocatingEvaluation(), kwargs...) Generate the state for the  proximal_bundle_method  on the manifold  M Keyword arguments Œ±‚ÇÄ=1.2 bundle_size=50 Œ¥=1.0 Œµ=1e-2 Œº=0.5 m=0.0125 retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions inverse_retraction_method= default_inverse_retraction_method (M, typeof(p)) : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value stopping_criterion= StopWhenLagrangeMultiplierLess (1e-8) | StopAfterIteration (5000) : a functor indicating that the stopping criterion is fulfilled sub_problem= proximal_bundle_method_subsolver `:  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= AllocatingEvaluation :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports X= zero_vector (M, p)  specify the type of tangent vector to use. source"},{"id":3166,"pagetitle":"Proximal bundle method","title":"Helpers and internal functions","ref":"/manopt/stable/solvers/proximal_bundle_method/#Helpers-and-internal-functions","content":" Helpers and internal functions"},{"id":3167,"pagetitle":"Proximal bundle method","title":"Manopt.proximal_bundle_method_subsolver","ref":"/manopt/stable/solvers/proximal_bundle_method/#Manopt.proximal_bundle_method_subsolver","content":" Manopt.proximal_bundle_method_subsolver  ‚Äî  Function Œª = proximal_bundle_method_subsolver(M, p_last_serious, Œº, approximation_errors, transported_subgradients)\nproximal_bundle_method_subsolver!(M, Œª, p_last_serious, Œº, approximation_errors, transported_subgradients) solver for the subproblem of the proximal bundle method. The subproblem for the proximal bundle method is \\[\\begin{align*}\n    \\operatorname*{arg\\,min}_{Œª ‚àà ‚Ñù^{\\lvert L_l\\rvert}} &\n    \\frac{1}{2 \\mu_l} \\Bigl\\lVert \\sum_{j ‚àà L_l} Œª_j \\mathrm{P}_{p_k‚Üêq_j} X_{q_j} \\Bigr\\rVert^2\n    + \\sum_{j ‚àà L_l} Œª_j \\, c_j^k\n    \\\\\n    \\text{s. t.} \\quad &\n    \\sum_{j ‚àà L_l} Œª_j = 1,\n    \\quad Œª_j ‚â• 0\n    \\quad \\text{for all } j ‚àà L_l,\n\\end{align*}\\] where  $L_l = \\{k\\}$  if  $q_k$  is a serious iterate, and  $L_l = L_{l-1} \\cup \\{k\\}$  otherwise. See [ HNP23 ]. Tip A default subsolver based on  RipQP .jl  and  QuadraticModels  is available if these two packages are loaded. source"},{"id":3168,"pagetitle":"Proximal bundle method","title":"Literature","ref":"/manopt/stable/solvers/proximal_bundle_method/#Literature","content":" Literature [HNP23] N.¬†Hoseini Monjezi, S.¬†Nobakhtian and M.¬†R.¬†Pouryayevali.  A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds .  IMA¬†Journal¬†of¬†Numerical¬†Analysis  43 , 293‚Äì325  (2023)."},{"id":3171,"pagetitle":"Proximal Gradient Method","title":"Proximal gradient method","ref":"/manopt/stable/solvers/proximal_gradient_method/#Proximal-gradient-method","content":" Proximal gradient method"},{"id":3172,"pagetitle":"Proximal Gradient Method","title":"Manopt.proximal_gradient_method","ref":"/manopt/stable/solvers/proximal_gradient_method/#Manopt.proximal_gradient_method","content":" Manopt.proximal_gradient_method  ‚Äî  Function proximal_gradient_method(M, f, g, grad_g, p=rand(M); prox_nonsmooth=nothing, kwargs...)\nproximal_gradient_method(M, mpgo::ManifoldProximalGradientObjective, p=rand(M); kwargs...)\nproximal_gradient_method!(M, f, g, grad_g, p; prox_nonsmooth=nothing, kwargs...)\nproximal_gradient_method!(M, mpgo::ManifoldProximalGradientObjective, p; kwargs...) Perform the proximal gradient method as introduced in [ BJJP25 ]. Given the minimization problem \\[\\operatorname*{arg\\,min}_{p‚àà\\mathcal M} f(p),\n\\quad \\text{ where } \\quad f(p) = g(p) + h(p).\\] This method performs the (intrinsic) proximal gradient method algorithm. Let  $Œª_k ‚â• 0$  be a sequence of (proximal) parameters, initialize  $p^{(0)} = p$ , and  $k=0$ . Then perform as long as the stopping criterion is not fulfilled \\[p^{(k+1)} = prox_{Œª_kh}\\Bigl(\n\\operatorname{retr}_{a^{(k)}}\\bigl(-Œª_k \\operatorname{grad} g(a^{(k)}\\bigr)\n\\Bigr),\\] where  $a^{(k)}=p^{(k)}$  by default, but it allows to introduce some acceleration before computing the gradient step. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v total cost function  f = g + h g :              the smooth part of the cost function grad_g :           a gradient  (M,p) -> X  or  (M, X, p) -> X  of the smooth part  $g$  of the problem Keyword Arguments acceleration=(p, s, k) -> (copyto!(get_manifold(M), s.a, s.p); s) : a function  (problem, state, k) -> state  to compute an acceleration, that is performed before the gradient step - the default is to copy the current point to the acceleration point, i.e. no acceleration is performed evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. prox_nonsmooth :          a proximal map  (M,Œª,p) -> q  or  (M, q, Œª, p) -> q  for the (possibly) nonsmoooth part  $h$  of  $f$ p : a point on the manifold  $\\mathcal M$ stepsize= default_stepsize (M, ProximalGradientMethodState) : a functor inheriting from  Stepsize  to determine a step size that by default uses a  ProximalGradientMethodBacktracking . retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopAfterIteration (100) : a functor indicating that the stopping criterion is fulfilled sub_problem= nothing:  specify a problem for a solver or a closed form solution function, which can be allocating or in-place.or nothing to take the proximal map from the  ManifoldProximalGradientObjective sub_state= evaluation:  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function.This field is ignored, if the  sub_problem  is  Nothing X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing the gradient at the current iterate All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3173,"pagetitle":"Proximal Gradient Method","title":"Manopt.proximal_gradient_method!","ref":"/manopt/stable/solvers/proximal_gradient_method/#Manopt.proximal_gradient_method!","content":" Manopt.proximal_gradient_method!  ‚Äî  Function proximal_gradient_method(M, f, g, grad_g, p=rand(M); prox_nonsmooth=nothing, kwargs...)\nproximal_gradient_method(M, mpgo::ManifoldProximalGradientObjective, p=rand(M); kwargs...)\nproximal_gradient_method!(M, f, g, grad_g, p; prox_nonsmooth=nothing, kwargs...)\nproximal_gradient_method!(M, mpgo::ManifoldProximalGradientObjective, p; kwargs...) Perform the proximal gradient method as introduced in [ BJJP25 ]. Given the minimization problem \\[\\operatorname*{arg\\,min}_{p‚àà\\mathcal M} f(p),\n\\quad \\text{ where } \\quad f(p) = g(p) + h(p).\\] This method performs the (intrinsic) proximal gradient method algorithm. Let  $Œª_k ‚â• 0$  be a sequence of (proximal) parameters, initialize  $p^{(0)} = p$ , and  $k=0$ . Then perform as long as the stopping criterion is not fulfilled \\[p^{(k+1)} = prox_{Œª_kh}\\Bigl(\n\\operatorname{retr}_{a^{(k)}}\\bigl(-Œª_k \\operatorname{grad} g(a^{(k)}\\bigr)\n\\Bigr),\\] where  $a^{(k)}=p^{(k)}$  by default, but it allows to introduce some acceleration before computing the gradient step. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v total cost function  f = g + h g :              the smooth part of the cost function grad_g :           a gradient  (M,p) -> X  or  (M, X, p) -> X  of the smooth part  $g$  of the problem Keyword Arguments acceleration=(p, s, k) -> (copyto!(get_manifold(M), s.a, s.p); s) : a function  (problem, state, k) -> state  to compute an acceleration, that is performed before the gradient step - the default is to copy the current point to the acceleration point, i.e. no acceleration is performed evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. prox_nonsmooth :          a proximal map  (M,Œª,p) -> q  or  (M, q, Œª, p) -> q  for the (possibly) nonsmoooth part  $h$  of  $f$ p : a point on the manifold  $\\mathcal M$ stepsize= default_stepsize (M, ProximalGradientMethodState) : a functor inheriting from  Stepsize  to determine a step size that by default uses a  ProximalGradientMethodBacktracking . retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopAfterIteration (100) : a functor indicating that the stopping criterion is fulfilled sub_problem= nothing:  specify a problem for a solver or a closed form solution function, which can be allocating or in-place.or nothing to take the proximal map from the  ManifoldProximalGradientObjective sub_state= evaluation:  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function.This field is ignored, if the  sub_problem  is  Nothing X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing the gradient at the current iterate All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3174,"pagetitle":"Proximal Gradient Method","title":"Manopt.ProximalGradientMethodAcceleration","ref":"/manopt/stable/solvers/proximal_gradient_method/#Manopt.ProximalGradientMethodAcceleration","content":" Manopt.ProximalGradientMethodAcceleration  ‚Äî  Type ProximalGradientMethodAcceleration{P, T, F} Compute an acceleration step \\[a^{(k)} = \\operatorname{retr}_{p^{(k)}}\\bigl(\n  -Œ≤_k\\operatorname{retr}^{-1}_{p^{(k)}}(p)\n\\bigr)\\] where  p^{(k)}  is the current iterate from the  ProximalGradientMethodState s field  p  and the result is stored in  state.a . The field  p  in this struct stores the last iterate. The retraction and its inverse are taken from the state. Fields p  - the last iterate Œ≤  - acceleration parameter function or value inverse_retraction_method  - method for inverse retraction X  - tangent vector for computations Constructor ProximalGradientMethodAcceleration(M::AbstractManifold; kwargs...) Generate the state for a given manifold  M  with initial iterate  p . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ Keyword arguments Œ≤ = k -> (k-1)/(k+2)  - acceleration parameter function or value inverse_retraction_method  - method for inverse retraction p  - initial point X  - initial tangent vector source"},{"id":3175,"pagetitle":"Proximal Gradient Method","title":"State","ref":"/manopt/stable/solvers/proximal_gradient_method/#State","content":" State"},{"id":3176,"pagetitle":"Proximal Gradient Method","title":"Manopt.ProximalGradientMethodState","ref":"/manopt/stable/solvers/proximal_gradient_method/#Manopt.ProximalGradientMethodState","content":" Manopt.ProximalGradientMethodState  ‚Äî  Type ProximalGradientMethodState <: AbstractManoptSolverState State for the  proximal_gradient_method  solver. Fields inverse_retraction_method::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses a  - point after acceleration step p::P : a point on the manifold  $\\mathcal M$  storing the current iterate q  - point for storing gradient step retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions X  - tangent vector for storing gradient stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled acceleration  - a function  (problem, state, k) -> state  to compute an acceleration before the gradient step stepsize  - a function or  Stepsize  object to compute the stepsize last_stepsize  - stores the last computed stepsize sub_problem::Union{AbstractManoptProblem, F} :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place.or nothing to take the proximal map from the  ManifoldProximalGradientObjective sub_state::Union{AbstractManoptProblem, F} :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function.This field is ignored, if the  sub_problem  is  Nothing Constructor ProximalGradientMethodState(M::AbstractManifold; kwargs...) Generate the state for a given manifold  M  with initial iterate  p . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ Keyword arguments stepsize=default_stepsize(M, ProximalGradientMethodState) inverse_retraction_method::AbstractInverseRetractionMethod : an inverse retraction  $\\operatorname{retr}^{-1}$  to use, see  the section on retractions and their inverses p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions acceleration=(p, s, k) -> (copyto!(get_manifold(M), s.a, s.p); s)  by default no acceleration is performed stopping_criterion= StopAfterIteration (100) : a functor indicating that the stopping criterion is fulfilled sub_problem= nothing:  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= AllocatingEvaluation () :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ to specify the representation of a tangent vector source"},{"id":3177,"pagetitle":"Proximal Gradient Method","title":"Helping functions","ref":"/manopt/stable/solvers/proximal_gradient_method/#Helping-functions","content":" Helping functions"},{"id":3178,"pagetitle":"Proximal Gradient Method","title":"Manopt.ProximalGradientNonsmoothSubgradient","ref":"/manopt/stable/solvers/proximal_gradient_method/#Manopt.ProximalGradientNonsmoothSubgradient","content":" Manopt.ProximalGradientNonsmoothSubgradient  ‚Äî  Type ProximalGradientNonsmoothSubgradient{F, R, P} Stores a subgradient of the nonsmooth part  h  of the proximal gradient objective  f = g + h , as well as the stepsize parameter  $Œª ‚àà ‚Ñù$ . This struct is also a functor in both formats     *  (M, p) -> X  to compute the gradient in allocating fashion. This is primarily used for computing a subgradient of the cost function  h(q) + \\frac{1}{2Œª} d^2(q, p)  that defines proximal map in the proximal gradient method. This reads \\[    \\partial h(q) - \\frac{1}{Œª} \\operatorname{log}_q p\\] where  p  is the proximity point where the proximal map is evaluated, i.e. the argument  p  of the proximal map  \\operatorname{prox}_{Œª} h . Fields X::F  - the subgradient of the nonsmooth part of the total objective, i.e. the part of the objective whose proximal map is sought Œª::R  - the stepsize parameter for the proximal map proximity_point::P  - point where the proximal map is evaluated, i.e. the argument of the proximal map that we want to solve for Constructor ProximalGradientNonsmoothSubgradient(cost, Œª, proximity_point) source"},{"id":3179,"pagetitle":"Proximal Gradient Method","title":"Manopt.ProximalGradientNonsmoothCost","ref":"/manopt/stable/solvers/proximal_gradient_method/#Manopt.ProximalGradientNonsmoothCost","content":" Manopt.ProximalGradientNonsmoothCost  ‚Äî  Type ProximalGradientNonsmoothCost{F, R, P} Stores the nonsmooth part  h  of the proximal gradient objective  f = g + h , as well as the stepsize parameter  $Œª ‚àà ‚Ñù$ . This struct is also a functor  (M, q) -> v  that can be used as a cost function within a solver, primarily for solving the proximal map subproblem formulation in the proximal gradient method, which reads \\[    \\operatorname{prox}_{Œª} h(p) = \\operatorname{argmin}_{q \\in \\mathcal M} \\left( h(q) + \\frac{1}{2Œª} d^2(q, p) \\right)\\] Hence, the functor reads \\[    (M, q) \\mapsto h(q) + \\frac{1}{2Œª} d^2(q, p)\\] and  p  is the proximity point where the proximal map is evaluated, i.e. the argument  p  of the proximal map  \\operatorname{prox}_{Œª} h . Fields cost::F  - the nonsmooth part  h  of the proximal gradient objective, i.e. the part of the objective whose proximal map is sought Œª::R  - the stepsize parameter for the proximal map proximity_point::P  - point where the proximal map is evaluated, i.e. the argument  p  of the proximal map  \\operatorname{prox}_{Œª} h  that we want to solve for Constructor ProximalGradientNonsmoothCost(cost, Œª, proximity_point) source"},{"id":3180,"pagetitle":"Proximal Gradient Method","title":"Stopping criteria","ref":"/manopt/stable/solvers/proximal_gradient_method/#Stopping-criteria","content":" Stopping criteria"},{"id":3181,"pagetitle":"Proximal Gradient Method","title":"Manopt.StopWhenGradientMappingNormLess","ref":"/manopt/stable/solvers/proximal_gradient_method/#Manopt.StopWhenGradientMappingNormLess","content":" Manopt.StopWhenGradientMappingNormLess  ‚Äî  Type StopWhenGradientMappingNormLess <: StoppingCriterion A stopping criterion based on the gradient mapping norm for proximal gradient methods. Fields at_iteration::Int : an integer indicating at which the stopping criterion last indicted to stop, which might also be before the solver started ( 0 ). Any negative value indicates that this was not yet the case; last_change::Real : the last change recorded in this stopping criterion threshold : the threshold for the change to check (run under to stop) Constructor StopWhenGradientMappingNormLess(Œµ) Create a stopping criterion with threshold  Œµ  for the gradient mapping for the  proximal_gradient_method . That is, this criterion indicates to stop when the gradient mapping has a norm less than  Œµ . The gradient mapping G Œª(p) is defined as -(1/Œª) * log p(T Œª(p)), where T Œª(p) is the proximal mapping prox Œª f(exp p(-Œª * grad f(p))). source"},{"id":3182,"pagetitle":"Proximal Gradient Method","title":"Stepsize","ref":"/manopt/stable/solvers/proximal_gradient_method/#Stepsize","content":" Stepsize"},{"id":3183,"pagetitle":"Proximal Gradient Method","title":"Manopt.ProximalGradientMethodBacktracking","ref":"/manopt/stable/solvers/proximal_gradient_method/#Manopt.ProximalGradientMethodBacktracking","content":" Manopt.ProximalGradientMethodBacktracking  ‚Äî  Function ProximalGradientMethodBacktracking(; kwargs...)\nProximalGradientMethodBacktracking(M::AbstractManifold; kwargs...) Compute a stepsize for the proximal gradient method using a backtracking line search. For the nonconvex case, the condition is: \\[f(p) - f(T_{Œª}(p)) ‚â• Œ≥Œª\\lVert G_{Œª}(p) \\rVert_{}^2\\] where  G_{Œª}(p) = (1/Œª) * \\log_p(T_{Œª}(p))  is the gradient mapping. For the convex case, the condition is: \\[g(T_{Œª}(p)) ‚â§ g(p) + ‚ü®\\operatorname{grad} g(p), \\log_p T_{Œª}(p)‚ü© + \\frac{1}{2Œª} \\mathrm{d}^2(p, T_{Œª}(p))\\] Returns a stepsize  Œª  that satisfies the specified condition. Info This function generates a  ManifoldDefaultsFactory  for  ProximalGradientMethodBacktrackingStepsize . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source"},{"id":3184,"pagetitle":"Proximal Gradient Method","title":"Manopt.ProximalGradientMethodBacktrackingStepsize","ref":"/manopt/stable/solvers/proximal_gradient_method/#Manopt.ProximalGradientMethodBacktrackingStepsize","content":" Manopt.ProximalGradientMethodBacktrackingStepsize  ‚Äî  Type ProximalGradientMethodBacktrackingStepsize <: Stepsize A functor for backtracking line search in proximal gradient methods. Fields initial_stepsize::T  - initial step size guess sufficient_decrease::T  - sufficient decrease parameter (default: 0.5) contraction_factor::T  - step size reduction factor (default: 0.5) strategy::Symbol  -  :nonconvex  or  :convex  (default:  :nonconvex ) candidate_point::P  - a working point used during backtracking last_stepsize::T  - the last computed stepsize Constructor ProximalGradientMethodBacktrackingStepsize(M::AbstractManifold; kwargs...) Keyword arguments initial_stepsize=1.0 : initial stepsize to try stop_when_stepsize_less=1e-8 : smallest stepsize when to stop (the last one before is taken) sufficient_decrease=0.5 : sufficient decrease parameter contraction_factor=0.5 : step size reduction factor strategy=:nonconvex : backtracking strategy, either  :convex  or  :nonconvex source"},{"id":3185,"pagetitle":"Proximal Gradient Method","title":"Debug functions","ref":"/manopt/stable/solvers/proximal_gradient_method/#Debug-functions","content":" Debug functions"},{"id":3186,"pagetitle":"Proximal Gradient Method","title":"Manopt.DebugWarnIfStepsizeCollapsed","ref":"/manopt/stable/solvers/proximal_gradient_method/#Manopt.DebugWarnIfStepsizeCollapsed","content":" Manopt.DebugWarnIfStepsizeCollapsed  ‚Äî  Type DebugWarnIfStepsizeCollapsed <: DebugAction print a warning if the backtracking stopped because the stepsize fell below a given threshold in the  proximal_gradient_method . This threshold is specified by the  stop_when_stepsize_less  field of the  ProximalGradientMethodBacktrackingStepsize . Constructor DebugWarnIfStepsizeCollapsed(warn=:Once;) Initialize the warning to warning level ( :Once ). The  warn  level can be set to  :Once  to only warn the first time the cost increases, to  :Always  to report an increase every time it happens, and it can be set to  :No  to deactivate the warning, then this  DebugAction  is inactive. All other symbols are handled as if they were  :Always source"},{"id":3187,"pagetitle":"Proximal Gradient Method","title":"Internal functions","ref":"/manopt/stable/solvers/proximal_gradient_method/#Internal-functions","content":" Internal functions"},{"id":3188,"pagetitle":"Proximal Gradient Method","title":"Manopt.get_cost_smooth","ref":"/manopt/stable/solvers/proximal_gradient_method/#Manopt.get_cost_smooth","content":" Manopt.get_cost_smooth  ‚Äî  Function get_cost_smooth(M::AbstractManifold, objective, p) Helper function to extract the smooth part  g  of a proximal gradient objective at the point  p . source"},{"id":3189,"pagetitle":"Proximal Gradient Method","title":"Manopt.default_stepsize","ref":"/manopt/stable/solvers/proximal_gradient_method/#Manopt.default_stepsize-Tuple{AbstractManifold, Type{<:ProximalGradientMethodState}}","content":" Manopt.default_stepsize  ‚Äî  Method default_stepsize(M::AbstractManifold, ::Type{<:ProximalGradientMethodState}) Returns the default proximal stepsize, which is a nonconvex backtracking strategy. source"},{"id":3190,"pagetitle":"Proximal Gradient Method","title":"Literature","ref":"/manopt/stable/solvers/proximal_gradient_method/#Literature","content":" Literature [BJJP25] R.¬†Bergmann, H.¬†Jasa, P.¬†John and M.¬†Pfeffer.  The Intrinsic Riemannian Proximal Gradient Method for Nonconvex Optimization , preprint (2025),  arXiv:2506.09775 ."},{"id":3193,"pagetitle":"Proximal point method","title":"Proximal point method","ref":"/manopt/stable/solvers/proximal_point/#Proximal-point-method","content":" Proximal point method"},{"id":3194,"pagetitle":"Proximal point method","title":"Manopt.proximal_point","ref":"/manopt/stable/solvers/proximal_point/#Manopt.proximal_point","content":" Manopt.proximal_point  ‚Äî  Function proximal_point(M, prox_f, p=rand(M); kwargs...)\nproximal_point(M, mpmo, p=rand(M); kwargs...)\nproximal_point!(M, prox_f, p; kwargs...)\nproximal_point!(M, mpmo, p; kwargs...) Perform the proximal point algoritm from [ FO02 ] which reads \\[p^{(k+1)} = \\operatorname{prox}_{Œª_kf}(p^{(k)})\\] Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ prox_f : a proximal map  (M,Œª,p) -> q  or  (M, q, Œª, p) -> q  for the summands of  $f$  (see  evaluation ) Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. f=nothing : a cost function  $f: \\mathcal M‚Üí‚Ñù$  to minimize. For running the algorithm,  $f$  is not required, but for example when recording the cost or using a stopping criterion that requires a cost function. Œª= k -> 1.0 : a function returning the (square summable but not summable) sequence of  $Œª_i$ stopping_criterion= StopAfterIteration (200) | StopWhenChangeLess (1e-12) ): a functor indicating that the stopping criterion is fulfilled All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3195,"pagetitle":"Proximal point method","title":"Manopt.proximal_point!","ref":"/manopt/stable/solvers/proximal_point/#Manopt.proximal_point!","content":" Manopt.proximal_point!  ‚Äî  Function proximal_point(M, prox_f, p=rand(M); kwargs...)\nproximal_point(M, mpmo, p=rand(M); kwargs...)\nproximal_point!(M, prox_f, p; kwargs...)\nproximal_point!(M, mpmo, p; kwargs...) Perform the proximal point algoritm from [ FO02 ] which reads \\[p^{(k+1)} = \\operatorname{prox}_{Œª_kf}(p^{(k)})\\] Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ prox_f : a proximal map  (M,Œª,p) -> q  or  (M, q, Œª, p) -> q  for the summands of  $f$  (see  evaluation ) Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. f=nothing : a cost function  $f: \\mathcal M‚Üí‚Ñù$  to minimize. For running the algorithm,  $f$  is not required, but for example when recording the cost or using a stopping criterion that requires a cost function. Œª= k -> 1.0 : a function returning the (square summable but not summable) sequence of  $Œª_i$ stopping_criterion= StopAfterIteration (200) | StopWhenChangeLess (1e-12) ): a functor indicating that the stopping criterion is fulfilled All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3196,"pagetitle":"Proximal point method","title":"State","ref":"/manopt/stable/solvers/proximal_point/#State","content":" State"},{"id":3197,"pagetitle":"Proximal point method","title":"Manopt.ProximalPointState","ref":"/manopt/stable/solvers/proximal_point/#Manopt.ProximalPointState","content":" Manopt.ProximalPointState  ‚Äî  Type ProximalPointState{P} <: AbstractGradientSolverState Fields p::P : a point on the manifold  $\\mathcal M$  storing the current iterate stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled Œª :         a function for the values of  $Œª_k$  per iteration(cycle  $k$ Constructor ProximalPointState(M::AbstractManifold; kwargs...) Initialize the proximal point method solver state, where Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ Keyword arguments Œª=k -> 1.0  a function to compute the  $Œª_k, k ‚àà \\mathcal N$ , p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value stopping_criterion= StopAfterIteration (100) : a functor indicating that the stopping criterion is fulfilled See also proximal_point source [FO02] O.¬†Ferreira and P.¬†R.¬†Oliveira.  Proximal point algorithm on Riemannian manifolds .  Optimization.¬†A¬†Journal¬†of¬†Mathematical¬†Programming¬†and¬†Operations¬†Research  51 , 257‚Äì270  (2002)."},{"id":3200,"pagetitle":"Quasi-Newton","title":"Riemannian quasi-Newton methods","ref":"/manopt/stable/solvers/quasi_Newton/#Riemannian-quasi-Newton-methods","content":" Riemannian quasi-Newton methods"},{"id":3201,"pagetitle":"Quasi-Newton","title":"Manopt.quasi_Newton","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.quasi_Newton","content":" Manopt.quasi_Newton  ‚Äî  Function quasi_Newton(M, f, grad_f, p; kwargs...)\nquasi_Newton!(M, f, grad_f, p; kwargs...) Perform a quasi Newton iteration to solve \\[\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} f(p)\\] with start point  p . The iterations can be done in-place of  p $=p^{(0)}$ . The  $k$ th iteration consists of Compute the search direction  $Œ∑^{(k)} = -\\mathcal B_k [\\operatorname{grad}f (p^{(k)})]$  or solve  $\\mathcal H_k [Œ∑^{(k)}] = -\\operatorname{grad}f (p^{(k)})]$ . Determine a suitable stepsize  $Œ±_k$  along the curve  $Œ≥(Œ±) = R_{p^{(k)}}(Œ± Œ∑^{(k)})$ , usually by using  WolfePowellLinesearch . Compute  $p^{(k+1)} = R_{p^{(k)}}(Œ±_k Œ∑^{(k)})$ . Define  $s_k = \\mathcal T_{p^{(k)}, Œ±_k Œ∑^{(k)}}(Œ±_k Œ∑^{(k)})$  and  $y_k = \\operatorname{grad}f(p^{(k+1)}) - \\mathcal T_{p^{(k)}, Œ±_k Œ∑^{(k)}}(\\operatorname{grad}f(p^{(k)}))$ , where  $\\mathcal T$  denotes a vector transport. Compute the new approximate Hessian  $H_{k+1}$  or its inverse  $B_{k+1}$ . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place p : a point on the manifold  $\\mathcal M$ Keyword arguments basis= DefaultOrthonormalBasis () : basis to use within each of the the tangent spaces to represent the Hessian (inverse) for the cases where it is stored in full (matrix) form. cautious_update=false :  whether or not to use the  QuasiNewtonCautiousDirectionUpdate   which wraps the  direction_upate . cautious_function=(x) -> x * 1e-4 : a monotone increasing function for the cautious update that is zero at  $x=0$  and strictly increasing at  $0$ differential= nothing : specify a specific function to evaluate the differential. By default,  $Df(p)[X] = ‚ü®\\operatorname{grad}f(p),X‚ü©$ . is used direction_update= InverseBFGS () : the  AbstractQuasiNewtonUpdateRule  to use. evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second.For example  grad_f(M,p)  allocates, but  grad_f!(M, X, p)  computes the result in-place of  X . initial_operator= initial_scale*Matrix{Float64}(I, n, n) :  initial matrix to use in case the Hessian (inverse) approximation is stored as a full matrix,  that is  n=manifold_dimension(M) . This matrix is only allocated for the full matrix case.  See also  initial_scale . initial_scale=1.0 : scale initial  s  to use in with  $\\frac{s‚ü®s_k,y_k‚ü©_{p_k}}{\\lVert y_k\\rVert_{p_k}}$  in the computation of the limited memory approach. see also  initial_operator memory_size=20 : limited memory, number of  $s_k, y_k$  to store.  Set to a negative value to use a full memory (matrix) representation nondescent_direction_behavior=:reinitialize_direction_update : specify how non-descent direction is handled. This can be :step_towards_negative_gradient : the direction is replaced with negative gradient, a message is stored. :ignore : the verification is not performed, so any computed direction is accepted. No message is stored. :reinitialize_direction_update : discards operator state stored in direction update rules. any other value performs the verification, keeps the direction but stores a message. A stored message can be displayed using  DebugMessages . preconditioner=nothing  specify a preconditioner, either the default  nothing  does not activate a preconditioning a function of the form  (M, p, X) -> Y  or mutating  (M, Y, p, X) -> Y  depending on the  evaluation a  PreconditionedDirection . See also their docs for mor details on the preconditioner. Note that the preconditioner is applied to the gradient, i.e. the right hand side  before  solving the linear system. project!=copyto! : for numerical stability it is possible to project onto the tangent space after every iteration. the function has to work inplace of  Y , that is  (M, Y, p, X) -> Y , where  X  and  Y  can be the same memory. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= WolfePowellLinesearch (retraction_method, vector_transport_method) : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (max(1000, memory_size)) | StopWhenGradientNormLess (1e-6) : a functor indicating that the stopping criterion is fulfilled vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3202,"pagetitle":"Quasi-Newton","title":"Manopt.quasi_Newton!","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.quasi_Newton!","content":" Manopt.quasi_Newton!  ‚Äî  Function quasi_Newton(M, f, grad_f, p; kwargs...)\nquasi_Newton!(M, f, grad_f, p; kwargs...) Perform a quasi Newton iteration to solve \\[\\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} f(p)\\] with start point  p . The iterations can be done in-place of  p $=p^{(0)}$ . The  $k$ th iteration consists of Compute the search direction  $Œ∑^{(k)} = -\\mathcal B_k [\\operatorname{grad}f (p^{(k)})]$  or solve  $\\mathcal H_k [Œ∑^{(k)}] = -\\operatorname{grad}f (p^{(k)})]$ . Determine a suitable stepsize  $Œ±_k$  along the curve  $Œ≥(Œ±) = R_{p^{(k)}}(Œ± Œ∑^{(k)})$ , usually by using  WolfePowellLinesearch . Compute  $p^{(k+1)} = R_{p^{(k)}}(Œ±_k Œ∑^{(k)})$ . Define  $s_k = \\mathcal T_{p^{(k)}, Œ±_k Œ∑^{(k)}}(Œ±_k Œ∑^{(k)})$  and  $y_k = \\operatorname{grad}f(p^{(k+1)}) - \\mathcal T_{p^{(k)}, Œ±_k Œ∑^{(k)}}(\\operatorname{grad}f(p^{(k)}))$ , where  $\\mathcal T$  denotes a vector transport. Compute the new approximate Hessian  $H_{k+1}$  or its inverse  $B_{k+1}$ . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place p : a point on the manifold  $\\mathcal M$ Keyword arguments basis= DefaultOrthonormalBasis () : basis to use within each of the the tangent spaces to represent the Hessian (inverse) for the cases where it is stored in full (matrix) form. cautious_update=false :  whether or not to use the  QuasiNewtonCautiousDirectionUpdate   which wraps the  direction_upate . cautious_function=(x) -> x * 1e-4 : a monotone increasing function for the cautious update that is zero at  $x=0$  and strictly increasing at  $0$ differential= nothing : specify a specific function to evaluate the differential. By default,  $Df(p)[X] = ‚ü®\\operatorname{grad}f(p),X‚ü©$ . is used direction_update= InverseBFGS () : the  AbstractQuasiNewtonUpdateRule  to use. evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second.For example  grad_f(M,p)  allocates, but  grad_f!(M, X, p)  computes the result in-place of  X . initial_operator= initial_scale*Matrix{Float64}(I, n, n) :  initial matrix to use in case the Hessian (inverse) approximation is stored as a full matrix,  that is  n=manifold_dimension(M) . This matrix is only allocated for the full matrix case.  See also  initial_scale . initial_scale=1.0 : scale initial  s  to use in with  $\\frac{s‚ü®s_k,y_k‚ü©_{p_k}}{\\lVert y_k\\rVert_{p_k}}$  in the computation of the limited memory approach. see also  initial_operator memory_size=20 : limited memory, number of  $s_k, y_k$  to store.  Set to a negative value to use a full memory (matrix) representation nondescent_direction_behavior=:reinitialize_direction_update : specify how non-descent direction is handled. This can be :step_towards_negative_gradient : the direction is replaced with negative gradient, a message is stored. :ignore : the verification is not performed, so any computed direction is accepted. No message is stored. :reinitialize_direction_update : discards operator state stored in direction update rules. any other value performs the verification, keeps the direction but stores a message. A stored message can be displayed using  DebugMessages . preconditioner=nothing  specify a preconditioner, either the default  nothing  does not activate a preconditioning a function of the form  (M, p, X) -> Y  or mutating  (M, Y, p, X) -> Y  depending on the  evaluation a  PreconditionedDirection . See also their docs for mor details on the preconditioner. Note that the preconditioner is applied to the gradient, i.e. the right hand side  before  solving the linear system. project!=copyto! : for numerical stability it is possible to project onto the tangent space after every iteration. the function has to work inplace of  Y , that is  (M, Y, p, X) -> Y , where  X  and  Y  can be the same memory. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= WolfePowellLinesearch (retraction_method, vector_transport_method) : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (max(1000, memory_size)) | StopWhenGradientNormLess (1e-6) : a functor indicating that the stopping criterion is fulfilled vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3203,"pagetitle":"Quasi-Newton","title":"Background","ref":"/manopt/stable/solvers/quasi_Newton/#Background","content":" Background The aim is to minimize a real-valued function on a Riemannian manifold, that is \\[\\min f(p), \\quad p ‚àà \\mathcal{M}.\\] Riemannian quasi-Newtonian methods are as generalizations of their Euclidean counterparts Riemannian line search methods. These methods determine a search direction  $Œ∑_k ‚àà T_{p_k} \\mathcal{M}$  at the current iterate  $p_k$  and a suitable stepsize  $Œ±_k$  along  $\\gamma(Œ±) = R_{p_k}(Œ± Œ∑_k)$ , where  $R: T \\mathcal{M} ‚Üí\\mathcal{M}$  is a retraction. The next iterate is obtained by \\[p_{k+1} = R_{p_k}(Œ±_k Œ∑_k).\\] In quasi-Newton methods, the search direction is given by \\[Œ∑_k = -{\\mathcal{H}_k}^{-1}[\\operatorname{grad}f (p_k)] = -\\mathcal{B}_k [\\operatorname{grad} (p_k)],\\] where  $\\mathcal{H}_k : T_{p_k} \\mathcal{M} ‚ÜíT_{p_k} \\mathcal{M}$  is a positive definite self-adjoint operator, which approximates the action of the Hessian  $\\operatorname{Hess} f (p_k)[‚ãÖ]$  and  $\\mathcal{B}_k = {\\mathcal{H}_k}^{-1}$ . The idea of quasi-Newton methods is instead of creating a complete new approximation of the Hessian operator  $\\operatorname{Hess} f(p_{k+1})$  or its inverse at every iteration, the previous operator  $\\mathcal{H}_k$  or  $\\mathcal{B}_k$  is updated by a convenient formula using the obtained information about the curvature of the objective function during the iteration. The resulting operator  $\\mathcal{H}_{k+1}$  or  $\\mathcal{B}_{k+1}$  acts on the tangent space  $T_{p_{k+1}} \\mathcal{M}$  of the freshly computed iterate  $p_{k+1}$ . In order to get a well-defined method, the following requirements are placed on the new operator  $\\mathcal{H}_{k+1}$  or  $\\mathcal{B}_{k+1}$  that is created by an update. Since the Hessian  $\\operatorname{Hess} f(p_{k+1})$  is a self-adjoint operator on the tangent space  $T_{p_{k+1}} \\mathcal{M}$ , and  $\\mathcal{H}_{k+1}$  approximates it, one requirement is, that  $\\mathcal{H}_{k+1}$  or  $\\mathcal{B}_{k+1}$  is also self-adjoint on  $T_{p_{k+1}} \\mathcal{M}$ . In order to achieve a steady descent, the next requirement is that  $Œ∑_k$  is a descent direction in each iteration. Hence a further requirement is that  $\\mathcal{H}_{k+1}$  or  $\\mathcal{B}_{k+1}$  is a positive definite operator on  $T_{p_{k+1}} \\mathcal{M}$ . In order to get information about the curvature of the objective function into the new operator  $\\mathcal{H}_{k+1}$  or  $\\mathcal{B}_{k+1}$ , the last requirement is a form of a Riemannian quasi-Newton equation: \\[\\mathcal{H}_{k+1} [T_{p_k \\rightarrow p_{k+1}}({R_{p_k}}^{-1}(p_{k+1}))] = \\operatorname{grad}(p_{k+1}) - T_{p_k \\rightarrow p_{k+1}}(\\operatorname{grad}f(p_k))\\] or \\[\\mathcal{B}_{k+1} [\\operatorname{grad}f(p_{k+1}) - T_{p_k \\rightarrow p_{k+1}}(\\operatorname{grad}f(p_k))] = T_{p_k \\rightarrow p_{k+1}}({R_{p_k}}^{-1}(p_{k+1}))\\] where  $T_{p_k \\rightarrow p_{k+1}} : T_{p_k} \\mathcal{M} ‚ÜíT_{p_{k+1}} \\mathcal{M}$  and the chosen retraction  $R$  is the associated retraction of  $T$ . Note that, of course, not all updates in all situations meet these conditions in every iteration. For specific quasi-Newton updates, the fulfilment of the Riemannian curvature condition, which requires that \\[g_{p_{k+1}}(s_k, y_k) > 0\\] holds, is a requirement for the inheritance of the self-adjointness and positive definiteness of the  $\\mathcal{H}_k$  or  $\\mathcal{B}_k$  to the operator  $\\mathcal{H}_{k+1}$  or  $\\mathcal{B}_{k+1}$ . Unfortunately, the fulfilment of the Riemannian curvature condition is not given by a step size  $\\alpha_k > 0$  that satisfies the generalized Wolfe conditions. However, to create a positive definite operator  $\\mathcal{H}_{k+1}$  or  $\\mathcal{B}_{k+1}$  in each iteration, the so-called locking condition was introduced in [ HGA15 ], which requires that the isometric vector transport  $T^S$ , which is used in the update formula, and its associate retraction  $R$  fulfil \\[T^{S}{p, Œæ_p}(Œæ_p) = Œ≤ T^{R}{p, Œæ_p}(Œæ_p), \\quad Œ≤ = \\frac{\\lVert Œæ_p \\rVert_p}{\\lVert T^{R}{p, Œæ_p}(Œæ_p) \\rVert_{R_{p}(Œæ_p)}},\\] where  $T^R$  is the vector transport by differentiated retraction. With the requirement that the isometric vector transport  $T^S$  and its associated retraction  $R$  satisfies the locking condition and using the tangent vector \\[y_k = {Œ≤_k}^{-1} \\operatorname{grad}f(p_{k+1}) - T^{S}{p_k, Œ±_k Œ∑_k}(\\operatorname{grad}f(p_k)),\\] where \\[Œ≤_k = \\frac{\\lVert Œ±_k Œ∑_k \\rVert_{p_k}}{\\lVert T^{R}{p_k, Œ±_k Œ∑_k}(Œ±_k Œ∑_k) \\rVert_{p_{k+1}}},\\] in the update, it can be shown that choosing a stepsize  $Œ±_k > 0$  that satisfies the Riemannian Wolfe conditions leads to the fulfilment of the Riemannian curvature condition, which in turn implies that the operator generated by the updates is positive definite. In the following the specific operators are denoted in matrix notation and hence use  $H_k$  and  $B_k$ , respectively."},{"id":3204,"pagetitle":"Quasi-Newton","title":"Direction updates","ref":"/manopt/stable/solvers/quasi_Newton/#Direction-updates","content":" Direction updates In general there are different ways to compute a fixed  AbstractQuasiNewtonUpdateRule . In general these are represented by"},{"id":3205,"pagetitle":"Quasi-Newton","title":"Manopt.AbstractQuasiNewtonDirectionUpdate","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.AbstractQuasiNewtonDirectionUpdate","content":" Manopt.AbstractQuasiNewtonDirectionUpdate  ‚Äî  Type AbstractQuasiNewtonDirectionUpdate An abstract representation of an Quasi Newton Update rule to determine the next direction given current  QuasiNewtonState . All subtypes should be functors, they should be callable as  H(M,x,d)  to compute a new direction update. source"},{"id":3206,"pagetitle":"Quasi-Newton","title":"Manopt.QuasiNewtonMatrixDirectionUpdate","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.QuasiNewtonMatrixDirectionUpdate","content":" Manopt.QuasiNewtonMatrixDirectionUpdate  ‚Äî  Type QuasiNewtonMatrixDirectionUpdate <: AbstractQuasiNewtonDirectionUpdate The  QuasiNewtonMatrixDirectionUpdate  represent a quasi-Newton update rule, where the operator is stored as a matrix. A distinction is made between the update of the approximation of the Hessian,  $H_k \\mapsto H_{k+1}$ , and the update of the approximation of the Hessian inverse,  $B_k \\mapsto B_{k+1}$ . For the first case, the coordinates of the search direction  $Œ∑_k$  with respect to a basis  $\\{b_i\\}_{i=1}^{n}$  are determined by solving a linear system of equations \\[\\text{Solve} \\quad \\hat{Œ∑_k} = - H_k \\widehat{\\operatorname{grad}f(x_k)},\\] where  $H_k$  is the matrix representing the operator with respect to the basis  $\\{b_i\\}_{i=1}^{n}$  and  $\\widehat{\\operatorname{grad}} f(p_k)$  represents the coordinates of the gradient of the objective function  $f$  in  $x_k$  with respect to the basis  $\\{b_i\\}_{i=1}^{n}$ . If a method is chosen where Hessian inverse is approximated, the coordinates of the search direction  $Œ∑_k$  with respect to a basis  $\\{b_i\\}_{i=1}^{n}$  are obtained simply by matrix-vector multiplication \\[\\hat{Œ∑_k} = - B_k \\widehat{\\operatorname{grad}f(x_k)},\\] where  $B_k$  is the matrix representing the operator with respect to the basis  $\\{b_i\\}_{i=1}^{n}$  and  $\\widehat{\\operatorname{grad}} f(p_k)$ . In the end, the search direction  $Œ∑_k$  is generated from the coordinates  $\\hat{eta_k}$  and the vectors of the basis  $\\{b_i\\}_{i=1}^{n}$  in both variants. The  AbstractQuasiNewtonUpdateRule  indicates which quasi-Newton update rule is used. In all of them, the Euclidean update formula is used to generate the matrix  $H_{k+1}$  and  $B_{k+1}$ , and the basis  $\\{b_i\\}_{i=1}^{n}$  is transported into the upcoming tangent space  $T_{p_{k+1}} \\mathcal M$ , preferably with an isometric vector transport, or generated there. Provided functors (mp::AbstractManoptproblem, st::QuasiNewtonState) -> Œ∑  to compute the update direction (Œ∑, mp::AbstractManoptproblem, st::QuasiNewtonState) -> Œ∑  to compute the update direction in-place of  Œ∑ Fields basis :                  an  AbstractBasis  to use in the tangent spaces matrix :                 the matrix which represents the approximating operator. initial_scale :          when initialising the update, a unit matrix is used as initial approximation, scaled by this factor update :                 a  AbstractQuasiNewtonUpdateRule . vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Constructor QuasiNewtonMatrixDirectionUpdate(\n    M::AbstractManifold,\n    update,\n    basis::B=default_basis(M),\n    m=Matrix{Float64}(I, manifold_dimension(M), manifold_dimension(M));\n    kwargs...\n) Keyword arguments initial_scale=1.0  ‚Äì this can also be deactivated by passing  nothing . vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Generate the Update rule with defaults from a manifold and the names corresponding to the fields. See also QuasiNewtonLimitedMemoryDirectionUpdate ,  QuasiNewtonCautiousDirectionUpdate ,  AbstractQuasiNewtonDirectionUpdate , source"},{"id":3207,"pagetitle":"Quasi-Newton","title":"Manopt.QuasiNewtonLimitedMemoryDirectionUpdate","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.QuasiNewtonLimitedMemoryDirectionUpdate","content":" Manopt.QuasiNewtonLimitedMemoryDirectionUpdate  ‚Äî  Type QuasiNewtonLimitedMemoryDirectionUpdate <: AbstractQuasiNewtonDirectionUpdate This  AbstractQuasiNewtonDirectionUpdate  represents the limited-memory Riemannian BFGS update, where the approximating operator is represented by  $m$  stored pairs of tangent vectors  $\\{\\widehat{s}_i\\}_{i=k-m}^{k-1}$  and  $\\{\\widehat{y}_i\\}_{i=k-m}^{k-1}$  in the  $k$ -th iteration. For the calculation of the search direction  $X_k$ , the generalisation of the two-loop recursion is used (see [ HGA15 ]), since it only requires inner products and linear combinations of tangent vectors in  $T_{p_k}\\mathcal M$ . For that the stored pairs of tangent vectors  $\\widehat{s}_i,  \\widehat{y}_i$ , the gradient  $\\operatorname{grad} f(p_k)$  of the objective function  $f$  in  $p_k$  and the positive definite self-adjoint operator \\[\\mathcal{B}^{(0)}_k[‚ãÖ] = \\frac{g_{p_k}(s_{k-1}, y_{k-1})}{g_{p_k}(y_{k-1}, y_{k-1})} \\; \\mathrm{id}_{T_{p_k} \\mathcal{M}}[‚ãÖ]\\] are used. The two-loop recursion can be understood as that the  InverseBFGS  update is executed  $m$  times in a row on  $\\mathcal B^{(0)}_k[‚ãÖ]$  using the tangent vectors  $\\widehat{s}_i,\\widehat{y}_i$ , and in the same time the resulting operator  $\\mathcal B^{LRBFGS}_k [‚ãÖ]$  is directly applied on  $\\operatorname{grad}f(x_k)$ . When updating there are two cases: if there is still free memory,  $k < m$ , the previously stored vector pairs  $\\widehat{s}_i,\\widehat{y}_i$  have to be transported into the upcoming tangent space  $T_{p_{k+1}}\\mathcal M$ . If there is no free memory, the oldest pair  $\\widehat{s}_i,\\widehat{y}_i$  has to be discarded and then all the remaining vector pairs  $\\widehat{s}_i,\\widehat{y}_i$  are transported into the tangent space  $T_{p_{k+1}}\\mathcal M$ . After that the new values  $s_k = \\widehat{s}_k = T^{S}_{x_k, Œ±_k Œ∑_k}(Œ±_k Œ∑_k)$  and  $y_k = \\widehat{y}_k$  are stored at the beginning. This process ensures that new information about the objective function is always included and the old, probably no longer relevant, information is discarded. Provided functors (mp::AbstractManoptproblem, st::QuasiNewtonState) -> Œ∑  to compute the update direction (Œ∑, mp::AbstractManoptproblem, st::QuasiNewtonState) -> Œ∑  to compute the update direction in-place of  Œ∑ Fields memory_s :                the set of the stored (and transported) search directions times step size  $\\{\\widehat{s}_i\\}_{i=k-m}^{k-1}$ . memory_y :                set of the stored gradient differences  $\\{\\widehat{y}_i\\}_{i=k-m}^{k-1}$ . Œæ :                       a variable used in the two-loop recursion. œÅ L                       a variable used in the two-loop recursion. initial_scale :           initial scaling of the Hessian, deactivate (e.g. when using a preconditioner) by passing  nothing vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports message :                 a string containing a potential warning that might have appeared project! :                a function to stabilize the update by projecting on the tangent space Constructor QuasiNewtonLimitedMemoryDirectionUpdate(\n    M::AbstractManifold,\n    x,\n    update::AbstractQuasiNewtonUpdateRule,\n    memory_size::Int;\n    initial_vector=zero_vector(M,x),\n    initial_scale::Real=1.0\n    project!=copyto!\n) See also InverseBFGS QuasiNewtonCautiousDirectionUpdate AbstractQuasiNewtonDirectionUpdate source"},{"id":3208,"pagetitle":"Quasi-Newton","title":"Manopt.QuasiNewtonCautiousDirectionUpdate","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.QuasiNewtonCautiousDirectionUpdate","content":" Manopt.QuasiNewtonCautiousDirectionUpdate  ‚Äî  Type QuasiNewtonCautiousDirectionUpdate <: AbstractQuasiNewtonDirectionUpdate These  AbstractQuasiNewtonDirectionUpdate s represent any quasi-Newton update rule, which are based on the idea of a so-called cautious update. The search direction is calculated as given in  QuasiNewtonMatrixDirectionUpdate  or  QuasiNewtonLimitedMemoryDirectionUpdate , butut the update  then is only executed if \\[\\frac{g_{x_{k+1}}(y_k,s_k)}{\\lVert s_k \\rVert^{2}_{x_{k+1}}} ‚â• Œ∏(\\lVert \\operatorname{grad}f(x_k) \\rVert_{x_k}),\\] is satisfied, where  $Œ∏$  is a monotone increasing function satisfying  $Œ∏(0) = 0$  and  $Œ∏$  is strictly increasing at  $0$ . If this is not the case, the corresponding update is skipped, which means that for  QuasiNewtonMatrixDirectionUpdate  the matrix  $H_k$  or  $B_k$  is not updated. The basis  $\\{b_i\\}^{n}_{i=1}$  is nevertheless transported into the upcoming tangent space  $T_{x_{k+1}} \\mathcal{M}$ , and for  QuasiNewtonLimitedMemoryDirectionUpdate  neither the oldest vector pair  $\\{ \\widetilde{s}_{k‚àím}, \\widetilde{y}_{k‚àím}\\}$  is discarded nor the newest vector pair  $\\{ \\widetilde{s}_{k}, \\widetilde{y}_{k}\\}$  is added into storage, but all stored vector pairs  $\\{ \\widetilde{s}_i, \\widetilde{y}_i\\}_{i=k-m}^{k-1}$  are transported into the tangent space  $T_{x_{k+1}} \\mathcal{M}$ . If  InverseBFGS  or  InverseBFGS  is chosen as update, then the resulting method follows the method of [ HAG18 ], taking into account that the corresponding step size is chosen. Provided functors (mp::AbstractManoptproblem, st::QuasiNewtonState) -> Œ∑  to compute the update direction (Œ∑, mp::AbstractManoptproblem, st::QuasiNewtonState) -> Œ∑  to compute the update direction in-place of  Œ∑ Fields update : an  AbstractQuasiNewtonDirectionUpdate Œ∏ :      a monotone increasing function satisfying  $Œ∏(0) = 0$  and  $Œ∏$  is strictly increasing at  $0$ . Constructor QuasiNewtonCautiousDirectionUpdate(U::QuasiNewtonMatrixDirectionUpdate; Œ∏ = identity)\nQuasiNewtonCautiousDirectionUpdate(U::QuasiNewtonLimitedMemoryDirectionUpdate; Œ∏ = identity) Generate a cautious update for either a matrix based or a limited memory based update rule. See also QuasiNewtonMatrixDirectionUpdate QuasiNewtonLimitedMemoryDirectionUpdate source"},{"id":3209,"pagetitle":"Quasi-Newton","title":"Manopt.initialize_update!","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.initialize_update!","content":" Manopt.initialize_update!  ‚Äî  Function initialize_update!(s::AbstractQuasiNewtonDirectionUpdate) Initialize direction update. By default no change is made. source initialize_update!(d::QuasiNewtonLimitedMemoryDirectionUpdate) Initialize the limited memory direction update by emptying the memory buffers. source"},{"id":3210,"pagetitle":"Quasi-Newton","title":"Manopt.QuasiNewtonPreconditioner","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.QuasiNewtonPreconditioner","content":" Manopt.QuasiNewtonPreconditioner  ‚Äî  Type QuasiNewtonPreconditioner{E<:AbstractEvaluationType, F} Add a preconditioning Fields preconditioner::F : the preconditioner function Constructors QuasiNewtonPreconditioner(\n    preconditioner;\n    evaluation::AbstractEvaluationType=AllocatingEvaluation()\n) Add preconditioning to a gradient problem. Input preconditioner :   preconditioner function, either as a  (M, p, X) -> Y  allocating or  (M, Y, p, X) -> Y  mutating function Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. source"},{"id":3211,"pagetitle":"Quasi-Newton","title":"Hessian update rules","ref":"/manopt/stable/solvers/quasi_Newton/#Hessian-update-rules","content":" Hessian update rules Using"},{"id":3212,"pagetitle":"Quasi-Newton","title":"Manopt.update_hessian!","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.update_hessian!","content":" Manopt.update_hessian!  ‚Äî  Function update_hessian!(d::AbstractQuasiNewtonDirectionUpdate, amp, st, p_old, k) update the Hessian within the  QuasiNewtonState st  given a  AbstractManoptProblem amp  as well as the an  AbstractQuasiNewtonDirectionUpdate d  and the last iterate  p_old . Note that the current ( k th) iterate is already stored in  get_iterate (st) . See also  AbstractQuasiNewtonUpdateRule  and its subtypes for the different rules that are available within  d . source the following update formulae for either  $H_{k+1}$  or  $B_{k+1}$  are available."},{"id":3213,"pagetitle":"Quasi-Newton","title":"Manopt.AbstractQuasiNewtonUpdateRule","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.AbstractQuasiNewtonUpdateRule","content":" Manopt.AbstractQuasiNewtonUpdateRule  ‚Äî  Type AbstractQuasiNewtonUpdateRule Specify a type for the different  AbstractQuasiNewtonDirectionUpdate s, that is for a  QuasiNewtonMatrixDirectionUpdate  there are several different updates to the matrix, while the default for  QuasiNewtonLimitedMemoryDirectionUpdate  the most prominent is  InverseBFGS . source"},{"id":3214,"pagetitle":"Quasi-Newton","title":"Manopt.BFGS","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.BFGS","content":" Manopt.BFGS  ‚Äî  Type BFGS <: AbstractQuasiNewtonUpdateRule indicates in  AbstractQuasiNewtonDirectionUpdate  that the Riemannian BFGS update is used in the Riemannian quasi-Newton method. Denote by  $\\widetilde{H}_k^\\mathrm{BFGS}$  the operator concatenated with a vector transport and its inverse before and after to act on  $x_{k+1} = R_{x_k}(Œ±_k Œ∑_k)$ . Then the update formula reads \\[H^\\mathrm{BFGS}_{k+1} = \\widetilde{H}^\\mathrm{BFGS}_k  + \\frac{y_k y^{\\mathrm{T}}_k }{s^{\\mathrm{T}}_k y_k} - \\frac{\\widetilde{H}^\\mathrm{BFGS}_k s_k s^{\\mathrm{T}}_k \\widetilde{H}^\\mathrm{BFGS}_k }{s^{\\mathrm{T}}_k \\widetilde{H}^\\mathrm{BFGS}_k s_k}\\] where  $s_k$  and  $y_k$  are the coordinate vectors with respect to the current basis (from  QuasiNewtonState ) of \\[T^{S}_{x_k, Œ±_k Œ∑_k}(Œ±_k Œ∑_k) \\quad\\text{and}\\quad\n\\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, Œ±_k Œ∑_k}(\\operatorname{grad}f(x_k)) ‚àà T_{x_{k+1}} \\mathcal{M},\\] respectively. source"},{"id":3215,"pagetitle":"Quasi-Newton","title":"Manopt.DFP","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.DFP","content":" Manopt.DFP  ‚Äî  Type DFP <: AbstractQuasiNewtonUpdateRule indicates in an  AbstractQuasiNewtonDirectionUpdate  that the Riemannian DFP update is used in the Riemannian quasi-Newton method. Denote by  $\\widetilde{H}_k^\\mathrm{DFP}$  the operator concatenated with a vector transport and its inverse before and after to act on  $x_{k+1} = R_{x_k}(Œ±_k Œ∑_k)$ . Then the update formula reads \\[H^\\mathrm{DFP}_{k+1} = \\Bigl(\n  \\mathrm{id}_{T_{x_{k+1}} \\mathcal{M}} - \\frac{y_k s^{\\mathrm{T}}_k}{s^{\\mathrm{T}}_k y_k}\n\\Bigr)\n\\widetilde{H}^\\mathrm{DFP}_k\n\\Bigl(\n  \\mathrm{id}_{T_{x_{k+1}} \\mathcal{M}} - \\frac{s_k y^{\\mathrm{T}}_k}{s^{\\mathrm{T}}_k y_k}\n\\Bigr) + \\frac{y_k y^{\\mathrm{T}}_k}{s^{\\mathrm{T}}_k y_k}\\] where  $s_k$  and  $y_k$  are the coordinate vectors with respect to the current basis (from  QuasiNewtonState ) of \\[T^{S}_{x_k, Œ±_k Œ∑_k}(Œ±_k Œ∑_k) \\quad\\text{and}\\quad\n\\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, Œ±_k Œ∑_k}(\\operatorname{grad}f(x_k)) ‚àà T_{x_{k+1}} \\mathcal{M},\\] respectively. source"},{"id":3216,"pagetitle":"Quasi-Newton","title":"Manopt.Broyden","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.Broyden","content":" Manopt.Broyden  ‚Äî  Type Broyden <: AbstractQuasiNewtonUpdateRule indicates in  AbstractQuasiNewtonDirectionUpdate  that the Riemannian Broyden update is used in the Riemannian quasi-Newton method, which is as a convex combination of  BFGS  and  DFP . Denote by  $\\widetilde{H}_k^\\mathrm{Br}$  the operator concatenated with a vector transport and its inverse before and after to act on  $x_{k+1} = R_{x_k}(Œ±_k Œ∑_k)$ . Then the update formula reads \\[H^\\mathrm{Br}_{k+1} = \\widetilde{H}^\\mathrm{Br}_k\n  - \\frac{\\widetilde{H}^\\mathrm{Br}_k s_k s^{\\mathrm{T}}_k \\widetilde{H}^\\mathrm{Br}_k}{s^{\\mathrm{T}}_k \\widetilde{H}^\\mathrm{Br}_k s_k} + \\frac{y_k y^{\\mathrm{T}}_k}{s^{\\mathrm{T}}_k y_k}\n  + œÜ_k s^{\\mathrm{T}}_k \\widetilde{H}^\\mathrm{Br}_k s_k\n  \\Bigl(\n        \\frac{y_k}{s^{\\mathrm{T}}_k y_k} - \\frac{\\widetilde{H}^\\mathrm{Br}_k s_k}{s^{\\mathrm{T}}_k \\widetilde{H}^\\mathrm{Br}_k s_k}\n  \\Bigr)\n  \\Bigl(\n        \\frac{y_k}{s^{\\mathrm{T}}_k y_k} - \\frac{\\widetilde{H}^\\mathrm{Br}_k s_k}{s^{\\mathrm{T}}_k \\widetilde{H}^\\mathrm{Br}_k s_k}\n  \\Bigr)^{\\mathrm{T}}\\] where  $s_k$  and  $y_k$  are the coordinate vectors with respect to the current basis (from  QuasiNewtonState ) of \\[T^{S}_{x_k, Œ±_k Œ∑_k}(Œ±_k Œ∑_k) \\quad\\text{and}\\quad\n\\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, Œ±_k Œ∑_k}(\\operatorname{grad}f(x_k)) ‚àà T_{x_{k+1}} \\mathcal{M},\\] respectively, and  $œÜ_k$  is the Broyden factor which is  :constant  by default but can also be set to  :Davidon . Constructor Broyden(œÜ, update_rule::Symbol = :constant) source"},{"id":3217,"pagetitle":"Quasi-Newton","title":"Manopt.SR1","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.SR1","content":" Manopt.SR1  ‚Äî  Type SR1 <: AbstractQuasiNewtonUpdateRule indicates in  AbstractQuasiNewtonDirectionUpdate  that the Riemannian SR1 update is used in the Riemannian quasi-Newton method. Denote by  $\\widetilde{H}_k^\\mathrm{SR1}$  the operator concatenated with a vector transport and its inverse before and after to act on  $x_{k+1} = R_{x_k}(Œ±_k Œ∑_k)$ . Then the update formula reads \\[H^\\mathrm{SR1}_{k+1} = \\widetilde{H}^\\mathrm{SR1}_k\n+ \\frac{\n  (y_k - \\widetilde{H}^\\mathrm{SR1}_k s_k) (y_k - \\widetilde{H}^\\mathrm{SR1}_k s_k)^{\\mathrm{T}}\n}{\n(y_k - \\widetilde{H}^\\mathrm{SR1}_k s_k)^{\\mathrm{T}} s_k\n}\\] where  $s_k$  and  $y_k$  are the coordinate vectors with respect to the current basis (from  QuasiNewtonState ) of \\[T^{S}_{x_k, Œ±_k Œ∑_k}(Œ±_k Œ∑_k) \\quad\\text{and}\\quad\n\\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, Œ±_k Œ∑_k}(\\operatorname{grad}f(x_k)) ‚àà T_{x_{k+1}} \\mathcal{M},\\] respectively. This method can be stabilized by only performing the update if denominator is larger than  $r\\lVert s_k\\rVert_{x_{k+1}}\\lVert y_k - \\widetilde{H}^\\mathrm{SR1}_k s_k \\rVert_{x_{k+1}}$  for some  $r>0$ . For more details, see Section 6.2 in [ NW06 ]. Constructor SR1(r::Float64=-1.0) Generate the  SR1  update. source"},{"id":3218,"pagetitle":"Quasi-Newton","title":"Manopt.InverseBFGS","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.InverseBFGS","content":" Manopt.InverseBFGS  ‚Äî  Type InverseBFGS <: AbstractQuasiNewtonUpdateRule indicates in  AbstractQuasiNewtonDirectionUpdate  that the inverse Riemannian BFGS update is used in the Riemannian quasi-Newton method. Denote by  $\\widetilde{B}_k^\\mathrm{BFGS}$  the operator concatenated with a vector transport and its inverse before and after to act on  $x_{k+1} = R_{x_k}(Œ±_k Œ∑_k)$ . Then the update formula reads \\[B^\\mathrm{BFGS}_{k+1}  = \\Bigl(\n  \\mathrm{id}_{T_{x_{k+1}} \\mathcal{M}} - \\frac{s_k y^{\\mathrm{T}}_k }{s^{\\mathrm{T}}_k y_k}\n\\Bigr)\n\\widetilde{B}^\\mathrm{BFGS}_k\n\\Bigl(\n  \\mathrm{id}_{T_{x_{k+1}} \\mathcal{M}} - \\frac{y_k s^{\\mathrm{T}}_k }{s^{\\mathrm{T}}_k y_k}\n\\Bigr) + \\frac{s_k s^{\\mathrm{T}}_k}{s^{\\mathrm{T}}_k y_k}\\] where  $s_k$  and  $y_k$  are the coordinate vectors with respect to the current basis (from  QuasiNewtonState ) of \\[T^{S}_{x_k, Œ±_k Œ∑_k}(Œ±_k Œ∑_k) \\quad\\text{and}\\quad\n\\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, Œ±_k Œ∑_k}(\\operatorname{grad}f(x_k)) ‚àà T_{x_{k+1}} \\mathcal{M},\\] respectively. source"},{"id":3219,"pagetitle":"Quasi-Newton","title":"Manopt.InverseDFP","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.InverseDFP","content":" Manopt.InverseDFP  ‚Äî  Type InverseDFP <: AbstractQuasiNewtonUpdateRule indicates in  AbstractQuasiNewtonDirectionUpdate  that the inverse Riemannian DFP update is used in the Riemannian quasi-Newton method. Denote by  $\\widetilde{B}_k^\\mathrm{DFP}$  the operator concatenated with a vector transport and its inverse before and after to act on  $x_{k+1} = R_{x_k}(Œ±_k Œ∑_k)$ . Then the update formula reads \\[B^\\mathrm{DFP}_{k+1} = \\widetilde{B}^\\mathrm{DFP}_k + \\frac{s_k s^{\\mathrm{T}}_k}{s^{\\mathrm{T}}_k y_k}\n  - \\frac{\\widetilde{B}^\\mathrm{DFP}_k y_k y^{\\mathrm{T}}_k \\widetilde{B}^\\mathrm{DFP}_k}{y^{\\mathrm{T}}_k \\widetilde{B}^\\mathrm{DFP}_k y_k}\\] where  $s_k$  and  $y_k$  are the coordinate vectors with respect to the current basis (from  QuasiNewtonState ) of \\[T^{S}_{x_k, Œ±_k Œ∑_k}(Œ±_k Œ∑_k) \\quad\\text{and}\\quad\n\\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, Œ±_k Œ∑_k}(\\operatorname{grad}f(x_k)) ‚àà T_{x_{k+1}} \\mathcal{M},\\] respectively. source"},{"id":3220,"pagetitle":"Quasi-Newton","title":"Manopt.InverseBroyden","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.InverseBroyden","content":" Manopt.InverseBroyden  ‚Äî  Type InverseBroyden <: AbstractQuasiNewtonUpdateRule Indicates in  AbstractQuasiNewtonDirectionUpdate  that the Riemannian Broyden update is used in the Riemannian quasi-Newton method, which is as a convex combination of  InverseBFGS  and  InverseDFP . Denote by  $\\widetilde{H}_k^\\mathrm{Br}$  the operator concatenated with a vector transport and its inverse before and after to act on  $x_{k+1} = R_{x_k}(Œ±_k Œ∑_k)$ . Then the update formula reads \\[B^\\mathrm{Br}_{k+1} = \\widetilde{B}^\\mathrm{Br}_k\n - \\frac{\\widetilde{B}^\\mathrm{Br}_k y_k y^{\\mathrm{T}}_k \\widetilde{B}^\\mathrm{Br}_k}{y^{\\mathrm{T}}_k \\widetilde{B}^\\mathrm{Br}_k y_k}\n   + \\frac{s_k s^{\\mathrm{T}}_k}{s^{\\mathrm{T}}_k y_k}\n + œÜ_k y^{\\mathrm{T}}_k \\widetilde{B}^\\mathrm{Br}_k y_k\n \\Bigl(\n     \\frac{s_k}{s^{\\mathrm{T}}_k y_k} - \\frac{\\widetilde{B}^\\mathrm{Br}_k y_k}{y^{\\mathrm{T}}_k \\widetilde{B}^\\mathrm{Br}_k y_k}\n    \\Bigr) \\Bigl(\n        \\frac{s_k}{s^{\\mathrm{T}}_k y_k} - \\frac{\\widetilde{B}^\\mathrm{Br}_k y_k}{y^{\\mathrm{T}}_k \\widetilde{B}^\\mathrm{Br}_k y_k}\n \\Bigr)^{\\mathrm{T}}\\] where  $s_k$  and  $y_k$  are the coordinate vectors with respect to the current basis (from  QuasiNewtonState ) of \\[T^{S}_{x_k, Œ±_k Œ∑_k}(Œ±_k Œ∑_k) \\quad\\text{and}\\quad\n\\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, Œ±_k Œ∑_k}(\\operatorname{grad}f(x_k)) ‚àà T_{x_{k+1}} \\mathcal{M},\\] respectively, and  $œÜ_k$  is the Broyden factor which is  :constant  by default but can also be set to  :Davidon . Constructor InverseBroyden(œÜ, update_rule::Symbol = :constant) source"},{"id":3221,"pagetitle":"Quasi-Newton","title":"Manopt.InverseSR1","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.InverseSR1","content":" Manopt.InverseSR1  ‚Äî  Type InverseSR1 <: AbstractQuasiNewtonUpdateRule indicates in  AbstractQuasiNewtonDirectionUpdate  that the inverse Riemannian SR1 update is used in the Riemannian quasi-Newton method. Denote by  $\\widetilde{B}_k^\\mathrm{SR1}$  the operator concatenated with a vector transport and its inverse before and after to act on  $x_{k+1} = R_{x_k}(Œ±_k Œ∑_k)$ . Then the update formula reads \\[B^\\mathrm{SR1}_{k+1} = \\widetilde{B}^\\mathrm{SR1}_k\n+ \\frac{\n  (s_k - \\widetilde{B}^\\mathrm{SR1}_k y_k) (s_k - \\widetilde{B}^\\mathrm{SR1}_k y_k)^{\\mathrm{T}}\n}{\n  (s_k - \\widetilde{B}^\\mathrm{SR1}_k y_k)^{\\mathrm{T}} y_k\n}\\] where  $s_k$  and  $y_k$  are the coordinate vectors with respect to the current basis (from  QuasiNewtonState ) of \\[T^{S}_{x_k, Œ±_k Œ∑_k}(Œ±_k Œ∑_k) \\quad\\text{and}\\quad\n\\operatorname{grad}f(x_{k+1}) - T^{S}_{x_k, Œ±_k Œ∑_k}(\\operatorname{grad}f(x_k)) ‚àà T_{x_{k+1}} \\mathcal{M},\\] respectively. This method can be stabilized by only performing the update if denominator is larger than  $r\\lVert y_k\\rVert_{x_{k+1}}\\lVert s_k - \\widetilde{H}^\\mathrm{SR1}_k y_k \\rVert_{x_{k+1}}$  for some  $r>0$ . For more details, see Section 6.2 in [ NW06 ]. Constructor InverseSR1(r::Float64=-1.0) Generate the  InverseSR1 . source"},{"id":3222,"pagetitle":"Quasi-Newton","title":"State","ref":"/manopt/stable/solvers/quasi_Newton/#State","content":" State The quasi Newton algorithm is based on a  DefaultManoptProblem ."},{"id":3223,"pagetitle":"Quasi-Newton","title":"Manopt.QuasiNewtonState","ref":"/manopt/stable/solvers/quasi_Newton/#Manopt.QuasiNewtonState","content":" Manopt.QuasiNewtonState  ‚Äî  Type QuasiNewtonState <: AbstractManoptSolverState The  AbstractManoptSolverState  represent any quasi-Newton based method and stores all necessary fields. Fields direction_update :              an  AbstractQuasiNewtonDirectionUpdate  rule. Œ∑ :                             the current update direction nondescent_direction_behavior : a  Symbol  to specify how to handle direction that are not descent ones. nondescent_direction_value :    the value from the last inner product from checking for descent directions p::P : a point on the manifold  $\\mathcal M$  storing the current iterate p_old :                         the last iterate preconditioner                  an  QuasiNewtonPreconditioner sk :                            the current step yk :                            the current gradient difference retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize::Stepsize : a functor inheriting from  Stepsize  to determine a step size stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ storing the gradient at the current iterate X_old :                         the last gradient Constructor QuasiNewtonState(M::AbstractManifold, p; kwargs...) Generate the Quasi Newton state on the manifold  M  with start point  p . Keyword arguments direction_update= QuasiNewtonLimitedMemoryDirectionUpdate (M, p, InverseBFGS(), memory_size; vector_transport_method=vector_transport_method) stopping_criterion= StopAfterIteration (1000) | StopWhenGradientNormLess (1e-6) : a functor indicating that the stopping criterion is fulfilled initial_scale=1.0 : a realtive initial scale. By default deactivated when using a preconditioner. memory_size=20 : a shortcut to set the memory in the default direction update preconditioner::Union{ QuasiNewtonPreconditioner , Nothing} = nothing  specify a preconditioner or deactivate by passing  nothing . retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= default_stepsize (M, QuasiNewtonState) : a functor inheriting from  Stepsize  to determine a step size vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ to specify the representation of a tangent vector See also quasi_Newton source"},{"id":3224,"pagetitle":"Quasi-Newton","title":"Technical details","ref":"/manopt/stable/solvers/quasi_Newton/#sec-qn-technical-details","content":" Technical details The  quasi_Newton  solver requires the following functions of a manifold to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified. A  vector_transport_to! M, Y, p, X, q) ; it is recommended to set the  default_vector_transport_method  to a favourite retraction. If this default is set, a  vector_transport_method=  or  vector_transport_method_dual=  (for  $\\mathcal N$ ) does not have to be specified. By default quasi Newton uses  ArmijoLinesearch  which requires  max_stepsize (M)  to be set and an implementation of  inner (M, p, X) . the  norm  as well, to stop when the norm of the gradient is small, but if you implemented  inner , the norm is provided already. A  copyto! (M, q, p)  and  copy (M,p)  for points and similarly  copy(M, p, X)  for tangent vectors. By default the tangent vector storing the gradient is initialized calling  zero_vector (M,p) . Most Hessian approximations further require  get_coordinates (M, p, X, b)  with respect to the  AbstractBasis b  provided, which is  DefaultOrthonormalBasis  by default from the  basis=  keyword."},{"id":3225,"pagetitle":"Quasi-Newton","title":"Literature","ref":"/manopt/stable/solvers/quasi_Newton/#Literature","content":" Literature [HAG18] W.¬†Huang, P.-A.¬†Absil and K.¬†A.¬†Gallivan.  A Riemannian BFGS method without differentiated retraction for nonconvex optimization problems .  SIAM¬†Journal¬†on¬†Optimization  28 , 470‚Äì495  (2018). [HGA15] W.¬†Huang, K.¬†A.¬†Gallivan and P.-A.¬†Absil.  A Broyden class of quasi-Newton methods for Riemannian optimization .  SIAM¬†Journal¬†on¬†Optimization  25 , 1660‚Äì1685  (2015). [NW06] J.¬†Nocedal and S.¬†J.¬†Wright.  Numerical Optimization . 2¬†Edition (Springer, New York, 2006)."},{"id":3228,"pagetitle":"Stochastic Gradient Descent","title":"Stochastic gradient descent","ref":"/manopt/stable/solvers/stochastic_gradient_descent/#Stochastic-gradient-descent","content":" Stochastic gradient descent"},{"id":3229,"pagetitle":"Stochastic Gradient Descent","title":"Manopt.stochastic_gradient_descent","ref":"/manopt/stable/solvers/stochastic_gradient_descent/#Manopt.stochastic_gradient_descent","content":" Manopt.stochastic_gradient_descent  ‚Äî  Function stochastic_gradient_descent(M, grad_f, p=rand(M); kwargs...)\nstochastic_gradient_descent(M, msgo; kwargs...)\nstochastic_gradient_descent!(M, grad_f, p; kwargs...)\nstochastic_gradient_descent!(M, msgo, p; kwargs...) perform a stochastic gradient descent. This can be perfomed in-place of  p . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ grad_f : a gradient function, that either returns a vector of the gradients or is a vector of gradient functions p : a point on the manifold  $\\mathcal M$ alternatively to the gradient you can provide an  ManifoldStochasticGradientObjective msgo , then using the  cost=  keyword does not have any effect since if so, the cost is already within the objective. Keyword arguments cost=missing : you can provide a cost function for example to track the function value direction= StochasticGradient ([ zero vector ](@extref ManifoldsBase.zero vector-Tuple{AbstractManifold, Any} ) (M, p)`) evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. evaluation_order=:Random : specify whether to use a randomly permuted sequence ( :FixedRandom :, a per cycle permuted sequence ( :Linear ) or the default  :Random  one. order_type=:RandomOder : a type of ordering of gradient evaluations. Possible values are  :RandomOrder , a  :FixedPermutation ,  :LinearOrder stopping_criterion= StopAfterIteration (1000) : a functor indicating that the stopping criterion is fulfilled stepsize= default_stepsize (M, StochasticGradientDescentState) : a functor inheriting from  Stepsize  to determine a step size order=[1:n] : the initial permutation, where  n  is the number of gradients in  gradF . retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3230,"pagetitle":"Stochastic Gradient Descent","title":"Manopt.stochastic_gradient_descent!","ref":"/manopt/stable/solvers/stochastic_gradient_descent/#Manopt.stochastic_gradient_descent!","content":" Manopt.stochastic_gradient_descent!  ‚Äî  Function stochastic_gradient_descent(M, grad_f, p=rand(M); kwargs...)\nstochastic_gradient_descent(M, msgo; kwargs...)\nstochastic_gradient_descent!(M, grad_f, p; kwargs...)\nstochastic_gradient_descent!(M, msgo, p; kwargs...) perform a stochastic gradient descent. This can be perfomed in-place of  p . Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ grad_f : a gradient function, that either returns a vector of the gradients or is a vector of gradient functions p : a point on the manifold  $\\mathcal M$ alternatively to the gradient you can provide an  ManifoldStochasticGradientObjective msgo , then using the  cost=  keyword does not have any effect since if so, the cost is already within the objective. Keyword arguments cost=missing : you can provide a cost function for example to track the function value direction= StochasticGradient ([ zero vector ](@extref ManifoldsBase.zero vector-Tuple{AbstractManifold, Any} ) (M, p)`) evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. evaluation_order=:Random : specify whether to use a randomly permuted sequence ( :FixedRandom :, a per cycle permuted sequence ( :Linear ) or the default  :Random  one. order_type=:RandomOder : a type of ordering of gradient evaluations. Possible values are  :RandomOrder , a  :FixedPermutation ,  :LinearOrder stopping_criterion= StopAfterIteration (1000) : a functor indicating that the stopping criterion is fulfilled stepsize= default_stepsize (M, StochasticGradientDescentState) : a functor inheriting from  Stepsize  to determine a step size order=[1:n] : the initial permutation, where  n  is the number of gradients in  gradF . retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. source"},{"id":3231,"pagetitle":"Stochastic Gradient Descent","title":"State","ref":"/manopt/stable/solvers/stochastic_gradient_descent/#State","content":" State"},{"id":3232,"pagetitle":"Stochastic Gradient Descent","title":"Manopt.StochasticGradientDescentState","ref":"/manopt/stable/solvers/stochastic_gradient_descent/#Manopt.StochasticGradientDescentState","content":" Manopt.StochasticGradientDescentState  ‚Äî  Type StochasticGradientDescentState <: AbstractGradientDescentSolverState Store the following fields for a default stochastic gradient descent algorithm, see also  ManifoldStochasticGradientObjective  and  stochastic_gradient_descent . Fields p::P : a point on the manifold  $\\mathcal M$  storing the current iterate direction :  a direction update to use stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled stepsize::Stepsize : a functor inheriting from  Stepsize  to determine a step size evaluation_order : specify whether to use a randomly permuted sequence ( :FixedRandom :), a per cycle permuted sequence ( :Linear ) or the default, a  :Random  sequence. order : stores the current permutation retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions Constructor StochasticGradientDescentState(M::AbstractManifold; kwargs...) Create a  StochasticGradientDescentState  with start point  p . Keyword arguments direction= StochasticGradientRule (M, [ zero vector ](@extref ManifoldsBase.zero vector-Tuple{AbstractManifold, Any} ) (M, p)`) order_type=:RandomOrder ` order=Int[] : specify how to store the order of indices for the next epoche retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value stopping_criterion= StopAfterIteration (1000) : a functor indicating that the stopping criterion is fulfilled stepsize= default_stepsize (M, StochasticGradientDescentState) : a functor inheriting from  Stepsize  to determine a step size X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ to specify the representation of a tangent vector source"},{"id":3233,"pagetitle":"Stochastic Gradient Descent","title":"Manopt.default_stepsize","ref":"/manopt/stable/solvers/stochastic_gradient_descent/#Manopt.default_stepsize-Tuple{AbstractManifold, Type{StochasticGradientDescentState}}","content":" Manopt.default_stepsize  ‚Äî  Method default_stepsize(M::AbstractManifold, ::Type{StochasticGradientDescentState}) Deinfe the default step size computed for the  StochasticGradientDescentState , which is  ConstantStepsize M . source Additionally, the options share a  DirectionUpdateRule , so you can also apply  MomentumGradient  and  AverageGradient  here. The most inner one should always be."},{"id":3234,"pagetitle":"Stochastic Gradient Descent","title":"Manopt.StochasticGradient","ref":"/manopt/stable/solvers/stochastic_gradient_descent/#Manopt.StochasticGradient","content":" Manopt.StochasticGradient  ‚Äî  Function StochasticGradient(; kwargs...)\nStochasticGradient(M::AbstractManifold; kwargs...) Keyword arguments initial_gradient= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value Info This function generates a  ManifoldDefaultsFactory  for  StochasticGradientRule . For default values, that depend on the manifold, this factory postpones the construction until the manifold from for example a corresponding  AbstractManoptSolverState  is available. source which internally uses"},{"id":3235,"pagetitle":"Stochastic Gradient Descent","title":"Manopt.AbstractGradientGroupDirectionRule","ref":"/manopt/stable/solvers/stochastic_gradient_descent/#Manopt.AbstractGradientGroupDirectionRule","content":" Manopt.AbstractGradientGroupDirectionRule  ‚Äî  Type AbstractStochasticGradientDescentSolverState <: AbstractManoptSolverState A generic type for all options related to gradient descent methods working with parts of the total gradient source"},{"id":3236,"pagetitle":"Stochastic Gradient Descent","title":"Manopt.StochasticGradientRule","ref":"/manopt/stable/solvers/stochastic_gradient_descent/#Manopt.StochasticGradientRule","content":" Manopt.StochasticGradientRule  ‚Äî  Type StochasticGradientRule<: AbstractGradientGroupDirectionRule Create a functor  (problem, state k) -> (s,X)  to evaluate the stochatsic gradient, that is chose a random index from the  state  and use the internal field for evaluation of the gradient in-place. The default gradient processor, which just evaluates the (stochastic) gradient or a subset thereof. Fields X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ Constructor StochasticGradientRule(M::AbstractManifold; p=rand(M), X=zero_vector(M, p)) Initialize the stochastic gradient processor with tangent vector type of  X , where both  M  and  p  are just help variables. See also stochastic_gradient_descent , [ StochasticGradient ])@ref) source"},{"id":3237,"pagetitle":"Stochastic Gradient Descent","title":"Technical details","ref":"/manopt/stable/solvers/stochastic_gradient_descent/#sec-sgd-technical-details","content":" Technical details The  stochastic_gradient_descent  solver requires the following functions of a manifold to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified."},{"id":3240,"pagetitle":"Subgradient method","title":"Subgradient method","ref":"/manopt/stable/solvers/subgradient/#sec-subgradient-method","content":" Subgradient method"},{"id":3241,"pagetitle":"Subgradient method","title":"Manopt.subgradient_method","ref":"/manopt/stable/solvers/subgradient/#Manopt.subgradient_method","content":" Manopt.subgradient_method  ‚Äî  Function subgradient_method(M, f, ‚àÇf, p=rand(M); kwargs...)\nsubgradient_method(M, sgo, p=rand(M); kwargs...)\nsubgradient_method!(M, f, ‚àÇf, p; kwargs...)\nsubgradient_method!(M, sgo, p; kwargs...) perform a subgradient method  $p^{(k+1)} = \\operatorname{retr}\\bigl(p^{(k)}, s^{(k)}‚àÇf(p^{(k)})\\bigr)$ , where  $\\operatorname{retr}$  is a retraction,  $s^{(k)}$  is a step size. Though the subgradient might be set valued, the argument  ‚àÇf  should always return  one  element from the subgradient, but not necessarily deterministic. For more details see [ FO98 ]. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v ‚àÇf : the (sub)gradient  $‚àÇ f: \\mathcal M ‚Üí T\\mathcal M$  of f p : a point on the manifold  $\\mathcal M$ alternatively to  f  and  ‚àÇf  a  ManifoldSubgradientObjective sgo  can be provided. Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= default_stepsize (M, SubGradientMethodState) : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (5000) : a functor indicating that the stopping criterion is fulfilled X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ to specify the representation of a tangent vector and the ones that are passed to  decorate_state!  for decorators. Output the obtained (approximate) minimizer  $p^*$ , see  get_solver_return  for details source"},{"id":3242,"pagetitle":"Subgradient method","title":"Manopt.subgradient_method!","ref":"/manopt/stable/solvers/subgradient/#Manopt.subgradient_method!","content":" Manopt.subgradient_method!  ‚Äî  Function subgradient_method(M, f, ‚àÇf, p=rand(M); kwargs...)\nsubgradient_method(M, sgo, p=rand(M); kwargs...)\nsubgradient_method!(M, f, ‚àÇf, p; kwargs...)\nsubgradient_method!(M, sgo, p; kwargs...) perform a subgradient method  $p^{(k+1)} = \\operatorname{retr}\\bigl(p^{(k)}, s^{(k)}‚àÇf(p^{(k)})\\bigr)$ , where  $\\operatorname{retr}$  is a retraction,  $s^{(k)}$  is a step size. Though the subgradient might be set valued, the argument  ‚àÇf  should always return  one  element from the subgradient, but not necessarily deterministic. For more details see [ FO98 ]. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v ‚àÇf : the (sub)gradient  $‚àÇ f: \\mathcal M ‚Üí T\\mathcal M$  of f p : a point on the manifold  $\\mathcal M$ alternatively to  f  and  ‚àÇf  a  ManifoldSubgradientObjective sgo  can be provided. Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize= default_stepsize (M, SubGradientMethodState) : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (5000) : a functor indicating that the stopping criterion is fulfilled X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ to specify the representation of a tangent vector and the ones that are passed to  decorate_state!  for decorators. Output the obtained (approximate) minimizer  $p^*$ , see  get_solver_return  for details source"},{"id":3243,"pagetitle":"Subgradient method","title":"State","ref":"/manopt/stable/solvers/subgradient/#State","content":" State"},{"id":3244,"pagetitle":"Subgradient method","title":"Manopt.SubGradientMethodState","ref":"/manopt/stable/solvers/subgradient/#Manopt.SubGradientMethodState","content":" Manopt.SubGradientMethodState  ‚Äî  Type SubGradientMethodState <: AbstractManoptSolverState stores option values for a  subgradient_method  solver Fields p::P : a point on the manifold  $\\mathcal M$  storing the current iterate p_star : optimal value retraction_method::AbstractRetractionMethod : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stepsize::Stepsize : a functor inheriting from  Stepsize  to determine a step size stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled X : the current element from the possible subgradients at  p  that was last evaluated. Constructor SubGradientMethodState(M::AbstractManifold; kwargs...) Initialise the Subgradient method state Keyword arguments retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value stepsize= default_stepsize (M, SubGradientMethodState) : a functor inheriting from  Stepsize  to determine a step size stopping_criterion= StopAfterIteration (5000) : a functor indicating that the stopping criterion is fulfilled X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ to specify the representation of a tangent vector source For  DebugAction s and  RecordAction s to record (sub)gradient, its norm and the step sizes, see the  gradient descent  actions."},{"id":3245,"pagetitle":"Subgradient method","title":"Technical details","ref":"/manopt/stable/solvers/subgradient/#sec-sgm-technical-details","content":" Technical details The  subgradient_method  solver requires the following functions of a manifold to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified."},{"id":3246,"pagetitle":"Subgradient method","title":"Literature","ref":"/manopt/stable/solvers/subgradient/#Literature","content":" Literature [FO98] O.¬†Ferreira and P.¬†R.¬†Oliveira.  Subgradient algorithm on Riemannian manifolds .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  97 , 93‚Äì104  (1998)."},{"id":3249,"pagetitle":"Steihaug-Toint TCG Method","title":"Steihaug-Toint truncated conjugate gradient method","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#tCG","content":" Steihaug-Toint truncated conjugate gradient method Solve the constraint optimization problem on the tangent space \\[\\begin{align*}\n\\operatorname*{arg\\,min}_{Y  ‚àà  T_p\\mathcal{M}}&\\ m_p(Y) = f(p) +\n‚ü®\\operatorname{grad}f(p), Y‚ü©_p + \\frac{1}{2} ‚ü®\\mathcal{H}_p[Y], Y‚ü©_p\\\\\n\\text{such that}& \\ \\lVert Y \\rVert_p ‚â§ Œî\n\\end{align*}\\] on the tangent space  $T_p\\mathcal M$  of a Riemannian manifold  $\\mathcal M$  by using the Steihaug-Toint truncated conjugate-gradient (tCG) method, see [ ABG06 ], Algorithm 2, and [ CGT00 ]. Here  $\\mathcal H_p$  is either the Hessian  $\\operatorname{Hess} f(p)$  or a linear symmetric operator on the tangent space approximating the Hessian."},{"id":3250,"pagetitle":"Steihaug-Toint TCG Method","title":"Interface","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Interface","content":" Interface"},{"id":3251,"pagetitle":"Steihaug-Toint TCG Method","title":"Manopt.truncated_conjugate_gradient_descent","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Manopt.truncated_conjugate_gradient_descent","content":" Manopt.truncated_conjugate_gradient_descent  ‚Äî  Function truncated_conjugate_gradient_descent(M, f, grad_f, Hess_f, p=rand(M), X=rand(M); vector_at=p);\n    kwargs...\n)\ntruncated_conjugate_gradient_descent(M, mho::ManifoldHessianObjective, p=rand(M), X=rand(M; vector_at=p);\n    kwargs...\n)\ntruncated_conjugate_gradient_descent(M, trmo::TrustRegionModelObjective, p=rand(M), X=rand(M; vector_at=p);\n    kwargs...\n) solve the trust-region subproblem \\[\\begin{align*}\n\\operatorname*{arg\\,min}_{Y  ‚àà  T_p\\mathcal{M}}&\\ m_p(Y) = f(p) +\n‚ü®\\operatorname{grad}f(p), Y‚ü©_p + \\frac{1}{2} ‚ü®\\mathcal{H}_p[Y], Y‚ü©_p\\\\\n\\text{such that}& \\ \\lVert Y \\rVert_p ‚â§ Œî\n\\end{align*}\\] on a manifold  $\\mathcal M$  by using the Steihaug-Toint truncated conjugate-gradient (tCG) method. This can be done inplace of  X . For a description of the algorithm and theorems offering convergence guarantees, see [ ABG06 ,  CGT00 ]. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place Hess_f : the (Riemannian) Hessian  $\\operatorname{Hess}f: T_{p}\\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p, X) -> Y  or a function  (M, Y, p, X) -> Y  computing  Y  in-place p : a point on the manifold  $\\mathcal M$ X : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ Instead of the three functions, you either provide a  ManifoldHessianObjective mho  which is then used to build the trust region model, or a  TrustRegionModelObjective trmo  directly. Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. preconditioner :       a preconditioner for the Hessian H. This is either an allocating function  (M, p, X) -> Y  or an in-place function  (M, Y, p, X) -> Y , see  evaluation , and by default set to the identity. Œ∏=1.0 :                the superlinear convergence target rate of  $1+Œ∏$ Œ∫=0.1 :                the linear convergence target rate. project!=copyto! : for numerical stability it is possible to project onto the tangent space after every iteration. the function has to work inplace of  Y , that is  (M, Y, p, X) -> Y , where  X  and  Y  can be the same memory. randomize=false :      indicate whether  X  is initialised to a random vector or not. This disables preconditioning. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopAfterIteration ( manifold_dimension (base_manifold(Tpm)) ) | StopWhenResidualIsReducedByFactorOrPower (; Œ∫=Œ∫, Œ∏=Œ∏) | StopWhenTrustRegionIsExceeded () | StopWhenCurvatureIsNegative () | StopWhenModelIncreased () : a functor indicating that the stopping criterion is fulfilled trust_region_radius= injectivity_radius (M) / 4 : the initial trust-region radius All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. See also trust_regions source"},{"id":3252,"pagetitle":"Steihaug-Toint TCG Method","title":"Manopt.truncated_conjugate_gradient_descent!","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Manopt.truncated_conjugate_gradient_descent!","content":" Manopt.truncated_conjugate_gradient_descent!  ‚Äî  Function truncated_conjugate_gradient_descent(M, f, grad_f, Hess_f, p=rand(M), X=rand(M); vector_at=p);\n    kwargs...\n)\ntruncated_conjugate_gradient_descent(M, mho::ManifoldHessianObjective, p=rand(M), X=rand(M; vector_at=p);\n    kwargs...\n)\ntruncated_conjugate_gradient_descent(M, trmo::TrustRegionModelObjective, p=rand(M), X=rand(M; vector_at=p);\n    kwargs...\n) solve the trust-region subproblem \\[\\begin{align*}\n\\operatorname*{arg\\,min}_{Y  ‚àà  T_p\\mathcal{M}}&\\ m_p(Y) = f(p) +\n‚ü®\\operatorname{grad}f(p), Y‚ü©_p + \\frac{1}{2} ‚ü®\\mathcal{H}_p[Y], Y‚ü©_p\\\\\n\\text{such that}& \\ \\lVert Y \\rVert_p ‚â§ Œî\n\\end{align*}\\] on a manifold  $\\mathcal M$  by using the Steihaug-Toint truncated conjugate-gradient (tCG) method. This can be done inplace of  X . For a description of the algorithm and theorems offering convergence guarantees, see [ ABG06 ,  CGT00 ]. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place Hess_f : the (Riemannian) Hessian  $\\operatorname{Hess}f: T_{p}\\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p, X) -> Y  or a function  (M, Y, p, X) -> Y  computing  Y  in-place p : a point on the manifold  $\\mathcal M$ X : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ Instead of the three functions, you either provide a  ManifoldHessianObjective mho  which is then used to build the trust region model, or a  TrustRegionModelObjective trmo  directly. Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. preconditioner :       a preconditioner for the Hessian H. This is either an allocating function  (M, p, X) -> Y  or an in-place function  (M, Y, p, X) -> Y , see  evaluation , and by default set to the identity. Œ∏=1.0 :                the superlinear convergence target rate of  $1+Œ∏$ Œ∫=0.1 :                the linear convergence target rate. project!=copyto! : for numerical stability it is possible to project onto the tangent space after every iteration. the function has to work inplace of  Y , that is  (M, Y, p, X) -> Y , where  X  and  Y  can be the same memory. randomize=false :      indicate whether  X  is initialised to a random vector or not. This disables preconditioning. retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopAfterIteration ( manifold_dimension (base_manifold(Tpm)) ) | StopWhenResidualIsReducedByFactorOrPower (; Œ∫=Œ∫, Œ∏=Œ∏) | StopWhenTrustRegionIsExceeded () | StopWhenCurvatureIsNegative () | StopWhenModelIncreased () : a functor indicating that the stopping criterion is fulfilled trust_region_radius= injectivity_radius (M) / 4 : the initial trust-region radius All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. See also trust_regions source"},{"id":3253,"pagetitle":"Steihaug-Toint TCG Method","title":"State","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#State","content":" State"},{"id":3254,"pagetitle":"Steihaug-Toint TCG Method","title":"Manopt.TruncatedConjugateGradientState","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Manopt.TruncatedConjugateGradientState","content":" Manopt.TruncatedConjugateGradientState  ‚Äî  Type TruncatedConjugateGradientState <: AbstractHessianSolverState describe the Steihaug-Toint truncated conjugate-gradient method, with Fields Let  T  denote the type of a tangent vector and  R <: Real . Œ¥::T :                     the conjugate gradient search direction Œ¥HŒ¥ ,  YPŒ¥ ,  Œ¥PŒ¥ ,  YPŒ¥ : temporary inner products with  HŒ¥  and preconditioned inner products. HŒ¥ ,  HY :                 temporary results of the Hessian applied to  Œ¥  and  Y , respectively. Œ∫::R :                     the linear convergence target rate. project! :                 for numerical stability it is possible to project onto the tangent space after every iteration. the function has to work inplace of  Y , that is  (M, Y, p, X) -> Y , where  X  and  Y  can be the same memory. randomize :          indicate whether  X  is initialised to a random vector or not residual::T :                 the gradient of the model  $m(Y)$ stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled Œ∏::R :                     the superlinear convergence target rate of  $1+Œ∏$ trust_region_radius::R :   the trust-region radius X::T :                     the gradient  $\\operatorname{grad}f(p)$ Y::T :                     current iterate tangent vector z::T :                     the preconditioned residual z_r::R :                   inner product of the residual and  z Constructor TruncatedConjugateGradientState(TpM::TangentSpace, Y=rand(TpM); kwargs...) Initialise the TCG state. Input TpM : a  TangentSpace Keyword arguments Œ∫=0.1 project!::F=copyto! : initialise the numerical stabilisation to just copy the result randomize=false Œ∏=1.0 trust_region_radius= injectivity_radius (base_manifold(TpM)) / 4 stopping_criterion= StopAfterIteration ( manifold_dimension (base_manifold(Tpm)) ) | StopWhenResidualIsReducedByFactorOrPower (; Œ∫=Œ∫, Œ∏=Œ∏) | StopWhenTrustRegionIsExceeded () | StopWhenCurvatureIsNegative () | StopWhenModelIncreased () : a functor indicating that the stopping criterion is fulfilled X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ See also truncated_conjugate_gradient_descent ,  trust_regions source"},{"id":3255,"pagetitle":"Steihaug-Toint TCG Method","title":"Stopping criteria","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Stopping-criteria","content":" Stopping criteria"},{"id":3256,"pagetitle":"Steihaug-Toint TCG Method","title":"Manopt.StopWhenResidualIsReducedByFactorOrPower","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Manopt.StopWhenResidualIsReducedByFactorOrPower","content":" Manopt.StopWhenResidualIsReducedByFactorOrPower  ‚Äî  Type StopWhenResidualIsReducedByFactorOrPower <: StoppingCriterion A functor for testing if the norm of residual at the current iterate is reduced either by a power of 1+Œ∏ or by a factor Œ∫ compared to the norm of the initial residual. The criterion hence reads $\\lVert r_k \\rVert_{p} ‚â¶ \\lVert r_0 \\rVert_{p^{(0)}} \\min \\bigl( Œ∫, \\lVert r_0 \\rVert_{p^{(0)}}  \\bigr)$ . Fields Œ∫ :      the reduction factor Œ∏ :      part of the reduction power at_iteration::Int : an integer indicating at which the stopping criterion last indicted to stop, which might also be before the solver started ( 0 ). Any negative value indicates that this was not yet the case; Constructor StopWhenResidualIsReducedByFactorOrPower(; Œ∫=0.1, Œ∏=1.0) Initialize the StopWhenResidualIsReducedByFactorOrPower functor to indicate to stop after the norm of the current residual is lesser than either the norm of the initial residual to the power of 1+Œ∏ or the norm of the initial residual times Œ∫. See also truncated_conjugate_gradient_descent ,  trust_regions source"},{"id":3257,"pagetitle":"Steihaug-Toint TCG Method","title":"Manopt.StopWhenTrustRegionIsExceeded","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Manopt.StopWhenTrustRegionIsExceeded","content":" Manopt.StopWhenTrustRegionIsExceeded  ‚Äî  Type StopWhenTrustRegionIsExceeded <: StoppingCriterion A functor for testing if the norm of the next iterate in the Steihaug-Toint truncated conjugate gradient method is larger than the trust-region radius  $Œ∏ ‚â§ \\lVert Y^{(k)}^{*} \\rVert_{p^{(k)}}$  and to end the algorithm when the trust region has been left. Fields at_iteration::Int : an integer indicating at which the stopping criterion last indicted to stop, which might also be before the solver started ( 0 ). Any negative value indicates that this was not yet the case; trr  the trust region radius YPY  the computed norm of  $Y$ . Constructor StopWhenTrustRegionIsExceeded() initialize the StopWhenTrustRegionIsExceeded functor to indicate to stop after the norm of the next iterate is greater than the trust-region radius. See also truncated_conjugate_gradient_descent ,  trust_regions source"},{"id":3258,"pagetitle":"Steihaug-Toint TCG Method","title":"Manopt.StopWhenCurvatureIsNegative","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Manopt.StopWhenCurvatureIsNegative","content":" Manopt.StopWhenCurvatureIsNegative  ‚Äî  Type StopWhenCurvatureIsNegative <: StoppingCriterion A functor for testing if the curvature of the model is negative,  $‚ü®Œ¥_k, \\operatorname{Hess} F(p)[Œ¥_k]‚ü©_p ‚â¶ 0$ . In this case, the model is not strictly convex, and the stepsize as computed does not yield a reduction of the model. Fields at_iteration::Int : an integer indicating at which the stopping criterion last indicted to stop, which might also be before the solver started ( 0 ). Any negative value indicates that this was not yet the case; value  store the value of the inner product. reason : stores a reason of stopping if the stopping criterion has been reached, see  get_reason . Constructor StopWhenCurvatureIsNegative() See also truncated_conjugate_gradient_descent ,  trust_regions source"},{"id":3259,"pagetitle":"Steihaug-Toint TCG Method","title":"Manopt.StopWhenModelIncreased","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Manopt.StopWhenModelIncreased","content":" Manopt.StopWhenModelIncreased  ‚Äî  Type StopWhenModelIncreased <: StoppingCriterion A functor for testing if the curvature of the model value increased. Fields at_iteration::Int : an integer indicating at which the stopping criterion last indicted to stop, which might also be before the solver started ( 0 ). Any negative value indicates that this was not yet the case; model_value stre the last model value inc_model_value  store the model value that increased Constructor StopWhenModelIncreased() See also truncated_conjugate_gradient_descent ,  trust_regions source"},{"id":3260,"pagetitle":"Steihaug-Toint TCG Method","title":"Manopt.set_parameter!","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Manopt.set_parameter!-Tuple{StopWhenResidualIsReducedByFactorOrPower, Val{:ResidualPower}, Any}","content":" Manopt.set_parameter!  ‚Äî  Method set_parameter!(c::StopWhenResidualIsReducedByFactorOrPower, :ResidualPower, v) Update the residual Power  Œ∏   to  v . source"},{"id":3261,"pagetitle":"Steihaug-Toint TCG Method","title":"Manopt.set_parameter!","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Manopt.set_parameter!-Tuple{StopWhenResidualIsReducedByFactorOrPower, Val{:ResidualFactor}, Any}","content":" Manopt.set_parameter!  ‚Äî  Method set_parameter!(c::StopWhenResidualIsReducedByFactorOrPower, :ResidualFactor, v) Update the residual Factor  Œ∫  to  v . source"},{"id":3262,"pagetitle":"Steihaug-Toint TCG Method","title":"Trust region model","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Trust-region-model","content":" Trust region model"},{"id":3263,"pagetitle":"Steihaug-Toint TCG Method","title":"Manopt.TrustRegionModelObjective","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Manopt.TrustRegionModelObjective","content":" Manopt.TrustRegionModelObjective  ‚Äî  Type TrustRegionModelObjective{O<:AbstractManifoldHessianObjective} <: AbstractManifoldSubObjective{O} A trust region model of the form \\[    m(X) = f(p) + ‚ü®\\operatorname{grad} f(p), X‚ü©_p + \\frac{1}(2} ‚ü®\\operatorname{Hess} f(p)[X], X‚ü©_p\\] Fields objective : an  AbstractManifoldHessianObjective  proving  $f$ , its gradient and Hessian Constructors TrustRegionModelObjective(objective) with either an  AbstractManifoldHessianObjective objective  or an decorator containing such an objective source"},{"id":3264,"pagetitle":"Steihaug-Toint TCG Method","title":"Technical details","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#sec-tr-technical-details","content":" Technical details The  trust_regions  solver requires the following functions of a manifold to be available if you do not provide a  trust_region_radius= , then  injectivity_radius  on the manifold  M  is required. the  norm  as well, to stop when the norm of the gradient is small, but if you implemented  inner , the norm is provided already. A  zero_vector! (M,X,p) . A  copyto! (M, q, p)  and  copy (M,p)  for points."},{"id":3265,"pagetitle":"Steihaug-Toint TCG Method","title":"Literature","ref":"/manopt/stable/solvers/truncated_conjugate_gradient_descent/#Literature","content":" Literature [ABG06] P.-A.¬†Absil, C.¬†Baker and K.¬†Gallivan.  Trust-Region Methods on Riemannian Manifolds .  Foundations¬†of¬†Computational¬†Mathematics  7 , 303‚Äì330  (2006). [CGT00] A.¬†R.¬†Conn, N.¬†I.¬†Gould and P.¬†L.¬†Toint.  Trust Region Methods  (Society for Industrial and Applied Mathematics, 2000)."},{"id":3268,"pagetitle":"Trust-Regions Solver","title":"The Riemannian trust regions solver","ref":"/manopt/stable/solvers/trust_regions/#The-Riemannian-trust-regions-solver","content":" The Riemannian trust regions solver Minimize a function \\[\\operatorname*{\\arg\\,min}_{p ‚àà \\mathcal{M}}\\ f(p)\\] by using the Riemannian trust-regions solver following [ ABG06 ] a model is build by lifting the objective at the  $k$ th iterate  $p_k$  by locally mapping the cost function  $f$  to the tangent space as  $f_k: T_{p_k}\\mathcal M ‚Üí ‚Ñù$  as  $f_k(X) = f(\\operatorname{retr}_{p_k}(X))$ . The trust region subproblem is then defined as \\[\\operatorname*{arg\\,min}_{X ‚àà T_{p_k}\\mathcal M}\\ m_k(X),\\] where \\[\\begin{align*}\nm_k&: T_{p_K}\\mathcal M ‚Üí ‚Ñù,\\\\\nm_k(X) &= f(p_k) + ‚ü®\\operatorname{grad} f(p_k), X‚ü©_{p_k} + \\frac{1}{2}\\langle \\mathcal H_k(X),X‚ü©_{p_k}\\\\\n\\text{such that}&\\ \\lVert X \\rVert_{p_k} ‚â§ Œî_k.\n\\end{align*}\\] Here  $Œî_k$  is a trust region radius, that is adapted every iteration, and  $\\mathcal H_k$  is some symmetric linear operator that approximates the Hessian  $\\operatorname{Hess} f$  of  $f$ ."},{"id":3269,"pagetitle":"Trust-Regions Solver","title":"Interface","ref":"/manopt/stable/solvers/trust_regions/#Interface","content":" Interface"},{"id":3270,"pagetitle":"Trust-Regions Solver","title":"Manopt.trust_regions","ref":"/manopt/stable/solvers/trust_regions/#Manopt.trust_regions","content":" Manopt.trust_regions  ‚Äî  Function trust_regions(M, f, grad_f, Hess_f, p=rand(M); kwargs...)\ntrust_regions(M, f, grad_f, p=rand(M); kwargs...)\ntrust_regions!(M, f, grad_f, Hess_f, p; kwargs...)\ntrust_regions!(M, f, grad_f, p; kwargs...) run the Riemannian trust-regions solver for optimization on manifolds to minimize  f , see on [ ABG06 ,  CGT00 ]. For the case that no Hessian is provided, the Hessian is computed using finite differences, see  ApproxHessianFiniteDifference . For solving the inner trust-region subproblem of finding an update-vector, by default the  truncated_conjugate_gradient_descent  is used. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place Hess_f : the (Riemannian) Hessian  $\\operatorname{Hess}f: T_{p}\\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p, X) -> Y  or a function  (M, Y, p, X) -> Y  computing  Y  in-place p : a point on the manifold  $\\mathcal M$ Keyword arguments acceptance_rate :        accept/reject threshold: if œÅ (the performance ratio for the iterate) is at least the acceptance rate œÅ', the candidate is accepted. This value should  be between  $0$  and  $rac{1}{4}$ augmentation_threshold=0.75 : trust-region augmentation threshold: if œÅ is larger than this threshold, a solution is on the trust region boundary and negative curvature, and the radius is extended (augmented) augmentation_factor=2.0 : trust-region augmentation factor evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. Œ∫=0.1 : the linear convergence target rate of the tCG method    truncated_conjugate_gradient_descent , and is used in a stopping criterion therein max_trust_region_radius : the maximum trust-region radius preconditioner :       a preconditioner for the Hessian H. This is either an allocating function  (M, p, X) -> Y  or an in-place function  (M, Y, p, X) -> Y , see  evaluation , and by default set to the identity. project!=copyto! : for numerical stability it is possible to project onto the tangent space after every iteration. the function has to work inplace of  Y , that is  (M, Y, p, X) -> Y , where  X  and  Y  can be the same memory. randomize=false :      indicate whether  X  is initialised to a random vector or not. This disables preconditioning. œÅ_regularization=1e3 : regularize the performance evaluation  $œÅ$  to avoid numerical inaccuracies. reduction_factor=0.25 : trust-region reduction factor reduction_threshold=0.1 : trust-region reduction threshold: if œÅ is below this threshold, the trust region radius is reduced by  reduction_factor . retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopAfterIteration (1000) | StopWhenGradientNormLess (1e-6) : a functor indicating that the stopping criterion is fulfilled sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. sub_stopping_criterion= ( see  truncated_conjugate_gradient_descent ): a functor indicating that the stopping criterion is fulfilled sub_problem= DefaultManoptProblem (M, ConstrainedManifoldObjective (subcost, subgrad; evaluation=evaluation)) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= QuasiNewtonState :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. where  QuasiNewtonLimitedMemoryDirectionUpdate  with  InverseBFGS  is used Œ∏=1.0 :                the superlinear convergence target rate of  $1+Œ∏$  of the tCG-method  truncated_conjugate_gradient_descent , and is used in a stopping criterion therein trust_region_radius= injectivity_radius (M) / 4 : the initial trust-region radius For the case that no Hessian is provided, the Hessian is computed using finite difference, see  ApproxHessianFiniteDifference . All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. See also truncated_conjugate_gradient_descent source"},{"id":3271,"pagetitle":"Trust-Regions Solver","title":"Manopt.trust_regions!","ref":"/manopt/stable/solvers/trust_regions/#Manopt.trust_regions!","content":" Manopt.trust_regions!  ‚Äî  Function trust_regions(M, f, grad_f, Hess_f, p=rand(M); kwargs...)\ntrust_regions(M, f, grad_f, p=rand(M); kwargs...)\ntrust_regions!(M, f, grad_f, Hess_f, p; kwargs...)\ntrust_regions!(M, f, grad_f, p; kwargs...) run the Riemannian trust-regions solver for optimization on manifolds to minimize  f , see on [ ABG06 ,  CGT00 ]. For the case that no Hessian is provided, the Hessian is computed using finite differences, see  ApproxHessianFiniteDifference . For solving the inner trust-region subproblem of finding an update-vector, by default the  truncated_conjugate_gradient_descent  is used. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ f : a cost function  $f: \\mathcal M‚Üí ‚Ñù$  implemented as  (M, p) -> v grad_f : the (Riemannian) gradient  $\\operatorname{grad}f: \\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p) -> X  or a function  (M, X, p) -> X  computing  X  in-place Hess_f : the (Riemannian) Hessian  $\\operatorname{Hess}f: T_{p}\\mathcal M ‚Üí T_{p}\\mathcal M$  of f as a function  (M, p, X) -> Y  or a function  (M, Y, p, X) -> Y  computing  Y  in-place p : a point on the manifold  $\\mathcal M$ Keyword arguments acceptance_rate :        accept/reject threshold: if œÅ (the performance ratio for the iterate) is at least the acceptance rate œÅ', the candidate is accepted. This value should  be between  $0$  and  $rac{1}{4}$ augmentation_threshold=0.75 : trust-region augmentation threshold: if œÅ is larger than this threshold, a solution is on the trust region boundary and negative curvature, and the radius is extended (augmented) augmentation_factor=2.0 : trust-region augmentation factor evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. Œ∫=0.1 : the linear convergence target rate of the tCG method    truncated_conjugate_gradient_descent , and is used in a stopping criterion therein max_trust_region_radius : the maximum trust-region radius preconditioner :       a preconditioner for the Hessian H. This is either an allocating function  (M, p, X) -> Y  or an in-place function  (M, Y, p, X) -> Y , see  evaluation , and by default set to the identity. project!=copyto! : for numerical stability it is possible to project onto the tangent space after every iteration. the function has to work inplace of  Y , that is  (M, Y, p, X) -> Y , where  X  and  Y  can be the same memory. randomize=false :      indicate whether  X  is initialised to a random vector or not. This disables preconditioning. œÅ_regularization=1e3 : regularize the performance evaluation  $œÅ$  to avoid numerical inaccuracies. reduction_factor=0.25 : trust-region reduction factor reduction_threshold=0.1 : trust-region reduction threshold: if œÅ is below this threshold, the trust region radius is reduced by  reduction_factor . retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions stopping_criterion= StopAfterIteration (1000) | StopWhenGradientNormLess (1e-6) : a functor indicating that the stopping criterion is fulfilled sub_kwargs= (;) : a named tuple of keyword arguments that are passed to  decorate_objective!  of the sub solvers objective, the  decorate_state!  of the subsovlers state, and the sub state constructor itself. sub_stopping_criterion= ( see  truncated_conjugate_gradient_descent ): a functor indicating that the stopping criterion is fulfilled sub_problem= DefaultManoptProblem (M, ConstrainedManifoldObjective (subcost, subgrad; evaluation=evaluation)) :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state= QuasiNewtonState :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. where  QuasiNewtonLimitedMemoryDirectionUpdate  with  InverseBFGS  is used Œ∏=1.0 :                the superlinear convergence target rate of  $1+Œ∏$  of the tCG-method  truncated_conjugate_gradient_descent , and is used in a stopping criterion therein trust_region_radius= injectivity_radius (M) / 4 : the initial trust-region radius For the case that no Hessian is provided, the Hessian is computed using finite difference, see  ApproxHessianFiniteDifference . All other keyword arguments are passed to  decorate_state!  for state decorators or  decorate_objective!  for objective decorators, respectively. Output The obtained approximate minimizer  $p^*$ . To obtain the whole final state of the solver, see  get_solver_return  for details, especially the  return_state=  keyword. See also truncated_conjugate_gradient_descent source"},{"id":3272,"pagetitle":"Trust-Regions Solver","title":"State","ref":"/manopt/stable/solvers/trust_regions/#State","content":" State"},{"id":3273,"pagetitle":"Trust-Regions Solver","title":"Manopt.TrustRegionsState","ref":"/manopt/stable/solvers/trust_regions/#Manopt.TrustRegionsState","content":" Manopt.TrustRegionsState  ‚Äî  Type TrustRegionsState <: AbstractHessianSolverState Store the state of the trust-regions solver. Fields acceptance_rate :         a lower bound of the performance ratio for the iterate that decides if the iteration is accepted or not. HX ,  HY ,  HZ :          interim storage (to avoid allocation) of ` \\operatorname{Hess} f(p)[‚ãÖ]  of  X ,  Y ,  Z max_trust_region_radius : the maximum trust-region radius p::P : a point on the manifold  $\\mathcal M$  storing the current iterate project! :                for numerical stability it is possible to project onto the tangent space after every iteration. the function has to work inplace of  Y , that is  (M, Y, p, X) -> Y , where  X  and  Y  can be the same memory. stop::StoppingCriterion : a functor indicating that the stopping criterion is fulfilled randomize :               indicate whether  X  is initialised to a random vector or not œÅ_regularization :        regularize the model fitness  $œÅ$  to avoid division by zero sub_problem::Union{AbstractManoptProblem, F} :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state::Union{AbstractManoptProblem, F} :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. œÉ :                       Gaussian standard deviation when creating the random initial tangent vector This field has no effect, when  randomize  is false. trust_region_radius : the trust-region radius X::T : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ Y :                       the solution (tangent vector) of the subsolver Z :                       the Cauchy point (only used if random is activated) Constructors TrustRegionsState(M, mho::AbstractManifoldHessianObjective; kwargs...)\nTrustRegionsState(M, sub_problem, sub_state; kwargs...)\nTrustRegionsState(M, sub_problem; evaluation=AllocatingEvaluation(), kwargs...) create a trust region state. given a  AbstractManifoldHessianObjective mho , the default sub solver, a  TruncatedConjugateGradientState  with  mho  used to define the problem on a tangent space is created given a  sub_problem  and an  evaluation=  keyword, the sub problem solver is assumed to be the closed form solution, where  evaluation  determines how to call the sub function. Input M:: AbstractManifold : a Riemannian manifold  $\\mathcal M$ sub_problem :  specify a problem for a solver or a closed form solution function, which can be allocating or in-place. sub_state :  a state to specify the sub solver to use. For a closed form solution, this indicates the type of function. Keyword arguments acceptance_rate=0.1 max_trust_region_radius=sqrt(manifold_dimension(M)) p= rand (M) : a point on the manifold  $\\mathcal M$  to specify the initial value project!=copyto! stopping_criterion= StopAfterIteration (1000) | StopWhenGradientNormLess (1e-6) : a functor indicating that the stopping criterion is fulfilled randomize=false œÅ_regularization=10000.0 Œ∏=1.0 trust_region_radius=max_trust_region_radius / 8 X= zero_vector (M, p) : a tangent vector at the point  $p$  on the manifold  $\\mathcal M$ to specify the representation of a tangent vector See also trust_regions source"},{"id":3274,"pagetitle":"Trust-Regions Solver","title":"Approximation of the Hessian","ref":"/manopt/stable/solvers/trust_regions/#Approximation-of-the-Hessian","content":" Approximation of the Hessian Several different methods to approximate the Hessian are available."},{"id":3275,"pagetitle":"Trust-Regions Solver","title":"Manopt.ApproxHessianFiniteDifference","ref":"/manopt/stable/solvers/trust_regions/#Manopt.ApproxHessianFiniteDifference","content":" Manopt.ApproxHessianFiniteDifference  ‚Äî  Type ApproxHessianFiniteDifference{E, P, T, G, RTR, VTR, R <: Real} <: AbstractApproxHessian A functor to approximate the Hessian by a finite difference of gradient evaluation. Given a point  p  and a direction  X  and the gradient  $\\operatorname{grad} f(p)$  of a function  $f$  the Hessian is approximated as follows: let  $c$  be a stepsize,  $X ‚àà T_{p}\\mathcal M$  a tangent vector and  $q = \\operatorname{retr}_p(\\frac{c}{\\lVert X \\rVert_p}X)$  be a step in direction  $X$  of length  $c$  following a retraction Then the Hessian is approximated by the finite difference of the gradients, where  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  is a vector transport. \\[\\operatorname{Hess}f(p)[X] ‚âà\n\\frac{\\lVert X \\rVert_p}{c}\\Bigl(\n  \\mathcal T_{p\\gets q}\\bigr(\\operatorname{grad}f(q)\\bigl) - \\operatorname{grad}f(p)\n\\Bigl)\\] Fields gradient!! :              the gradient function (either allocating or mutating, see  evaluation  parameter) step_length :             a step length for the finite difference retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Internal temporary fields grad_tmp :     a temporary storage for the gradient at the current  p grad_dir_tmp : a temporary storage for the gradient at the current  p_dir p_dir::P :     a temporary storage to the forward direction (or the  $q$  in the formula) Constructor ApproximateFiniteDifference(M, p, grad_f; kwargs...) Keyword arguments evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. steplength= 2^{-14} $: step length$ c`` to approximate the gradient evaluations retraction_method= default_retraction_method (M, typeof(p)) : a retraction  $\\operatorname{retr}$  to use, see  the section on retractions vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports source"},{"id":3276,"pagetitle":"Trust-Regions Solver","title":"Manopt.ApproxHessianSymmetricRankOne","ref":"/manopt/stable/solvers/trust_regions/#Manopt.ApproxHessianSymmetricRankOne","content":" Manopt.ApproxHessianSymmetricRankOne  ‚Äî  Type ApproxHessianSymmetricRankOne{E, P, G, T, B<:AbstractBasis{‚Ñù}, VTR, R<:Real} <: AbstractApproxHessian A functor to approximate the Hessian by the symmetric rank one update. Fields gradient!! : the gradient function (either allocating or mutating, see  evaluation  parameter). ŒΩ : a small real number to ensure that the denominator in the update does not become too small and thus the method does not break down. vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports . Internal temporary fields p_tmp : a temporary storage the current point  p . grad_tmp : a temporary storage for the gradient at the current  p . matrix : a temporary storage for the matrix representation of the approximating operator. basis : a temporary storage for an orthonormal basis at the current  p . Constructor ApproxHessianSymmetricRankOne(M, p, gradF; kwargs...) Keyword arguments initial_operator  ( Matrix{Float64}(I, manifold_dimension(M), manifold_dimension(M)) ) the matrix representation of the initial approximating operator. basis  ( DefaultOrthonormalBasis() ) an orthonormal basis in the tangent space of the initial iterate p. nu  ( -1 ) evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports source"},{"id":3277,"pagetitle":"Trust-Regions Solver","title":"Manopt.ApproxHessianBFGS","ref":"/manopt/stable/solvers/trust_regions/#Manopt.ApproxHessianBFGS","content":" Manopt.ApproxHessianBFGS  ‚Äî  Type ApproxHessianBFGS{E, P, G, T, B<:AbstractBasis{‚Ñù}, VTR, R<:Real} <: AbstractApproxHessian A functor to approximate the Hessian by the BFGS update. Fields gradient!!  the gradient function (either allocating or mutating, see  evaluation  parameter). scale vector_transport_method::AbstractVectorTransportMethodP : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports Internal temporary fields p_tmp  a temporary storage the current point  p . grad_tmp  a temporary storage for the gradient at the current  p . matrix  a temporary storage for the matrix representation of the approximating operator. basis  a temporary storage for an orthonormal basis at the current  p . Constructor ApproxHessianBFGS(M, p, gradF; kwargs...) Keyword arguments initial_operator  ( Matrix{Float64}(I, manifold_dimension(M), manifold_dimension(M)) ) the matrix representation of the initial approximating operator. basis  ( DefaultOrthonormalBasis() ) an orthonormal basis in the tangent space of the initial iterate p. nu  ( -1 ) evaluation= AllocatingEvaluation () : specify whether the functions that return an array, for example a point or a tangent vector, work by allocating its result ( AllocatingEvaluation ) or whether they modify their input argument to return the result therein ( InplaceEvaluation ). Since usually the first argument is the manifold, the modified argument is the second. vector_transport_method= default_vector_transport_method (M, typeof(p)) : a vector transport  $\\mathcal T_{‚ãÖ‚Üê‚ãÖ}$  to use, see  the section on vector transports source as well as their (non-exported) common supertype"},{"id":3278,"pagetitle":"Trust-Regions Solver","title":"Manopt.AbstractApproxHessian","ref":"/manopt/stable/solvers/trust_regions/#Manopt.AbstractApproxHessian","content":" Manopt.AbstractApproxHessian  ‚Äî  Type AbstractApproxHessian <: Function An abstract supertype for approximate Hessian functions, declares them also to be functions. source"},{"id":3279,"pagetitle":"Trust-Regions Solver","title":"Technical details","ref":"/manopt/stable/solvers/trust_regions/#sec-tr-technical-details","content":" Technical details The  trust_regions  solver requires the following functions of a manifold to be available A  retract! (M, q, p, X) ; it is recommended to set the  default_retraction_method  to a favourite retraction. If this default is set, a  retraction_method=  does not have to be specified. By default the stopping criterion uses the  norm  as well, to stop when the norm of the gradient is small, but if you implemented  inner , the norm is provided already. if you do not provide an initial  max_trust_region_radius , a  manifold_dimension  is required. A  copyto! (M, q, p)  and  copy (M,p)  for points. By default the tangent vectors are initialized calling  zero_vector (M,p) ."},{"id":3280,"pagetitle":"Trust-Regions Solver","title":"Literature","ref":"/manopt/stable/solvers/trust_regions/#Literature","content":" Literature [ABG06] P.-A.¬†Absil, C.¬†Baker and K.¬†Gallivan.  Trust-Region Methods on Riemannian Manifolds .  Foundations¬†of¬†Computational¬†Mathematics  7 , 303‚Äì330  (2006). [CGT00] A.¬†R.¬†Conn, N.¬†I.¬†Gould and P.¬†L.¬†Toint.  Trust Region Methods  (Society for Industrial and Applied Mathematics, 2000)."},{"id":3283,"pagetitle":"Use automatic differentiation","title":"Using automatic differentiation in Manopt.jl","ref":"/manopt/stable/tutorials/AutomaticDifferentiation/#Using-automatic-differentiation-in-Manopt.jl","content":" Using automatic differentiation in Manopt.jl Since  Manifolds.jl  0.7, the support of automatic differentiation support has been extended. This tutorial explains how to use Euclidean tools to derive a gradient for a real-valued function  $f:  \\mathcal M ‚Üí ‚Ñù$ . Two methods are considered: an intrinsic variant and a variant employing the embedding. These gradients can then be used within any gradient based optimization algorithm in  Manopt.jl . While by default  FiniteDifferences.jl are used, one can also use  FiniteDiff.jl ,  ForwardDiff.jl ,  ReverseDiff.jl , or  Zygote.jl . This tutorial looks at a few possibilities to approximate or derive the gradient of a function  $f:\\mathcal M ‚Üí ‚Ñù$  on a Riemannian manifold, without computing it yourself. There are mainly two different philosophies: Working  intrinsically , that is staying on the manifold and in the tangent spaces, considering to approximate the gradient by forward differences. Working in an embedding where all tools from functions on Euclidean spaces can be used, like finite differences or automatic differentiation, and then compute the corresponding Riemannian gradient from there. First, load all necessary packages using Manopt, Manifolds, Random, LinearAlgebra\nusing FiniteDifferences, ManifoldDiff, ADTypes\nRandom.seed!(42);"},{"id":3284,"pagetitle":"Use automatic differentiation","title":"1. (Intrinsic) forward differences","ref":"/manopt/stable/tutorials/AutomaticDifferentiation/#1.-(Intrinsic)-forward-differences","content":" 1. (Intrinsic) forward differences A first idea is to generalize (multivariate) finite differences to Riemannian manifolds. Let  $X_1,\\ldots,X_d ‚àà T_p\\mathcal M$  denote an orthonormal basis of the tangent space  $T_p\\mathcal M$  at the point  $p‚àà\\mathcal M$  on the Riemannian manifold. The notion of a directional derivative is generalized to a ‚Äúdirection‚Äù  $Y‚ààT_p\\mathcal M$ . Let  $c:  [-Œµ,Œµ]$ ,  $Œµ>0$ , be a curve with  $c(0) = p$ ,  $\\dot c(0) = Y$ , for example  $c(t)= \\exp_p(tY)$ . This yields \\[    Df(p)[Y] = \\left. \\frac{d}{dt} \\right|_{t=0} f(c(t)) = \\lim_{t ‚Üí 0} \\frac{1}{t}(f(\\exp_p(tY))-f(p))\\] The differential  $Df(p)[X]$  is approximated by a finite difference scheme for an  $h>0$  as \\[DF(p)[Y] ‚âà G_h(Y) := \\frac{1}{h}(f(\\exp_p(hY))-f(p))\\] Furthermore the gradient  $\\operatorname{grad}f$  is the Riesz representer of the differential: \\[    Df(p)[Y] = g_p(\\operatorname{grad}f(p), Y),\\qquad \\text{ for all } Y ‚àà T_p\\mathcal M\\] and since it is a tangent vector, we can write it in terms of a basis as \\[    \\operatorname{grad}f(p) = \\sum_{i=1}^{d} g_p(\\operatorname{grad}f(p),X_i)X_i\n    = \\sum_{i=1}^{d} Df(p)[X_i]X_i\\] and perform the approximation from before to obtain \\[    \\operatorname{grad}f(p) ‚âà \\sum_{i=1}^{d} G_h(X_i)X_i\\] for some suitable step size  $h$ . This comes at the cost of  $d+1$  function evaluations and  $d$  exponential maps. This is the first variant we can use. An advantage is that it is  intrinsic  in the sense that it does not require any embedding of the manifold."},{"id":3285,"pagetitle":"Use automatic differentiation","title":"An example: the Rayleigh quotient","ref":"/manopt/stable/tutorials/AutomaticDifferentiation/#An-example:-the-Rayleigh-quotient","content":" An example: the Rayleigh quotient The Rayleigh quotient is concerned with finding eigenvalues (and eigenvectors) of a symmetric matrix  $A ‚àà ‚Ñù^{(n+1)√ó(n+1)}$ . The optimization problem reads \\[F:  ‚Ñù^{n+1} ‚Üí ‚Ñù,\\quad F(\\mathbf x) = \\frac{\\mathbf x^\\mathrm{T}A\\mathbf x}{\\mathbf x^\\mathrm{T}\\mathbf x}\\] Minimizing this function yields the smallest eigenvalue  $\\lambda_1$  as a value and the corresponding minimizer  $\\mathbf x^*$  is a corresponding eigenvector. Since the length of an eigenvector is irrelevant, there is an ambiguity in the cost function. It can be better phrased on the sphere  $  ùïä^n $  of unit vectors in  $‚Ñù^{n+1}$ , \\[\\operatorname*{arg\\,min}_{p ‚àà ùïä^n}\\ f(p) = \\operatorname*{arg\\,min}_{\\ p ‚àà ùïä^n} p^\\mathrm{T}Ap\\] We can compute the Riemannian gradient exactly as \\[\\operatorname{grad} f(p) = 2(Ap - pp^\\mathrm{T}Ap)\\] so we can compare it to the approximation by finite differences. n = 200\nA = randn(n + 1, n + 1)\nA = Symmetric(A)\nM = Sphere(n);\n\nf1(p) = p' * A'p\ngradf1(p) = 2 * (A * p - p * p' * A * p) gradf1 (generic function with 1 method) Manifolds provides a finite difference scheme in tangent spaces, that you can introduce to use an existing framework (if the wrapper is implemented) form Euclidean space. Here we use  FiniteDiff.jl . r_backend = ManifoldDiff.TangentDiffBackend(\n    AutoFiniteDifferences(central_fdm(5, 1))\n)\ngradf1_FD(p) = ManifoldDiff.gradient(M, f1, p, r_backend)\n\np = zeros(n + 1)\np[1] = 1.0\nX1 = gradf1(p)\nX2 = gradf1_FD(p)\nnorm(M, p, X1 - X2) 1.0156376260445835e-12 We obtain quite a good approximation of the gradient."},{"id":3286,"pagetitle":"Use automatic differentiation","title":"2. Conversion of a Euclidean gradient in the embedding to a Riemannian Gradient of a (not Necessarily Isometrically) embedded manifold","ref":"/manopt/stable/tutorials/AutomaticDifferentiation/#EmbeddedGradient","content":" 2. Conversion of a Euclidean gradient in the embedding to a Riemannian Gradient of a (not Necessarily Isometrically) embedded manifold Let  $\\tilde f: ‚Ñù^m ‚Üí ‚Ñù$  be a function on the embedding of an  $n$ -dimensional manifold  $\\mathcal M \\subset ‚Ñù^m$ and let  $f:  \\mathcal M ‚Üí ‚Ñù$  denote the restriction of  $\\tilde f$  to the manifold  $\\mathcal M$ . Since we can use the pushforward of the embedding to also embed the tangent space  $T_p\\mathcal M$ ,  $p‚àà\\mathcal M$ , we can similarly obtain the differential  $Df(p):  T_p\\mathcal M ‚Üí ‚Ñù$  by restricting the differential  $D\\tilde f(p)$  to the tangent space. If both  $T_p\\mathcal M$  and  $T_p‚Ñù^m$  have the same inner product, or in other words the manifold is isometrically embedded in  $‚Ñù^m$  (like for example the sphere  $\\mathbb S^n\\subset‚Ñù^{m+1}$ ), then this restriction of the differential directly translates to a projection of the gradient \\[\\operatorname{grad}f(p) = \\operatorname{Proj}_{T_p\\mathcal M}(\\operatorname{grad} \\tilde f(p))\\] More generally take a change of the metric into account as \\[\\langle  \\operatorname{Proj}_{T_p\\mathcal M}(\\operatorname{grad} \\tilde f(p)), X \\rangle\n= Df(p)[X] = g_p(\\operatorname{grad}f(p), X)\\] or in words: we have to change the Riesz representer of the (restricted/projected) differential of  $f$  ( $\\tilde f$ ) to the one with respect to the Riemannian metric. This is done using  change_representer ."},{"id":3287,"pagetitle":"Use automatic differentiation","title":"A continued example","ref":"/manopt/stable/tutorials/AutomaticDifferentiation/#A-continued-example","content":" A continued example We continue with the Rayleigh Quotient from before, now just starting with the definition of the Euclidean case in the embedding, the function  $F$ . F(x) = x' * A * x / (x' * x); The cost function is the same by restriction f2(M, p) = F(p); The gradient is now computed combining our gradient scheme with FiniteDifferences. function grad_f2_AD(M, p)\n    b = Manifolds.RiemannianProjectionBackend(AutoFiniteDifferences(central_fdm(5, 1)))\n    return Manifolds.gradient(M, F, p, b)\nend\nX3 = grad_f2_AD(M, p)\nnorm(M, p, X1 - X3) 1.7224975655660473e-12"},{"id":3288,"pagetitle":"Use automatic differentiation","title":"An example for a non-isometrically embedded manifold","ref":"/manopt/stable/tutorials/AutomaticDifferentiation/#An-example-for-a-non-isometrically-embedded-manifold","content":" An example for a non-isometrically embedded manifold on the manifold  $\\mathcal P(3)$  of symmetric positive definite matrices. The following function computes (half) the distance squared (with respect to the linear affine metric) on the manifold  $\\mathcal P(3)$  to the identity matrix  $I_3$ . Denoting the unit matrix we consider the function \\[    G(q)\n    = \\frac{1}{2}d^2_{\\mathcal P(3)}(q,I_3)\n    = \\lVert \\operatorname{Log}(q) \\rVert_F^2,\\] where  $\\operatorname{Log}$  denotes the matrix logarithm and  $\\lVert \\cdot \\rVert_F$  is the Frobenius norm. This can be computed for symmetric positive definite matrices by summing the squares of the logarithms of the eigenvalues of  $q$  and dividing by two: G(q) = sum(log.(eigvals(Symmetric(q))) .^ 2) / 2 G (generic function with 1 method) We can also interpret this as a function on the space of matrices and apply the Euclidean finite differences machinery; in this way we can easily derive the Euclidean gradient. But when computing the Riemannian gradient, we have to change the representer (see again  change_representer ) after projecting onto the tangent space  $T_p\\mathcal P(n)$  at  $p$ . Let‚Äôs first define a point and the manifold  $N=\\mathcal P(3)$ . rotM(Œ±) = [1.0 0.0 0.0; 0.0 cos(Œ±) sin(Œ±); 0.0 -sin(Œ±) cos(Œ±)]\nq = rotM(œÄ / 6) * [1.0 0.0 0.0; 0.0 2.0 0.0; 0.0 0.0 3.0] * transpose(rotM(œÄ / 6))\nN = SymmetricPositiveDefinite(3)\nis_point(N, q) true We could first just compute the gradient using  FiniteDifferences.jl , but this yields the Euclidean gradient: FiniteDifferences.grad(central_fdm(5, 1), G, q) ([3.240417492806275e-14 -2.3531899864903462e-14 0.0; 0.0 0.3514812167654708 0.017000516835452926; 0.0 0.0 0.36129646973723023],) Instead, we use the  RiemannianProjectedBackend  of  ManifoldDiff.jl , which in this case internally uses  FiniteDifferences.jl  to compute a Euclidean gradient but then uses the conversion explained before to derive the Riemannian gradient. We define this here again as a function  grad_G_FD  that could be used in the  Manopt.jl  framework within a gradient based optimization. function grad_G_FD(N, q)\n    return Manifolds.gradient(\n        N,\n        G,\n        q,\n        ManifoldDiff.RiemannianProjectionBackend(AutoFiniteDifferences(central_fdm(5, 1))),\n    )\nend\nG1 = grad_G_FD(N, q) 3√ó3 Matrix{Float64}:\n  3.24042e-14  -2.64734e-14  -5.09481e-15\n -2.64734e-14   1.86368       0.826856\n -5.09481e-15   0.826856      2.81845 Now, we can again compare this to the (known) solution of the gradient, namely the gradient of (half of) the distance squared  $G(q) = \\frac{1}{2}d^2_{\\mathcal P(3)}(q,I_3)$  is given by  $\\operatorname{grad} G(q) = -\\operatorname{log}_q I_3$ , where  $\\operatorname{log}$  is th  logarithmic map  on the manifold. G2 = -log(N, q, Matrix{Float64}(I, 3, 3)) 3√ó3 Matrix{Float64}:\n -0.0  -0.0       -0.0\n -0.0   1.86368    0.826856\n -0.0   0.826856   2.81845 Both terms agree up to  $1.8√ó10^{-12}$ : norm(G1 - G2)\nisapprox(M, q, G1, G2; atol=2 * 1e-12) true"},{"id":3289,"pagetitle":"Use automatic differentiation","title":"Summary","ref":"/manopt/stable/tutorials/AutomaticDifferentiation/#Summary","content":" Summary This tutorial illustrates how to use tools from Euclidean spaces, finite differences or automatic differentiation, to compute gradients on Riemannian manifolds. The scheme allows to use  any  differentiation framework within the embedding to derive a Riemannian gradient."},{"id":3290,"pagetitle":"Use automatic differentiation","title":"Technical details","ref":"/manopt/stable/tutorials/AutomaticDifferentiation/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. Status `~/work/Manopt.jl/Manopt.jl/tutorials/Project.toml`\n  [47edcb42] ADTypes v1.15.0\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [5ae59095] Colors v0.13.1\n  [31c24e10] Distributions v0.25.120\n  [26cc04aa] FiniteDifferences v0.12.32\n  [7073ff75] IJulia v1.29.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [af67fdf4] ManifoldDiff v0.4.4\n  [1cead3c2] Manifolds v0.10.22\n  [3362f125] ManifoldsBase v1.2.0\n  [0fc0a36d] Manopt v0.5.20 `..`\n  [91a5bcdd] Plots v1.40.16\n  [731186ca] RecursiveArrayTools v3.34.1\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0 This tutorial was last rendered July 8, 2025, 18:41:53."},{"id":3293,"pagetitle":"Do constrained optimization","title":"How to do constrained optimization","ref":"/manopt/stable/tutorials/ConstrainedOptimization/#How-to-do-constrained-optimization","content":" How to do constrained optimization Ronny Bergmann This tutorial is a short introduction to using solvers for constraint optimisation in  Manopt.jl ."},{"id":3294,"pagetitle":"Do constrained optimization","title":"Introduction","ref":"/manopt/stable/tutorials/ConstrainedOptimization/#Introduction","content":" Introduction A constraint optimisation problem is given by \\[\\tag{P}\n\\begin{align*}\n\\operatorname*{arg\\,min}_{p‚àà\\mathcal M} & f(p)\\\\\n\\text{such that} &\\quad g(p) \\leq 0\\\\\n&\\quad h(p) = 0,\\\\\n\\end{align*}\\] where  $f:  \\mathcal M ‚Üí ‚Ñù$  is a cost function, and  $g:  \\mathcal M ‚Üí ‚Ñù^m$  and  $h:  \\mathcal M ‚Üí ‚Ñù^n$  are the inequality and equality constraints, respectively. The  $\\leq$  and  $=$  in (P) are meant element-wise. This can be seen as a balance between moving constraints into the geometry of a manifold  $\\mathcal M$  and keeping some, since they can be handled well in algorithms, see [ BH19 ], [ LB19 ] for details. using Distributions, LinearAlgebra, Manifolds, Manopt, Random\nRandom.seed!(42); In this tutorial we want to look at different ways to specify the problem and its implications. We start with specifying an example problems to illustrate the different available forms. We consider the problem of a Nonnegative PCA, cf.¬†Section 5.1.2 in [ LB19 ] let  $v_0 ‚àà ‚Ñù^d$ ,  $\\lVert v_0 \\rVert=1$  be given spike signal, that is a signal that is sparse with only  $s=\\lfloor Œ¥d \\rfloor$  nonzero entries. \\[Z = \\sqrt{œÉ} v_0v_0^{\\mathrm{T}}+N,\\] where  $\\sigma$  is a signal-to-noise ratio and  $N$  is a matrix with random entries, where the diagonal entries are distributed with zero mean and standard deviation  $1/d$  on the off-diagonals and  $2/d$  on the diagonal d = 150; # dimension of v0\nœÉ = 0.1^2; # SNR\nŒ¥ = 0.1; sp = Int(floor(Œ¥ * d)); # Sparsity\nS = sample(1:d, sp; replace=false);\nv0 =  [i ‚àà S ? 1 / sqrt(sp) : 0.0 for i in 1:d];\nN = rand(Normal(0, 1 / d), (d, d)); N[diagind(N, 0)] .= rand(Normal(0, 2 / d), d);\nZ = Z = sqrt(œÉ) * v0 * transpose(v0) + N; In order to recover  $v_0$  we consider the constrained optimisation problem on the sphere  $\\mathcal S^{d-1}$  given by \\[\\begin{align*}\n\\operatorname*{arg\\,min}_{p‚àà\\mathcal S^{d-1}} & -p^{\\mathrm{T}}Zp^{\\mathrm{T}}\\\\\n\\text{such that} &\\quad p \\geq 0\\\\\n\\end{align*}\\] or in the previous notation  $f(p) = -p^{\\mathrm{T}}Zp^{\\mathrm{T}}$  and  $g(p) = -p$ . We first initialize the manifold under consideration M = Sphere(d - 1) Sphere(149, ‚Ñù)"},{"id":3295,"pagetitle":"Do constrained optimization","title":"A first augmented Lagrangian run","ref":"/manopt/stable/tutorials/ConstrainedOptimization/#A-first-augmented-Lagrangian-run","content":" A first augmented Lagrangian run We first defined  $f$  and  $g$  as usual functions f(M, p) = -transpose(p) * Z * p;\ng(M, p) = -p; since  $f$  is a functions defined in the embedding  $‚Ñù^d$  as well, we obtain its gradient by projection. grad_f(M, p) = project(M, p, -transpose(Z) * p - Z * p); For the constraints this is a little more involved, since each function  $g_i=g(p)_i=p_i$  has to return its own gradient. These are again in the embedding just  $\\operatorname{grad} g_i(p) = -e_i$  the  $i$  th unit vector. We can project these again onto the tangent space at  $p$ : grad_g(M, p) = project.(\n    Ref(M), Ref(p), [[i == j ? -1.0 : 0.0 for j in 1:d] for i in 1:d]\n); We further start in a random point: p0 = rand(M); Let‚Äôs verify a few things for the initial point f(M, p0) 0.005667399180991248 How much the function g is positive maximum(g(M, p0)) 0.17885478285466855 Now as a first method we can just call the  Augmented Lagrangian Method  with a simple call: @time v1 = augmented_Lagrangian_method(\n    M, f, grad_f, p0; g=g, grad_g=grad_g,\n    debug=[:Iteration, :Cost, :Stop, \" | \", (:Change, \"Œîp : %1.5e\"), 20, \"\\n\"],\n    stopping_criterion = StopAfterIteration(300) | (\n        StopWhenSmallerOrEqual(:œµ, 1e-5) & StopWhenChangeLess(M, 1e-8)\n    )\n); Initial f(x): 0.005667 | \n# 20    f(x): -0.123557 | Œîp : 1.00133e+00\n# 40    f(x): -0.123557 | Œîp : 3.77088e-08\n# 60    f(x): -0.123557 | Œîp : 2.40619e-05\nThe value of the variable (œµ) is smaller than or equal to its threshold (1.0e-5).\nAt iteration 68 the algorithm performed a step with a change (7.600544776224794e-11) less than 9.77237220955808e-6.\n  5.986342 seconds (18.32 M allocations: 1.604 GiB, 3.10% gc time, 97.22% compilation time) Now we have both a lower function value and the point is nearly within the constraints, namely up to numerical inaccuracies f(M, v1) -0.12353580883894738 maximum( g(M, v1) ) 4.577229036010474e-12"},{"id":3296,"pagetitle":"Do constrained optimization","title":"A faster augmented Lagrangian run","ref":"/manopt/stable/tutorials/ConstrainedOptimization/#A-faster-augmented-Lagrangian-run","content":" A faster augmented Lagrangian run Now this is a little slow, so we can modify two things: Gradients should be evaluated in place, so for example grad_f!(M, X, p) = project!(M, X, p, -transpose(Z) * p - Z * p); The constraints are currently always evaluated all together, since the function  grad_g  always returns a vector of gradients.  We first change the constraints function into a vector of functions.  We further change the gradient  both  into a vector of gradient functions  $\\operatorname{grad} g_i,i=1,\\ldots,d$ ,  as well as  gradients that are computed in place. g2 = [(M, p) -> -p[i] for i in 1:d];\ngrad_g2! = [\n    (M, X, p) -> project!(M, X, p, [i == j ? -1.0 : 0.0 for j in 1:d]) for i in 1:d\n]; We obtain @time v2 = augmented_Lagrangian_method(\n        M, f, grad_f!, p0; g=g2, grad_g=grad_g2!, evaluation=InplaceEvaluation(),\n        debug=[:Iteration, :Cost, :Stop, \" | \", (:Change, \"Œîp : %1.5e\"), 20, \"\\n\"],\n        stopping_criterion = StopAfterIteration(300) | (\n          StopWhenSmallerOrEqual(:œµ, 1e-5) & StopWhenChangeLess(M, 1e-8)\n        )\n    ); Initial f(x): 0.005667 | \n# 20    f(x): -0.123557 | Œîp : 1.00133e+00\n# 40    f(x): -0.123557 | Œîp : 3.77088e-08\n# 60    f(x): -0.123557 | Œîp : 2.40619e-05\nThe value of the variable (œµ) is smaller than or equal to its threshold (1.0e-5).\nAt iteration 68 the algorithm performed a step with a change (7.600544776224794e-11) less than 9.77237220955808e-6.\n  2.162446 seconds (5.31 M allocations: 738.839 MiB, 2.63% gc time, 93.84% compilation time) As a technical remark: note that (by default) the change to  InplaceEvaluation s affects both the constrained solver as well as the inner solver of the subproblem in each iteration. f(M, v2) -0.12353580883894738 maximum(g(M, v2)) 4.577229036010474e-12 These are the very similar to the previous values but the solver took much less time and less memory allocations."},{"id":3297,"pagetitle":"Do constrained optimization","title":"Exact penalty method","ref":"/manopt/stable/tutorials/ConstrainedOptimization/#Exact-penalty-method","content":" Exact penalty method As a second solver, we have the  Exact Penalty Method , which currently is available with two smoothing variants, which make an inner solver for smooth optimization, that is by default again [quasi Newton] possible:  LogarithmicSumOfExponentials  and  LinearQuadraticHuber . We compare both here as well. The first smoothing technique is the default, so we can just call @time v3 = exact_penalty_method(\n    M, f, grad_f!, p0; g=g2, grad_g=grad_g2!, evaluation=InplaceEvaluation(),\n    debug=[:Iteration, :Cost, :Stop, \" | \", :Change, 50, \"\\n\"],\n); Initial f(x): 0.005667 | \n# 50    f(x): -0.122792 | Last Change: 0.982159\n# 100   f(x): -0.123555 | Last Change: 0.013515\nThe value of the variable (œµ) is smaller than or equal to its threshold (1.0e-6).\nAt iteration 102 the algorithm performed a step with a change (3.0244885037602495e-7) less than 1.0e-6.\n  2.651971 seconds (14.89 M allocations: 5.793 GiB, 9.32% gc time, 58.25% compilation time) We obtain a similar cost value as for the Augmented Lagrangian Solver from before, but here the constraint is actually fulfilled and not just numerically ‚Äúon the boundary‚Äù. f(M, v3) -0.12355544268449432 maximum(g(M, v3)) -3.589798060999793e-6 The second smoothing technique is often beneficial, when we have a lot of constraints (in the previously mentioned vectorial manner), since we can avoid several gradient evaluations for the constraint functions here. This leads to a faster iteration time. @time v4 = exact_penalty_method(\n    M, f, grad_f!, p0; g=g2, grad_g=grad_g2!,\n    evaluation=InplaceEvaluation(),\n    smoothing=LinearQuadraticHuber(),\n    debug=[:Iteration, :Cost, :Stop, \" | \", :Change, 50, \"\\n\"],\n); Initial f(x): 0.005667 | \n# 50    f(x): -0.123559 | Last Change: 0.008024\n# 100   f(x): -0.123557 | Last Change: 0.000026\nThe value of the variable (œµ) is smaller than or equal to its threshold (1.0e-6).\nAt iteration 101 the algorithm performed a step with a change (1.0069976577931588e-8) less than 1.0e-6.\n  1.918182 seconds (8.49 M allocations: 2.557 GiB, 7.01% gc time, 79.38% compilation time) For the result we see the same behaviour as for the other smoothing. f(M, v4) -0.12355667846565418 maximum(g(M, v4)) 2.6974802196316014e-8"},{"id":3298,"pagetitle":"Do constrained optimization","title":"Comparing to the unconstrained solver","ref":"/manopt/stable/tutorials/ConstrainedOptimization/#Comparing-to-the-unconstrained-solver","content":" Comparing to the unconstrained solver We can compare this to the  global  optimum on the sphere, which is the unconstrained optimisation problem, where we can just use Quasi Newton. Note that this is much faster, since every iteration of the algorithm does a quasi-Newton call as well. @time w1 = quasi_Newton(\n    M, f, grad_f!, p0; evaluation=InplaceEvaluation()\n);   0.650542 seconds (1.49 M allocations: 99.134 MiB, 1.97% gc time, 97.08% compilation time) f(M, w1) -0.13990874034056555 But for sure here the constraints here are not fulfilled and we have quite positive entries in  $g(w_1)$ maximum(g(M, w1)) 0.11803200739746737"},{"id":3299,"pagetitle":"Do constrained optimization","title":"Technical details","ref":"/manopt/stable/tutorials/ConstrainedOptimization/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. Status `~/work/Manopt.jl/Manopt.jl/tutorials/Project.toml`\n  [47edcb42] ADTypes v1.15.0\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [5ae59095] Colors v0.13.1\n  [31c24e10] Distributions v0.25.120\n  [26cc04aa] FiniteDifferences v0.12.32\n  [7073ff75] IJulia v1.29.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [af67fdf4] ManifoldDiff v0.4.4\n  [1cead3c2] Manifolds v0.10.22\n  [3362f125] ManifoldsBase v1.2.0\n  [0fc0a36d] Manopt v0.5.20 `..`\n  [91a5bcdd] Plots v1.40.16\n  [731186ca] RecursiveArrayTools v3.34.1\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0 This tutorial was last rendered July 8, 2025, 18:42:25."},{"id":3300,"pagetitle":"Do constrained optimization","title":"Literature","ref":"/manopt/stable/tutorials/ConstrainedOptimization/#Literature","content":" Literature [BH19] R.¬†Bergmann and R.¬†Herzog.  Intrinsic formulation of KKT conditions and constraint qualifications on smooth manifolds .  SIAM¬†Journal¬†on¬†Optimization  29 , 2423‚Äì2444  (2019),  arXiv:1804.06214 . [LB19] C.¬†Liu and N.¬†Boumal.  Simple algorithms for optimization on Riemannian manifolds with constraints .  Applied¬†Mathematics¬†&¬†Optimization  (2019),  arXiv:1091.10000 ."},{"id":3303,"pagetitle":"Count and use a cache","title":"How to count and cache function calls","ref":"/manopt/stable/tutorials/CountAndCache/#How-to-count-and-cache-function-calls","content":" How to count and cache function calls Ronny Bergmann In this tutorial, we want to investigate the caching and counting (statistics) features of  Manopt.jl . We reuse the optimization tasks from the introductory tutorial  üèîÔ∏è Get started with Manopt.jl ."},{"id":3304,"pagetitle":"Count and use a cache","title":"Introduction","ref":"/manopt/stable/tutorials/CountAndCache/#Introduction","content":" Introduction There are surely many ways to keep track for example of how often the cost function is called, for example with a  functor , as we used in an example in  How to Record Data mutable struct MyCost{I<:Integer}\n    count::I\nend\nMyCost() = MyCost{Int64}(0)\nfunction (c::MyCost)(M, x)\n    c.count += 1\n    # [ .. Actual implementation of the cost here ]\nend This still leaves a bit of work to the user, especially for tracking more than just the number of cost function evaluations. When a function like the objective or gradient is expensive to compute, it may make sense to cache its results. Manopt.jl tries to minimize the number of repeated calls but sometimes they are necessary and harmless when the function is cheap to compute. Caching of expensive function calls can for example be added using  Memoize.jl  by the user. The approach in the solvers of  Manopt.jl  aims to simplify adding both these capabilities on the level of calling a solver."},{"id":3305,"pagetitle":"Count and use a cache","title":"Technical background","ref":"/manopt/stable/tutorials/CountAndCache/#Technical-background","content":" Technical background The two ingredients for a solver in  Manopt.jl  are the  AbstractManoptProblem  and the  AbstractManoptSolverState , where the former consists of the domain, that is the  AsbtractManifold  and  AbstractManifoldObjective . Both recording and debug capabilities are implemented in a decorator pattern to the solver state. They can be easily added using the  record=  and  debug=  in any solver call. This pattern was recently extended, such that also the objective can be decorated. This is how both caching and counting are implemented, as decorators of the  AbstractManifoldObjective  and hence for example changing/extending the behaviour of a call to  get_cost . Let‚Äôs finish off the technical background by loading the necessary packages. Besides  Manopt.jl  and  Manifolds.jl  we also need  LRUCaches.jl  which are (since Julia 1.9) a weak dependency and provide the  least recently used  strategy for our caches. using Manopt, Manifolds, Random, LRUCache, LinearAlgebra, ManifoldDiff\nusing ManifoldDiff: grad_distance"},{"id":3306,"pagetitle":"Count and use a cache","title":"Counting","ref":"/manopt/stable/tutorials/CountAndCache/#Counting","content":" Counting We first define our task, the Riemannian Center of Mass from the  üèîÔ∏è Get started with Manopt.jl  tutorial. n = 100\nœÉ = œÄ / 8\nM = Sphere(2)\np = 1 / sqrt(2) * [1.0, 0.0, 1.0]\nRandom.seed!(42)\ndata = [exp(M, p,  œÉ * rand(M; vector_at=p)) for i in 1:n];\nf(M, p) = sum(1 / (2 * n) * distance.(Ref(M), Ref(p), data) .^ 2)\ngrad_f(M, p) = sum(1 / n * grad_distance.(Ref(M), data, Ref(p))); to now count how often the cost and the gradient are called, we use the  count=  keyword argument that works in any solver to specify the elements of the objective whose calls we want to count calls to. A full list is available in the documentation of the  AbstractManifoldObjective . To also see the result, we have to set  return_objective=true . This returns  (objective, p)  instead of just the solver result  p . We can further also set  return_state=true  to get even more information about the solver run. gradient_descent(M, f, grad_f, data[1]; count=[:Cost, :Gradient], return_objective=true, return_state=true) # Solver state for `Manopt.jl`s Gradient Descent\nAfter 66 iterations\n\n## Parameters\n* retraction method: ManifoldsBase.ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0\n    retraction_method=ManifoldsBase.ExponentialRetraction()\n    contraction_factor=0.95\n    sufficient_decrease=0.1\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 200:  not reached\n  * |grad f| < 1.0e-8: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Statistics on function calls\n  * :Gradient : 199\n  * :Cost     : 275 And we see that statistics are shown in the end."},{"id":3307,"pagetitle":"Count and use a cache","title":"Caching","ref":"/manopt/stable/tutorials/CountAndCache/#Caching","content":" Caching To now also cache these calls, we can use the  cache=  keyword argument. Since now both the cache and the count ‚Äúextend‚Äù the capability of the objective, the order is important: on the high-level interface, the  count  is treated first, which means that only actual function calls and not cache look-ups are counted. With the proper initialisation, you can use any caches here that support the  get!(function, cache, key)!  update. All parts of the objective that can currently be cached are listed at  ManifoldCachedObjective . The solver call has a keyword  cache  that takes a tuple (c, vs, n)  of three arguments, where  c  is a symbol for the type of cache,  vs  is a vector of symbols, which calls to cache and  n  is the size of the cache. If the last element is not provided, a suitable default (currently n=10 ) is used. Here we want to use  c=:LRU  caches for  vs=[Cost, :Gradient]  with a size of  n=25 . r = gradient_descent(M, f, grad_f, data[1];\n    count=[:Cost, :Gradient],\n    cache=(:LRU, [:Cost, :Gradient], 25),\n    return_objective=true, return_state=true) # Solver state for `Manopt.jl`s Gradient Descent\nAfter 66 iterations\n\n## Parameters\n* retraction method: ManifoldsBase.ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0\n    retraction_method=ManifoldsBase.ExponentialRetraction()\n    contraction_factor=0.95\n    sufficient_decrease=0.1\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 200:  not reached\n  * |grad f| < 1.0e-8: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Cache\n  * :Cost     : 25/25 entries of type Float64 used\n  * :Gradient : 25/25 entries of type Vector{Float64} used\n\n## Statistics on function calls\n  * :Gradient : 66\n  * :Cost     : 149 Since the default setup with  ArmijoLinesearch  needs the gradient and the cost, and similarly the stopping criterion might (independently) evaluate the gradient, the caching is quite helpful here. And of course also for this advanced return value of the solver, we can still access the result as usual: get_solver_result(r) 3-element Vector{Float64}:\n 0.6868392807355564\n 0.006531599748261925\n 0.7267799809043942"},{"id":3308,"pagetitle":"Count and use a cache","title":"Advanced caching examples","ref":"/manopt/stable/tutorials/CountAndCache/#Advanced-caching-examples","content":" Advanced caching examples There are more options other than caching single calls to specific parts of the objective. For example you may want to cache intermediate results of computing the cost and share that with the gradient computation. We present three solutions to this: An easy approach from within  Manopt.jl : the  ManifoldCostGradientObjective A shared storage approach using a functor A shared (internal) cache approach also using a functor For that we switch to another example: the Rayleigh quotient. We aim to maximize the Rayleigh quotient  $\\displaystyle\\frac{x^{\\mathrm{T}}Ax}{x^{\\mathrm{T}}x}$ , for some  $A‚àà‚Ñù^{m+1\\times m+1}$  and  $x‚àà‚Ñù^{m+1}$  but since we consider this on the sphere and  Manopt.jl  (as many other optimization toolboxes) minimizes, we consider \\[g(p) = -p^{\\mathrm{T}}Ap,\\qquad p‚àà\\mathbb S^{m}\\] The Euclidean gradient (that is in  $  R^{m+1} $ ) is actually just  $\\nabla g(p) = -2Ap$ , the Riemannian gradient the projection of  $\\nabla g(p)$  onto the tangent space  $T_p\\mathbb S^{m}$ . m = 25\nRandom.seed!(42)\nA = randn(m + 1, m + 1)\nA = Symmetric(A)\np_star = eigvecs(A)[:, end] # minimizer (or similarly -p)\nf_star = -eigvals(A)[end] # cost (note that we get - the largest Eigenvalue)\n\nN = Sphere(m);\n\ng(M, p) = -p' * A*p\n‚àág(p) = -2 * A * p\ngrad_g(M,p) = project(M, p, ‚àág(p))\ngrad_g!(M,X, p) = project!(M, X, p, ‚àág(p)) grad_g! (generic function with 1 method) But since both the cost and the gradient require the computation of the matrix-vector product  $Ap$ , it might be beneficial to only compute this once."},{"id":3309,"pagetitle":"Count and use a cache","title":"The ManifoldCostGradientObjective approach","ref":"/manopt/stable/tutorials/CountAndCache/#The-[ManifoldCostGradientObjective](@ref)-approach","content":" The  ManifoldCostGradientObjective  approach The  ManifoldCostGradientObjective  uses a combined function to compute both the gradient and the cost at the same time. We define the in-place variant as function g_grad_g!(M::AbstractManifold, X, p)\n    X .= -A*p\n    c = p'*X\n    X .*= 2\n    project!(M, X, p, X)\n    return (c, X)\nend g_grad_g! (generic function with 1 method) where we only compute the matrix-vector product once. The small disadvantage might be, that we always compute  both , the gradient and the cost. Luckily, the cache we used before, takes this into account and caches both results, such that we indeed end up computing  A*p  only once when asking to a cost and a gradient. Let‚Äôs compare both methods p0 = [(1/5 .* ones(5))..., zeros(m-4)...];\n@time s1 = gradient_descent(N, g, grad_g!, p0;\n    stopping_criterion =¬†StopWhenGradientNormLess(1e-5),\n    evaluation=InplaceEvaluation(),\n    count=[:Cost, :Gradient],\n    cache=(:LRU, [:Cost, :Gradient], 25),\n    return_objective=true,\n)   1.489144 seconds (2.46 M allocations: 124.664 MiB, 1.26% gc time, 99.70% compilation time)\n\n## Cache\n  * :Cost     : 25/25 entries of type Float64 used\n  * :Gradient : 25/25 entries of type Vector{Float64} used\n\n## Statistics on function calls\n  * :Gradient : 602\n  * :Cost     : 1449\n\nTo access the solver result, call `get_solver_result` on this variable. versus obj = ManifoldCostGradientObjective(g_grad_g!; evaluation=InplaceEvaluation())\n@time s2 = gradient_descent(N, obj, p0;\n    stopping_criterion=StopWhenGradientNormLess(1e-5),\n    count=[:Cost, :Gradient],\n    cache=(:LRU, [:Cost, :Gradient], 25),\n    return_objective=true,\n)   0.653669 seconds (1.14 M allocations: 69.572 MiB, 2.77% gc time, 99.03% compilation time)\n\n## Cache\n  * :Cost     : 25/25 entries of type Float64 used\n  * :Gradient : 25/25 entries of type Vector{Float64} used\n\n## Statistics on function calls\n  * :Gradient : 602\n  * :Cost     : 2051\n\nTo access the solver result, call `get_solver_result` on this variable. first of all both yield the same result p1 = get_solver_result(s1)\np2 = get_solver_result(s2)\n[distance(N, p1, p2), g(N, p1), g(N, p2), f_star] 4-element Vector{Float64}:\n  0.0\n -7.8032957637779\n -7.8032957637779\n -7.803295763793949 and we can see that the combined number of evaluations is once 2051, once just the number of cost evaluations 1449. Note that the involved additional 847 gradient evaluations are merely a multiplication with 2. On the other hand, the additional caching of the gradient in these cases might be less beneficial. It is beneficial, when the gradient and the cost are very often required together."},{"id":3310,"pagetitle":"Count and use a cache","title":"A shared storage approach using a functor","ref":"/manopt/stable/tutorials/CountAndCache/#A-shared-storage-approach-using-a-functor","content":" A shared storage approach using a functor An alternative to the previous approach is the usage of a functor that introduces a ‚Äúshared storage‚Äù of the result of computing  A*p . We additionally have to store  p  though, since we have to make sure that we are still evaluating the cost and/or gradient at the same point at which the cached  A*p  was computed. We again consider the (more efficient) in-place variant. This can be done as follows struct StorageG{T,M}\n    A::M\n    Ap::T\n    p::T\nend\nfunction (g::StorageG)(::Val{:Cost}, M::AbstractManifold, p)\n    if !(p==g.p) #We are at a new point -> Update\n        g.Ap .= g.A*p\n        g.p .= p\n    end\n    return -g.p'*g.Ap\nend\nfunction (g::StorageG)(::Val{:Gradient}, M::AbstractManifold, X, p)\n    if !(p==g.p) #We are at a new point -> Update\n        g.Ap .= g.A*p\n        g.p .= p\n    end\n    X .= -2 .* g.Ap\n    project!(M, X, p, X)\n    return X\nend Here we use the first parameter to distinguish both functions. For the mutating case the signatures are different regardless of the additional argument but for the allocating case, the signatures of the cost and the gradient function are the same. #Define the new functor\nstorage_g = StorageG(A, zero(p0), zero(p0))\n# and cost and gradient that use this functor as\ng3(M,p) = storage_g(Val(:Cost), M, p)\ngrad_g3!(M, X, p) = storage_g(Val(:Gradient), M, X, p)\n@time s3 = gradient_descent(N, g3, grad_g3!, p0;\n    stopping_criterion =¬†StopWhenGradientNormLess(1e-5),\n    evaluation=InplaceEvaluation(),\n    count=[:Cost, :Gradient],\n    cache=(:LRU, [:Cost, :Gradient], 2),\n    return_objective=true#, return_state=true\n)   0.564653 seconds (639.66 k allocations: 33.791 MiB, 99.25% compilation time)\n\n## Cache\n  * :Cost     : 2/2 entries of type Float64 used\n  * :Gradient : 2/2 entries of type Vector{Float64} used\n\n## Statistics on function calls\n  * :Gradient : 602\n  * :Cost     : 1449\n\nTo access the solver result, call `get_solver_result` on this variable. This of course still yields the same result p3 = get_solver_result(s3)\ng(N, p3) - f_star 1.6049384043981263e-11 And while we again have a split off the cost and gradient evaluations, we can observe that the allocations are less than half of the previous approach."},{"id":3311,"pagetitle":"Count and use a cache","title":"A local cache approach","ref":"/manopt/stable/tutorials/CountAndCache/#A-local-cache-approach","content":" A local cache approach This variant is very similar to the previous one, but uses a whole cache instead of just one place to store  A*p . This makes the code a bit nicer, and it is possible to store more than just the last  p  either cost or gradient was called with. struct CacheG{C,M}\n    A::M\n    cache::C\nend\nfunction (g::CacheG)(::Val{:Cost}, M, p)\n    Ap = get!(g.cache, copy(M,p)) do\n        g.A*p\n    end\n    return -p'*Ap\nend\nfunction (g::CacheG)(::Val{:Gradient}, M, X, p)\n    Ap = get!(g.cache, copy(M,p)) do\n        g.A*p\n    end\n    X .= -2 .* Ap\n    project!(M, X, p, X)\n    return X\nend However, the resulting solver run is not always faster, since the whole cache instead of storing just  Ap  and  p  is a bit more costly. Then the tradeoff is, whether this pays off. #Define the new functor\ncache_g = CacheG(A, LRU{typeof(p0),typeof(p0)}(; maxsize=25))\n# and cost and gradient that use this functor as\ng4(M,p) = cache_g(Val(:Cost), M, p)\ngrad_g4!(M, X, p) = cache_g(Val(:Gradient), M, X, p)\n@time s4 = gradient_descent(N, g4, grad_g4!, p0;\n    stopping_criterion =¬†StopWhenGradientNormLess(1e-5),\n    evaluation=InplaceEvaluation(),\n    count=[:Cost, :Gradient],\n    cache=(:LRU, [:Cost, :Gradient], 25),\n    return_objective=true,\n)   0.486628 seconds (528.90 k allocations: 28.380 MiB, 98.94% compilation time)\n\n## Cache\n  * :Cost     : 25/25 entries of type Float64 used\n  * :Gradient : 25/25 entries of type Vector{Float64} used\n\n## Statistics on function calls\n  * :Gradient : 602\n  * :Cost     : 1449\n\nTo access the solver result, call `get_solver_result` on this variable. and for safety let‚Äôs verify that we are reasonably close p4 = get_solver_result(s4)\ng(N, p4) - f_star 1.6049384043981263e-11 For this example, or maybe even  gradient_descent  in general it seems, this additional (second, inner) cache does not improve the result further, it is about the same effort both time and allocation-wise."},{"id":3312,"pagetitle":"Count and use a cache","title":"Summary","ref":"/manopt/stable/tutorials/CountAndCache/#Summary","content":" Summary While the second approach of  ManifoldCostGradientObjective  is very easy to implement, both the storage and the (local) cache approach are more efficient. All three are an improvement over the first implementation without sharing interim results. The results with storage or cache have further advantage of being more flexible, since the stored information could also be reused in a third function, for example when also computing the Hessian."},{"id":3313,"pagetitle":"Count and use a cache","title":"Technical details","ref":"/manopt/stable/tutorials/CountAndCache/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. Status `~/work/Manopt.jl/Manopt.jl/tutorials/Project.toml`\n  [47edcb42] ADTypes v1.15.0\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [5ae59095] Colors v0.13.1\n  [31c24e10] Distributions v0.25.120\n  [26cc04aa] FiniteDifferences v0.12.32\n  [7073ff75] IJulia v1.29.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [af67fdf4] ManifoldDiff v0.4.4\n  [1cead3c2] Manifolds v0.10.22\n  [3362f125] ManifoldsBase v1.2.0\n  [0fc0a36d] Manopt v0.5.20 `..`\n  [91a5bcdd] Plots v1.40.16\n  [731186ca] RecursiveArrayTools v3.34.1\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0 This tutorial was last rendered July 8, 2025, 18:42:49."},{"id":3316,"pagetitle":"Define objectives in the embedding","title":"How to define the cost in the embedding","ref":"/manopt/stable/tutorials/EmbeddingObjectives/#How-to-define-the-cost-in-the-embedding","content":" How to define the cost in the embedding Ronny Bergmann Specifying a cost function  $f:  \\mathcal M ‚Üí ‚Ñù$  on a manifold is usually the model one starts with. Specifying its gradient  $\\operatorname{grad} f: \\mathcal M ‚Üí T\\mathcal M$ , or more precisely  $\\operatorname{grad}f(p) ‚àà T_p\\mathcal M$ , and eventually a Hessian  $\\operatorname{Hess} f:  T_p\\mathcal M ‚Üí T_p\\mathcal M$  are then necessary to perform optimization. Since these might be challenging to compute, especially when manifolds and differential geometry are not the main area of a user,¬†easier to use methods might be welcome. This tutorial discusses how to specify  $f$  in the embedding as  $\\tilde f$ , maybe only locally around the manifold, and use the Euclidean gradient  $‚àá \\tilde f$  and Hessian  $‚àá^2 \\tilde f$  within  Manopt.jl . For the theoretical background see  convert an Euclidean to an Riemannian Gradient , or Section 4.7 of [ Bou23 ] for the gradient part or Section 5.11 as well as [ Ngu23 ] for the background on converting Hessians. Here we use the Examples 9.40 and 9.49 of [ Bou23 ] and compare the different methods, one can call the solver, depending on which gradient and/or Hessian one provides. using Manifolds, Manopt, ManifoldDiff\nusing LinearAlgebra, Random, Colors, Plots\nRandom.seed!(123) We consider the cost function on the  Grassmann  manifold given by n = 5\nk = 2\nM = Grassmann(5,2)\nA = Symmetric(rand(n,n)); f(M, p) = 1 / 2 * tr(p' * A * p) Note that this implementation is already also a valid implementation / continuation of  $f$  into the (lifted) embedding of the Grassmann manifold. In the implementation we can use  f  for both the Euclidean  $\\tilde f$  and the Grassmann case  $f$ . Its Euclidean gradient  $\\nabla f$  and Hessian  $\\nabla^2f$  are easy to compute as ‚àáf(M, p) = A * p\n‚àá¬≤f(M,p,X) = A*X On the other hand, from the aforementioned Example 9.49 we can also state the Riemannian gradient and Hessian for comparison as grad_f(M, p) = A * p - p * (p' * A * p)\nHess_f(M, p, X) = A * X - p * p' * A * X - X * p' * A * p We can verify that these are the correct at least numerically by calling the  check_gradient check_gradient(M, f, grad_f; plot=true) and the  check_Hessian , which requires a bit more tolerance in its linearity verification check_Hessian(M, f, grad_f, Hess_f; plot=true, error=:error, atol=1e-15) While they look reasonable here and were already derived, for the general case this derivation might be more complicated. Luckily there exist two functions in  ManifoldDiff.jl  that are implemented for several manifolds from  Manifolds.jl , namely  riemannian_gradient (M, p, eG)  that converts a Riemannian gradient  eG= $\\nabla \\tilde f(p)$  into a the Riemannian one  $\\operatorname{grad} f(p)$  and  riemannian_Hessian (M, p, eG, eH, X)  which converts the Euclidean Hessian  eH= $\\nabla^2 \\tilde f(p)[X]$  into  $\\operatorname{Hess} f(p)[X]$ , where we also require the Euclidean gradient  eG= $\\nabla \\tilde f(p)$ . So we can define grad2_f(M, p) = riemannian_gradient(M, p, ‚àáf(get_embedding(M), embed(M, p))) where only formally we here call  embed(M,p)  before passing  p  to the Euclidean gradient, though here (for the Grassmann manifold with Stiefel representation) the embedding function is the identity. Similarly for the Hessian, where in our example the embeddings of both the points and tangent vectors are the identity. function Hess2_f(M, p, X)\n    return riemannian_Hessian(\n        M,\n        p,\n        ‚àáf(get_embedding(M), embed(M, p)),\n        ‚àá¬≤f(get_embedding(M), embed(M, p), embed(M, p, X)),\n        X\n    )\nend And we can again verify these numerically, check_gradient(M, f, grad2_f; plot=true) and check_Hessian(M, f, grad2_f, Hess2_f; plot=true, error=:error, atol=1e-14) which yields the same result, but we see that the Euclidean conversion might be a bit less stable. Now if we want to use these in optimization we would require these two functions to call e.g. p0 = [1.0 0.0; 0.0 1.0; 0.0 0.0; 0.0 0.0; 0.0 0.0]\nr1 = adaptive_regularization_with_cubics(\n    M,\n    f,\n    grad_f,\n    Hess_f,\n    p0;\n    debug=[:Iteration, :Cost, \"\\n\"],\n    return_objective=true,\n    return_state=true,\n)\nq1 = get_solver_result(r1)\nr1 Initial f(x): 0.666814\n# 1     f(x): 0.329582\n# 2     f(x): -0.251913\n# 3     f(x): -0.451908\n# 4     f(x): -0.604753\n# 5     f(x): -0.608791\n# 6     f(x): -0.608797\n# 7     f(x): -0.608797\n\n# Solver state for `Manopt.jl`s Adaptive Regularization with Cubics (ARC)\nAfter 7 iterations\n\n## Parameters\n* Œ∑1 | Œ∑2              : 0.1 | 0.9\n* Œ≥1 | Œ≥2              : 0.1 | 2.0\n* œÉ (œÉmin)             : 0.0004082482904638632 (1.0e-10)\n* œÅ (œÅ_regularization) : 1.0002163851951777 (1000.0)\n* retraction method    : ManifoldsBase.ExponentialRetraction()\n* sub solver state     :\n    | # Solver state for `Manopt.jl`s Lanczos Iteration\n    | After 6 iterations\n    | \n    | ## Parameters\n    | * œÉ                         : 0.0040824829046386315\n    | * # of Lanczos vectors used : 6\n    | \n    | ## Stopping criteria\n    | (a) For the Lanczos Iteration\n    | Stop When _one_ of the following are fulfilled:\n    |   * Max Iteration 6:  reached\n    |   * First order progress with Œ∏=0.5:  not reached\n    | Overall: reached\n    | (b) For the Newton sub solver\n    | Max Iteration 200:    not reached\n    | This indicates convergence: No\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 40:   not reached\n  * |grad f| < 1.0e-9: reached\n  * All Lanczos vectors (5) used:   not reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Iteration = [ (:Iteration, \"# %-6d\"), (:Cost, \"f(x): %f\"), \"\\n\" ] but if you choose to go for the conversions, then, thinking of the embedding and defining two new functions might be tedious. There is a shortcut for these, which performs the change internally, when necessary by specifying  objective_type=:Euclidean . r2 = adaptive_regularization_with_cubics(\n    M,\n    f,\n    ‚àáf,\n    ‚àá¬≤f,\n    p0;\n    # The one line different to specify our grad/Hess are Eucldiean:\n    objective_type=:Euclidean,\n    debug=[:Iteration, :Cost, \"\\n\"],\n    return_objective=true,\n    return_state=true,\n)\nq2 = get_solver_result(r2)\nr2 Initial f(x): 0.666814\n# 1     f(x): 0.329582\n# 2     f(x): -0.251913\n# 3     f(x): -0.451908\n# 4     f(x): -0.604753\n# 5     f(x): -0.608791\n# 6     f(x): -0.608797\n# 7     f(x): -0.608797\n\n# Solver state for `Manopt.jl`s Adaptive Regularization with Cubics (ARC)\nAfter 7 iterations\n\n## Parameters\n* Œ∑1 | Œ∑2              : 0.1 | 0.9\n* Œ≥1 | Œ≥2              : 0.1 | 2.0\n* œÉ (œÉmin)             : 0.0004082482904638632 (1.0e-10)\n* œÅ (œÅ_regularization) : 1.000409105075989 (1000.0)\n* retraction method    : ManifoldsBase.ExponentialRetraction()\n* sub solver state     :\n    | # Solver state for `Manopt.jl`s Lanczos Iteration\n    | After 6 iterations\n    | \n    | ## Parameters\n    | * œÉ                         : 0.0040824829046386315\n    | * # of Lanczos vectors used : 6\n    | \n    | ## Stopping criteria\n    | (a) For the Lanczos Iteration\n    | Stop When _one_ of the following are fulfilled:\n    |   * Max Iteration 6:  reached\n    |   * First order progress with Œ∏=0.5:  not reached\n    | Overall: reached\n    | (b) For the Newton sub solver\n    | Max Iteration 200:    not reached\n    | This indicates convergence: No\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 40:   not reached\n  * |grad f| < 1.0e-9: reached\n  * All Lanczos vectors (5) used:   not reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Iteration = [ (:Iteration, \"# %-6d\"), (:Cost, \"f(x): %f\"), \"\\n\" ] which returns the same result, see distance(M, q1, q2) 5.599906634890012e-16 This conversion also works for the gradients of constraints, and is passed down to sub solvers by default when these are created using the Euclidean objective  $f$ ,  $\\nabla f$  and  $\\nabla^2 f$ ."},{"id":3317,"pagetitle":"Define objectives in the embedding","title":"Summary","ref":"/manopt/stable/tutorials/EmbeddingObjectives/#Summary","content":" Summary If you have the Euclidean gradient (or Hessian) available for a solver call, all you need to provide is  objective_type=:Euclidean  to convert the objective to a Riemannian one."},{"id":3318,"pagetitle":"Define objectives in the embedding","title":"Literature","ref":"/manopt/stable/tutorials/EmbeddingObjectives/#Literature","content":" Literature [Bou23] N.¬†Boumal.  An Introduction to Optimization on Smooth Manifolds . First¬†Edition ( Cambridge University Press, 2023 ). [Ngu23] D.¬†Nguyen.  Operator-Valued Formulas for Riemannian Gradient and Hessian and Families of Tractable Metrics in Riemannian Optimization .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  198 , 135‚Äì164  (2023),  arXiv:2009.10159 ."},{"id":3319,"pagetitle":"Define objectives in the embedding","title":"Technical details","ref":"/manopt/stable/tutorials/EmbeddingObjectives/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. Status `~/work/Manopt.jl/Manopt.jl/tutorials/Project.toml`\n  [47edcb42] ADTypes v1.15.0\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [5ae59095] Colors v0.13.1\n  [31c24e10] Distributions v0.25.120\n  [26cc04aa] FiniteDifferences v0.12.32\n  [7073ff75] IJulia v1.29.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [af67fdf4] ManifoldDiff v0.4.4\n  [1cead3c2] Manifolds v0.10.22\n  [3362f125] ManifoldsBase v1.2.0\n  [0fc0a36d] Manopt v0.5.20 `..`\n  [91a5bcdd] Plots v1.40.16\n  [731186ca] RecursiveArrayTools v3.34.1\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0 This tutorial was last rendered July 8, 2025, 18:43:30."},{"id":3322,"pagetitle":"Print debug output","title":"How to print debug output","ref":"/manopt/stable/tutorials/HowToDebug/#How-to-print-debug-output","content":" How to print debug output Ronny Bergmann This tutorial aims to illustrate how to perform debug output. For that we consider an example that includes a subsolver, to also consider their debug capabilities. The problem itself is hence not the main focus. We consider a nonnegative PCA which we can write as a constraint problem on the Sphere Let‚Äôs first load the necessary packages. using Manopt, Manifolds, Random, LinearAlgebra\nRandom.seed!(42); d = 4\nM = Sphere(d - 1)\nv0 = project(M, [ones(2)..., zeros(d - 2)...])\nZ = v0 * v0'\n#Cost and gradient\nf(M, p) = -tr(transpose(p) * Z * p) / 2\ngrad_f(M, p) = project(M, p, -transpose.(Z) * p / 2 - Z * p / 2)\n# Constraints\ng(M, p) = -p # now p ‚â• 0\nmI = -Matrix{Float64}(I, d, d)\n# Vector of gradients of the constraint components\ngrad_g(M, p) = [project(M, p, mI[:, i]) for i in 1:d] Then we can take a starting point p0 = project(M, [ones(2)..., zeros(d - 3)..., 0.1])"},{"id":3323,"pagetitle":"Print debug output","title":"Simple debug output","ref":"/manopt/stable/tutorials/HowToDebug/#Simple-debug-output","content":" Simple debug output Any solver accepts the keyword  debug= , which in the simplest case can be set to an array of strings, symbols and a number. Strings are printed in every iteration as is (cf.¬† DebugDivider ) and should be used to finish the array with a line break. the last number in the array is used with  DebugEvery  to print the debug only every  $i$ th iteration. Any Symbol is converted into certain debug prints Certain symbols starting with a capital letter are mapped to certain prints, for example  :Cost  is mapped to  DebugCost ()  to print the current cost function value. A full list is provided in the  DebugActionFactory . A special keyword is  :Stop , which is only added to the final debug hook to print the stopping criterion. Any symbol with a small letter is mapped to fields of the  AbstractManoptSolverState  which is used. This way you can easily print internal data, if you know their names. Let‚Äôs look at an example first: if we want to print the current iteration number, the current cost function value as well as the value  œµ  from the  ExactPenaltyMethodState . To keep the amount of print at a reasonable level, we want to only print the debug every twenty-fifth iteration. Then we can write p1 = exact_penalty_method(\n    M, f, grad_f, p0; g=g, grad_g=grad_g,\n    debug = [:Iteration, :Cost, \" | \", (:œµ,\"œµ: %.8f\"), 25, \"\\n\", :Stop]\n); Initial f(x): -0.497512 | œµ: 0.00100000\n# 25    f(x): -0.499449 | œµ: 0.00017783\n# 50    f(x): -0.499996 | œµ: 0.00003162\n# 75    f(x): -0.500000 | œµ: 0.00000562\n# 100   f(x): -0.500000 | œµ: 0.00000100\nThe value of the variable (œµ) is smaller than or equal to its threshold (1.0e-6).\nAt iteration 102 the algorithm performed a step with a change (4.2533629774851707e-7) less than 1.0e-6."},{"id":3324,"pagetitle":"Print debug output","title":"Specifying when to print something","ref":"/manopt/stable/tutorials/HowToDebug/#Specifying-when-to-print-something","content":" Specifying when to print something While in the last step, we specified what to print, this can be extend to even specify  when  to print it. Currently the following four ‚Äúplaces‚Äù are available, ordered by when they appear in an algorithm run. :Start  to print something at the start of the algorithm. At this place all other (the following) places are ‚Äúreset‚Äù, by triggering each of them with an iteration number  0 :BeforeIteration  to print something before an iteration starts :Iteration  to print something  after  an iteration. For example the group of prints from the last code block  [:Iteration, :Cost, \" | \", :œµ, 25,]  is added to this entry. :Stop  to print something when the algorithm stops. In the example, the  :Stop  adds the  DebugStoppingCriterion  is added to this place. Specifying something especially for one of these places is done by specifying a  Pair , so for example  :BeforeIteration => :Iteration  would add the display of the iteration number to be printed  before  the iteration is performed. Changing this in the run does not change the output. Being more precise for the other entries, we could also write p1 = exact_penalty_method(\n    M, f, grad_f, p0; g=g, grad_g=grad_g,\n    debug = [\n        :BeforeIteration => [:Iteration],\n        :Iteration => [:Cost, \" | \", :œµ, \"\\n\"],\n        :Stop => DebugStoppingCriterion(),\n        25,\n    ],\n); Initial f(x): -0.497512 | œµ: 0.001\n# 25    f(x): -0.499449 | œµ: 0.0001778279410038921\n# 50    f(x): -0.499996 | œµ: 3.1622776601683734e-5\n# 75    f(x): -0.500000 | œµ: 5.623413251903474e-6\n# 100   f(x): -0.500000 | œµ: 1.0e-6\nThe value of the variable (œµ) is smaller than or equal to its threshold (1.0e-6).\nAt iteration 102 the algorithm performed a step with a change (4.2533629774851707e-7) less than 1.0e-6. This also illustrates, that instead of  Symbol s we can also always pass down a  DebugAction  directly, for example when there is a reason to create or configure the action more individually than the default from the symbol. Note that the number ( 25 ) yields that all but  :Start  and  :Stop  are only displayed every twenty-fifth iteration."},{"id":3325,"pagetitle":"Print debug output","title":"Subsolver debug","ref":"/manopt/stable/tutorials/HowToDebug/#Subsolver-debug","content":" Subsolver debug Sub solvers have a  sub_kwargs  keyword, such that you can pass keywords to the sub solver as well. This works well if you do not plan to change the subsolver. If you do you can wrap your own  solver_state=  argument in a  decorate_state!  and pass a  debug=  password to this function call. Keywords in a keyword have to be passed as pairs ( :debug => [...] ). For most debugs, there further exists a longer form to specify the format to print. We want to use this to specify the format to print  œµ . This is done by putting the corresponding symbol together with the string to use in formatting into a tuple like  (:œµ,\" | œµ: %.8f\") , where we can already include the divider as well. A main problem now is, that this debug is issued every sub solver call or initialisation, as the following print of just a  .  per sub solver test/call illustrates p3 = exact_penalty_method(\n    M, f, grad_f, p0; g=g, grad_g=grad_g,\n    debug = [\"\\n\",:Iteration, DebugCost(), (:œµ,\" | œµ: %.8f\"), 25, \"\\n\", :Stop],\n    sub_kwargs = [:debug => [\".\"]]\n); Initial f(x): -0.497512 | œµ: 0.00100000\n....................................................................................\n# 25    f(x): -0.499449 | œµ: 0.00017783\n.......................................................................\n# 50    f(x): -0.499996 | œµ: 0.00003162\n..................................................\n# 75    f(x): -0.500000 | œµ: 0.00000562\n..................................................\n# 100   f(x): -0.500000 | œµ: 0.00000100\n....The value of the variable (œµ) is smaller than or equal to its threshold (1.0e-6).\nAt iteration 102 the algorithm performed a step with a change (4.2533629774851707e-7) less than 1.0e-6. The different lengths of the dotted lines come from the fact that ‚Äîat least in the beginning‚Äî the subsolver performs a few steps and each sub solvers step prints a dot. For this issue, there is the next symbol (similar to the  :Stop ) to indicate that a debug set is a subsolver set  :WhenActive , which introduces a  DebugWhenActive  that is only activated when the outer debug is actually active, or another words  DebugEvery  is active itself. Furthermore, we want to print the iteration number  before  printing the sub solvers steps, so we put this into a  Pair , but we can leave the remaining ones as single entries. Finally we also prefix  :Stop  with  \" | \"  and print the iteration number at the time we stop. We get p4 = exact_penalty_method(\n    M,\n    f,\n    grad_f,\n    p0;\n    g=g,\n    grad_g=grad_g,\n    debug=[\n        :BeforeIteration => [:Iteration, \"\\n\"],\n        :Iteration => [DebugCost(), (:œµ, \" | œµ: %.8f\"), \"\\n\"],\n        :Stop,\n        25,\n    ],\n    sub_kwargs=[\n        :debug => [\n            \" | \",\n            :Iteration,\n            :Cost,\n            \"\\n\",\n            :WhenActive,\n            :Stop => [(:Stop, \" | \"), \" | stopped after iteration \", :Iteration, \"\\n\"],\n        ],\n    ],\n); Initial \nf(x): -0.497512 | œµ: 0.00100000\n | Initial f(x): -0.498944\n | # 1     f(x): -0.498969\n | The algorithm reached approximately critical point after 1 iterations; the gradient norm (3.4995246389869776e-5) is less than 0.001.\n | stopped after iteration # 1     \n# 25    \nf(x): -0.499449 | œµ: 0.00017783\n | Initial f(x): -0.499992\n | # 1     f(x): -0.499992\n | # 2     f(x): -0.499992\n | The algorithm reached approximately critical point after 2 iterations; the gradient norm (0.00027436723916614346) is less than 0.001.\n | stopped after iteration # 2     \n# 50    \nf(x): -0.499996 | œµ: 0.00003162\n | Initial f(x): -0.500000\n | # 1     f(x): -0.500000\n | The algorithm reached approximately critical point after 1 iterations; the gradient norm (5.000404403277298e-6) is less than 0.001.\n | stopped after iteration # 1     \n# 75    \nf(x): -0.500000 | œµ: 0.00000562\n | Initial f(x): -0.500000\n | # 1     f(x): -0.500000\n | The algorithm reached approximately critical point after 1 iterations; the gradient norm (4.202215558182483e-6) is less than 0.001.\n | stopped after iteration # 1     \n# 100   \nf(x): -0.500000 | œµ: 0.00000100\nThe value of the variable (œµ) is smaller than or equal to its threshold (1.0e-6).\nAt iteration 102 the algorithm performed a step with a change (4.2533629774851707e-7) less than 1.0e-6. where we now see that the subsolver always only requires one step. Note that since debug of an iteration is happening  after  a step, we see the sub solver run  before  the debug for an iteration number."},{"id":3326,"pagetitle":"Print debug output","title":"Advanced debug output","ref":"/manopt/stable/tutorials/HowToDebug/#Advanced-debug-output","content":" Advanced debug output There is two more advanced variants that can be used. The first is a tuple of a symbol and a string, where the string is used as the format print, that most  DebugAction s have. The second is, to directly provide a  DebugAction . We can for example change the way the  :œµ  is printed by adding a format string and use  DebugCost ()  which is equivalent to using  :Cost . Especially with the format change, the lines are more consistent in length. p2 = exact_penalty_method(\n    M, f, grad_f, p0; g=g, grad_g=grad_g,\n    debug = [:Iteration, DebugCost(), (:œµ,\" | œµ: %.8f\"), 25, \"\\n\", :Stop]\n); Initial f(x): -0.497512 | œµ: 0.00100000\n# 25    f(x): -0.499449 | œµ: 0.00017783\n# 50    f(x): -0.499996 | œµ: 0.00003162\n# 75    f(x): -0.500000 | œµ: 0.00000562\n# 100   f(x): -0.500000 | œµ: 0.00000100\nThe value of the variable (œµ) is smaller than or equal to its threshold (1.0e-6).\nAt iteration 102 the algorithm performed a step with a change (4.2533629774851707e-7) less than 1.0e-6. You can also write your own  DebugAction  functor, where the function to implement has the same signature as the  step  function, that is an  AbstractManoptProblem , an  AbstractManoptSolverState , as well as the current iterate. For example the already mentioned DebugDivider (s)  is given as mutable struct DebugDivider{TIO<:IO} <: DebugAction\n    io::TIO\n    divider::String\n    DebugDivider(divider=\" | \"; io::IO=stdout) = new{typeof(io)}(io, divider)\nend\nfunction (d::DebugDivider)(::AbstractManoptProblem, ::AbstractManoptSolverState, k::Int)\n    (k >= 0) && (!isempty(d.divider)) && (print(d.io, d.divider))\n    return nothing\nend"},{"id":3327,"pagetitle":"Print debug output","title":"Using callbacks","ref":"/manopt/stable/tutorials/HowToDebug/#Using-callbacks","content":" Using callbacks If you prefer to write debugs as callbacks, this is also possible since Manopt 0.5.18. There are two variants, a simple and a default variant, that maybe fits a bit better the scheme introduced before. For the simple variant, you can just implement a function  cb()  to perform what ever you like. We illustrate this as follows, where we wrap the code in a function for better scoping. Here we just count the number of iterations. function run_with_callback()\n    n = 0\n    callback() = (n += 1)\n    exact_penalty_method(M, f, grad_f, p0; g=g, grad_g=grad_g, callback=callback);\n    return n\nend\nrun_with_callback() 103 This ‚Äúsimple‚Äù mode has the disadvantage, that we do not have access to anything else from ‚Äúwithin‚Äù the solver and it is called both in the initialisation (at ‚Äúiterate 0‚Äù), hence it counts one step more than the previous stopping criterion. Therefore, passing a function to  debug=  is the way to activate the (extended) variant, where the callback has to have the same form as the action functor. The following example stores the last gradient the subsolver computed in  last_X , to illustrate how to access elements from even the subsolvers state. function run_with_callback2()\n    last_X = zero_vector(M, p0)\n    callback2(problem, state, k) = copyto!(M, last_X, get_iterate(state), get_gradient(state.sub_state))\n    exact_penalty_method(M, f, grad_f, p0; g=g, grad_g=grad_g, debug=callback2);\n    return last_X\nend\nrun_with_callback2() 4-element Vector{Float64}:\n  1.7887803827258762e-11\n  1.7887803827258762e-11\n -1.1196817936117585e-6\n -1.1196817936117585e-6 The full form here would also be possible, calling  Manopt.DebugCallback (callback2)  or analogously  Manopt.DebugCallback (callback; simple=true)  and use that in arrays or  DebugGroup s as before. The  callback2  can also be part of a whole  debug = [...]  array, similarly within a dictionary to add callbacks only to the end of an algorithm ( :Stop => callback2 ) or to  :BeforeIteration  as illustrated."},{"id":3328,"pagetitle":"Print debug output","title":"Technical details","ref":"/manopt/stable/tutorials/HowToDebug/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. Status `~/work/Manopt.jl/Manopt.jl/tutorials/Project.toml`\n  [47edcb42] ADTypes v1.15.0\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [5ae59095] Colors v0.13.1\n  [31c24e10] Distributions v0.25.120\n  [26cc04aa] FiniteDifferences v0.12.32\n  [7073ff75] IJulia v1.29.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [af67fdf4] ManifoldDiff v0.4.4\n  [1cead3c2] Manifolds v0.10.22\n  [3362f125] ManifoldsBase v1.2.0\n  [0fc0a36d] Manopt v0.5.20 `..`\n  [91a5bcdd] Plots v1.40.16\n  [731186ca] RecursiveArrayTools v3.34.1\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0 This tutorial was last rendered July 8, 2025, 18:43:54."},{"id":3331,"pagetitle":"Record values","title":"How to record data during the iterations","ref":"/manopt/stable/tutorials/HowToRecord/#How-to-record-data-during-the-iterations","content":" How to record data during the iterations Ronny Bergmann The recording and debugging features make it possible to record nearly any data during the iterations. This tutorial illustrates how to: record one value during the iterations; record multiple values during the iterations and access them afterwards; record within a subsolver define an own  RecordAction  to perform individual recordings. Several predefined recordings exist, for example  RecordCost  or  RecordGradient , if the problem the solver uses provides a gradient. For fields of the  State  the recording can also be done  RecordEntry . For other recordings, for example more advanced computations before storing a value, an own  RecordAction  can be defined. We illustrate these using the gradient descent from the  Get started: optimize  tutorial. Here the focus is put on ways to investigate the behaviour during iterations by using Recording techniques. Let‚Äôs first load the necessary packages. using Manopt, Manifolds, Random, ManifoldDiff, LinearAlgebra\nusing ManifoldDiff: grad_distance\nRandom.seed!(42);"},{"id":3332,"pagetitle":"Record values","title":"The objective","ref":"/manopt/stable/tutorials/HowToRecord/#The-objective","content":" The objective We generate data and define our cost and gradient: Random.seed!(42)\nm = 30\nM = Sphere(m)\nn = 800\nœÉ = œÄ / 8\nx = zeros(Float64, m + 1)\nx[2] = 1.0\ndata = [exp(M, x, œÉ * rand(M; vector_at=x)) for i in 1:n]\nf(M, p) = sum(1 / (2 * n) * distance.(Ref(M), Ref(p), data) .^ 2)\ngrad_f(M, p) = sum(1 / n * grad_distance.(Ref(M), data, Ref(p))) grad_f (generic function with 1 method)"},{"id":3333,"pagetitle":"Record values","title":"First examples","ref":"/manopt/stable/tutorials/HowToRecord/#First-examples","content":" First examples For the high level interfaces of the solvers, like  gradient_descent  we have to set  return_state  to  true  to obtain the whole  solver state  and not only the resulting minimizer. Then we can easily use the  record=  option to add recorded values. This keyword accepts  RecordAction s as well as several symbols as shortcuts, for example  :Cost  to record the cost, or if your options have a field  f ,  :f  would record that entry. An overview of the symbols that can be used is given  here . We first just record the cost after every iteration R = gradient_descent(M, f, grad_f, data[1]; record=:Cost, return_state=true) # Solver state for `Manopt.jl`s Gradient Descent\nAfter 58 iterations\n\n## Parameters\n* retraction method: ManifoldsBase.ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0\n    retraction_method=ManifoldsBase.ExponentialRetraction()\n    contraction_factor=0.95\n    sufficient_decrease=0.1\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 200:  not reached\n  * |grad f| < 1.0e-8: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Record\n(Iteration = RecordCost(),) From the returned state, we see that the  GradientDescentState  are encapsulated (decorated) within a  RecordSolverState . For such a state, one can attach different recorders to some operations, currently to  :Start .  :Stop , and  :Iteration , where  :Iteration  is the default when using the  record=  keyword with a  RecordAction  or a  Symbol  as we just did. We can access all values recorded during the iterations by calling  get_record(R, :Iteation)  or since this is the default even shorter get_record(R) 58-element Vector{Float64}:\n 0.6870172325261714\n 0.6239221496686211\n 0.5900244338953802\n 0.569312079535616\n 0.551804825865545\n 0.5429045359832491\n 0.5383847696671529\n 0.5360322830268692\n 0.5348144739486789\n 0.5341773307679919\n ‚ãÆ\n 0.5334801024530125\n 0.5334801024530096\n 0.5334801024530081\n 0.5334801024530073\n 0.5334801024530066\n 0.5334801024530061\n 0.5334801024530059\n 0.5334801024530059\n 0.5334801024530059 To record more than one value, you can pass an array of a mix of symbols and  RecordAction s which formally introduces  RecordGroup . Such a group records a tuple of values in every iteration: R2 = gradient_descent(M, f, grad_f, data[1]; record=[:Iteration, :Cost], return_state=true) # Solver state for `Manopt.jl`s Gradient Descent\nAfter 58 iterations\n\n## Parameters\n* retraction method: ManifoldsBase.ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0\n    retraction_method=ManifoldsBase.ExponentialRetraction()\n    contraction_factor=0.95\n    sufficient_decrease=0.1\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 200:  not reached\n  * |grad f| < 1.0e-8: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),) Here, the symbol  :Cost  is mapped to using the  RecordCost  action. The same holds for  :Iteration  obviously records the current iteration number  i . To access these you can first extract the group of records (that is where the  :Iteration s are recorded; note the plural) and then access the  :Cost  ‚Äú‚Äú‚Äù get_record_action(R2, :Iteration) RecordGroup([RecordIteration(), RecordCost()]) Since  iteration  is the default, we can also omit it here again. To access single recorded values, one can use get_record_action(R2)[:Cost] 58-element Vector{Float64}:\n 0.6870172325261714\n 0.6239221496686211\n 0.5900244338953802\n 0.569312079535616\n 0.551804825865545\n 0.5429045359832491\n 0.5383847696671529\n 0.5360322830268692\n 0.5348144739486789\n 0.5341773307679919\n ‚ãÆ\n 0.5334801024530125\n 0.5334801024530096\n 0.5334801024530081\n 0.5334801024530073\n 0.5334801024530066\n 0.5334801024530061\n 0.5334801024530059\n 0.5334801024530059\n 0.5334801024530059 This can be also done by using a the high level interface  get_record get_record(R2, :Iteration, :Cost) 58-element Vector{Float64}:\n 0.6870172325261714\n 0.6239221496686211\n 0.5900244338953802\n 0.569312079535616\n 0.551804825865545\n 0.5429045359832491\n 0.5383847696671529\n 0.5360322830268692\n 0.5348144739486789\n 0.5341773307679919\n ‚ãÆ\n 0.5334801024530125\n 0.5334801024530096\n 0.5334801024530081\n 0.5334801024530073\n 0.5334801024530066\n 0.5334801024530061\n 0.5334801024530059\n 0.5334801024530059\n 0.5334801024530059 Note that the first symbol again refers to the point where we record (not to the thing we record). We can also pass a tuple as second argument to have our own order within the tuples returned. Switching the order of recorded cost and Iteration can be done using ‚Äú‚Äú‚Äù get_record(R2, :Iteration, (:Iteration, :Cost)) 58-element Vector{Tuple{Int64, Float64}}:\n (1, 0.6870172325261714)\n (2, 0.6239221496686211)\n (3, 0.5900244338953802)\n (4, 0.569312079535616)\n (5, 0.551804825865545)\n (6, 0.5429045359832491)\n (7, 0.5383847696671529)\n (8, 0.5360322830268692)\n (9, 0.5348144739486789)\n (10, 0.5341773307679919)\n ‚ãÆ\n (50, 0.5334801024530125)\n (51, 0.5334801024530096)\n (52, 0.5334801024530081)\n (53, 0.5334801024530073)\n (54, 0.5334801024530066)\n (55, 0.5334801024530061)\n (56, 0.5334801024530059)\n (57, 0.5334801024530059)\n (58, 0.5334801024530059)"},{"id":3334,"pagetitle":"Record values","title":"A more complex example","ref":"/manopt/stable/tutorials/HowToRecord/#A-more-complex-example","content":" A more complex example To illustrate a complicated example let‚Äôs record: the iteration number, cost and gradient field, but only every sixth iteration; the iteration at which we stop. We first generate the problem and the state, to also illustrate the low-level works when not using the high-level interface  gradient_descent . p = DefaultManoptProblem(M, ManifoldGradientObjective(f, grad_f))\ns = GradientDescentState(\n    M;\n    p=copy(data[1]),\n    stopping_criterion=StopAfterIteration(200) | StopWhenGradientNormLess(10.0^-9),\n) # Solver state for `Manopt.jl`s Gradient Descent\n\n## Parameters\n* retraction method: ManifoldsBase.ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0\n    retraction_method=ManifoldsBase.ExponentialRetraction()\n    contraction_factor=0.95\n    sufficient_decrease=0.1\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 200:  not reached\n  * |grad f| < 1.0e-9: not reached\nOverall: not reached\nThis indicates convergence: No We now first build a  RecordGroup  to group the three entries we want to record per iteration. We then put this into a  RecordEvery  to only record this every sixth iteration rI = RecordEvery(\n    RecordGroup([\n        RecordIteration() => :Iteration,\n        RecordCost() => :Cost,\n        RecordEntry(similar(data[1]), :X) => :Gradient,\n    ]),\n    6,\n) RecordEvery(RecordGroup([RecordIteration(), RecordCost(), RecordEntry(:X)]), 6, true) where the notation as a pair with the symbol can be read as ‚ÄúIs accessible by‚Äù. The  record=  keyword with the symbol  :Iteration  is actually the same as we specified here for the first group entry. For recording the final iteration number sI = RecordIteration() RecordIteration() We now combine both into the  RecordSolverState  decorator. It acts completely the same as any  AbstractManoptSolverState  but records something in every iteration additionally. This is stored in a dictionary of  RecordAction s, where  :Iteration  is the action (here the only every sixth iteration group) and the  sI  which is executed at stop. Note that the keyword  record=  in the high level interface  gradient_descent  only would fill the  :Iteration  symbol of said dictionary, but we could also pass pairs like in the following, that is in the form  Symbol => RecordAction  into that keyword to obtain the same as in r = RecordSolverState(s, Dict(:Iteration => rI, :Stop => sI)) # Solver state for `Manopt.jl`s Gradient Descent\n\n## Parameters\n* retraction method: ManifoldsBase.ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0\n    retraction_method=ManifoldsBase.ExponentialRetraction()\n    contraction_factor=0.95\n    sufficient_decrease=0.1\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 200:  not reached\n  * |grad f| < 1.0e-9: not reached\nOverall: not reached\nThis indicates convergence: No\n\n## Record\n(Iteration = RecordEvery(RecordGroup([RecordIteration(), RecordCost(), RecordEntry(:X)]), 6, true), Stop = RecordIteration()) We now call the solver res = solve!(p, r) # Solver state for `Manopt.jl`s Gradient Descent\nAfter 63 iterations\n\n## Parameters\n* retraction method: ManifoldsBase.ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0\n    retraction_method=ManifoldsBase.ExponentialRetraction()\n    contraction_factor=0.95\n    sufficient_decrease=0.1\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 200:  not reached\n  * |grad f| < 1.0e-9: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Record\n(Iteration = RecordEvery(RecordGroup([RecordIteration(), RecordCost(), RecordEntry(:X)]), 6, true), Stop = RecordIteration()) And we can look at the recorded value at  :Stop  to see how many iterations were performed get_record(res, :Stop) 1-element Vector{Int64}:\n 63 and the other values during the iterations are get_record(res, :Iteration, (:Iteration, :Cost)) 10-element Vector{Tuple{Int64, Float64}}:\n (6, 0.5429045359832491)\n (12, 0.5336712822308554)\n (18, 0.5334840986243338)\n (24, 0.5334801877032023)\n (30, 0.5334801043129838)\n (36, 0.5334801024945817)\n (42, 0.5334801024539585)\n (48, 0.5334801024530282)\n (54, 0.5334801024530066)\n (60, 0.5334801024530057) where the last tuple contains the names from the pairs when we generated the record group. So similarly we can use  :Gradient  as specified before to access the recorded gradient."},{"id":3335,"pagetitle":"Record values","title":"Recording from a Subsolver","ref":"/manopt/stable/tutorials/HowToRecord/#Recording-from-a-Subsolver","content":" Recording from a Subsolver One can also record from a subsolver. For that we need a problem that actually requires a subsolver. We take the constraint example from the  How to print debug  tutorial. Maybe read that part for more details on the problem d = 4\nM2 = Sphere(d - 1)\nv0 = project(M2, [ones(2)..., zeros(d - 2)...])\nZ = v0 * v0'\n#Cost and gradient\nf2(M, p) = -tr(transpose(p) * Z * p) / 2\ngrad_f2(M, p) = project(M, p, -transpose.(Z) * p / 2 - Z * p / 2)\n# Constraints\ng(M, p) = -p # now p ‚â• 0\nmI = -Matrix{Float64}(I, d, d)\n# Vector of gradients of the constraint components\ngrad_g(M, p) = [project(M, p, mI[:, i]) for i in 1:d]\np0 = project(M2, [ones(2)..., zeros(d - 3)..., 0.1]) We directly start with recording the sub solvers Iteration. We can specify what to record in the subsolver using the  sub_kwargs  keyword argument with a  Symbol => value  pair. Here we specify to record the iteration and the cost in every sub solvers step. Furthermore, we have to ‚Äúcollect‚Äù this recording after every sub solver run. This is done with the  :Subsolver  keyword in the main  record=  keyword. s1 = exact_penalty_method(\n    M2,\n    f2,\n    grad_f2,\n    p0;\n    g = g,\n    grad_g = grad_g,\n    record = [:Iteration, :Cost, :Subsolver],\n    sub_kwargs = [:record => [:Iteration, :Cost]],\n    return_state=true,\n); Then the first entry of the record contains the iterate, the (main solvers) cost, and the third entry is the recording of the subsolver. get_record(s1)[1] (1, -0.4733019623455375, [(1, -0.4288382393589549), (2, -0.43669534259556914), (3, -0.4374036673499917), (4, -0.43744087180862923)]) When adding a number to not record on every iteration, the  :Subsolver  keyword of course still also only ‚Äúcopies over‚Äù the subsolver recordings when active. But one could avoid allocations on the other runs. This is done, by specifying the sub solver as  :WhenActive s2 = exact_penalty_method(\n    M2,\n    f2,\n    grad_f2,\n    p0;\n    g = g,\n    grad_g = grad_g,\n    record = [:Iteration, :Cost, :Subsolver, 25],\n    sub_kwargs = [:record => [:Iteration, :Cost, :WhenActive]],\n    return_state=true,\n); Then get_record(s2) 4-element Vector{Tuple{Int64, Float64, Vector{Tuple{Int64, Float64}}}}:\n (25, -0.4994494108530985, [(1, -0.4991469152295235)])\n (50, -0.49999564261147317, [(1, -0.49999366842932896)])\n (75, -0.49999997420136083, [(1, -0.4999999614701454)])\n (100, -0.4999999998337046, [(1, -0.49999999981081666)]) Finally, instead of recording iterations, we can also specify to record the stopping criterion and final cost by adding that to  :Stop  of the sub solvers record. Then we can specify, as usual in a tuple, that the  :Subsolver  should record  :Stop  (by default it takes over  :Iteration ) s3 = exact_penalty_method(\n    M2,\n    f2,\n    grad_f2,\n    p0;\n    g = g,\n    grad_g = grad_g,\n    record = [:Iteration, :Cost, (:Subsolver, :Stop), 25],\n    sub_kwargs = [:record => [:Stop => [:Stop, :Cost]]],\n    return_state=true,\n); Then the following displays also the reasons why each of the recorded sub solvers stopped and the corresponding cost get_record(s3) 4-element Vector{Tuple{Int64, Float64, Vector{Tuple{String, Float64}}}}:\n (25, -0.4994494108530985, [(\"The algorithm reached approximately critical point after 1 iterations; the gradient norm (0.00031307624887101047) is less than 0.001.\\n\", -0.4991469152295235)])\n (50, -0.49999564261147317, [(\"The algorithm reached approximately critical point after 1 iterations; the gradient norm (0.0009767910400237622) is less than 0.001.\\n\", -0.49999366842932896)])\n (75, -0.49999997420136083, [(\"The algorithm reached approximately critical point after 1 iterations; the gradient norm (0.0002239629119661262) is less than 0.001.\\n\", -0.4999999614701454)])\n (100, -0.4999999998337046, [(\"The algorithm reached approximately critical point after 1 iterations; the gradient norm (3.8129640908105967e-6) is less than 0.001.\\n\", -0.49999999981081666)])"},{"id":3336,"pagetitle":"Record values","title":"Writing an own RecordActions","ref":"/manopt/stable/tutorials/HowToRecord/#Writing-an-own-[RecordAction](https://manoptjl.org/stable/plans/record/#Manopt.RecordAction)s","content":" Writing an own  RecordAction s Let‚Äôs investigate where we want to count the number of function evaluations, again just to illustrate, since for the gradient this is just one evaluation per iteration. We first define a cost, that counts its own calls. mutable struct MyCost{T}\n    data::T\n    count::Int\nend\nMyCost(data::T) where {T} = MyCost{T}(data, 0)\nfunction (c::MyCost)(M, x)\n    c.count += 1\n    return sum(1 / (2 * length(c.data)) * distance.(Ref(M), Ref(x), c.data) .^ 2)\nend and we define an own, new  RecordAction , which is a functor, that is a struct that is also a function. The function we have to implement is similar to a single solver step in signature, since it might get called every iteration: mutable struct RecordCount <: RecordAction\n    recorded_values::Vector{Int}\n    RecordCount() = new(Vector{Int}())\nend\nfunction (r::RecordCount)(p::AbstractManoptProblem, ::AbstractManoptSolverState, i)\n    if i > 0\n        push!(r.recorded_values, Manopt.get_cost_function(get_objective(p)).count)\n    elseif i < 0 # reset if negative\n        r.recorded_values = Vector{Int}()\n    end\nend Now we can initialize the new cost and call the gradient descent. Note that this illustrates also the last use case since you can pass symbol-action pairs into the  record= array. f3 = MyCost(data) Now for the plain gradient descent, we have to modify the step (to a constant stepsize) and remove the default debug verification whether the cost increases (setting  debug  to  [] ). We also only look at the first 20 iterations to keep this example small in recorded values. We call R3 = gradient_descent(\n    M,\n    f3,\n    grad_f,\n    data[1];\n    record=[:Iteration => [\n        :Iteration,\n        RecordCount() => :Count,\n        :Cost],\n    ],\n    stepsize = ConstantLength(1.0),\n    stopping_criterion=StopAfterIteration(20),\n    debug=[],\n    return_state=true,\n) # Solver state for `Manopt.jl`s Gradient Descent\nAfter 20 iterations\n\n## Parameters\n* retraction method: ManifoldsBase.ExponentialRetraction()\n\n## Stepsize\nConstantLength(1.0; type=:relative)\n\n## Stopping criterion\n\nMax Iteration 20:   reached\nThis indicates convergence: No\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), Main.Notebook.RecordCount([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]), RecordCost()]),) For  :Cost  we already learned how to access them, the  => :Count  introduces an action to obtain the  :Count  symbol as its access. We can again access the whole sets of records get_record(R3) 20-element Vector{Tuple{Int64, Int64, Float64}}:\n (1, 1, 0.5823814423113639)\n (2, 2, 0.540804980234004)\n (3, 3, 0.5345550944722898)\n (4, 4, 0.5336375289938887)\n (5, 5, 0.5335031591890169)\n (6, 6, 0.5334834802310252)\n (7, 7, 0.5334805973984544)\n (8, 8, 0.5334801749902928)\n (9, 9, 0.5334801130855078)\n (10, 10, 0.5334801040117543)\n (11, 11, 0.5334801026815558)\n (12, 12, 0.5334801024865219)\n (13, 13, 0.5334801024579218)\n (14, 14, 0.5334801024537273)\n (15, 15, 0.5334801024531121)\n (16, 16, 0.5334801024530218)\n (17, 17, 0.5334801024530087)\n (18, 18, 0.5334801024530067)\n (19, 19, 0.5334801024530065)\n (20, 20, 0.5334801024530064) this is equivalent to calling  R[:Iteration] . Note that since we introduced  :Count  we can also access a single recorded value using R3[:Iteration, :Count] 20-element Vector{Int64}:\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20 and we see that the cost function is called once per iteration. If we use this counting cost and run the default gradient descent with Armijo line search, we can infer how many Armijo line search backtracks are preformed: f4 = MyCost(data) MyCost{Vector{Vector{Float64}}}([[-0.054658825167894595, -0.5592077846510423, -0.04738273828111257, -0.04682080720921302, 0.12279468849667038, 0.07171438895366239, -0.12930045409417057, -0.22102081626380404, -0.31805333254577767, 0.0065859500152017645  ‚Ä¶  -0.21999168261518043, 0.19570142227077295, 0.340909965798364, -0.0310802190082894, -0.04674431076254687, -0.006088297671169996, 0.01576037011323387, -0.14523596850249543, 0.14526158060820338, 0.1972125856685378], [-0.08192376929745249, -0.5097715132187676, -0.008339904915541005, 0.07289741328038676, 0.11422036270613797, -0.11546739299835748, 0.2296996932628472, 0.1490467170835958, -0.11124820565850364, -0.11790721606521781  ‚Ä¶  -0.16421249630470344, -0.2450575844467715, -0.07570080850379841, -0.07426218324072491, -0.026520181327346338, 0.11555341205250205, -0.0292955762365121, -0.09012096853677576, -0.23470556634911574, -0.026214242996704013], [-0.22951484264859257, -0.6083825348640186, 0.14273766477054015, -0.11947823367023377, 0.05984293499234536, 0.058820835498203126, 0.07577331705863266, 0.1632847202946857, 0.20244385489915745, 0.04389826920203656  ‚Ä¶  0.3222365119325929, 0.009728730325524067, -0.12094785371632395, -0.36322323926212824, -0.0689253407939657, 0.23356953371702974, 0.23489531397909744, 0.078303336494718, -0.14272984135578806, 0.07844539956202407], [-0.0012588500237817606, -0.29958740415089763, 0.036738459489123514, 0.20567651907595125, -0.1131046432541904, -0.06032435985370224, 0.3366633723165895, -0.1694687746143405, -0.001987171245125281, 0.04933779858684409  ‚Ä¶  -0.2399584473006256, 0.19889267065775063, 0.22468755918787048, 0.1780090580180643, 0.023703860700539356, -0.10212737517121755, 0.03807004103115319, -0.20569120952458983, -0.03257704254233959, 0.06925473452536687], [-0.035534309946938375, -0.06645560787329002, 0.14823972268208874, -0.23913346587232426, 0.038347027875883496, 0.10453333143286662, 0.050933995140290705, -0.12319549375687473, 0.12956684644537844, -0.23540367869989412  ‚Ä¶  -0.41471772859912864, -0.1418984610380257, 0.0038321446836859334, 0.23655566917750157, -0.17500681300994742, -0.039189751036839374, -0.08687860620942896, -0.11509948162959047, 0.11378233994840942, 0.38739450723013735], [-0.3122539912469438, -0.3101935557860296, 0.1733113629107006, 0.08968593616209351, -0.1836344261367962, -0.06480023695256802, 0.18165070013886545, 0.19618275767992124, -0.07956460275570058, 0.0325997354656551  ‚Ä¶  0.2845492418767769, 0.17406455870721682, -0.053101230371568706, -0.1382082812981627, 0.005830071475508364, 0.16739264037923055, 0.034365814374995335, 0.09107702398753297, -0.1877250428700409, 0.05116494897806923], [-0.04159442361185588, -0.7768029783272633, 0.06303616666722486, 0.08070518925253539, -0.07396265237309446, -0.06008109299719321, 0.07977141629715745, 0.019511027129056415, 0.08629917589924847, -0.11156298867318722  ‚Ä¶  0.0792587504128044, -0.016444383900170008, -0.181746064577005, -0.01888129512990984, -0.13523922089388968, 0.11358102175659832, 0.07929049608459493, 0.1689565359083833, 0.07673657951723721, -0.1128480905648813], [-0.21221814304651335, -0.5031823821503253, 0.010326342133992458, -0.12438192100961257, 0.04004758695231872, 0.2280527500843805, -0.2096243232022162, -0.16564828762420294, -0.28325749481138984, 0.17033534605245823  ‚Ä¶  -0.13599096505924074, 0.28437770540525625, 0.08424426798544583, -0.1266207606984139, 0.04917635557603396, -0.00012608938533809706, -0.04283220254770056, -0.08771365647566572, 0.14750169103093985, 0.11601120086036351], [0.10683290707435536, -0.17680836277740156, 0.23767458301899405, 0.12011180867097299, -0.029404774462600154, 0.11522028383799933, -0.3318174480974519, -0.17859266746938374, 0.04352373642537759, 0.2530382802667988  ‚Ä¶  0.08879861736692073, -0.004412506987801729, 0.19786810509925895, -0.1397104682727044, 0.09482328498485094, 0.05108149065160893, -0.14578343506951633, 0.3167479772660438, 0.10422673169182732, 0.21573150015891313], [-0.024895624707466164, -0.7473912016432697, -0.1392537238944721, -0.14948896791465557, -0.09765393283580377, 0.04413059403279867, -0.13865379004720355, -0.071032040283992, 0.15604054722246585, -0.10744260463413555  ‚Ä¶  -0.14748067081342833, -0.14743635071251024, 0.0643591937981352, 0.16138827697852615, -0.12656652133603935, -0.06463635704869083, 0.14329582429103488, -0.01113113793821713, 0.29295387893749997, 0.06774523575259782]  ‚Ä¶  [0.011874845316569967, -0.6910596618389588, 0.21275741439477827, -0.014042545524367437, -0.07883613103495014, -0.0021900966696246776, -0.033836430464220496, 0.2925813113264835, -0.04718187201980008, 0.03949680289730036  ‚Ä¶  0.0867736586603294, 0.0404682510051544, -0.24779813848587257, -0.28631514602877145, -0.07211767532456789, -0.15072898498180473, 0.017855923621826746, -0.09795357710255254, -0.14755229203084924, 0.1305005778855436], [0.013457629515450426, -0.3750353654626534, 0.12349883726772073, 0.3521803555005319, 0.2475921439420274, 0.006088649842999206, 0.31203183112392907, -0.036869203979483754, -0.07475746464056504, -0.029297797064479717  ‚Ä¶  0.16867368684091563, -0.09450564983271922, -0.0587273302122711, -0.1326667940553803, -0.25530237980444614, 0.37556905374043376, 0.04922612067677609, 0.2605362549983866, -0.21871556587505667, -0.22915883767386164], [0.03295085436260177, -0.971861604433394, 0.034748713521512035, -0.0494065013245799, -0.01767479281403355, 0.0465459739459587, 0.007470494722096038, 0.003227960072276129, 0.0058328596338402365, -0.037591237446692356  ‚Ä¶  0.03205152122876297, 0.11331109854742015, 0.03044900529526686, 0.017971704993311105, -0.009329252062960229, -0.02939354719650879, 0.022088835776251863, -0.02546111553658854, -0.0026257225461427582, 0.005702111697172774], [0.06968243992532257, -0.7119502191435176, -0.18136614593117445, -0.1695926215673451, 0.01725015359973796, -0.00694164951158388, -0.34621134287344574, 0.024709256792651912, -0.1632255805999673, -0.2158226433583082  ‚Ä¶  -0.14153772108081458, -0.11256850346909901, 0.045109821764180706, -0.1162754336222613, -0.13221711766357983, 0.005365354776191061, 0.012750671705879105, -0.018208207549835407, 0.12458753932455452, -0.31843587960340897], [-0.19830349374441875, -0.6086693423968884, 0.08552341811170468, 0.35781519334042255, 0.15790663648524367, 0.02712571268324985, 0.09855601327331667, -0.05840653973421127, -0.09546429767790429, -0.13414717696055448  ‚Ä¶  -0.0430935804718714, 0.2678584478951765, 0.08780994289014614, 0.01613469379498457, 0.0516187906322884, -0.07383067566731401, -0.1481272738354552, -0.010532317187265649, 0.06555344745952187, -0.1506167863762911], [-0.04347524125197773, -0.6327981074196994, -0.221116680035191, 0.0282207467940456, -0.0855024881522933, 0.12821801740178346, 0.1779499563280024, -0.10247384887512365, 0.0396432464100116, -0.0582580338112627  ‚Ä¶  0.1253893207083573, 0.09628202269764763, 0.3165295473947355, -0.14915034201394833, -0.1376727867817772, -0.004153096613530293, 0.09277957650773738, 0.05917264554031624, -0.12230262590034507, -0.19655728521529914], [-0.10173946348675116, -0.6475660153977272, 0.1260284619729566, -0.11933160462857616, -0.04774310633937567, 0.09093928358804217, 0.041662676324043114, -0.1264739543938265, 0.09605293126911392, -0.16790474428001648  ‚Ä¶  -0.04056684573478108, 0.09351665120940456, 0.15259195558799882, 0.0009949298312580497, 0.09461980828206303, 0.3067004514287283, 0.16129258773733715, -0.18893664085007542, -0.1806865244492513, 0.029319680436405825], [-0.251780954320053, -0.39147463259941456, -0.24359579328578626, 0.30179309757665723, 0.21658893985206484, 0.12304585275893232, 0.28281133086451704, 0.029187615341955325, 0.03616243507191924, 0.029375588909979152  ‚Ä¶  -0.08071746662465404, -0.2176101928258658, 0.20944684921170825, 0.043033273425352715, -0.040505542460853576, 0.17935596149079197, -0.08454569418519972, 0.0545941597033932, 0.12471741052450099, -0.24314124407858329], [0.28156471341150974, -0.6708572780452595, -0.1410302363738465, -0.08322589397277698, -0.022772599832907418, -0.04447265789199677, -0.016448068022011157, -0.07490911512503738, 0.2778432295769144, -0.10191899088372378  ‚Ä¶  -0.057272155080983836, 0.12817478092201395, 0.04623814480781884, -0.12184190164369117, 0.1987855635987229, -0.14533603246124993, -0.16334072868597016, -0.052369977381939437, 0.014904286931394959, -0.2440882678882144], [0.12108727495744157, -0.714787344982596, 0.01632521838262752, 0.04437570556908449, -0.041199280304144284, 0.052984488452616, 0.03796520200156107, 0.2791785910964288, 0.11530429924056099, 0.12178223160398421  ‚Ä¶  -0.07621847481721669, 0.18353870423743013, -0.19066653731436745, -0.09423224997242206, 0.14596847781388494, -0.09747986927777111, 0.16041150122587072, -0.02296513951256738, 0.06786878373578588, 0.15296635978447756]], 0) To not get too many entries let‚Äôs just look at the first 20 iterations again R4 = gradient_descent(\n    M,\n    f4,\n    grad_f,\n    data[1];\n    record=[RecordCount(),],\n    return_state=true,\n) # Solver state for `Manopt.jl`s Gradient Descent\nAfter 58 iterations\n\n## Parameters\n* retraction method: ManifoldsBase.ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0\n    retraction_method=ManifoldsBase.ExponentialRetraction()\n    contraction_factor=0.95\n    sufficient_decrease=0.1\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 200:  not reached\n  * |grad f| < 1.0e-8: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Record\n(Iteration = Main.Notebook.RecordCount([25, 29, 33, 37, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76, 80, 84, 88, 92, 96, 100, 104, 108, 112, 116, 120, 124, 128, 132, 136, 140, 144, 148, 152, 156, 160, 164, 168, 172, 176, 180, 184, 188, 192, 196, 200, 204, 208, 212, 216, 220, 224, 229, 232, 237, 241, 245, 247, 249]),) get_record(R4) 58-element Vector{Int64}:\n  25\n  29\n  33\n  37\n  40\n  44\n  48\n  52\n  56\n  60\n   ‚ãÆ\n 220\n 224\n 229\n 232\n 237\n 241\n 245\n 247\n 249 We can see that the number of cost function calls varies, depending on how many line search backtrack steps were required to obtain a good stepsize."},{"id":3337,"pagetitle":"Record values","title":"Technical details","ref":"/manopt/stable/tutorials/HowToRecord/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. Status `~/work/Manopt.jl/Manopt.jl/tutorials/Project.toml`\n  [47edcb42] ADTypes v1.15.0\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [5ae59095] Colors v0.13.1\n  [31c24e10] Distributions v0.25.120\n  [26cc04aa] FiniteDifferences v0.12.32\n  [7073ff75] IJulia v1.29.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [af67fdf4] ManifoldDiff v0.4.4\n  [1cead3c2] Manifolds v0.10.22\n  [3362f125] ManifoldsBase v1.2.0\n  [0fc0a36d] Manopt v0.5.20 `..`\n  [91a5bcdd] Plots v1.40.16\n  [731186ca] RecursiveArrayTools v3.34.1\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0 This tutorial was last rendered July 8, 2025, 18:44:25."},{"id":3340,"pagetitle":"Implement a solver","title":"How to implementing your own solver","ref":"/manopt/stable/tutorials/ImplementASolver/#How-to-implementing-your-own-solver","content":" How to implementing your own solver Ronny Bergmann When you have used a few solvers from  Manopt.jl  for example like in the opening tutorial  Get started: optimize  you might come to the idea of implementing a solver yourself. After a short introduction of the algorithm we aim to implement, this tutorial first discusses the structural details, for example what a solver consists of and ‚Äúworks with‚Äù. Afterwards, we show how to implement the algorithm. Finally, we discuss how to make the algorithm both nice for the user as well as initialized in a way, that it can benefit from features already available in  Manopt.jl . Note If you have implemented your own solver, we would be very happy to have that within  Manopt.jl  as well, so maybe consider  opening a Pull Request using Manopt, Manifolds, Random"},{"id":3341,"pagetitle":"Implement a solver","title":"Our guiding example: a random walk minimization","ref":"/manopt/stable/tutorials/ImplementASolver/#Our-guiding-example:-a-random-walk-minimization","content":" Our guiding example: a random walk minimization Since most serious algorithms should be implemented in  Manopt.jl  themselves directly, we implement a solver that randomly walks on the manifold and keeps track of the lowest point visited. As for algorithms in  Manopt.jl  we aim to implement this  generically  for any manifold that is implemented using  ManifoldsBase.jl . The random walk minimization Given: a manifold  $\\mathcal M$ a starting point  $p=p^{(0)}$ a cost function  $f: \\mathcal M ‚Üí ‚Ñù$ . a parameter  $\\sigma > 0$ . a retraction  $\\operatorname{retr}_p(X)$  that maps  $X ‚àà T_p\\mathcal M$  to the manifold. We can run the following steps of the algorithm set  $k=0$ set our best point  $q = p^{(0)}$ Repeat until a stopping criterion is fulfilled Choose a random tangent vector  $X^{(k)} ‚àà T_{p^{(k)}}\\mathcal M$  of length  $\\lVert X^{(k)} \\rVert = \\sigma$ ‚ÄúWalk‚Äù along this direction, that is  $p^{(k+1)} = \\operatorname{retr}_{p^{(k)}}(X^{(k)})$ If  $f(p^{(k+1)}) < f(q)$  set q = p^{(k+1)} $  as our new best visited point Return  $q$  as the resulting best point we visited"},{"id":3342,"pagetitle":"Implement a solver","title":"Preliminaries: elements a solver works on","ref":"/manopt/stable/tutorials/ImplementASolver/#Preliminaries:-elements-a-solver-works-on","content":" Preliminaries: elements a solver works on There are two main ingredients a solver needs: a problem to work on and the state of a solver, which ‚Äúidentifies‚Äù the solver and stores intermediate results."},{"id":3343,"pagetitle":"Implement a solver","title":"Specifying the task: an AbstractManoptProblem","ref":"/manopt/stable/tutorials/ImplementASolver/#Specifying-the-task:-an-AbstractManoptProblem","content":" Specifying the task: an  AbstractManoptProblem A problem in  Manopt.jl  usually consists of a manifold (an  AbstractManifold ) and an  AbstractManifoldObjective  describing the function we have and its features. In our case the objective is (just) a  ManifoldCostObjective  that stores cost function  f(M,p) -> R . More generally, it might for example store a gradient function or the Hessian or any other information we have about our task. This is something independent of the solver itself, since it only identifies the problem we want to solve independent of how we want to solve it,¬†or in other words, this type contains all information that is static and independent of the specific solver at hand. Usually the problems variable is called  mp ."},{"id":3344,"pagetitle":"Implement a solver","title":"Specifying a solver: an AbstractManoptSolverState","ref":"/manopt/stable/tutorials/ImplementASolver/#Specifying-a-solver:-an-AbstractManoptSolverState","content":" Specifying a solver: an  AbstractManoptSolverState Everything that is needed by a solver during the iterations, all its parameters, interim values that are needed beyond just one iteration, is stored in a subtype of the  AbstractManoptSolverState . This identifies the solver uniquely. In our case we want to store five things the current iterate  p $=p^{(k)}$ the best visited point  $q$ the variable  $\\sigma > 0$ the retraction  $\\operatorname{retr}$  to use (cf.¬† retractions and inverse retractions ) a criterion, when to stop: a  StoppingCriterion We can defined this as mutable struct RandomWalkState{\n    P,\n    R<:AbstractRetractionMethod,\n    S<:StoppingCriterion,\n} <: AbstractManoptSolverState\n  p::P\n  q::P\n  œÉ::Float64\n  retraction_method::R\n  stop::S\nend The stopping criterion is usually stored in the state‚Äôs  stop  field. If you have a reason to do otherwise, you have one more function to implement (see next section). For ease of use, a constructor can be provided, that for example chooses a good default for the retraction based on a given manifold. function RandomWalkState(M::AbstractManifold, p::P=rand(M);\n    œÉ = 0.1,\n    retraction_method::R=default_retraction_method(M, typeof(p)),\n    stopping_criterion::S=StopAfterIteration(200)\n) where {P, R<:AbstractRetractionMethod, S<:StoppingCriterion}\n    return RandomWalkState{P,R,S}(p, copy(M, p), œÉ, retraction_method, stopping_criterion)\nend Parametrising the state avoid that we have abstract typed fields. The keyword arguments for the retraction and stopping criterion are the ones usually used in  Manopt.jl  and provide an easy way to construct this state now. States usually have a shortened name as their variable, we use  rws  for our state here."},{"id":3345,"pagetitle":"Implement a solver","title":"Implementing your solver","ref":"/manopt/stable/tutorials/ImplementASolver/#Implementing-your-solver","content":" Implementing your solver There is basically only two methods we need to implement for our solver initialize_solver!(mp, rws)  which initialises the solver before the first iteration step_solver!(mp, rws, i)  which implements the  i th iteration, where  i  is given to you as the third parameter get_iterate(rws)  which accesses the iterate from other places in the solver get_solver_result(rws)  returning the solvers final (best) point we reached. By default this would return the last iterate  rws.p  (or more precisely calls  get_iterate ), but since we randomly walk and remember our best point in  q , this has to return  rws.q . The first two functions are in-place functions, that is they modify our solver state  rws . You implement these by multiple dispatch on the types after importing said functions from Manopt: import Manopt: initialize_solver!, step_solver!, get_iterate, get_solver_result The state we defined before has two fields where we use the common names used in  Manopt.jl , that is the  StoppingCriterion  is usually in  stop  and the iterate in  p . If your choice is different, you need to reimplement stop_solver!(mp, rws, i)  to determine whether or not to stop after the  i th iteration. get_iterate(rws)  to access the current iterate We recommend to follow the general scheme with the  stop  field. If you have specific criteria when to stop, consider implementing your own  stopping criterion  instead."},{"id":3346,"pagetitle":"Implement a solver","title":"Initialization and iterate access","ref":"/manopt/stable/tutorials/ImplementASolver/#Initialization-and-iterate-access","content":" Initialization and iterate access For our solver, there is not so much to initialize, just to be safe we should copy over the initial value in  p  we start with, to  q . We do not have to care about remembering the iterate, that is done by  Manopt.jl . For the iterate access we just have to pass  p . function initialize_solver!(mp::AbstractManoptProblem, rws::RandomWalkState)\n    copyto!(M, rws.q, rws.p) # Set p^{(0)} = q\n    return rws\nend\nget_iterate(rws::RandomWalkState) = rws.p\nget_solver_result(rws::RandomWalkState) = rws.q and similarly we implement the step. Here we make use of the fact that the problem (and also the objective in fact) have access functions for their elements, the one we need is  get_cost . function step_solver!(mp::AbstractManoptProblem, rws::RandomWalkState, i)\n    M = get_manifold(mp) # for ease of use get the manifold from the problem\n    X = rand(M; vector_at=p)     # generate a direction\n    X .*= rws.œÉ/norm(M, p, X)\n    # Walk\n    retract!(M, rws.p, rws.p, X, rws.retraction_method)\n    # is the new point better? Then store it\n    if get_cost(mp, rws.p) < get_cost(mp, rws.q)\n        copyto!(M, rws.p, rws.q)\n    end\n    return rws\nend Performance wise we could improve the number of allocations by making  X  also a field of our  rws  but let‚Äôs keep it simple here. We could also store the cost of  q  in the state, but we shall see how to easily also enable this solver to allow for  caching . In practice, however, it is preferable to cache intermediate values like cost of  q  in the state when it can be easily achieved. This way we do not have to deal with overheads of an external cache. Now we can just run the solver already. We take the same example as for the other tutorials We first define our task, the Riemannian Center of Mass from the  Get started: optimize  tutorial. Random.seed!(23)\nn = 100\nœÉ = œÄ / 8\nM = Sphere(2)\np = 1 / sqrt(2) * [1.0, 0.0, 1.0]\ndata = [exp(M, p,  œÉ * rand(M; vector_at=p)) for i in 1:n];\nf(M, p) = sum(1 / (2 * n) * distance.(Ref(M), Ref(p), data) .^ 2) We can now generate the problem with its objective and the state mp = DefaultManoptProblem(M, ManifoldCostObjective(f))\ns = RandomWalkState(M; œÉ = 0.2)\n\nsolve!(mp, s)\nget_solver_result(s) 3-element Vector{Float64}:\n -0.2412674850987521\n  0.8608618657176527\n -0.44800317943876844 The function  solve!  works also in place of  s , but the last line illustrates how to access the result in general; we could also just look at  s.p , but the function  get_iterate  is also used in several other places. We could for example easily set up a second solver to work from a specified starting point with a different  œÉ  like s2 = RandomWalkState(M, [1.0, 0.0, 0.0];  œÉ = 0.1)\nsolve!(mp, s2)\nget_solver_result(s2) 3-element Vector{Float64}:\n 1.0\n 0.0\n 0.0"},{"id":3347,"pagetitle":"Implement a solver","title":"Ease of use I: a high level interface","ref":"/manopt/stable/tutorials/ImplementASolver/#Ease-of-use-I:-a-high-level-interface","content":" Ease of use I: a high level interface Manopt.jl  offers a few additional features for solvers in their high level interfaces, for example  debug=  for debug ,  record=  keywords for debug and recording within solver states or  count=  and  cache  keywords for the objective. We can introduce these here as well with just a few lines of code. There are usually two steps. We further need three internal function from  Manopt.jl using Manopt: get_solver_return, indicates_convergence, status_summary"},{"id":3348,"pagetitle":"Implement a solver","title":"A high level interface using the objective","ref":"/manopt/stable/tutorials/ImplementASolver/#A-high-level-interface-using-the-objective","content":" A high level interface using the objective This could be considered as an interim step to the high-level interface: if objective,¬†a  ManifoldCostObjective  is already initialized, the high level interface consists of the steps possibly decorate the objective generate the problem generate and possibly generate the state call the solver determine the return value We illustrate the step with an in-place variant here. A variant that keeps the given start point unchanged would just add a  copy(M, p)  upfront.  Manopt.jl  provides both variants. function random_walk_algorithm!(\n    M::AbstractManifold,\n    mgo::ManifoldCostObjective,\n    p;\n    œÉ = 0.1,\n    retraction_method::AbstractRetractionMethod=default_retraction_method(M, typeof(p)),\n    stopping_criterion::StoppingCriterion=StopAfterIteration(200),\n    kwargs...,\n)\n    dmgo = decorate_objective!(M, mgo; kwargs...)\n    dmp = DefaultManoptProblem(M, dmgo)\n    s = RandomWalkState(M, [1.0, 0.0, 0.0];\n        œÉ=0.1,\n        retraction_method=retraction_method, stopping_criterion=stopping_criterion,\n    )\n    ds = decorate_state!(s; kwargs...)\n    solve!(dmp, ds)\n    return get_solver_return(get_objective(dmp), ds)\nend random_walk_algorithm! (generic function with 1 method)"},{"id":3349,"pagetitle":"Implement a solver","title":"The high level interface","ref":"/manopt/stable/tutorials/ImplementASolver/#The-high-level-interface","content":" The high level interface Starting from the last section, the usual call a user would prefer is just passing a manifold  M  the cost  f  and maybe a start point  p . function random_walk_algorithm!(M::AbstractManifold, f, p=rand(M); kwargs...)\n    mgo = ManifoldCostObjective(f)\n    return random_walk_algorithm!(M, mgo, p; kwargs...)\nend random_walk_algorithm! (generic function with 3 methods)"},{"id":3350,"pagetitle":"Implement a solver","title":"Ease of Use II: the state summary","ref":"/manopt/stable/tutorials/ImplementASolver/#Ease-of-Use-II:-the-state-summary","content":" Ease of Use II: the state summary For the case that you set  return_state=true  the solver should return a summary of the run. When a  show  method is provided, users can easily read such summary in a terminal. It should reflect its main parameters, if they are not too verbose and provide information about the reason it stopped and whether this indicates convergence. Here it would for example look like import Base: show\nfunction show(io::IO, rws::RandomWalkState)\n    i = get_count(rws, :Iterations)\n    Iter = (i > 0) ? \"After $i iterations\\n\" : \"\"\n    Conv = indicates_convergence(rws.stop) ? \"Yes\" : \"No\"\n    s = \"\"\"\n    # Solver state for `Manopt.jl`s Tutorial Random Walk\n    $Iter\n    ## Parameters\n    * retraction method: $(rws.retraction_method)\n    * œÉ                : $(rws.œÉ)\n\n    ## Stopping criterion\n\n    $(status_summary(rws.stop))\n    This indicates convergence: $Conv\"\"\"\n    return print(io, s)\nend Now the algorithm can be easily called and provides all features of a  Manopt.jl  algorithm. For example to see the summary, we could now just call q = random_walk_algorithm!(M, f; return_state=true) # Solver state for `Manopt.jl`s Tutorial Random Walk\nAfter 200 iterations\n\n## Parameters\n* retraction method: ManifoldsBase.ExponentialRetraction()\n* œÉ                : 0.1\n\n## Stopping criterion\n\nMax Iteration 200:  reached\nThis indicates convergence: No"},{"id":3351,"pagetitle":"Implement a solver","title":"Conclusion & beyond","ref":"/manopt/stable/tutorials/ImplementASolver/#Conclusion-and-beyond","content":" Conclusion & beyond We saw in this tutorial how to implement a simple cost-based algorithm, to illustrate how optimization algorithms are covered in  Manopt.jl . One feature we did not cover is that most algorithms allow for in-place and allocation functions, as soon as they work on more than just the cost, for example use gradients, proximal maps or Hessians. This is usually a keyword argument of the objective and hence also part of the high-level interfaces."},{"id":3352,"pagetitle":"Implement a solver","title":"Technical details","ref":"/manopt/stable/tutorials/ImplementASolver/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. Status `~/work/Manopt.jl/Manopt.jl/tutorials/Project.toml`\n  [47edcb42] ADTypes v1.15.0\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [5ae59095] Colors v0.13.1\n  [31c24e10] Distributions v0.25.120\n  [26cc04aa] FiniteDifferences v0.12.32\n  [7073ff75] IJulia v1.29.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [af67fdf4] ManifoldDiff v0.4.4\n  [1cead3c2] Manifolds v0.10.22\n  [3362f125] ManifoldsBase v1.2.0\n  [0fc0a36d] Manopt v0.5.20 `..`\n  [91a5bcdd] Plots v1.40.16\n  [731186ca] RecursiveArrayTools v3.34.1\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0 This tutorial was last rendered July 8, 2025, 18:44:42."},{"id":3355,"pagetitle":"Optimize on your own manifold","title":"Optimize on your own manifold","ref":"/manopt/stable/tutorials/ImplementOwnManifold/#Optimize-on-your-own-manifold","content":" Optimize on your own manifold Ronny Bergmann When you have used a few solvers from  Manopt.jl  for example like in the opening tutorial  üèîÔ∏è Get started with Manopt.jl  and also familiarized yourself with how to work with manifolds in general at  üöÄ Get Started with  Manifolds.jl , you might come across the point that you want to  implementing a manifold  yourself and use it within  Manopt.jl . A challenge might be, which functions are necessary, since the overall interface of  ManifoldsBase.jl  is maybe not completely necessary. This tutorial aims to help you through these steps to implement necessary parts of a manifold to get started with the  solver  you have in mind."},{"id":3356,"pagetitle":"Optimize on your own manifold","title":"An example problem","ref":"/manopt/stable/tutorials/ImplementOwnManifold/#An-example-problem","content":" An example problem We get started by loading the packages we need. using LinearAlgebra, Manifolds, ManifoldsBase, Random\nusing Manopt\nRandom.seed!(42) We also define the same manifold as in the  implementing a manifold  tutorial. \"\"\"\n    ScaledSphere <: AbstractManifold{‚Ñù}\n\nDefine a sphere of fixed radius\n\n# Fields\n\n* `dimension` dimension of the sphere\n* `radius` the radius of the sphere\n\n# Constructor\n\n    ScaledSphere(dimension,radius)\n\nInitialize the manifold to a certain `dimension` and `radius`,\nwhich by default is set to `1.0`\n\"\"\"\nstruct ScaledSphere <: AbstractManifold{‚Ñù}\n    dimension::Int\n    radius::Float64\nend We would like to compute a mean and/or median similar to  üèîÔ∏è Get started with Manopt.jl! . For given a set of points  $q_1,\\ldots,q_n$  we want to compute [ Kar77 ] \\[  \\operatorname*{arg\\,min}_{p‚àà\\mathcal M}\n  \\frac{1}{2n} \\sum_{i=1}^n d_{\\mathcal M}^2(p, q_i)\\] On the  ScaledSphere  we just defined. We define a few parameters first d = 5  # dimension of the sphere - embedded in R^{d+1}\nr = 2.0 # radius of the sphere\nN = 100 # data set size\n\nM = ScaledSphere(d,r) ScaledSphere(5, 2.0) If we generate a few points # generate 100 points around the north pole\npts = [ [zeros(d)..., M.radius] .+ 0.5.*([rand(d)...,0.5] .- 0.5) for _=1:N]\n# project them onto the r-sphere\npts = [ r/norm(p) .* p for p in pts] Then, before starting with optimization, we need the distance on the manifold, to define the cost function, as well as the logarithmic map to defined the gradient. For both, we here use the ‚Äúlazy‚Äù approach of using the  Sphere  as a fallback. Finally, we have to provide information about how points and tangent vectors are stored on the manifold by implementing their  representation_size  function, which is often required when allocating memory. While we could import ManifoldsBase: distance, log, representation_size\nfunction distance(M::ScaledSphere, p, q)\n    return M.radius * distance(Sphere(M.dimension), p ./ M.radius, q ./ M.radius)\nend\nfunction log(M::ScaledSphere, p, q)\n    return M.radius * log(Sphere(M.dimension), p ./ M.radius, q ./ M.radius)\nend\nrepresentation_size(M::ScaledSphere) = (M.dimension+1,)"},{"id":3357,"pagetitle":"Optimize on your own manifold","title":"Define the cost and gradient","ref":"/manopt/stable/tutorials/ImplementOwnManifold/#Define-the-cost-and-gradient","content":" Define the cost and gradient f(M, q) = sum(distance(M, q, p)^2 for p in pts)\ngrad_f(M,q) = sum( - log(M, q, p) for p in pts)"},{"id":3358,"pagetitle":"Optimize on your own manifold","title":"Defining the necessary functions to run a solver","ref":"/manopt/stable/tutorials/ImplementOwnManifold/#Defining-the-necessary-functions-to-run-a-solver","content":" Defining the necessary functions to run a solver The documentation usually lists the necessary functions in a section ‚ÄúTechnical Details‚Äù close to the end of the documentation of a solver, for our case that is  The gradient descent‚Äôs Technical Details , They list all details, but we can start even step by step here if we are a bit careful."},{"id":3359,"pagetitle":"Optimize on your own manifold","title":"A retraction","ref":"/manopt/stable/tutorials/ImplementOwnManifold/#A-retraction","content":" A retraction We first implement a  retract ion. Informally,¬†given a current point and a direction to ‚Äúwalk into‚Äù we need a function that performs that walk. Since we take an easy one that just projects onto the sphere, we use the  ProjectionRetraction  type. To be precise, we have to implement the  in-place variant retract_project! import ManifoldsBase: retract_project!\nfunction retract_project!(M::ScaledSphere, q, p, X)\n    q .= p .+ X\n    q .*= M.radius / norm(q)\n    return q\nend retract_project! (generic function with 18 methods) The other two technical remarks refer to the step size and the stopping criterion, so if we set these to something simpler, we should already be able to do a first run. We have to specify that we want to use the new retraction, a simple step size and stopping criterion We start with a certain point of cost p0 = [zeros(d)...,1.0]\nf(M,p0) 444.60374551157634 Then we can run our first solver,¬†where we have to overwrite a few defaults, which would use functions we do not (yet) have. Let‚Äôs discuss these in the next steps. q1 = gradient_descent(M, f, grad_f, p0;\n    retraction_method = ProjectionRetraction(),   # state, that we use the retraction from above\n    stepsize = DecreasingLength(M; length=1.0), # A simple step size\n    stopping_criterion = StopAfterIteration(10),  # A simple stopping criterion\n    X = zeros(d+1),                               # how we define/represent tangent vectors\n)\nf(M,q1) 162.4000287847332 We at least see, that the function value decreased."},{"id":3360,"pagetitle":"Optimize on your own manifold","title":"Norm and maximal step size","ref":"/manopt/stable/tutorials/ImplementOwnManifold/#Norm-and-maximal-step-size","content":" Norm and maximal step size To use more advanced stopping criteria and step sizes we first need an  inner (M, p, X) . We also need a  max_stepsize (M) , to avoid having too large steps on positively curved manifolds like our scaled sphere in this example import ManifoldsBase: inner\nimport Manopt: max_stepsize\ninner(M::ScaledSphere, p, X,Y) = dot(X,Y) # inherited from the embedding\n # set the maximal allowed stepsize to injectivity radius.\nManopt.max_stepsize(M::ScaledSphere) = M.radius*œÄ Then we can use the default step size ( ArmijoLinesearch ) and the default stopping criterion, which checks for a small gradient Norm q2 = gradient_descent(M, f, grad_f, p0;\n    retraction_method = ProjectionRetraction(), # as before\n    X = zeros(d+1), # as before\n)\nf(M, q2) 9.772830131357034"},{"id":3361,"pagetitle":"Optimize on your own manifold","title":"Making life easier: default retraction and zero vector","ref":"/manopt/stable/tutorials/ImplementOwnManifold/#Making-life-easier:-default-retraction-and-zero-vector","content":" Making life easier: default retraction and zero vector To initialize tangent vector memory, the function  zero_vector (M,p)  is called. Similarly, the most-used retraction is returned by  default_retraction_method We can use both here, to make subsequent calls to the solver less verbose. We define import ManifoldsBase: zero_vector, default_retraction_method\nzero_vector(M::ScaledSphere, p) = zeros(M.dimension+1)\ndefault_retraction_method(M::ScaledSphere) = ProjectionRetraction() default_retraction_method (generic function with 19 methods) and now we can even just call q3 = gradient_descent(M, f, grad_f, p0)\nf(M, q3) 9.772830131357034 But we for example automatically also get the possibility to obtain debug information like gradient_descent(M, f, grad_f, p0; debug = [:Iteration, :Cost, :Stepsize, 25, :GradientNorm, :Stop, \"\\n\"]); Initial f(x): 444.603746\n# 25    f(x): 9.772833s:0.018299583806109226|grad f(p)|:0.020516914880881486\n# 50    f(x): 9.772830s:0.018299583806109226|grad f(p)|:0.00013449321419330018\nThe algorithm reached approximately critical point after 72 iterations; the gradient norm (9.20733514568335e-9) is less than 1.0e-8. see  How to Print Debug Output  for more details."},{"id":3362,"pagetitle":"Optimize on your own manifold","title":"Technical details","ref":"/manopt/stable/tutorials/ImplementOwnManifold/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. Status `~/work/Manopt.jl/Manopt.jl/tutorials/Project.toml`\n  [47edcb42] ADTypes v1.15.0\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [5ae59095] Colors v0.13.1\n  [31c24e10] Distributions v0.25.120\n  [26cc04aa] FiniteDifferences v0.12.32\n  [7073ff75] IJulia v1.29.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [af67fdf4] ManifoldDiff v0.4.4\n  [1cead3c2] Manifolds v0.10.22\n  [3362f125] ManifoldsBase v1.2.0\n  [0fc0a36d] Manopt v0.5.20 `..`\n  [91a5bcdd] Plots v1.40.16\n  [731186ca] RecursiveArrayTools v3.34.1\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0 This tutorial was last rendered July 8, 2025, 18:45:03."},{"id":3363,"pagetitle":"Optimize on your own manifold","title":"Literature","ref":"/manopt/stable/tutorials/ImplementOwnManifold/#Literature","content":" Literature [Kar77] H.¬†Karcher.  Riemannian center of mass and mollifier smoothing .  Communications¬†on¬†Pure¬†and¬†Applied¬†Mathematics  30 , 509‚Äì541  (1977)."},{"id":3366,"pagetitle":"Speedup using in-place computations","title":"Speedup using in-place evaluation","ref":"/manopt/stable/tutorials/InplaceGradient/#Speedup-using-in-place-evaluation","content":" Speedup using in-place evaluation Ronny Bergmann When it comes to time critical operations, a main ingredient in Julia is given by mutating functions, that is those that compute in place without additional memory allocations. In the following, we illustrate how to do this with  Manopt.jl . Let‚Äôs start with the same function as in  üèîÔ∏è Get started with Manopt.jl  and compute the mean of some points, only that here we use the sphere  $\\mathbb S^{30}$  and  $n=800$  points. From the aforementioned example. We first load all necessary packages. using Manopt, Manifolds, Random, BenchmarkTools\nusing ManifoldDiff: grad_distance, grad_distance!\nRandom.seed!(42); And setup our data Random.seed!(42)\nm = 30\nM = Sphere(m)\nn = 800\nœÉ = œÄ / 8\np = zeros(Float64, m + 1)\np[2] = 1.0\ndata = [exp(M, p, œÉ * rand(M; vector_at=p)) for i in 1:n];"},{"id":3367,"pagetitle":"Speedup using in-place computations","title":"Classical definition","ref":"/manopt/stable/tutorials/InplaceGradient/#Classical-definition","content":" Classical definition The variant from the previous tutorial defines a cost  $f(x)$  and its gradient  $\\operatorname{grad}f(p)$  ‚Äú‚Äú‚Äù f(M, p) = sum(1 / (2 * n) * distance.(Ref(M), Ref(p), data) .^ 2)\ngrad_f(M, p) = sum(1 / n * grad_distance.(Ref(M), data, Ref(p))) grad_f (generic function with 1 method) We further set the stopping criterion to be a little more strict. Then we obtain sc = StopWhenGradientNormLess(3e-10)\np0 = zeros(Float64, m + 1); p0[1] = 1/sqrt(2); p0[2] = 1/sqrt(2)\nm1 = gradient_descent(M, f, grad_f, p0; stopping_criterion=sc); We can also benchmark this as @benchmark gradient_descent($M, $f, $grad_f, $p0; stopping_criterion=$sc) BenchmarkTools.Trial: 90 samples with 1 evaluation per sample.\n Range (min ‚Ä¶ max):  51.678 ms ‚Ä¶ 134.204 ms  ‚îä GC (min ‚Ä¶ max):  9.64% ‚Ä¶ 38.77%\n Time  (median):     53.536 ms               ‚îä GC (median):    11.71%\n Time  (mean ¬± œÉ):   55.776 ms ¬±   9.262 ms  ‚îä GC (mean ¬± œÉ):  12.53% ¬±  3.19%\n\n  ‚ñà‚ñá‚ñÅ‚ñá‚ñÅ‚ñÖ‚ñÇ     ‚ñÅ                                                 \n  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ ‚ñÅ\n  51.7 ms         Histogram: frequency by time         71.5 ms <\n\n Memory estimate: 173.76 MiB, allocs estimate: 1167364."},{"id":3368,"pagetitle":"Speedup using in-place computations","title":"In-place computation of the gradient","ref":"/manopt/stable/tutorials/InplaceGradient/#In-place-computation-of-the-gradient","content":" In-place computation of the gradient We can reduce the memory allocations by implementing the gradient to be evaluated in-place. We do this by using a  functor . The motivation is twofold: on one hand, we want to avoid variables from the global scope, for example the manifold  M  or the  data , being used within the function. Considering to do the same for more complicated cost functions might also be worth pursuing. Here, we store the data (as reference) and one introduce temporary memory to avoid reallocation of memory per  grad_distance  computation. We get struct GradF!{TD,TTMP}\n    data::TD\n    tmp::TTMP\nend\nfunction (grad_f!::GradF!)(M, X, p)\n    fill!(X, 0)\n    for di in grad_f!.data\n        grad_distance!(M, grad_f!.tmp, di, p)\n        X .+= grad_f!.tmp\n    end\n    X ./= length(grad_f!.data)\n    return X\nend For the actual call to the solver, we first have to generate an instance of  GradF!  and tell the solver, that the gradient is provided in an  InplaceEvaluation . We can further also use  gradient_descent!  to even work in-place of the initial point we pass. grad_f2! = GradF!(data, similar(data[1]))\nm2 = deepcopy(p0)\ngradient_descent!(\n    M, f, grad_f2!, m2; evaluation=InplaceEvaluation(), stopping_criterion=sc\n); We can again benchmark this @benchmark gradient_descent!(\n    $M, $f, $grad_f2!, m2; evaluation=$(InplaceEvaluation()), stopping_criterion=$sc\n) setup = (m2 = deepcopy($p0)) BenchmarkTools.Trial: 137 samples with 1 evaluation per sample.\n Range (min ‚Ä¶ max):  35.297 ms ‚Ä¶ 49.118 ms  ‚îä GC (min ‚Ä¶ max): 0.00% ‚Ä¶ 25.92%\n Time  (median):     35.863 ms              ‚îä GC (median):    0.00%\n Time  (mean ¬± œÉ):   36.604 ms ¬±  1.640 ms  ‚îä GC (mean ¬± œÉ):  0.67% ¬±  2.89%\n\n   ‚ñá‚ñá‚ñà                                                         \n  ‚ñá‚ñà‚ñà‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ ‚ñÉ\n  35.3 ms         Histogram: frequency by time        44.1 ms <\n\n Memory estimate: 3.72 MiB, allocs estimate: 6879. which is faster by about a factor of 2 compared to the first solver-call. Note that the results  m1  and  m2  are of course the same. distance(M, m1, m2) 4.8317610992693745e-11"},{"id":3369,"pagetitle":"Speedup using in-place computations","title":"Technical details","ref":"/manopt/stable/tutorials/InplaceGradient/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. Status `~/Repositories/Julia/Manopt.jl/tutorials/Project.toml`\n  [47edcb42] ADTypes v1.14.0\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [5ae59095] Colors v0.13.0\n  [31c24e10] Distributions v0.25.119\n  [26cc04aa] FiniteDifferences v0.12.32\n  [7073ff75] IJulia v1.27.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [af67fdf4] ManifoldDiff v0.4.2\n  [1cead3c2] Manifolds v0.10.17\n  [3362f125] ManifoldsBase v1.1.0\n  [0fc0a36d] Manopt v0.5.14 `..`\n  [91a5bcdd] Plots v1.40.13\n  [731186ca] RecursiveArrayTools v3.33.0 This tutorial was last rendered May 2, 2025, 15:48:41."},{"id":3372,"pagetitle":"How to run stochastic gradient descent","title":"How to run stochastic gradient descent","ref":"/manopt/stable/tutorials/StochasticGradientDescent/#How-to-run-stochastic-gradient-descent","content":" How to run stochastic gradient descent Ronny Bergmann This tutorial illustrates how to use the  stochastic_gradient_descent  solver and different  DirectionUpdateRule s to introduce the average or momentum variant, see  Stochastic Gradient Descent . Computationally, we look at a very simple but large scale problem, the Riemannian Center of Mass or  Fr√©chet mean : for given points  $p_i ‚àà\\mathcal M$ ,  $i=1,‚Ä¶,N$  this optimization problem reads \\[\\operatorname*{arg\\,min}_{x‚àà\\mathcal M} \\frac{1}{2}\\sum_{i=1}^{N}\n  \\operatorname{d}^2_{\\mathcal M}(x,p_i),\\] which of course can be (and is) solved by a gradient descent, see the introductory tutorial or  Statistics in Manifolds.jl . If  $N$  is very large, evaluating the complete gradient might be quite expensive. A remedy is to evaluate only one of the terms at a time and choose a random order for these. We first initialize the packages using Manifolds, Manopt, Random, BenchmarkTools, ManifoldDiff\nusing ManifoldDiff: grad_distance\nRandom.seed!(42); We next generate a (little) large(r) data set n = 5000\nœÉ = œÄ / 12\nM = Sphere(2)\np = 1 / sqrt(2) * [1.0, 0.0, 1.0]\ndata = [exp(M, p,  œÉ * rand(M; vector_at=p)) for i in 1:n]; Note that due to the construction of the points as zero mean tangent vectors, the mean should be very close to our initial point  p . In order to use the stochastic gradient, we now need a function that returns the vector of gradients. There are two ways to define it in  Manopt.jl : either as a single function that returns a vector, or as a vector of functions. The first variant is of course easier to define, but the second is more efficient when only evaluating one of the gradients. For the mean, the gradient is \\[\\operatorname{grad}f(p) = \\sum_{i=1}^N \\operatorname{grad}f_i(x) \\quad \\text{where} \\operatorname{grad}f_i(x) = -\\log_x p_i\\] which we define in  Manopt.jl  in two different ways: either as one function returning all gradients as a vector (see  gradF ), or, maybe more fitting for a large scale problem, as a vector of small gradient functions (see  gradf ) F(M, p) = 1 / (2 * n) * sum(map(q -> distance(M, p, q)^2, data))\ngradF(M, p) = [grad_distance(M, p, q) for q in data]\ngradf = [(M, p) -> grad_distance(M, q, p) for q in data];\np0 = 1 / sqrt(3) * [1.0, 1.0, 1.0] 3-element Vector{Float64}:\n 0.5773502691896258\n 0.5773502691896258\n 0.5773502691896258 The calls are only slightly different, but notice that accessing the second gradient element requires evaluating all logs in the first function, while we only call  one  of the functions in the second array of functions. So while you can use both  gradF  and  gradf  in the following call, the second one is (much) faster: p_opt1 = stochastic_gradient_descent(M, gradF, p) 3-element Vector{Float64}:\n -0.4124602512237471\n  0.7450900936719854\n  0.38494647999455556 @benchmark stochastic_gradient_descent($M, $gradF, $p0) BenchmarkTools.Trial: 1 sample with 1 evaluation per sample.\n Single result which took 6.855 s (9.53% GC) to evaluate,\n with a memory estimate of 7.83 GiB, over 200213003 allocations. p_opt2 = stochastic_gradient_descent(M, gradf, p0) 3-element Vector{Float64}:\n 0.6828818855405705\n 0.17545293717581142\n 0.7091463863243863 @benchmark stochastic_gradient_descent($M, $gradf, $p0) BenchmarkTools.Trial: 2494 samples with 1 evaluation per sample.\n Range (min ‚Ä¶ max):  666.857 Œºs ‚Ä¶ 28.248 ms  ‚îä GC (min ‚Ä¶ max): 0.00% ‚Ä¶ 93.29%\n Time  (median):       1.615 ms              ‚îä GC (median):    0.00%\n Time  (mean ¬± œÉ):     2.002 ms ¬±  1.462 ms  ‚îä GC (mean ¬± œÉ):  7.91% ¬± 11.97%\n\n  ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÅ                ‚ñà                                      \n  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ ‚ñÉ\n  667 Œºs          Histogram: frequency by time         7.24 ms <\n\n Memory estimate: 861.17 KiB, allocs estimate: 20050. This result is reasonably close. But we can improve it by using a  DirectionUpdateRule , namely: On the one hand  MomentumGradient , which requires both the manifold and the initial value, to keep track of the iterate and parallel transport the last direction to the current iterate. The necessary  vector_transport_method  keyword is set to a suitable default on every manifold, see  default_vector_transport_method . We get ‚Äú‚Äú‚Äù p_opt3 = stochastic_gradient_descent(\n    M, gradf, p0; direction=MomentumGradient(; direction=StochasticGradient())\n) 3-element Vector{Float64}:\n  0.8620522047922077\n -0.14633537373883504\n  0.4852339174105711 MG = MomentumGradient(; direction=StochasticGradient());\n@benchmark stochastic_gradient_descent($M, $gradf, p=$p0; direction=$MG) BenchmarkTools.Trial: 800 samples with 1 evaluation per sample.\n Range (min ‚Ä¶ max):  5.210 ms ‚Ä¶ 36.096 ms  ‚îä GC (min ‚Ä¶ max):  0.00% ‚Ä¶ 79.92%\n Time  (median):     5.358 ms              ‚îä GC (median):     0.00%\n Time  (mean ¬± œÉ):   6.247 ms ¬±  2.338 ms  ‚îä GC (mean ¬± œÉ):  10.56% ¬± 15.47%\n\n  ‚ñÜ‚ñà‚ñÖ‚ñÉ                                 ‚ñÇ      ‚ñÅ ‚ñÅ             \n  ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñà‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñÑ‚ñÑ ‚ñá\n  5.21 ms      Histogram: log(frequency) by time     10.6 ms <\n\n Memory estimate: 7.71 MiB, allocs estimate: 200052. And on the other hand the  AverageGradient  computes an average of the last  n  gradients. This is done by p_opt4 = stochastic_gradient_descent(\n    M, gradf, p0; direction=AverageGradient(; n=10, direction=StochasticGradient()), debug=[],\n) 3-element Vector{Float64}:\n  0.09593793200711084\n  0.9330329181600736\n -0.34676431020492327 AG = AverageGradient(; n=10, direction=StochasticGradient(M));\n@benchmark stochastic_gradient_descent($M, $gradf, p=$p0; direction=$AG, debug=[]) BenchmarkTools.Trial: 232 samples with 1 evaluation per sample.\n Range (min ‚Ä¶ max):  18.525 ms ‚Ä¶ 47.458 ms  ‚îä GC (min ‚Ä¶ max): 0.00% ‚Ä¶ 59.70%\n Time  (median):     19.025 ms              ‚îä GC (median):    0.00%\n Time  (mean ¬± œÉ):   21.638 ms ¬±  4.558 ms  ‚îä GC (mean ¬± œÉ):  9.24% ¬± 11.82%\n\n  ‚ñà‚ñÑ          ‚ñÉ‚ñÑ                                               \n  ‚ñà‚ñà‚ñá‚ñÑ‚ñá‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÑ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ ‚ñÖ\n  18.5 ms      Histogram: log(frequency) by time      43.8 ms <\n\n Memory estimate: 21.90 MiB, allocs estimate: 600077. Note that the default  StoppingCriterion  is a fixed number of iterations which helps the comparison here. For both update rules we have to internally specify that we are still in the stochastic setting, since both rules can also be used with the  IdentityUpdateRule  within  gradient_descent . For this not-that-large-scale example we can of course also use a gradient descent with  ArmijoLinesearch , fullGradF(M, p) = 1/n*sum(grad_distance(M, q, p) for q in data)\np_opt5 = gradient_descent(M, F, fullGradF, p0; stepsize=ArmijoLinesearch()) 3-element Vector{Float64}:\n  0.7050420977039097\n -0.006374163035874202\n  0.7091368066253959 but in general it is expected to be a bit slow. AL = ArmijoLinesearch();\n@benchmark gradient_descent($M, $F, $fullGradF, $p0; stepsize=$AL) BenchmarkTools.Trial: 23 samples with 1 evaluation per sample.\n Range (min ‚Ä¶ max):  211.643 ms ‚Ä¶ 270.269 ms  ‚îä GC (min ‚Ä¶ max): 8.55% ‚Ä¶ 17.22%\n Time  (median):     214.943 ms               ‚îä GC (median):    9.80%\n Time  (mean ¬± œÉ):   222.730 ms ¬±  16.469 ms  ‚îä GC (mean ¬± œÉ):  9.99% ¬±  2.97%\n\n    ‚ñà‚ñà     ‚ñÇ                                                     \n  ‚ñÖ‚ñà‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ ‚ñÅ\n  212 ms           Histogram: frequency by time          270 ms <\n\n Memory estimate: 230.57 MiB, allocs estimate: 6338643."},{"id":3373,"pagetitle":"How to run stochastic gradient descent","title":"Technical details","ref":"/manopt/stable/tutorials/StochasticGradientDescent/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. Status `~/work/Manopt.jl/Manopt.jl/tutorials/Project.toml`\n  [47edcb42] ADTypes v1.15.0\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [5ae59095] Colors v0.13.1\n  [31c24e10] Distributions v0.25.120\n  [26cc04aa] FiniteDifferences v0.12.32\n  [7073ff75] IJulia v1.29.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [af67fdf4] ManifoldDiff v0.4.4\n  [1cead3c2] Manifolds v0.10.22\n  [3362f125] ManifoldsBase v1.2.0\n  [0fc0a36d] Manopt v0.5.20 `..`\n  [91a5bcdd] Plots v1.40.16\n  [731186ca] RecursiveArrayTools v3.34.1\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0 This tutorial was last rendered July 8, 2025, 18:46:40."},{"id":3376,"pagetitle":"üèîÔ∏è Get started with Manopt.jl","title":"üèîÔ∏è Get started with Manopt.jl","ref":"/manopt/stable/tutorials/getstarted/#Get-started-with-Manopt.jl","content":" üèîÔ∏è Get started with Manopt.jl Ronny Bergmann This tutorial both introduces the basics of optimisation on manifolds as well as how to use  Manopt.jl  to perform optimisation on manifolds in  Julia . For more theoretical background, see for example [ Car92 ] for an introduction to Riemannian manifolds and [ AMS08 ] or [ Bou23 ] to read more about optimisation thereon. Let  $\\mathcal M$  denote a ( Riemannian manifold  and let  $f:  \\mathcal M ‚Üí ‚Ñù$  be a cost function. The aim is to determine or obtain a point  $p^*$  where  $f$  is  minimal  or in other words  $p^*$  is a  minimizer  of  $f$ . This can also be written as \\[    \\operatorname*{arg\\,min}_{p ‚àà \\mathcal M} f(p)\\] where the aim is to compute the minimizer  $p^*$  numerically. As an example, consider the generalisation of the  (arithmetic) mean . In the Euclidean case with  $d‚àà\\mathbb N$ , that is for  $n‚àà\\mathbb N$  data points  $y_1,\\ldots,y_n ‚àà ‚Ñù^d$  the mean \\[  \\frac{1}{n}\\sum_{i=1}^n y_i\\] can not be directly generalised to data  $q_1,\\ldots,q_n ‚àà \\mathcal M$ , since on a manifold there is no addition available. But the mean can also be characterised as the following minimizer \\[  \\operatorname*{arg\\,min}_{x‚àà‚Ñù^d} \\frac{1}{2n}\\sum_{i=1}^n \\lVert x - y_i\\rVert^2\\] and using the Riemannian distance  $d_\\mathcal M$ , this can be written on Riemannian manifolds, which is the so called  Riemannian Center of Mass  [ Kar77 ] \\[  \\operatorname*{arg\\,min}_{p‚àà\\mathcal M}\n  \\frac{1}{2n} \\sum_{i=1}^n d_{\\mathcal M}^2(p, q_i)\\] Fortunately the gradient can be computed and is \\[ \\frac{1}{n} \\sum_{i=1}^n -\\log_p q_i\\]"},{"id":3377,"pagetitle":"üèîÔ∏è Get started with Manopt.jl","title":"Loading the necessary packages","ref":"/manopt/stable/tutorials/getstarted/#Loading-the-necessary-packages","content":" Loading the necessary packages Let‚Äôs assume you have already installed both  Manopt.jl  and  Manifolds.jl  in Julia (using for example  using Pkg; Pkg.add([\"Manopt\", \"Manifolds\"]) ). Then we can get started by loading both packages as well as  Random.jl  for persistency in this tutorial. using Manopt, Manifolds, Random, LinearAlgebra, ManifoldDiff\nusing ManifoldDiff: grad_distance, prox_distance\nRandom.seed!(42); Now assume we are on the  Sphere $\\mathcal M = \\mathbb S^2$  and we generate some random points ‚Äúaround‚Äù some initial point  $p$ n = 100\nœÉ = œÄ / 8\nM = Sphere(2)\np = 1 / sqrt(2) * [1.0, 0.0, 1.0]\ndata = [exp(M, p,  œÉ * rand(M; vector_at=p)) for i in 1:n]; Now we can define the cost function  $f$  and its (Riemannian) gradient  $\\operatorname{grad} f$  for the Riemannian center of mass: f(M, p) = sum(1 / (2 * n) * distance.(Ref(M), Ref(p), data) .^ 2)\ngrad_f(M, p) = sum(1 / n * grad_distance.(Ref(M), data, Ref(p))); and just call  gradient_descent . For a first start, we do not have to provide more than the manifold, the cost, the gradient, and a starting point, which we just set to the first data point m1 = gradient_descent(M, f, grad_f, data[1]) 3-element Vector{Float64}:\n 0.6868392807355564\n 0.006531599748261925\n 0.7267799809043942 In order to get more details, we further add the  debug=  keyword argument, which act as a  decorator pattern . This way we can easily specify a certain debug to be printed. The goal is to get an output of the form # i | Last Change: [...] | F(x): [...] | but where we also want to fix the display format for the change and the cost numbers (the  [...] ) to have a certain format. Furthermore, the reason why the solver stopped should be printed at the end These can easily be specified using either a Symbol when using the default format for numbers, or a tuple of a symbol and a format-string in the  debug=  keyword that is available for every solver. We can also,¬†for illustration reasons,¬†just look at the first 6 steps by setting a  stopping_criterion= m2 = gradient_descent(M, f, grad_f, data[1];\n    debug=[:Iteration,(:Change, \"|Œîp|: %1.9f |\"),\n        (:Cost, \" F(x): %1.11f | \"), \"\\n\", :Stop],\n    stopping_criterion = StopAfterIteration(6)\n  ) Initial  F(x): 0.32487988924 | \n# 1     |Œîp|: 1.063609017 | F(x): 0.25232524046 | \n# 2     |Œîp|: 0.809858671 | F(x): 0.20966960102 | \n# 3     |Œîp|: 0.616665145 | F(x): 0.18546505598 | \n# 4     |Œîp|: 0.470841764 | F(x): 0.17121604104 | \n# 5     |Œîp|: 0.359345690 | F(x): 0.16300825911 | \n# 6     |Œîp|: 0.274597420 | F(x): 0.15818548927 | \nAt iteration 6 the algorithm reached its maximal number of iterations (6).\n\n3-element Vector{Float64}:\n  0.7533872481682505\n -0.06053107055583637\n  0.6547851890466334 See  here  for the list of available symbols. Technical Detail The  debug=  keyword is actually a list of  DebugActions  added to every iteration, allowing you to write your own ones even. Additionally,  :Stop  is an action added to the end of the solver to display the reason why the solver stopped. The default stopping criterion for  gradient_descent  is, to either stop when the gradient is small ( <1e-9 ) or a max number of iterations is reached (as a fallback). Combining stopping-criteria can be done by  |  or  & . We further pass a number  25  to  debug=  to only an output every  25 th iteration: m3 = gradient_descent(M, f, grad_f, data[1];\n    debug=[:Iteration,(:Change, \"|Œîp|: %1.9f |\"),\n        (:Cost, \" F(x): %1.11f | \"), \"\\n\", :Stop, 25],\n    stopping_criterion = StopWhenGradientNormLess(1e-14) |¬†StopAfterIteration(400),\n) Initial  F(x): 0.32487988924 | \n# 25    |Œîp|: 0.459715605 | F(x): 0.15145076374 | \n# 50    |Œîp|: 0.000551270 | F(x): 0.15145051509 | \nThe algorithm reached approximately critical point after 73 iterations; the gradient norm (9.988871119384563e-16) is less than 1.0e-14.\n\n3-element Vector{Float64}:\n 0.6868392794788668\n 0.006531600680779286\n 0.7267799820836411 We can finally use another way to determine the stepsize, for example a little more expensive  ArmijoLineSeach  than the default  stepsize  rule used on the Sphere. m4 = gradient_descent(M, f, grad_f, data[1];\n    debug=[:Iteration,(:Change, \"|Œîp|: %1.9f |\"),\n        (:Cost, \" F(x): %1.11f | \"), \"\\n\", :Stop, 2],\n      stepsize = ArmijoLinesearch(; contraction_factor=0.999, sufficient_decrease=0.5),\n    stopping_criterion = StopWhenGradientNormLess(1e-14) |¬†StopAfterIteration(400),\n) Initial  F(x): 0.32487988924 | \n# 2     |Œîp|: 0.001318138 | F(x): 0.15145051509 | \n# 4     |Œîp|: 0.000000004 | F(x): 0.15145051509 | \n# 6     |Œîp|: 0.000000000 | F(x): 0.15145051509 | \nThe algorithm reached approximately critical point after 7 iterations; the gradient norm (5.073696618059386e-15) is less than 1.0e-14.\n\n3-element Vector{Float64}:\n 0.6868392794788669\n 0.006531600680779358\n 0.7267799820836413 Then we reach approximately the same point as in the previous run, but in far less steps [f(M, m3)-f(M,m4), distance(M, m3, m4)] 2-element Vector{Float64}:\n 1.6653345369377348e-16\n 1.727269835930624e-16"},{"id":3378,"pagetitle":"üèîÔ∏è Get started with Manopt.jl","title":"Using the tutorial mode","ref":"/manopt/stable/tutorials/getstarted/#Using-the-tutorial-mode","content":" Using the tutorial mode Since a few things on manifolds are a bit different from (classical) Euclidean optimization,  Manopt.jl  has a mode to warn about a few pitfalls. It can be set using Manopt.set_parameter!(:Mode, \"Tutorial\") [ Info: Setting the `Manopt.jl` parameter :Mode to Tutorial. to activate these. Continuing from the example before, one might argue, that the minimizer of  $f$  does not depend on the scaling of the function. In theory this is of course also the case on manifolds, but for the optimizations there is a caveat. When we define the Riemannian mean without the scaling f2(M, p) = sum(1 / 2 * distance.(Ref(M), Ref(p), data) .^ 2)\ngrad_f2(M, p) = sum(grad_distance.(Ref(M), data, Ref(p))); And we consider the gradient at the starting point in norm norm(M, data[1], grad_f2(M, data[1])) 57.47318616893399 On the sphere, when we follow a geodesic, we ‚Äúreturn‚Äù to the start point after length  $2œÄ$ . If we ‚Äúland‚Äù short before the starting point due to a gradient of length just shy of  $2œÄ$ , the line search would take the gradient direction (and not the negative gradient direction) as a start. The line search is still performed, but in this case returns a much too small, maybe even nearly zero step size. In other words, we have to be careful that the optimisation stays a ‚Äúlocal‚Äù argument we use. This is also warned for in  \"Tutorial\"  mode. Calling mX = gradient_descent(M, f2, grad_f2, data[1]) ‚îå Warning: At iteration #0\n‚îÇ the gradient norm (57.47318616893399) is larger that 1.0 times the injectivity radius 3.141592653589793 at the current iterate.\n‚îî @ Manopt ~/work/Manopt.jl/Manopt.jl/src/plans/debug.jl:1164\n‚îå Warning: Further warnings will be suppressed, use DebugWarnIfGradientNormTooLarge(1.0, :Always) to get all warnings.\n‚îî @ Manopt ~/work/Manopt.jl/Manopt.jl/src/plans/debug.jl:1168\n\n3-element Vector{Float64}:\n 0.6868392794870684\n 0.006531600674920825\n 0.7267799820759485 So just by chance it seems we still got nearly the same point as before, but when we for example look when this one stops, that is takes more steps. gradient_descent(M, f2, grad_f2, data[1], debug=[:Stop]); The algorithm reached approximately critical point after 140 iterations; the gradient norm (6.807380063106406e-9) is less than 1.0e-8. This also illustrates one way to deactivate the hints, namely by overwriting the  debug=  keyword, that in  Tutorial  mode contains additional warnings. The other option is to globally reset the  :Mode  back to Manopt.set_parameter!(:Mode, \"\") [ Info: Resetting the `Manopt.jl` parameter :Mode to default."},{"id":3379,"pagetitle":"üèîÔ∏è Get started with Manopt.jl","title":"Example 2: computing the median of symmetric positive definite matrices","ref":"/manopt/stable/tutorials/getstarted/#Example-2:-computing-the-median-of-symmetric-positive-definite-matrices","content":" Example 2: computing the median of symmetric positive definite matrices For the second example let‚Äôs consider the manifold of  $3 √ó 3$  symmetric positive definite matrices  and again 100 random points N = SymmetricPositiveDefinite(3)\nm = 100\nœÉ = 0.005\nq = Matrix{Float64}(I, 3, 3)\ndata2 = [exp(N, q, œÉ * rand(N; vector_at=q)) for i in 1:m]; Instead of the mean, let‚Äôs consider a non-smooth optimisation task: the median can be generalized to Manifolds as the minimiser of the sum of distances, see [ Bac14 ]. We define g(N, q) = sum(1 / (2 * m) * distance.(Ref(N), Ref(q), data2)) g (generic function with 1 method) Since the function is non-smooth, we can not use a gradient-based approach. But since for every summand the  proximal map  is available, we can use the  cyclic proximal point algorithm (CPPA) . We hence define the vector of proximal maps as proxes_g = Function[(N, Œª, q) -> prox_distance(N, Œª / m, di, q, 1) for di in data2]; Besides also looking at a some debug prints, we can also easily record these values. Similarly to  debug= ,  record=  also accepts Symbols, see list  here , to indicate things to record. We further set  return_state  to true to obtain not just the (approximate) minimizer. res = cyclic_proximal_point(N, g, proxes_g, data2[1];\n  debug=[:Iteration,\" | \",:Change,\" | \",(:Cost, \"F(x): %1.12f\"),\"\\n\", 1000, :Stop,\n        ],\n        record=[:Iteration, :Change, :Cost, :Iterate],\n        return_state=true,\n    ); Initial  |  | F(x): 0.005875512856\n# 1000   | Last Change: 0.003704 | F(x): 0.003239019699\n# 2000   | Last Change: 0.000015 | F(x): 0.003238996105\n# 3000   | Last Change: 0.000005 | F(x): 0.003238991748\n# 4000   | Last Change: 0.000002 | F(x): 0.003238990225\n# 5000   | Last Change: 0.000001 | F(x): 0.003238989520\nAt iteration 5000 the algorithm reached its maximal number of iterations (5000). Technical Detail The recording is realised by  RecordActions  that are (also) executed at every iteration. These can also be individually implemented and added to the  record=  array instead of symbols. First, the computed median can be accessed as median = get_solver_result(res) 3√ó3 Matrix{Float64}:\n 1.0          2.12236e-5   0.000398721\n 2.12236e-5   1.00044      0.000141798\n 0.000398721  0.000141798  1.00041 but we can also look at the recorded values. For simplicity (of output), lets just look at the recorded values at iteration 42 get_record(res)[42] (42, 1.0569455860769079e-5, 0.003252547739370045, [0.9998583866917449 0.0002098880312604301 0.0002895445818451581; 0.00020988803126037459 1.0000931572564762 0.0002084371501681892; 0.00028954458184524134 0.0002084371501681892 1.000070920743257]) But we can also access whole series and see that the cost does not decrease that fast; actually, the CPPA might converge relatively slow. For that we can for example access the  :Cost  that was recorded every  :Iterate  as well as the (maybe a little boring)  :Iteration -number in a semi-log-plot. x = get_record(res, :Iteration, :Iteration)\ny = get_record(res, :Iteration, :Cost)\nusing Plots\nplot(x,y,xaxis=:log, label=\"CPPA Cost\")"},{"id":3380,"pagetitle":"üèîÔ∏è Get started with Manopt.jl","title":"Technical details","ref":"/manopt/stable/tutorials/getstarted/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. Status `~/work/Manopt.jl/Manopt.jl/tutorials/Project.toml`\n  [47edcb42] ADTypes v1.15.0\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [5ae59095] Colors v0.13.1\n  [31c24e10] Distributions v0.25.120\n  [26cc04aa] FiniteDifferences v0.12.32\n  [7073ff75] IJulia v1.29.0\n  [8ac3fa9e] LRUCache v1.6.2\n  [af67fdf4] ManifoldDiff v0.4.4\n  [1cead3c2] Manifolds v0.10.22\n  [3362f125] ManifoldsBase v1.2.0\n  [0fc0a36d] Manopt v0.5.20 `..`\n  [91a5bcdd] Plots v1.40.16\n  [731186ca] RecursiveArrayTools v3.34.1\n  [37e2e46d] LinearAlgebra v1.11.0\n  [9a3f8284] Random v1.11.0 This tutorial was last rendered July 8, 2025, 18:47:23."},{"id":3381,"pagetitle":"üèîÔ∏è Get started with Manopt.jl","title":"Literature","ref":"/manopt/stable/tutorials/getstarted/#Literature","content":" Literature [AMS08] P.-A.¬†Absil, R.¬†Mahony and R.¬†Sepulchre.  Optimization Algorithms on Matrix Manifolds  (Princeton University Press, 2008), available online at  press.princeton.edu/chapters/absil/ . [Bac14] M.¬†Baƒç√°k.  Computing medians and means in Hadamard spaces .  SIAM¬†Journal¬†on¬†Optimization  24 , 1542‚Äì1566  (2014),  arXiv:1210.2145 . [Bou23] N.¬†Boumal.  An Introduction to Optimization on Smooth Manifolds . First¬†Edition ( Cambridge University Press, 2023 ). [Car92] M.¬†P.¬†do¬†Carmo.  Riemannian Geometry .  Mathematics: Theory & Applications  (Birkh√§user Boston, Inc., Boston, MA, 1992); p.¬†xiv+300. [Kar77] H.¬†Karcher.  Riemannian center of mass and mollifier smoothing .  Communications¬†on¬†Pure¬†and¬†Applied¬†Mathematics  30 , 509‚Äì541  (1977)."},{"id":3384,"pagetitle":"Error estimation","title":"Error estimation","ref":"/manifolddiffeq/stable/#Error-estimation","content":" Error estimation Methods with time step adaptation require estimating the error of the solution. The error is then forwarded to algorithms from OrdinaryDiffEq.jl, see  documentation ."},{"id":3385,"pagetitle":"Error estimation","title":"ManifoldDiffEq.calculate_eest","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.calculate_eest-Tuple{AbstractManifold, Vararg{Any, 7}}","content":" ManifoldDiffEq.calculate_eest  ‚Äî  Method calculate_eest(M::AbstractManifold, utilde, uprev, u, abstol, reltol, internalnorm, t) Estimate error of a solution of an ODE on manifold  M . Arguments utilde  ‚Äì point on  M  for error estimation, uprev  ‚Äì point from before the current step, u  ‚Äì point after the current step`, abstol  - abolute tolerance, reltol  - relative tolerance, internalnorm  ‚Äì copied  internalnorm  from the integrator, t  ‚Äì time at which the error is estimated. source"},{"id":3386,"pagetitle":"Error estimation","title":"ManifoldDiffEq.reltol_norm","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.reltol_norm-Tuple{AbstractManifold, Any}","content":" ManifoldDiffEq.reltol_norm  ‚Äî  Method reltol_norm(M::AbstractManifold, u) Estimate the fraction  d_{min}/eps(number_eltype(u))  where  d_{min}  is the distance between  u , a point on  M , and the nearest distinct point on  M  representable in the representation of  u . source"},{"id":3387,"pagetitle":"Error estimation","title":"Literature","ref":"/manifolddiffeq/stable/#Literature","content":" Literature"},{"id":3390,"pagetitle":"Examples","title":"Examples","ref":"/manifolddiffeq/stable/#Examples","content":" Examples We take a look at the simple example from In the following code an ODE on a sphere is solved the introductory example from the  lecture notes  by  E. Hairer . We solve the ODE system on the sphere  $\\mathbb S^2$  given by \\[\\begin{pmatrix}\n    \\dot x \\\\\n    \\dot y \\\\\n    \\dot z\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n    0 & z/I_3 & -y/I_2 \\\\\n    -z/I_3 & 0 & x/I_1 \\\\\n    y/I_2& -x/I_1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n    x \\\\\n    y \\\\\n    z\n\\end{pmatrix}\\] using ManifoldDiffEq, Manifolds\nusing GLMakie, LinearAlgebra, Colors\n\nn = 25\n\nŒ∏ = [0;(0.5:n-0.5)/n;1]\nœÜ = [(0:2n-2)*2/(2n-1);2]\nx = [cospi(œÜ)*sinpi(Œ∏) for Œ∏ in Œ∏, œÜ in œÜ]\ny = [sinpi(œÜ)*sinpi(Œ∏) for Œ∏ in Œ∏, œÜ in œÜ]\nz = [cospi(Œ∏) for Œ∏ in Œ∏, œÜ in œÜ]\n\nfunction f2(x, y, z)\n    Iv = [1.6, 1.0, 2/3]\n    p = [x, y, z]\n    A = [0 -z y; z 0 -x; -y x 0]\n    return A * (p./Iv)\nend\n\ntans = f2.(vec(x), vec(y), vec(z))\nu = [a[1] for a in tans]\nv = [a[2] for a in tans]\nw = [a[3] for a in tans]\n\nf = Figure();\nAxis3(f[1,1])\n\narr = GLMakie.arrows!(\n           vec(x), vec(y), vec(z), u, v, w;\n           arrowsize = 0.02, linecolor = (:gray, 0.7), linewidth = 0.0075, lengthscale = 0.1\n)\nsave(\"docs/src/assets/img/first_example_vector_field.png\", f) which looks like Let's set up the manifold, the  sphere  and two different types of problems/solvers A first one that uses the Lie group action of the  Special orthogonal group  acting on data with 2 solvers and direct solvers on the sphere, using 3 other solvers using the idea of frozen coefficients. S2 = Manifolds.Sphere(2)\nu0 = [0.0, sqrt(9/10), sqrt(1/10)]\ntspan = (0, 20.0)\n\nA_lie = LieManifoldDiffEqOperator{Float64}() do u, p, t\n    return hat(SpecialOrthogonal(3), Matrix(I(3)), cross(u, f2(u...)))\nend\nprob_lie = ManifoldODEProblem(A_lie, u0, tspan, S2)\n\nA_frozen = FrozenManifoldDiffEqOperator{Float64}() do u, p, t\n    return f2(u...)\nend\nprob_frozen = ManifoldODEProblem(A_frozen, u0, tspan, S2)\n\naction = RotationAction(Euclidean(3), SpecialOrthogonal(3))\nalg_lie_euler = ManifoldDiffEq.ManifoldLieEuler(S2, ExponentialRetraction(), action)\nalg_lie_rkmk4 = ManifoldDiffEq.RKMK4(S2, ExponentialRetraction(), action)\n\nalg_manifold_euler = ManifoldDiffEq.ManifoldEuler(S2, ExponentialRetraction())\nalg_cg2 = ManifoldDiffEq.CG2(S2, ExponentialRetraction())\nalg_cg23 = ManifoldDiffEq.CG2_3(S2, ExponentialRetraction())\nalg_cg3 = ManifoldDiffEq.CG3(S2, ExponentialRetraction())\n\ndt = 0.1\nsol_lie = solve(prob_lie, alg_lie_euler, dt = dt)\nsol_rkmk4 = solve(prob_lie, alg_lie_rkmk4, dt = dt)\n\nsol_frozen = solve(prob_frozen, alg_manifold_euler, dt=dt)\nsol_frozen_cg2 = solve(prob_frozen, alg_cg2, dt = dt)\nsol_frozen_cg23 = solve(prob_frozen, alg_cg23)\nsol_frozen_cg3 = solve(prob_frozen, alg_cg3, dt = dt)\n\nplot_sol(sol, col) = GLMakie.lines!([u[1] for u in sol.u], [u[2] for u in sol.u], [u[3] for u in sol.u]; linewidth = 2, color=col)\n\nl1 = plot_sol(sol_lie, colorant\"#999933\")\nl2 = plot_sol(sol_rkmk4, colorant\"#DDCC77\")\nl3 = plot_sol(sol_frozen, colorant\"#332288\")\nl4 = plot_sol(sol_frozen_cg2, colorant\"#CCEE88\")\nl5 = plot_sol(sol_frozen_cg23, colorant\"#88CCEE\")\nl6 = plot_sol(sol_frozen_cg3, colorant\"#44AA99\")\nLegend(f[1, 2],\n    [l1, l2, l3, l4, l5, l6],\n    [\"Lie Euler\", \"RKMK4\", \"Euler\", \"CG2\", \"CG2(3)\", \"CG3\"]\n)\nsave(\"docs/src/assets/img/first_example_solutions.png\", f) And the solutions look like Note that  alg_cg23  uses adaptive time stepping."},{"id":3393,"pagetitle":"Frozen coefficients solvers","title":"Frozen coefficients solvers","ref":"/manifolddiffeq/stable/#Frozen-coefficients-solvers","content":" Frozen coefficients solvers An initial value problem manifold ordinary differential equation in the frozen coefficients formulation by Crouch and Grossman, see [ CG93 ]. A frozen coefficients ODE on manifold  $M$  is defined in terms a vector field  $F\\colon (M √ó P √ó ‚Ñù) \\to T_p M$  where  $p$  is the point given as the third argument to  $F$ , with an initial value  $y_0$  and  $P$  is the space of constant parameters. Frozen coefficients mean that we also have means to transport a vector  $X \\in T_p M$  obtained from  $F$  to a different point on a manifold or a different time (parameters are assumed to be constant). This is performed through  operator_vector_transport , an object of a subtype of  AbstractVectorTransportOperator , stored in  FrozenManifoldDiffEqOperator . A solution to this problem is a curve  $y\\colon ‚Ñù\\to M$  such that  $y(0)=y_0$  and for each  $t \\in [0, T]$  we have  $D_t y(t) = F(y(t), p, t)$ . The problem is usually studied for manifolds that are Lie groups or homogeneous manifolds, see[ CMO14 ]. Note that in this formulation  $s$ -stage explicit Runge-Kutta schemes that for  $\\mathbb{R}^n$  are defined by equations \\[\\begin{align*}\nX_1 &= f(u_n, p, t) \\\\\nX_2 &= f(u_n+h a_{2,1} X_1, p, t+c_2 h) \\\\\nX_3 &= f(u_n+h a_{3,1} X_1 + a_{3,2} X_2, p, t+c_3 h) \\\\\n&\\vdots \\\\\nX_s &= f(u_n+h a_{s,1} X_1 + a_{s,2} X_2 + \\dots + a_{s,s-1} X_{s-1}, p, t+c_s h) \\\\\nu_{n+1} &= u_n + h\\sum_{i=1}^s b_i X_i\n\\end{align*}\\] for general manifolds read \\[\\begin{align*}\nX_1 &= f(u_n, p, t) \\\\\nu_{n,2,1} &= \\exp_{u_n}(h a_{2,1} X_1) \\\\\nX_2 &= f(u_{n,2,1}, p, t+c_2 h) \\\\\nu_{n,3,1} &= \\exp_{u_n}(h a_{3,1} X_1) \\\\\nu_{n,3,2} &= \\exp_{u_{n,3,1}}(\\mathcal P_{u_{n,3,1}\\gets u_{n,2,1}} h a_{3,2} X_2) \\\\\nX_3 &= f(u_{n,3,2}, p, t+c_3 h) \\\\\n&\\vdots \\\\\nX_s &= f(u_{n,s,s-1}, p, t+c_s h) \\\\\nX_{b,1} &= X_1 \\\\\nu_{b,1} &= \\exp_{u_n}(h b_1 X_{b,1}) \\\\\nX_{b,2} &= \\mathcal P_{u_{b,1} \\gets u_{n,2,1}} X_2 \\\\\nu_{b,2} &= \\exp_{u_{b,1}}(h b_2 X_{b,2}) \\\\\n&\\vdots \\\\\nX_{b,s} &= \\mathcal P_{u_{b,s-1} \\gets u_{n,s,s-1}} X_s \\\\\nu_{n+1} &= \\exp_{u_{b,s-1}}(h b_s X_{b,s})\n\\end{align*}\\] Vector transports correspond to handling frozen coefficients. Note that the implementation allows for easy substitution of methods used for calculation of the exponential map (for example to use an approximation) and vector transport (if the default vector transport is not suitable for the problem). It is desirable to use a flat vector transport instead of a torsion-free one when available, for example the plus or minus Cartan-Schouten connections on Lie groups."},{"id":3394,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.CG2","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.CG2","content":" ManifoldDiffEq.CG2  ‚Äî  Type CG2 A Crouch-Grossmann algorithm of second order for problems in the  ExplicitManifoldODEProblemType  formulation. The Butcher tableau is identical to the Euclidean RK2: \\[\\begin{array}{c|cc}\n0 & 0 \\\\\n\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n\\hline\n& 0 & 1\n\\end{array}\\] source"},{"id":3395,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.CG2Cache","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.CG2Cache","content":" ManifoldDiffEq.CG2Cache  ‚Äî  Type CG2Cache Mutable cache for  CG2 . source"},{"id":3396,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.CG2_3","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.CG2_3","content":" ManifoldDiffEq.CG2_3  ‚Äî  Type CG2_3 A Crouch-Grossmann algorithm of order 2(3) for problems in the  ExplicitManifoldODEProblemType  formulation. The Butcher tableau reads (see tableau (5) of [ EM98 ]): \\[\\begin{array}{c|ccc}\n0 & 0 \\\\\n\\frac{3}{4} & \\frac{3}{4} & 0 \\\\\n\\frac{17}{24} & \\frac{119}{216} & \\frac{17}{108} & 0\\\\\n\\hline\n& \\frac{3}{4} & \\frac{31}{4} & \\frac{-15}{2}\n& \\frac{13}{51} & -\\frac{2}{3} & \\frac{24}{17}\n\\end{array}\\] The last row is used for error estimation. source"},{"id":3397,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.CG2_3Cache","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.CG2_3Cache","content":" ManifoldDiffEq.CG2_3Cache  ‚Äî  Type CG2_3Cache Cache for  CG2_3 . source"},{"id":3398,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.CG3","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.CG3","content":" ManifoldDiffEq.CG3  ‚Äî  Type CG3 A Crouch-Grossmann algorithm of second order for problems in the  ExplicitManifoldODEProblemType  formulation. See tableau 6.1 of [ OM99 ]: \\[\\begin{array}{c|ccc}\n0 & 0 \\\\\n\\frac{3}{4} & \\frac{3}{4} & 0 \\\\\n\\frac{17}{24} & \\frac{119}{216} & \\frac{17}{108} & 0\\\\\n\\hline\n& \\frac{13}{51} & -\\frac{2}{3} & \\frac{24}{17}\n\\end{array}\\] source"},{"id":3399,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.CG3Cache","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.CG3Cache","content":" ManifoldDiffEq.CG3Cache  ‚Äî  Type CG3Cache Mutable cache for  CG3 . source"},{"id":3400,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.CG4a","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.CG4a","content":" ManifoldDiffEq.CG4a  ‚Äî  Type CG4a A Crouch-Grossmann algorithm of second order for problems in the  ExplicitManifoldODEProblemType  formulation. See coefficients from Example 1 of [ JMO00 ]. source"},{"id":3401,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.CG4aCache","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.CG4aCache","content":" ManifoldDiffEq.CG4aCache  ‚Äî  Type CG4aCache Mutable cache for  CG4a . source"},{"id":3402,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.ManifoldEuler","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.ManifoldEuler","content":" ManifoldDiffEq.ManifoldEuler  ‚Äî  Type ManifoldEuler The manifold Euler algorithm for problems in the  ExplicitManifoldODEProblemType  formulation. source"},{"id":3403,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.ManifoldEulerCache","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.ManifoldEulerCache","content":" ManifoldDiffEq.ManifoldEulerCache  ‚Äî  Type ManifoldEulerCache Mutable cache for  ManifoldEuler . source"},{"id":3404,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.ManifoldEulerConstantCache","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.ManifoldEulerConstantCache","content":" ManifoldDiffEq.ManifoldEulerConstantCache  ‚Äî  Type ManifoldEulerConstantCache Cache for  ManifoldEuler . source"},{"id":3405,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.ExplicitManifoldODEProblemType","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.ExplicitManifoldODEProblemType","content":" ManifoldDiffEq.ExplicitManifoldODEProblemType  ‚Äî  Type ExplicitManifoldODEProblemType An initial value problem manifold ordinary differential equation in the frozen coefficients formulation by Crouch and Grossman, see [ CG93 ]. A frozen coefficients ODE on manifold  $M$  is defined in terms a vector field  $F: (M √ó P √ó ‚Ñù) \\to T_p M$  where  $p$  is the point given as the third argument to  $F$ , with an initial value  $y‚ÇÄ$  and  $P$  is the space of constant parameters. A solution to this problem is a curve  $y:‚Ñù\\to M$  such that  $y(0)=y‚ÇÄ$  and for each  $t ‚àà [0, T]$  we have  $D_t y(t) = F(y(t), p, t)$ , Note Proofs of convergence and order have several assumptions, including time-independence of  $F$ . Integrators may not work well if these assumptions do not hold. source"},{"id":3406,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.FrozenManifoldDiffEqOperator","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.FrozenManifoldDiffEqOperator","content":" ManifoldDiffEq.FrozenManifoldDiffEqOperator  ‚Äî  Type FrozenManifoldDiffEqOperator{T<:Number,TM<:AbstractManifold,TF,TVT} <: AbstractSciMLOperator{T} DiffEq operator on manifolds in the frozen vector field formulation. source"},{"id":3407,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.AbstractVectorTransportOperator","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.AbstractVectorTransportOperator","content":" ManifoldDiffEq.AbstractVectorTransportOperator  ‚Äî  Type AbstractVectorTransportOperator Abstract type for vector transport operators in the frozen coefficients formulation. source"},{"id":3408,"pagetitle":"Frozen coefficients solvers","title":"ManifoldDiffEq.DefaultVectorTransportOperator","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.DefaultVectorTransportOperator","content":" ManifoldDiffEq.DefaultVectorTransportOperator  ‚Äî  Type (vto::DefaultVectorTransportOperator)(M::AbstractManifold, p, X, q, params, t_from, t_to) In the frozen coefficient formulation, transport tangent vector  X  such that  X = f(p, params, t_from)  to point  q  at time  t_to . This provides a sort of estimation of  f(q, params, t_to) . source"},{"id":3409,"pagetitle":"Frozen coefficients solvers","title":"Literature","ref":"/manifolddiffeq/stable/#Literature","content":" Literature"},{"id":3412,"pagetitle":"Home","title":"ManifoldDiffEq","ref":"/manifolddiffeq/stable/#ManifoldDiffEq","content":" ManifoldDiffEq The package  ManifoldDiffEq  aims to provide a library of differential equation solvers on manifolds. The library is built on top of  Manifolds.jl  and follows the interface of  OrdinaryDiffEq.jl . The code below demonstrates usage of  ManifoldDiffEq  to solve a simple equation and visualize the results. Methods implemented in this library are described for example in[ HLW10 ]."},{"id":3413,"pagetitle":"Home","title":"ManifoldDiffEq.ManifoldODEProblem","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.ManifoldODEProblem","content":" ManifoldDiffEq.ManifoldODEProblem  ‚Äî  Type ManifoldODEProblem A general problem for ODE problems on Riemannian manifolds. Fields f  the tangent vector field  f(u,p,t) u0  the initial condition tspan  time interval for the solution p  constant parameters for  f ` kwargs  A callback to be applied to every solver which uses the problem. problem_type  type of problem manifold  the manifold the vector field is defined on source"},{"id":3414,"pagetitle":"Home","title":"Literature","ref":"/manifolddiffeq/stable/#Literature","content":" Literature"},{"id":3417,"pagetitle":"Internals","title":"Internal documentation","ref":"/manifolddiffeq/stable/#Internal-documentation","content":" Internal documentation This page documents the internal types and methods of  ManifoldDiffEq.jl ."},{"id":3418,"pagetitle":"Internals","title":"Functions","ref":"/manifolddiffeq/stable/#Functions","content":" Functions"},{"id":3419,"pagetitle":"Internals","title":"ManifoldDiffEq.AbstractManifoldDiffEqAlgorithm","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.AbstractManifoldDiffEqAlgorithm","content":" ManifoldDiffEq.AbstractManifoldDiffEqAlgorithm  ‚Äî  Type abstract type AbstractManifoldDiffEqAlgorithm end A subtype of  OrdinaryDiffEqAlgorithm  for manifold-aware algorithms. source"},{"id":3420,"pagetitle":"Internals","title":"ManifoldDiffEq.ManifoldInterpolationData","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.ManifoldInterpolationData","content":" ManifoldDiffEq.ManifoldInterpolationData  ‚Äî  Type struct ManifoldInterpolationData end Inspired by  OrdinaryDiffEq.InterpolationData . The main difference is using on-manifold interpolation instead of the Euclidean one. source"},{"id":3421,"pagetitle":"Internals","title":"ManifoldDiffEq.AbstractManifoldDiffEqAdaptiveAlgorithm","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.AbstractManifoldDiffEqAdaptiveAlgorithm","content":" ManifoldDiffEq.AbstractManifoldDiffEqAdaptiveAlgorithm  ‚Äî  Type AbstractManifoldDiffEqAdaptiveAlgorithm <: AbstractManifoldDiffEqAlgorithm An abstract subtype of  AbstractManifoldDiffEqAlgorithm  for adaptive algorithms. This is the manifold-aware analogue of  OrdinaryDiffEqAdaptiveAlgorithm . source"},{"id":3422,"pagetitle":"Internals","title":"ManifoldDiffEq.ManifoldODESolution","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.ManifoldODESolution","content":" ManifoldDiffEq.ManifoldODESolution  ‚Äî  Type struct ManifoldODESolution{T} end Counterpart of  SciMLBase.ODESolution . It doesn't use the  N  parameter (because it is not a generic manifold concept) and fields  u_analytic ,  errors ,  alg_choice ,  original ,  tslocation  and  resid  (because we don't use them currently in  ManifoldDiffEq.jl ). Type parameter  T  denotes scalar floating point type of the solution Fields: u : the representation of the ODE solution. Uses a nested power manifold representation. t : time points at which values in  u  were calculated. k : the representation of the  f  function evaluations at time points  k . Uses a nested power manifold representation. prob : original problem that was solved. alg :  AbstractManifoldDiffEqAlgorithm  used to obtain the solution. interp ManifoldInterpolationData . It is used for calculating solution values at times  t  other then the ones at which it was saved. dense :  true  if ODE solution is saved at every step and  false  otherwise. stats :  DEStats  of the solver retcode :  ReturnCode  of the solution. source"},{"id":3425,"pagetitle":"Lie group action solvers","title":"Lie group solvers","ref":"/manifolddiffeq/stable/#Lie-group-solvers","content":" Lie group solvers An initial value problem manifold ordinary differential equation in the Lie action formulation. A Lie ODE on manifold  $M$  is defined in terms a vector field  $F: (M √ó P √ó ‚Ñù) \\to ùî§$  where  $ùî§$  is the Lie algebra of a Lie group  $G$  acting on  $M$ , with an initial value  $y_0$  and  $P$  is the space of constant parameters. A solution to this problem is a curve  $y\\colon ‚Ñù\\to M$  such that  $y(0)=y_0$  and for each  $t \\in [0, T]$  we have  $D_t y(t) = f(y(t), p, t)\\circ y(t)$ , where the  $\\circ$  is defined as \\[X\\circ m = \\frac{d}{dt}\\vert_{t=0} \\exp(tZ)\\cdot m\\] and  $\\cdot$  is the group action of  $G$  on  $M$ . The Lie group  $G$  must act transitively on  $M$ , that is for each pair of points  $p, q$  on  $M$  there is an element  $a \\in G$  such that  $a\\cdot p = q$ . See for example [ CMO14 ] for details."},{"id":3426,"pagetitle":"Lie group action solvers","title":"ManifoldDiffEq.ManifoldLieEuler","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.ManifoldLieEuler","content":" ManifoldDiffEq.ManifoldLieEuler  ‚Äî  Type ManifoldLieEuler The manifold Lie-Euler algorithm for problems in the  LieODEProblemType  formulation. source"},{"id":3427,"pagetitle":"Lie group action solvers","title":"ManifoldDiffEq.ManifoldLieEulerCache","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.ManifoldLieEulerCache","content":" ManifoldDiffEq.ManifoldLieEulerCache  ‚Äî  Type ManifoldLieEulerCache Mutable cache for  ManifoldLieEuler . source"},{"id":3428,"pagetitle":"Lie group action solvers","title":"ManifoldDiffEq.ManifoldLieEulerConstantCache","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.ManifoldLieEulerConstantCache","content":" ManifoldDiffEq.ManifoldLieEulerConstantCache  ‚Äî  Type ManifoldLieEulerConstantCache Constant cache for  ManifoldLieEuler . source"},{"id":3429,"pagetitle":"Lie group action solvers","title":"ManifoldDiffEq.RKMK4","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.RKMK4","content":" ManifoldDiffEq.RKMK4  ‚Äî  Type RKMK4 The Lie group variant of fourth-order Runge-Kutta algorithm for problems in the  LieODEProblemType  formulation, called Runge-Kutta Munthe-Kaas. The Butcher tableau is: \\[\\begin{array}{c|cccc}\n0 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{2} & 0 \\\\\n\\frac{1}{2} & \\frac{1}{2} & 0 \\\\\n1 & 0 & 0 & 1 & 0\\\\\n\\hline\n& \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{6} & \\frac{1}{6}\n\\end{array}\\] For more details see [ MO99 ]. source"},{"id":3430,"pagetitle":"Lie group action solvers","title":"ManifoldDiffEq.RKMK4Cache","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.RKMK4Cache","content":" ManifoldDiffEq.RKMK4Cache  ‚Äî  Type RKMK4Cache Mutable cache for  RKMK4 . source"},{"id":3431,"pagetitle":"Lie group action solvers","title":"ManifoldDiffEq.RKMK4ConstantCache","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.RKMK4ConstantCache","content":" ManifoldDiffEq.RKMK4ConstantCache  ‚Äî  Type RKMK4ConstantCache Constant cache for  RKMK4 . source"},{"id":3432,"pagetitle":"Lie group action solvers","title":"ManifoldDiffEq.LieODEProblemType","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.LieODEProblemType","content":" ManifoldDiffEq.LieODEProblemType  ‚Äî  Type LieODEProblemType An initial value problem manifold ordinary differential equation in the Lie action formulation. A Lie ODE on manifold  $M$  is defined in terms a vector field  $F: (‚Ñù √ó P √ó M) \\to ùî§$  where  $ùî§$  is the Lie algebra of a Lie group  $G$  acting on  $M$ , with an initial value  $y‚ÇÄ$  and  $P$  is the space of constant parameters. A solution to this problem is a curve  $y:‚Ñù\\to M$  such that  $y(0)=y‚ÇÄ$  and for each  $t ‚àà [0, T]$  we have  $D_t y(t) = F(y(t), p, t)‚àòy(t)$ , where the  $‚àò$  is defined as \\[X‚àòm = \\frac{d}{dt}\\vert_{t=0} \\exp(tZ)‚ãÖm\\] and  $‚ãÖ$  is the group action of  $G$  on  $M$ . Note Proofs of convergence and order have several assumptions, including time-independence of  $F$ . Integrators may not work well if these assumptions do not hold. source"},{"id":3433,"pagetitle":"Lie group action solvers","title":"ManifoldDiffEq.LieManifoldDiffEqOperator","ref":"/manifolddiffeq/stable/#ManifoldDiffEq.LieManifoldDiffEqOperator","content":" ManifoldDiffEq.LieManifoldDiffEqOperator  ‚Äî  Type LieManifoldDiffEqOperator{T<:Number,TF} <: AbstractSciMLOperator{T} DiffEq operator on manifolds in the Lie group action formulation. source"},{"id":3434,"pagetitle":"Lie group action solvers","title":"Literature","ref":"/manifolddiffeq/stable/#Literature","content":" Literature"},{"id":3437,"pagetitle":"Notation","title":"Notation","ref":"/manifolddiffeq/stable/#Notation","content":" Notation Notation of ManifoldDiffEq.jl mostly follows the  notation of Manifolds.jl . There are, however, a few changes to more closely match the notation of the DiffEq ecosystem. Namely: u  is often used to denote points on a manifold. Tangent vectors are usually denoted by  $X$ ,  $Y$  but some places may use the symbol  $k$ . Parameters of the solved function are denoted either by  $p$  or  params , depending on the context."},{"id":3440,"pagetitle":"References","title":"Literature","ref":"/manifolddiffeq/stable/#Literature","content":" Literature [CMO14] E.¬†Celledoni, H.¬†Marthinsen and B.¬†Owren.  An introduction to Lie group integrators ‚Äì basics, new developments and applications .  Journal¬†of¬†Computational¬†Physics  257 , 1040‚Äì1061  (2014). Accessed on Jul 14, 2021, arXiv: 1207.0069. [CG93] P.¬†E.¬†Crouch and R.¬†Grossman.  Numerical integration of ordinary differential equations on manifolds .  Journal¬†of¬†Nonlinear¬†Science  3 , 1‚Äì33  (1993). Accessed on Jul 17, 2021. [EM98] K.¬†Eng√∏ and A.¬†Marthinsen.  Modeling and Solution of Some Mechanical Problems on Lie Groups .  Multibody¬†System¬†Dynamics  2 , 71‚Äì88  (1998). Accessed on Oct 13, 2021. [HLW10] E.¬†Hairer, C.¬†Lubich and G.¬†Wanner.  Geometric Numerical Integration: Structure-Preserving Algorithms for Ordinary Differential Equations . 2nd ed. 2006. 2nd printing 2010 edition¬†Edition (Springer, Heidelberg ; New York, 2010). [JMO00] Z.¬†Jackiewicz, A.¬†Marthinsen and B.¬†Owren.  Construction of Runge‚ÄìKutta methods of Crouch‚ÄìGrossman type of high order .  Advances¬†in¬†Computational¬†Mathematics  13 , 405‚Äì415  (2000). Accessed on Oct 15, 2021. [MO99] H.¬†Munthe‚ÄìKaas and B.¬†Owren.  Computations in a free Lie algebra .  Philosophical¬†Transactions¬†of¬†the¬†Royal¬†Society¬†of¬†London.¬†Series¬†A:¬†Mathematical,¬†Physical¬†and¬†Engineering¬†Sciences  357 , 957‚Äì981  (1999). Accessed on Jul 14, 2021. Publisher: Royal Society. [OM99] B.¬†Owren and A.¬†Marthinsen.  Runge-Kutta Methods Adapted to Manifolds and Based on Rigid Frames .  BIT¬†Numerical¬†Mathematics  39 , 116‚Äì142  (1999). Accessed on Jul 15, 2021."},{"id":3443,"pagetitle":"Home","title":"ManifoldDiff","ref":"/manifolddiff/stable/#ManifoldDiff","content":" ManifoldDiff The package  ManifoldDiff  aims to provide automatic calculation of Riemannian gradients of functions defined on manifolds. It builds upon  Manifolds.jl ."},{"id":3444,"pagetitle":"Home","title":"Naming scheme","ref":"/manifolddiff/stable/#Naming-scheme","content":" Naming scheme Providing a derivative, differential or gradient for a given function, this package adds that information to the function name. For example grad_f  for a gradient  $\\operatorname{grad} f$ subgrad_f  for a subgradient from the subdifferential $\\partial f$ differential_f  for  $Df$  (also called pushforward) differential_f_variable  if  f  has multiple variables / parameters, since a usual writing in math is  $f_x$  in this case adjoint_differential_f  for pullbacks adjoint_differential_f_variable  if  f  has multiple variables / parameters f_derivative  for  $f'$ jacobian_f  for Jacobian matrix of  $f$ . jacobian_f_variable  if  f  has multiple parameters. the scheme is not completely fixed but tries to follow the mathematical notation."},{"id":3447,"pagetitle":"Backends","title":"Differentiation backends","ref":"/manifolddiff/stable/backends/#Differentiation-backends","content":" Differentiation backends"},{"id":3448,"pagetitle":"Backends","title":"ManifoldDiff.set_default_differential_backend!","ref":"/manifolddiff/stable/backends/#ManifoldDiff.set_default_differential_backend!","content":" ManifoldDiff.set_default_differential_backend!  ‚Äî  Function set_default_differential_backend!(backend) Set current backend for differentiation to  backend . source"},{"id":3449,"pagetitle":"Backends","title":"ManifoldDiff.default_differential_backend","ref":"/manifolddiff/stable/backends/#ManifoldDiff.default_differential_backend","content":" ManifoldDiff.default_differential_backend  ‚Äî  Function default_differential_backend() Get the default differentiation backend. source"},{"id":3450,"pagetitle":"Backends","title":"Euclidian backends","ref":"/manifolddiff/stable/backends/#Euclidian-backends","content":" Euclidian backends Euclidian backend objects can be taken from  ADTypes.jl . See the documentation of  DifferentiationInterface.jl  for the list of supported packages."},{"id":3451,"pagetitle":"Backends","title":"EmbeddedDiff","ref":"/manifolddiff/stable/backends/#EmbeddedDiff","content":" EmbeddedDiff"},{"id":3452,"pagetitle":"Backends","title":"ManifoldDiff.ExplicitEmbeddedBackend","ref":"/manifolddiff/stable/backends/#ManifoldDiff.ExplicitEmbeddedBackend","content":" ManifoldDiff.ExplicitEmbeddedBackend  ‚Äî  Type ExplicitEmbeddedBackend{TF<:NamedTuple} A backend to use with the  RiemannianProjectionBackend  or the  TangentDiffBackend , when you have explicit formulae for the gradient in the embedding available. Constructor ExplicitEmbeddedBackend(M::AbstractManifold; kwargs) Construct an  ExplicitEmbeddedBackend  in the embedding  M , where currently the following keywords may be used gradient  for a(n allocating) gradient function  gradient(M, p)  defined in the embedding gradient!  for a mutating gradient function  gradient!(M, X, p) . Note that the gradient functions are defined on the embedding manifold  M  passed to the Backend as well source"},{"id":3455,"pagetitle":"Usage","title":"Basic usage","ref":"/manifolddiff/stable/basic_usage/#Basic-usage","content":" Basic usage You can calculate Riemannian gradient of a function defined in its embedding in multiple ways.  DifferentiationInterface.jl  can be used to select the backend. using ManifoldDiff\nusing DifferentiationInterface\nusing Manifolds, FiniteDifferences, ForwardDiff, Zygote\n\nrb_onb_fd51 = TangentDiffBackend(AutoFiniteDifferences(central_fdm(5, 1)))\nrb_onb_fwdd = TangentDiffBackend(AutoForwardDiff())\nrb_proj_zyg = RiemannianProjectionBackend(AutoZygote())\n\ns2 = Sphere(2)\n\nA = [1.0 2.0 5.0; 2.0 -1.0 4.0; 5.0 4.0 0.0]\n\nf(p) = p' * A * p\nq = [0.0, 1.0, 0.0]\n\nprintln(ManifoldDiff.gradient(s2, f, q, rb_onb_fd51))\nprintln(ManifoldDiff.gradient(s2, f, q, rb_onb_fwdd))\nprintln(ManifoldDiff.gradient(s2, f, q, rb_proj_zyg)) [3.999999999999511, 0.0, 7.999999999999624]\n[NaN, NaN, NaN]\n[4.0, 0.0, 8.0] In this example  rb_onb_fd51  corresponds to a finite differencing scheme,  rb_onb_fwdd  calculates gradient using  ForwardDiff.jl  and  rb_proj_zyg  uses  Zygote.jl  for reverse mode automatic differentiation. TangentDiffBackend  reduces dimensionality of the problem to the intrinsic dimension of the manifold, while  RiemannianProjectionBackend  relies on converting Euclidean gradient in the embedding to the Riemannian one."},{"id":3458,"pagetitle":"Internals","title":"Internal functions","ref":"/manifolddiff/stable/internals/#Internal-functions","content":" Internal functions"},{"id":3459,"pagetitle":"Internals","title":"ManifoldDiff.CurrentDiffBackend","ref":"/manifolddiff/stable/internals/#ManifoldDiff.CurrentDiffBackend","content":" ManifoldDiff.CurrentDiffBackend  ‚Äî  Type CurrentDiffBackend(backend) A mutable struct for storing the current differentiation backend in a global constant  _current_default_differential_backend . See also default_differential_backend ,  set_default_differential_backend! source"},{"id":3460,"pagetitle":"Internals","title":"ManifoldDiff._current_default_differential_backend","ref":"/manifolddiff/stable/internals/#ManifoldDiff._current_default_differential_backend","content":" ManifoldDiff._current_default_differential_backend  ‚Äî  Constant _current_default_differential_backend The instance of  CurrentDiffBackend  that stores the globally default differentiation backend. source"},{"id":3461,"pagetitle":"Internals","title":"ManifoldDiff._hessian","ref":"/manifolddiff/stable/internals/#ManifoldDiff._hessian","content":" ManifoldDiff._hessian  ‚Äî  Function _hessian(f, p[, backend]) Compute the Hessian of a callable  f  at point  p  computed using the given  backend . If the backend is not explicitly specified, it is obtained using the function  default_differential_backend . This function calculates plain Euclidean Hessian. Note Not specifying the backend explicitly will usually result in a type instability and decreased performance. source"},{"id":3462,"pagetitle":"Internals","title":"ManifoldDiff._jacobian","ref":"/manifolddiff/stable/internals/#ManifoldDiff._jacobian","content":" ManifoldDiff._jacobian  ‚Äî  Function _jacobian(f, p[, backend]) Compute the Jacobian of a callable  f  at point  p  computed using the given  backend . If the backend is not explicitly specified, it is obtained using the function  default_differential_backend . This function calculates plain Euclidean Jacobians, for Riemannian Jacobian calculation see for example  gradient . Note Not specifying the backend explicitly will usually result in a type instability and decreased performance. source"},{"id":3463,"pagetitle":"Internals","title":"ManifoldDiff._gradient","ref":"/manifolddiff/stable/internals/#ManifoldDiff._gradient","content":" ManifoldDiff._gradient  ‚Äî  Function _gradient(f, p[, backend]) Compute the gradient of a callable  f  at point  p  computed using the given  backend . If the backend is not explicitly specified, it is obtained using the function  default_differential_backend . This function calculates plain Euclidean gradients, for Riemannian gradient calculation see for example  gradient . Note Not specifying the backend explicitly will usually result in a type instability and decreased performance. source"},{"id":3464,"pagetitle":"Internals","title":"ManifoldDiff._derivative","ref":"/manifolddiff/stable/internals/#ManifoldDiff._derivative","content":" ManifoldDiff._derivative  ‚Äî  Function _derivative(f, t[, backend]) Compute the derivative of a callable  f  at time  t  computed using the given  backend . If the backend is not explicitly specified, it is obtained using the function  default_differential_backend . This function calculates plain Euclidean derivatives, for Riemannian differentiation see for example  differential . Note Not specifying the backend explicitly will usually result in a type instability and decreased performance. source"},{"id":3467,"pagetitle":"Library of functions","title":"Different library functions","ref":"/manifolddiff/stable/library/#Different-library-functions","content":" Different library functions Documentation for  ManifoldDiff.jl 's methods and types for finite differences and automatic differentiation."},{"id":3468,"pagetitle":"Library of functions","title":"Derivatives","ref":"/manifolddiff/stable/library/#Derivatives","content":" Derivatives"},{"id":3469,"pagetitle":"Library of functions","title":"ManifoldDiff.geodesic_derivative","ref":"/manifolddiff/stable/library/#ManifoldDiff.geodesic_derivative-Tuple{Any, Any, Any, Number}","content":" ManifoldDiff.geodesic_derivative  ‚Äî  Method Y = geodesic_derivative(M, p, X, t::Number; Œ≥t = geodesic(M, p, X, t))\ngeodesic_derivative!(M, Y, p, X, t::Number; Œ≥t = geodesic(M, p, X, t)) Evaluate the derivative of the geodesic  $Œ≥(t)$  with  $Œ≥_{p,X}(0) = p$  and  $\\dot Œ≥_{p,X}(0) = X$  at  $t$ . The formula reads \\[\\dot Œ≥(t) = \\mathcal P_{Œ≥(t) \\gets p} X\\] where  $\\mathcal P$  denotes the parallel transport. This computation can also be done in-place of  $Y$ . Optional Parameters Œ≥t  ‚Äì ( geodesic(M, p, X, t) ) the point on the geodesic at  $t$ . This way if the point was computed earlier it can be resued here. source"},{"id":3470,"pagetitle":"Library of functions","title":"ManifoldDiff.shortest_geodesic_derivative","ref":"/manifolddiff/stable/library/#ManifoldDiff.shortest_geodesic_derivative-Tuple{Any, Any, Any, Number}","content":" ManifoldDiff.shortest_geodesic_derivative  ‚Äî  Method Y = shortest_geodesic_derivative(M, p, X, t::Number; Œ≥t = shortest_geodesic(M, p, q, t))\nshortest_geodesic_derivative!(M, Y, p, X, t::Number; Œ≥t = shortest_geodesic(M, p, q, t)) Evaluate the derivative of the shortest geodesic  $Œ≥(t)$  with  $Œ≥_{p,q}(0) = p$  and  $\\dot Œ≥_{p,q}(1) = q$  at  $t$ . The formula reads \\[\\dot Œ≥(t) = \\mathcal P_{Œ≥(t) \\gets p} \\log_pq\\] where  $\\mathcal P$  denotes the parallel transport. This computation can also be done in-place of  $Y$ . Optional Parameters Œ≥t = geodesic(M, p, X, t)  the point on the geodesic at  $t$ . This way if the point was computed earlier it can be resued here. source"},{"id":3471,"pagetitle":"Library of functions","title":"Differentials and their adjoints","ref":"/manifolddiff/stable/library/#Differentials-and-their-adjoints","content":" Differentials and their adjoints"},{"id":3472,"pagetitle":"Library of functions","title":"ManifoldDiff.adjoint_differential_exp_argument","ref":"/manifolddiff/stable/library/#ManifoldDiff.adjoint_differential_exp_argument-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldDiff.adjoint_differential_exp_argument  ‚Äî  Method adjoint_differential_exp_argument(M, p, X, Y)\nadjoint_differential_exp_argument!(M, Z, p, X, Y) Compute the adjoint of  $D_X\\exp_p X[Y]$  (in place of  Z ). Note that  $X ‚àà  T_p(T_p\\mathcal M) = T_p\\mathcal M$  is still a tangent vector. See also differential_exp_argument ,  adjoint_Jacobi_field source"},{"id":3473,"pagetitle":"Library of functions","title":"ManifoldDiff.adjoint_differential_exp_basepoint","ref":"/manifolddiff/stable/library/#ManifoldDiff.adjoint_differential_exp_basepoint-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldDiff.adjoint_differential_exp_basepoint  ‚Äî  Method adjoint_differential_exp_basepoint(M, p, X, Y)\nadjoint_differential_exp_basepoint!(M, Z, p, X, Y) Computes the adjoint of  $D_p \\exp_p X[Y]$  (in place of  Z ). See also differential_exp_basepoint ,  adjoint_Jacobi_field source"},{"id":3474,"pagetitle":"Library of functions","title":"ManifoldDiff.adjoint_differential_log_argument","ref":"/manifolddiff/stable/library/#ManifoldDiff.adjoint_differential_log_argument-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldDiff.adjoint_differential_log_argument  ‚Äî  Method adjoint_differential_log_argument(M, p, q, X)\nadjoint_differential_log_argument!(M, Y, p, q, X) Compute the adjoint of  $D_q log_p q[X]$  (in place of  Y ). See also differential_log_argument ,  adjoint_Jacobi_field source"},{"id":3475,"pagetitle":"Library of functions","title":"ManifoldDiff.adjoint_differential_log_basepoint","ref":"/manifolddiff/stable/library/#ManifoldDiff.adjoint_differential_log_basepoint-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldDiff.adjoint_differential_log_basepoint  ‚Äî  Method adjoint_differential_log_basepoint(M, p, q, X)\nadjoint_differential_log_basepoint!(M, Y, p, q, X) computes the adjoint of  $D_p log_p q[X]$  (in place of  Y ). See also differential_log_basepoint ,  adjoint_Jacobi_field source"},{"id":3476,"pagetitle":"Library of functions","title":"ManifoldDiff.adjoint_differential_shortest_geodesic_endpoint","ref":"/manifolddiff/stable/library/#ManifoldDiff.adjoint_differential_shortest_geodesic_endpoint-Tuple{AbstractManifold, Vararg{Any, 4}}","content":" ManifoldDiff.adjoint_differential_shortest_geodesic_endpoint  ‚Äî  Method adjoint_differential_shortest_geodesic_endpoint(M, p, q, t, X)\nadjoint_differential_shortest_geodesic_endpoint!(M, Y, p, q, t, X) Compute the adjoint of  $D_q Œ≥(t; p, q)[X]$  (in place of  Y ). See also differential_shortest_geodesic_endpoint ,  adjoint_Jacobi_field source"},{"id":3477,"pagetitle":"Library of functions","title":"ManifoldDiff.adjoint_differential_shortest_geodesic_startpoint","ref":"/manifolddiff/stable/library/#ManifoldDiff.adjoint_differential_shortest_geodesic_startpoint-Tuple{AbstractManifold, Vararg{Any, 4}}","content":" ManifoldDiff.adjoint_differential_shortest_geodesic_startpoint  ‚Äî  Method adjoint_differential_shortest_geodesic_startpoint(M, p, q, t, X)\nadjoint_differential_shortest_geodesic_startpoint!(M, Y, p, q, t, X) Compute the adjoint of  $D_p Œ≥(t; p, q)[X]$  (in place of  Y ). See also differential_shortest_geodesic_startpoint ,  adjoint_Jacobi_field source"},{"id":3478,"pagetitle":"Library of functions","title":"ManifoldDiff.differential_exp_argument","ref":"/manifolddiff/stable/library/#ManifoldDiff.differential_exp_argument-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldDiff.differential_exp_argument  ‚Äî  Method Z = differential_exp_argument(M, p, X, Y)\ndifferential_exp_argument!(M, Z, p, X, Y) Compute  $D_X\\exp_pX[Y]$  (in place of  Z ). Note that  $X ‚àà  T_X(T_p\\mathcal M) = T_p\\mathcal M$  is still a tangent vector. See also differential_exp_basepoint ,  jacobi_field source"},{"id":3479,"pagetitle":"Library of functions","title":"ManifoldDiff.differential_exp_argument_lie_approx","ref":"/manifolddiff/stable/library/#ManifoldDiff.differential_exp_argument_lie_approx-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldDiff.differential_exp_argument_lie_approx  ‚Äî  Method differential_exp_argument_lie_approx(M::AbstractManifold, p, X, Y; n) Approximate differential of exponential map based on Lie group exponential. The formula reads (see Theorem 1.7 of  [Helgason1978] ) \\[D_X \\exp_{p}(X)[Y] = (\\mathrm{d}L_{\\exp_e(X)})_e\\left(\\sum_{k=0}^{n}\\frac{(-1)^k}{(k+1)!}(\\operatorname{ad}_X)^k(Y)\\right)\\] where  $(\\operatorname{ad}_X)^k(Y)$  is defined recursively as  $(\\operatorname{ad}_X)^0(Y) = Y$ ,  $\\operatorname{ad}_X^{k+1}(Y) = [X, \\operatorname{ad}_X^k(Y)]$ . source"},{"id":3480,"pagetitle":"Library of functions","title":"ManifoldDiff.differential_exp_basepoint","ref":"/manifolddiff/stable/library/#ManifoldDiff.differential_exp_basepoint-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldDiff.differential_exp_basepoint  ‚Äî  Method Z = differential_exp_basepoint(M, p, X, Y)\ndifferential_exp_basepoint!(M, Z, p, X, Y) Compute  $D_p\\exp_p X[Y]$  (in place of  Z ). See also differential_exp_argument ,  jacobi_field source"},{"id":3481,"pagetitle":"Library of functions","title":"ManifoldDiff.differential_inverse_retract_argument_fd_approx","ref":"/manifolddiff/stable/library/#ManifoldDiff.differential_inverse_retract_argument_fd_approx-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldDiff.differential_inverse_retract_argument_fd_approx  ‚Äî  Method differential_inverse_retract_argument_fd_approx(\n    M::AbstractManifold,\n    p,\n    q,\n    X;\n    retr::AbstractRetractionMethod = default_retraction_method(M),\n    invretr::AbstractInverseRetractionMethod = default_inverse_retraction_method(M),\n    h::Real=sqrt(eps(eltype(X))),\n) Approximate the differential of the inverse retraction  invretr  using a finite difference formula (see Eq. (16) in [ Zim20 ] \\[\\frac{\\operatorname{retr}^{-1}_q(\\operatorname{retr}_p(hX)) - \\operatorname{retr}^{-1}_q(\\operatorname{retr}_p(-hX))}{2h}\\] where  $h$  is the finite difference step  h ,  $\\operatorname{retr}^{-1}$  is the inverse retraction  invretr  and  $\\operatorname{retr}$  is the retraction  retr . source"},{"id":3482,"pagetitle":"Library of functions","title":"ManifoldDiff.differential_log_argument","ref":"/manifolddiff/stable/library/#ManifoldDiff.differential_log_argument-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldDiff.differential_log_argument  ‚Äî  Method Y = differential_log_argument(M, p, q, X)\ndifferential_log_argument!(M, Y, p, q, X) computes  $D_q\\log_pq[X]$  (in place of  Y ). See also differential_log_basepoint ,  jacobi_field source"},{"id":3483,"pagetitle":"Library of functions","title":"ManifoldDiff.differential_log_basepoint","ref":"/manifolddiff/stable/library/#ManifoldDiff.differential_log_basepoint-Tuple{AbstractManifold, Any, Any, Any}","content":" ManifoldDiff.differential_log_basepoint  ‚Äî  Method Y = differential_log_basepoint(M, p, q, X)\ndifferential_log_basepoint!(M, Y, p, q, X) computes  $D_p\\log_pq[X]$  (in place of  Y ). See also differential_log_argument ,  jacobi_field source"},{"id":3484,"pagetitle":"Library of functions","title":"ManifoldDiff.differential_shortest_geodesic_endpoint","ref":"/manifolddiff/stable/library/#ManifoldDiff.differential_shortest_geodesic_endpoint-Tuple{AbstractManifold, Vararg{Any, 4}}","content":" ManifoldDiff.differential_shortest_geodesic_endpoint  ‚Äî  Method Y = differential_shortest_geodesic_endpoint(M, p, q, t, X)\ndifferential_shortest_geodesic_endpoint!(M, Y, p, q, t, X) Compute  $D_qŒ≥(t;p,q)[X]$  (in place of  Y ). See also differential_shortest_geodesic_startpoint ,  jacobi_field source"},{"id":3485,"pagetitle":"Library of functions","title":"ManifoldDiff.differential_shortest_geodesic_startpoint","ref":"/manifolddiff/stable/library/#ManifoldDiff.differential_shortest_geodesic_startpoint-Tuple{AbstractManifold, Vararg{Any, 4}}","content":" ManifoldDiff.differential_shortest_geodesic_startpoint  ‚Äî  Method Y = differential_shortest_geodesic_startpoint(M, p, q, t, X)\ndifferential_shortest_geodesic_startpoint!(M, Y, p, q, t, X) Compute  $D_p Œ≥(t;p,q)[Œ∑]$  (in place of  Y ). See also differential_shortest_geodesic_endpoint ,  jacobi_field source"},{"id":3486,"pagetitle":"Library of functions","title":"ManifoldDiff.AbstractProjector","ref":"/manifolddiff/stable/library/#ManifoldDiff.AbstractProjector","content":" ManifoldDiff.AbstractProjector  ‚Äî  Type abstract type AbstractProjector end An abstract type for projectors on a tangent space  $T_pM$  for fixed values of  p  and  M . Calling a projector on a tangent vector returns a new tangent vector: (Œ†::AbstractProjector)(X) -> Y Projectors assume that  X  is a valid vector from  $T_pM$ . source"},{"id":3487,"pagetitle":"Library of functions","title":"ManifoldDiff.CoprojectorOntoVector","ref":"/manifolddiff/stable/library/#ManifoldDiff.CoprojectorOntoVector","content":" ManifoldDiff.CoprojectorOntoVector  ‚Äî  Type CoprojectorOntoVector{TM<:AbstractManifold,TP,TX} A structure that represents projector onto the subspace of the tangent space at  p  from manifold  M  othogonal to vector  X  of unit norm. Constructor CoprojectorOntoVector(M::AbstractManifold, p, X) source"},{"id":3488,"pagetitle":"Library of functions","title":"ManifoldDiff.ProjectorOntoVector","ref":"/manifolddiff/stable/library/#ManifoldDiff.ProjectorOntoVector","content":" ManifoldDiff.ProjectorOntoVector  ‚Äî  Type ProjectorOntoVector{TM<:AbstractManifold,TP,TX} A structure that represents projector onto the subspace of the tangent space at  p  from manifold  M  spanned by tangent vector  X  of unit norm. Constructor ProjectorOntoVector(M::AbstractManifold, p, X) source"},{"id":3489,"pagetitle":"Library of functions","title":"ManifoldDiff.diagonalizing_projectors","ref":"/manifolddiff/stable/library/#ManifoldDiff.diagonalizing_projectors-Tuple{AbstractManifold, Any, Any}","content":" ManifoldDiff.diagonalizing_projectors  ‚Äî  Method diagonalizing_projectors(M::AbstractManifold, p, X) Compute eigenvalues of the Jacobi operator  $Y ‚Üí R(Y,X)X$ , where  $R$  is the curvature endomorphism, together with projectors onto eigenspaces of the operator. Projectors are objects of subtypes of  AbstractProjector . By default constructs projectors using the  DiagonalizingOrthonormalBasis . source"},{"id":3490,"pagetitle":"Library of functions","title":"Gradients","ref":"/manifolddiff/stable/library/#Gradients","content":" Gradients"},{"id":3491,"pagetitle":"Library of functions","title":"ManifoldDiff.grad_distance","ref":"/manifolddiff/stable/library/#ManifoldDiff.grad_distance","content":" ManifoldDiff.grad_distance  ‚Äî  Function grad_distance(M, q, p[, c=2])\ngrad_distance!(M, X, q, p[, c=2]) compute the (sub)gradient of the distance (default: squared), in place of  X . \\[f(p) = \\frac{1}{c} d^c_{\\mathcal M}(p, q)\\] to a fixed point  q  on the manifold  M  and  c  is an integer. The (sub-)gradient reads \\[\\operatorname{grad}f(p) = -d_{\\mathcal M}^{c-2}(p, q)\\log_pq\\] for  $c\\neq 1$  or  $p\\neq  q$ . Note that for the remaining case  $c=1$ ,  $p=q$ , the function is not differentiable. In this case, the function returns the corresponding zero tangent vector, since this is an element of the subdifferential. Optional c  ‚Äì ( 2 ) the exponent of the distance,  i.e. the default is the squared distance source"},{"id":3492,"pagetitle":"Library of functions","title":"ManifoldDiff.subgrad_distance","ref":"/manifolddiff/stable/library/#ManifoldDiff.subgrad_distance","content":" ManifoldDiff.subgrad_distance  ‚Äî  Function subgrad_distance(M, q, p[, c = 2; atol = 0])\nsubgrad_distance!(M, X, q, p[, c = 2; atol = 0]) compute the subgradient of the distance (in place of  X ) \\[f(p) = \\frac{1}{c} d^c_{\\mathcal M}(p, q)\\] to a fixed point  q  on the manifold  M  and  c  is an integer. The subgradient reads \\[\\partial f(p) = -d_{\\mathcal M}^{c-2}(p, q)\\log_pq\\] for  $c\\neq 1$  or  $p\\neq  q$ . Note that for the remaining case  $c=1$ ,  $p=q$ , the function is not differentiable. In this case, the subgradient is given by a tangent vector at  p  with norm less than or equal to one. Optional c  ‚Äì ( 2 ) the exponent of the distance,  i.e. the default is the distance atol  ‚Äì ( 0 ) the tolerance to use when evaluating the distance between  p  and  q . source"},{"id":3493,"pagetitle":"Library of functions","title":"Jacobi fields","ref":"/manifolddiff/stable/library/#Jacobi-fields","content":" Jacobi fields"},{"id":3494,"pagetitle":"Library of functions","title":"ManifoldDiff.adjoint_Jacobi_field","ref":"/manifolddiff/stable/library/#ManifoldDiff.adjoint_Jacobi_field-Union{Tuple{TŒ≤}, Tuple{AbstractManifold, Any, Any, Any, Any, TŒ≤}} where TŒ≤","content":" ManifoldDiff.adjoint_Jacobi_field  ‚Äî  Method Y = adjoint_Jacobi_field(M, p, q, t, X, Œ≤)\nadjoint_Jacobi_field!(M, Y, p, q, t, X, Œ≤) Compute the AdjointJacobiField  $J$  along the geodesic  $Œ≥_{p,q}$  on the manifold  $\\mathcal M$  with initial conditions (depending on the application)  $X ‚àà T_{Œ≥_{p,q}(t)}\\mathcal M$  and weights  $Œ≤$ . The result is a vector  $Y ‚àà T_p\\mathcal M$ . The main difference to  jacobi_field  is the, that the input  X  and the output  Y  switched tangent spaces. The computation can be done in place of  Y . For details see  jacobi_field source"},{"id":3495,"pagetitle":"Library of functions","title":"ManifoldDiff.jacobi_field","ref":"/manifolddiff/stable/library/#ManifoldDiff.jacobi_field-Union{Tuple{TŒ≤}, Tuple{AbstractManifold, Any, Any, Any, Any, TŒ≤}} where TŒ≤","content":" ManifoldDiff.jacobi_field  ‚Äî  Method Y = jacobi_field(M, p, q, t, X, Œ≤)\njacobi_field!(M, Y, p, q, t, X, Œ≤) compute the Jacobi field  $J$  along the geodesic  $Œ≥_{p,q}$  on the manifold  $\\mathcal M$  with initial conditions (depending on the application)  $X ‚àà T_p\\mathcal M$  and weights  $Œ≤$ . The result is a tangent vector  Y  from  $T_{Œ≥_{p,q}(t)}\\mathcal M$ . The computation can be done in place of  Y . See also adjoint_Jacobi_field source"},{"id":3496,"pagetitle":"Library of functions","title":"ManifoldDiff.Œ≤differential_exp_argument","ref":"/manifolddiff/stable/library/#ManifoldDiff.Œ≤differential_exp_argument-Tuple{Any, Number, Any}","content":" ManifoldDiff.Œ≤differential_exp_argument  ‚Äî  Method Œ≤differential_exp_argument(Œ∫,t,d) weights for the  jacobi_field  corresponding to the differential of the geodesic with respect to its start point  $D_X \\exp_p X[Y]$ . They are \\[Œ≤(Œ∫) = \\begin{cases}\n\\frac{\\sinh(d\\sqrt{-Œ∫})}{d\\sqrt{-Œ∫}}&\\text{ if }Œ∫ < 0,\\\\\n1 & \\text{ if } Œ∫ = 0,\\\\\n\\frac{\\sin(d\\sqrt{Œ∫})}{d\\sqrt{Œ∫}}&\\text{ if }Œ∫ > 0.\n\\end{cases}\\] See also differential_exp_argument ,  jacobi_field source"},{"id":3497,"pagetitle":"Library of functions","title":"ManifoldDiff.Œ≤differential_exp_basepoint","ref":"/manifolddiff/stable/library/#ManifoldDiff.Œ≤differential_exp_basepoint-Tuple{Any, Number, Any}","content":" ManifoldDiff.Œ≤differential_exp_basepoint  ‚Äî  Method Œ≤differential_exp_basepoint(Œ∫,t,d) weights for the  jacobi_field  corresponding to the differential of the geodesic with respect to its start point  $D_p \\exp_p X [Y]$ . They are \\[Œ≤(Œ∫) = \\begin{cases}\n\\cosh(\\sqrt{-Œ∫})&\\text{ if }Œ∫ < 0,\\\\\n1 & \\text{ if } Œ∫ = 0,\\\\\n\\cos(\\sqrt{Œ∫}) &\\text{ if }Œ∫ > 0.\n\\end{cases}\\] See also differential_exp_basepoint ,  jacobi_field source"},{"id":3498,"pagetitle":"Library of functions","title":"ManifoldDiff.Œ≤differential_log_argument","ref":"/manifolddiff/stable/library/#ManifoldDiff.Œ≤differential_log_argument-Tuple{Any, Number, Any}","content":" ManifoldDiff.Œ≤differential_log_argument  ‚Äî  Method Œ≤differential_log_argument(Œ∫,t,d) weights for the JacobiField corresponding to the differential of the logarithmic map with respect to its argument  $D_q \\log_p q[X]$ . They are \\[Œ≤(Œ∫) = \\begin{cases}\n\\frac{ d\\sqrt{-Œ∫} }{\\sinh(d\\sqrt{-Œ∫})}&\\text{ if }Œ∫ < 0,\\\\\n1 & \\text{ if } Œ∫ = 0,\\\\\n\\frac{ d\\sqrt{Œ∫} }{\\sin(d\\sqrt{Œ∫})}&\\text{ if }Œ∫ > 0.\n\\end{cases}\\] See also differential_log_basepoint ,  jacobi_field source"},{"id":3499,"pagetitle":"Library of functions","title":"ManifoldDiff.Œ≤differential_log_basepoint","ref":"/manifolddiff/stable/library/#ManifoldDiff.Œ≤differential_log_basepoint-Tuple{Any, Number, Any}","content":" ManifoldDiff.Œ≤differential_log_basepoint  ‚Äî  Method Œ≤differential_log_basepoint(Œ∫,t,d) weights for the  jacobi_field  corresponding to the differential of the geodesic with respect to its start point  $D_p \\log_p q[X]$ . They are \\[Œ≤(Œ∫) = \\begin{cases}\n-\\sqrt{-Œ∫}d\\frac{\\cosh(d\\sqrt{-Œ∫})}{\\sinh(d\\sqrt{-Œ∫})}&\\text{ if }Œ∫ < 0,\\\\\n-1 & \\text{ if } Œ∫ = 0,\\\\\n-\\sqrt{Œ∫}d\\frac{\\cos(d\\sqrt{Œ∫})}{\\sin(d\\sqrt{Œ∫})}&\\text{ if }Œ∫ > 0.\n\\end{cases}\\] See also differential_log_argument ,  differential_log_argument ,  jacobi_field source"},{"id":3500,"pagetitle":"Library of functions","title":"ManifoldDiff.Œ≤differential_shortest_geodesic_startpoint","ref":"/manifolddiff/stable/library/#ManifoldDiff.Œ≤differential_shortest_geodesic_startpoint-Tuple{Any, Any, Any}","content":" ManifoldDiff.Œ≤differential_shortest_geodesic_startpoint  ‚Äî  Method Œ≤differential_shortest_geodesic_startpoint(Œ∫,t,d) weights for the  jacobi_field  corresponding to the differential of the geodesic with respect to its start point  $D_x g(t;p,q)[X]$ . They are \\[Œ≤(Œ∫) = \\begin{cases}\n\\frac{\\sinh(d(1-t)\\sqrt{-Œ∫})}{\\sinh(d\\sqrt{-Œ∫})}\n&\\text{ if }Œ∫ < 0,\\\\\n1-t & \\text{ if } Œ∫ = 0,\\\\\n\\frac{\\sin((1-t)d\\sqrt{Œ∫})}{\\sinh(d\\sqrt{Œ∫})}\n&\\text{ if }Œ∫ > 0.\n\\end{cases}\\] Due to a symmetry argument, these are also used to compute  $D_q g(t; p,q)[Œ∑]$ See also differential_shortest_geodesic_endpoint ,  differential_shortest_geodesic_startpoint ,  jacobi_field source"},{"id":3501,"pagetitle":"Library of functions","title":"Jacobians","ref":"/manifolddiff/stable/library/#Jacobians","content":" Jacobians"},{"id":3502,"pagetitle":"Library of functions","title":"ManifoldDiff.allocate_jacobian","ref":"/manifolddiff/stable/library/#ManifoldDiff.allocate_jacobian-Tuple{AbstractManifold, AbstractManifold, Any, Any}","content":" ManifoldDiff.allocate_jacobian  ‚Äî  Method allocate_jacobian(\n    M_domain::AbstractManifold,\n    M_codomain::AbstractManifold,\n    f,\n    p;\n    basis_domain::AbstractBasis = DefaultOrthonormalBasis(),\n    basis_codomain::AbstractBasis = DefaultOrthonormalBasis(),\n) Allocate Jacobian of function  f  with given domain and codomain at point  p .  basis_domain  and  basis_codomain  denote bases of tangent spaces at, respectively,  p  and  f(p) . source"},{"id":3503,"pagetitle":"Library of functions","title":"ManifoldDiff.jacobian_exp_argument","ref":"/manifolddiff/stable/library/#ManifoldDiff.jacobian_exp_argument","content":" ManifoldDiff.jacobian_exp_argument  ‚Äî  Function jacobian_exp_argument(\n    M::AbstractManifold,\n    p,\n    X,\n    basis_domain::AbstractBasis=DefaultOrthonormalBasis(),\n    basis_codomain::AbstractBasis=DefaultOrthonormalBasis(),\n) Compute Jacobian of the exponential map with respect to its argument (tangent vector). Differential of the exponential map is here considered as a function from  $T_p \\mathcal{M}$  to  $T_{\\exp_p X} \\mathcal{M}$ . Jacobian coefficients are represented in basis  basis_domain  in the domain and in  basis_codomain  in the codomain. source"},{"id":3504,"pagetitle":"Library of functions","title":"ManifoldDiff.jacobian_exp_basepoint","ref":"/manifolddiff/stable/library/#ManifoldDiff.jacobian_exp_basepoint","content":" ManifoldDiff.jacobian_exp_basepoint  ‚Äî  Function jacobian_exp_basepoint(\n    M::AbstractManifold,\n    p,\n    X,\n    basis_domain::AbstractBasis=DefaultOrthonormalBasis(),\n    basis_codomain::AbstractBasis=DefaultOrthonormalBasis(),\n) Compute Jacobian of the exponential map with respect to the basepoint. Differential of the exponential map is here considered as a function from  $T_p \\mathcal{M}$  to  $T_{\\exp_p X} \\mathcal{M}$ . Jacobian coefficients are represented in basis  basis_domain  in the domain and in  basis_codomain  in the codomain. source"},{"id":3505,"pagetitle":"Library of functions","title":"ManifoldDiff.jacobian_log_argument","ref":"/manifolddiff/stable/library/#ManifoldDiff.jacobian_log_argument","content":" ManifoldDiff.jacobian_log_argument  ‚Äî  Function jacobian_log_argument(\n    M::AbstractManifold,\n    p,\n    q,\n    basis_domain::AbstractBasis=DefaultOrthonormalBasis(),\n    basis_codomain::AbstractBasis=DefaultOrthonormalBasis(),\n) Compute Jacobian of the logarithmic map with respect to its argument (point  q ). Differential of the logarithmic map is here considered as a function from  $T_q \\mathcal{M}$  to  $T_p \\mathcal{M}$ . Jacobian coefficients are represented in basis  basis_domain  in the domain and in  basis_codomain  in the codomain. source"},{"id":3506,"pagetitle":"Library of functions","title":"ManifoldDiff.jacobian_log_basepoint","ref":"/manifolddiff/stable/library/#ManifoldDiff.jacobian_log_basepoint","content":" ManifoldDiff.jacobian_log_basepoint  ‚Äî  Function jacobian_log_basepoint(\n    M::AbstractManifold,\n    p,\n    q,\n    basis_domain::AbstractBasis=DefaultOrthonormalBasis(),\n    basis_codomain::AbstractBasis=DefaultOrthonormalBasis(),\n) Compute Jacobian of the logarithmic map with respect to the basepoint. Differential of the logarithmic map is here considered as a function from  $T_q \\mathcal{M}$  to  $T_p \\mathcal{M}$ . Jacobian coefficients are represented in basis  basis_domain  in the domain and in  basis_codomain  in the codomain. source"},{"id":3507,"pagetitle":"Library of functions","title":"Riemannian differentials","ref":"/manifolddiff/stable/library/#Riemannian-differentials","content":" Riemannian differentials"},{"id":3508,"pagetitle":"Library of functions","title":"ManifoldDiff.AbstractRiemannianDiffBackend","ref":"/manifolddiff/stable/library/#ManifoldDiff.AbstractRiemannianDiffBackend","content":" ManifoldDiff.AbstractRiemannianDiffBackend  ‚Äî  Type AbstractRiemannianDiffBackend An abstract type for backends for differentiation. source"},{"id":3509,"pagetitle":"Library of functions","title":"ManifoldDiff.RiemannianProjectionBackend","ref":"/manifolddiff/stable/library/#ManifoldDiff.RiemannianProjectionBackend","content":" ManifoldDiff.RiemannianProjectionBackend  ‚Äî  Type RiemannianProjectionBackend <: AbstractRiemannianDiffBackend This backend computes the differentiation in the embedding, which is currently limited to the gradient. Let  $mathcal M$  denote a manifold embedded in some  $R^m$ , where  $m$  is usually (much) larger than the manifold dimension. Then we require three tools A function  $fÃÉ: ‚Ñù^m ‚Üí ‚Ñù$  such that its restriction to the manifold yields the cost function  $f$  of interest. A  project  function to project tangent vectors from the embedding (at  $T_p‚Ñù^m$ ) back onto the tangent space  $T_p\\mathcal M$ . This also includes possible changes of the representation of the tangent vector (e.g. in the Lie algebra or in a different data format). A  change_representer  for non-isometrically embedded manifolds, i.e. where the tangent space  $T_p\\mathcal M$  of the manifold does not inherit the inner product from restriction of the inner product from the tangent space  $T_p‚Ñù^m$  of the embedding see also  riemannian_gradient  and [ AMS08 ], Section 3.6.1 for a derivation on submanifolds. source"},{"id":3510,"pagetitle":"Library of functions","title":"ManifoldDiff.TangentDiffBackend","ref":"/manifolddiff/stable/library/#ManifoldDiff.TangentDiffBackend","content":" ManifoldDiff.TangentDiffBackend  ‚Äî  Type TangentDiffBackend <: AbstractRiemannianDiffBackend A backend that uses tangent spaces and bases therein to derive an intrinsic differentiation scheme. Since it works in tangent spaces at argument and function value, methods might require a retraction and an inverse retraction as well as a basis. In the tangent space itself, this backend then employs a (Euclidean) backend. Constructor TangentDiffBackend(diff_backend) where  diff_backend  is a (Euclidean) backend to be used on the tangent space. With the keyword arguments retraction  an  AbstractRetractionMethod  ( ExponentialRetraction  by default) inverse_retraction  an  AbstractInverseRetractionMethod LogarithmicInverseRetraction  by default) basis_arg  an  AbstractBasis  ( DefaultOrthogonalBasis  by default) basis_val  an  AbstractBasis  ( DefaultOrthogonalBasis  by default) source"},{"id":3511,"pagetitle":"Library of functions","title":"ManifoldDiff.differential","ref":"/manifolddiff/stable/library/#ManifoldDiff.differential-Tuple{AbstractManifold, Any, Real, ManifoldDiff.AbstractRiemannianDiffBackend}","content":" ManifoldDiff.differential  ‚Äî  Method differential(M::AbstractManifold, f, t::Real, backend)\ndifferential!(M::AbstractManifold, f, X, t::Real, backend) Compute the Riemannian differential of a curve  $f: ‚Ñù\\to M$  on a manifold  M  represented by function  f  at time  t  using the given backend. It is calculated as the tangent vector equal to  $\\mathrm{d}f_t(t)[1]$ . The mutating variant computes the differential in place of  X . source"},{"id":3512,"pagetitle":"Library of functions","title":"ManifoldDiff.gradient","ref":"/manifolddiff/stable/library/#ManifoldDiff.gradient-Tuple{AbstractManifold, Any, Any, ManifoldDiff.AbstractRiemannianDiffBackend}","content":" ManifoldDiff.gradient  ‚Äî  Method gradient(M::AbstractManifold, f, p, backend::AbstractRiemannianDiffBackend)\ngradient!(M::AbstractManifold, f, X, p, backend::AbstractRiemannianDiffBackend) Compute the Riemannian gradient  $‚àáf(p)$  of a real-valued function  $f:\\mathcal M \\to ‚Ñù$  at point  p  on the manifold  M  using the specified  AbstractRiemannianDiffBackend . The mutating variant computes the gradient in place of  X . source"},{"id":3513,"pagetitle":"Library of functions","title":"ManifoldDiff.gradient","ref":"/manifolddiff/stable/library/#ManifoldDiff.gradient-Tuple{AbstractManifold, Any, Any, TangentDiffBackend}","content":" ManifoldDiff.gradient  ‚Äî  Method gradient(M, f, p, backend::TangentDiffBackend) This method uses the internal  backend.diff_backend  (Euclidean) on the function \\[    f(\\operatorname{retr}_p(\\cdot))\\] which is given on the tangent space. In detail, the gradient can be written in terms of the  backend.basis_arg . We illustrate it here for an  AbstractOrthonormalBasis , since that simplifies notations: \\[\\operatorname{grad}f(p) = \\operatorname{grad}f(p) = \\sum_{i=1}^{d} g_p(\\operatorname{grad}f(p),X_i)X_i\n\t= \\sum_{i=1}^{d} Df(p)[X_i]X_i\\] where the last equality is due to the definition of the gradient as the Riesz representer of the differential. If the backend is a forward (or backward) finite difference, these coefficients in the sum can be approximates as \\[DF(p)[Y] ‚âà \\frac{1}{h}\\bigl( f(\\exp_p(hY)) - f(p) \\bigr)\\] writing  $p=\\exp_p(0)$  we see that this is a finite difference of  $f\\circ\\exp_p$ , i.e. for a function on the tangent space, so we can also use other (Euclidean) backends source"},{"id":3514,"pagetitle":"Library of functions","title":"ManifoldDiff.hessian","ref":"/manifolddiff/stable/library/#ManifoldDiff.hessian-Tuple{AbstractManifold, Any, Any, TangentDiffBackend}","content":" ManifoldDiff.hessian  ‚Äî  Method hessian(M::AbstractManifold, f, p, backend::TangentDiffBackend) Compute the Hessian of function  f  at point  p  using the given  backend . The formula for normal coordinate systems from [SommerFletcherPennec2020]  is used. source"},{"id":3515,"pagetitle":"Library of functions","title":"ManifoldDiff.riemannian_Hessian","ref":"/manifolddiff/stable/library/#ManifoldDiff.riemannian_Hessian-Tuple{AbstractManifold, Vararg{Any, 4}}","content":" ManifoldDiff.riemannian_Hessian  ‚Äî  Method riemannian_Hessian(M, p, eG, eH, X)\nriemannian_Hessian!(M, Y, p, eG, eH, X) Convert the Euclidean Hessian  eH= $\\operatorname{Hess} \\tilde f(p) [X]$  of a function  $f \\colon \\mathcal M \\to \\mathbb R$ , which is the restriction of  $\\tilde f$  to  $\\mathcal M$ , given additionally the (Euclidean) gradient  $\\operatorname{grad} \\tilde f(p)$ . The Riemannian Hessian is then computed by \\[\\operatorname{Hess} f(p)[X]\n= \\operatorname{proj}_{T_p\\mathcal M}\\bigl(\\operatorname{Hess} \\tilde f(p)[X])\n+ \\mathcal W_p\\Bigl( X, \\operatorname{proj}_{N_p\\mathcal M}\\bigl( \\operatorname{grad} \\tilde f (p) \\bigr) \\Bigr),\\] where  $N_p\\mathcal M$  denotes the normal space, i.e. the orthogonal complement of the tangent space in the embedding, and  $\\mathcal W_p$  denotes the  Weingarten  map. See [ Bou23 ] for more details The function is inspired by  ehess2rhess  in the  Matlab package Manopt . source"},{"id":3516,"pagetitle":"Library of functions","title":"ManifoldDiff.riemannian_gradient","ref":"/manifolddiff/stable/library/#ManifoldDiff.riemannian_gradient-Tuple{AbstractManifold, Any, Any}","content":" ManifoldDiff.riemannian_gradient  ‚Äî  Method riemannian_gradient(M, p, Y; embedding_metric=EuclideanMetric())\nriemannian_gradient!(M, X, p, Y; embedding_metric=EuclideanMetric()) For a given gradient  $Y = \\operatorname{grad} \\tilde f(p)$  in the embedding of a manifold, this function computes the Riemannian gradient  $\\operatorname{grad} f(p)$  of the function  $\\tilde f$  restricted to the manifold  $M$ . This can also be done in place of  X . By default it uses the following computation: Let the projection  $Z = \\operatorname{proj}_{T_p\\mathcal M}(Y)$  of  $Y$  onto the tangent space at  $p$  be given, that is with respect to the inner product in the embedding. Then \\[‚ü®Z-Y, W‚ü© = 0 \\text{ for all } W \\in T_p\\mathcal M,\\] or rearranged  $‚ü®Y,W‚ü© = ‚ü®Z,W‚ü©$ . We then use the definition of the Riemannian gradient \\[‚ü®\\operatorname{grad} f(p), W‚ü©_p = Df(p)[X] = ‚ü®\\operatorname{grad}f(p), W‚ü© = ‚ü®\\operatorname{proj}_{T_p\\mathcal M}(\\operatorname{grad}f(p)),W‚ü©\n\\quad\\text{for all } W \\in T_p\\mathcal M.\\] Comparing the first and the last term, the remaining computation is the function  change_representer . This method can also be implemented directly, if a more efficient/stable version is known. The function is inspired by  egrad2rgrad  in the  Matlab package Manopt . source"},{"id":3517,"pagetitle":"Library of functions","title":"Proximal Maps","ref":"/manifolddiff/stable/library/#Proximal-Maps","content":" Proximal Maps Given a convex, lower semi-continuous function  $f\\colon \\mathcal M \\to \\mathbb R$ , its proximal map is defined for some  $Œª>0$  as [ Bac14 ] \\[\\operatorname{prox}_{Œªf}(p) := \\operatorname*{arg\\,min}_{q\\in\\mathcal M} \\frac{1}{2Œª}d^2_{\\mathcal M}(p,q) + f(q).\\] Another name for the proximal map is  resolvent . Intuitively this means to minimize the function  $f$  while at the same timme ‚Äústaying close‚Äù to the argument  $p$ ."},{"id":3518,"pagetitle":"Library of functions","title":"ManifoldDiff.prox_distance","ref":"/manifolddiff/stable/library/#ManifoldDiff.prox_distance","content":" ManifoldDiff.prox_distance  ‚Äî  Function y = prox_distance(M::AbstractManifold, Œª::Real, p_data, p [, r=2])\nprox_distance!(M::AbstractManifold, q, Œª::Real, p_data, p [, r=2]) Compute the proximal map  $\\operatorname{prox}_{Œªf}$  with parameter Œª of  $f(p) = \\frac{1}{r}d_{\\mathcal M}^r(p_{\\mathrm{data}},p)$ . For the in-place variant the computation is done in place of  q . Input M       a manifold  M Œª       the prox parameter, a positive real number. p_data  a point on  M . p       the argument of the proximal map r       ( 2 ) exponent of the distance. Output q  ‚Äì the result of the proximal map of  $f$ For more details see [ WDS14 ] source Helgason1978 S. Helgason, Differential Geometry, Lie Groups, and Symmetric Spaces, First Edition. Academic Press, 1978. SommerFletcherPennec2020 S. Sommer, T. Fletcher, and X. Pennec, ‚Äú1 - Introduction to differential and Riemannian geometry,‚Äù in Riemannian Geometric Statistics in Medical Image Analysis, X. Pennec, S. Sommer, and T. Fletcher, Eds. Academic Press, 2020, pp. 3‚Äì37. doi: 10.1016/B978-0-12-814725-2.00008-X."},{"id":3521,"pagetitle":"Literature","title":"Literature","ref":"/manifolddiff/stable/references/#Literature","content":" Literature [AMS08] P.-A.¬†Absil, R.¬†Mahony and R.¬†Sepulchre.  Optimization Algorithms on Matrix Manifolds  (Princeton University Press, 2008), available online at  press.princeton.edu/chapters/absil/ . [Bac14] M.¬†Baƒç√°k.  Computing medians and means in Hadamard spaces .  SIAM¬†Journal¬†on¬†Optimization  24 , 1542‚Äì1566  (2014). [Bou23] N.¬†Boumal.  An Introduction to Optimization on Smooth Manifolds . First¬†Edition (Cambridge University Press, 2023). Homepage to the book:  nicolasboumal.net/book/index.html . [WDS14] A.¬†Weinmann, L.¬†Demaret and M.¬†Storath.  Total variation regularization for manifold-valued data .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  7 , 2226‚Äì2257  (2014). [Zim20] R.¬†Zimmermann.  Hermite Interpolation and Data Processing Errors on Riemannian Matrix Manifolds .  SIAM¬†Journal¬†on¬†Scientific¬†Computing  42 , A2593-A2619  (2020)."},{"id":3524,"pagetitle":"Home","title":"Welcome to ManoptExample.jl","ref":"/manoptexamples/stable/#Welcome-to-ManoptExample.jl","content":" Welcome to ManoptExample.jl"},{"id":3525,"pagetitle":"Home","title":"ManoptExamples.ManoptExamples","ref":"/manoptexamples/stable/#ManoptExamples.ManoptExamples","content":" ManoptExamples.ManoptExamples  ‚Äî  Module üèîÔ∏è‚õ∑Ô∏è ManoptExamples.jl ‚Äì A collection of research and tutorial example problems for  Manopt.jl üìö Documentation:  juliamanifolds.github.io/ManoptExamples.jl üì¶ Repository:  github.com/JuliaManifolds/ManoptExamples.jl üí¨ Discussions:  github.com/JuliaManifolds/ManoptExamples.jl/discussions üéØ Issues:  github.com/JuliaManifolds/ManoptExamples.jl/issues source This package provides a set of example tasks for  Manopt.jl  based on either generic manifolds from the  ManifoldsBase.jl  interface or specific manifolds from  Manifolds.jl . Each example usually consists of a cost function and additional objects, like the gradient or proximal maps, see  objectives an example explaining how to use these, see  examples Helping functions that are used in one or more examples can be found in the section of functions in the menu."},{"id":3528,"pagetitle":"Changelog","title":"Changelog","ref":"/manoptexamples/stable/changelog/#Changelog","content":" Changelog All notable changes to this Julia package will be documented in this file. The format is based on  Keep a Changelog , and this project adheres to  Semantic Versioning ."},{"id":3529,"pagetitle":"Changelog","title":"[0.1.13] ‚Äì 21/03/2025","ref":"/manoptexamples/stable/changelog/#[0.1.13]-‚Äì-21/03/2025","content":" [0.1.13] ‚Äì 21/03/2025"},{"id":3530,"pagetitle":"Changelog","title":"Changed","ref":"/manoptexamples/stable/changelog/#Changed","content":" Changed Updated numerical experiments from the Riemannian Convex Bundle paper."},{"id":3531,"pagetitle":"Changelog","title":"[0.1.12] ‚Äì 11/02/2025","ref":"/manoptexamples/stable/changelog/#[0.1.12]-‚Äì-11/02/2025","content":" [0.1.12] ‚Äì 11/02/2025"},{"id":3532,"pagetitle":"Changelog","title":"Changed","ref":"/manoptexamples/stable/changelog/#Changed-2","content":" Changed Update ManifoldDiff.jl compat to 0.4."},{"id":3533,"pagetitle":"Changelog","title":"[0.1.11] ‚Äì 10/02/2025","ref":"/manoptexamples/stable/changelog/#[0.1.11]-‚Äì-10/02/2025","content":" [0.1.11] ‚Äì 10/02/2025"},{"id":3534,"pagetitle":"Changelog","title":"Changed","ref":"/manoptexamples/stable/changelog/#Changed-3","content":" Changed Bump dependencies on CI Adapt to ManifoldsBase 1.0"},{"id":3535,"pagetitle":"Changelog","title":"[0.1.10] ‚Äì 18/10/2024","ref":"/manoptexamples/stable/changelog/#[0.1.10]-‚Äì-18/10/2024","content":" [0.1.10] ‚Äì 18/10/2024"},{"id":3536,"pagetitle":"Changelog","title":"Changed","ref":"/manoptexamples/stable/changelog/#Changed-4","content":" Changed Bump dependencies"},{"id":3537,"pagetitle":"Changelog","title":"[0.1.9] ‚Äì 28/06/2024","ref":"/manoptexamples/stable/changelog/#[0.1.9]-‚Äì-28/06/2024","content":" [0.1.9] ‚Äì 28/06/2024"},{"id":3538,"pagetitle":"Changelog","title":"Added","ref":"/manoptexamples/stable/changelog/#Added","content":" Added Three numerical experiments from the Riemannian Convex Bundle paper."},{"id":3539,"pagetitle":"Changelog","title":"[0.1.8] ‚Äì 12/06/2024","ref":"/manoptexamples/stable/changelog/#[0.1.8]-‚Äì-12/06/2024","content":" [0.1.8] ‚Äì 12/06/2024"},{"id":3540,"pagetitle":"Changelog","title":"Changed","ref":"/manoptexamples/stable/changelog/#Changed-5","content":" Changed use  range  compatible with Julia 1.6 and hence lower the compatibility entry for Julia again."},{"id":3541,"pagetitle":"Changelog","title":"[0.1.7] ‚Äì 07/06/2024","ref":"/manoptexamples/stable/changelog/#[0.1.7]-‚Äì-07/06/2024","content":" [0.1.7] ‚Äì 07/06/2024"},{"id":3542,"pagetitle":"Changelog","title":"Changed","ref":"/manoptexamples/stable/changelog/#Changed-6","content":" Changed make  Manopt.jl  a weak dependency and load functions that require parts of it only load as an extension. This makes it easier to use the examples in the tests of Manopt itself."},{"id":3543,"pagetitle":"Changelog","title":"[0.1.6] ‚Äì 22/03/2024","ref":"/manoptexamples/stable/changelog/#[0.1.6]-‚Äì-22/03/2024","content":" [0.1.6] ‚Äì 22/03/2024"},{"id":3544,"pagetitle":"Changelog","title":"Added","ref":"/manoptexamples/stable/changelog/#Added-2","content":" Added Hyperparameter optimization example."},{"id":3545,"pagetitle":"Changelog","title":"[0.1.3] ‚Äì 11/12/2023","ref":"/manoptexamples/stable/changelog/#[0.1.3]-‚Äì-11/12/2023","content":" [0.1.3] ‚Äì 11/12/2023"},{"id":3546,"pagetitle":"Changelog","title":"Added","ref":"/manoptexamples/stable/changelog/#Added-3","content":" Added Total variation Minimization cost, proxes, and an example B√©zier curve cost, gradients, and an example."},{"id":3547,"pagetitle":"Changelog","title":"[0.1.3] ‚Äì 16/09/2023","ref":"/manoptexamples/stable/changelog/#[0.1.3]-‚Äì-16/09/2023","content":" [0.1.3] ‚Äì 16/09/2023"},{"id":3548,"pagetitle":"Changelog","title":"Added","ref":"/manoptexamples/stable/changelog/#Added-4","content":" Added Rayleigh Quotient functions added an example illustrating Euclidean gradient/HEssian conversion Add Literature with DocumenterCitations"},{"id":3549,"pagetitle":"Changelog","title":"[0.1.2] ‚Äì 13/06/2023","ref":"/manoptexamples/stable/changelog/#[0.1.2]-‚Äì-13/06/2023","content":" [0.1.2] ‚Äì 13/06/2023"},{"id":3550,"pagetitle":"Changelog","title":"Added","ref":"/manoptexamples/stable/changelog/#Added-5","content":" Added Update examples to use Quarto Add DC examples"},{"id":3551,"pagetitle":"Changelog","title":"[0.1.1] ‚Äì 01/03/2023","ref":"/manoptexamples/stable/changelog/#[0.1.1]-‚Äì-01/03/2023","content":" [0.1.1] ‚Äì 01/03/2023"},{"id":3552,"pagetitle":"Changelog","title":"Added","ref":"/manoptexamples/stable/changelog/#Added-6","content":" Added Rosenbrock function and examples"},{"id":3553,"pagetitle":"Changelog","title":"[0.1.0] ‚Äì 18/02/2023","ref":"/manoptexamples/stable/changelog/#[0.1.0]-‚Äì-18/02/2023","content":" [0.1.0] ‚Äì 18/02/2023"},{"id":3554,"pagetitle":"Changelog","title":"Added","ref":"/manoptexamples/stable/changelog/#Added-7","content":" Added Setup the project to collect example objectives for  Manopt.jl  which are well-documented and well-tested Setup Documentation to provide one example Quarto file for every example objective to illustrate how to use them."},{"id":3557,"pagetitle":"Contributing to ManoptExamples.jl","title":"Contributing to Manopt.jl","ref":"/manoptexamples/stable/contributing/#Contributing-to-Manopt.jl","content":" Contributing to  Manopt.jl First, thanks for taking the time to contribute. Any contribution is appreciated and welcome. The following is a set of guidelines to  ManoptExamples.jl ."},{"id":3558,"pagetitle":"Contributing to ManoptExamples.jl","title":"Table of Contents","ref":"/manoptexamples/stable/contributing/#Table-of-Contents","content":" Table of Contents Contributing to  Manopt.jl Table of Contents I just have a question How can I file an issue? How can I contribute? Add an objective Code style"},{"id":3559,"pagetitle":"Contributing to ManoptExamples.jl","title":"I just have a question","ref":"/manoptexamples/stable/contributing/#I-just-have-a-question","content":" I just have a question The developer can most easily be reached in the Julia Slack channel  #manifolds . You can apply for the Julia Slack workspace  here  if you haven't joined yet. You can also ask your question on  our GitHub discussion ."},{"id":3560,"pagetitle":"Contributing to ManoptExamples.jl","title":"How can I file an issue?","ref":"/manoptexamples/stable/contributing/#How-can-I-file-an-issue?","content":" How can I file an issue? If you found a bug or want to propose a feature, we track our issues within the  GitHub repository ."},{"id":3561,"pagetitle":"Contributing to ManoptExamples.jl","title":"How can I contribute?","ref":"/manoptexamples/stable/contributing/#How-can-I-contribute?","content":" How can I contribute?"},{"id":3562,"pagetitle":"Contributing to ManoptExamples.jl","title":"Add an objective","ref":"/manoptexamples/stable/contributing/#Add-an-objective","content":" Add an objective The  objective  in  Manopt.jl  represents the task to be optimised, usually phrased on an arbitrary manifold. The manifold is later specified when wrapping the objective inside a  Problem . If you have a specific objective you would like to provide here, feel free to start a new file in the  src/objectives/  folder in your own fork and propose it later as a  Pull Request . If you objective works without reusing any other objective functions, then they can all just be placed in this one file. If you notice, that you are reusing for example another objectives gradient as part of your objective, please refactor the code, such that the gradient, or other function is in the corresponding file in  src/functions/  and follows the naming scheme: cost functions are always of the form  cost_  and a fitting name gradient functions are always of the  gradient_  and a fitting name, followed by an  ! for in-place gradients and by  !!  if it is a  struct  that can provide both. It would be great if you could also add a small test for the functions and the problem you defined in the  test/  section."},{"id":3563,"pagetitle":"Contributing to ManoptExamples.jl","title":"Add an example","ref":"/manoptexamples/stable/contributing/#Add-an-example","content":" Add an example If you have used one of the problems from here in an example or you are providing a problem together with an example, please add a corresponding  Quarto  Markdown file to the  examples/  folder. The Markdown file should provide a short introduction to the problem and provide links to further details, maybe a paper or a preprint. Use the  bib/literature.yaml  file to add references (in  CSL_YAML , which can for example be exported e.g. from Zotero). Add any packages you need to the  examples/  environment (see the containting  Project.toml ). The examples will not be run on CI, but their rendered  CommonMark  outpout should be included in the list of examples in the documentation of this package."},{"id":3564,"pagetitle":"Contributing to ManoptExamples.jl","title":"Code style","ref":"/manoptexamples/stable/contributing/#Code-style","content":" Code style We try to follow the  documentation guidelines  from the Julia documentation as well as  Blue Style . We run  JuliaFormatter.jl  on the repo in the way set in the  .JuliaFormatter.toml  file, which enforces a number of conventions consistent with the Blue Style. We also follow a few internal conventions: Any implemented function should be accompanied by its mathematical formulae if a closed form exists. within a file the structs should come first and functions second. The only exception are constructors for the structs within both blocks an alphabetical order is preferable. The above implies that the mutating variant of a function follows the non-mutating variant. There should be no dangling  =  signs. Always add a newline between things of different types (struct/method/const). Always add a newline between methods for different functions (including in-place/non-mutating variants). Prefer to have no newline between methods for the same function; when reasonable, merge the docstrings into a generic function signature. All  import / using / include  should be in the main module file. There should only be a minimum of  export s within this file, all problems should usually be later addressed as  ManoptExamples.[...] the Quarto Markdown files are excluded from this formatting."},{"id":3567,"pagetitle":"Data","title":"Data sets","ref":"/manoptexamples/stable/data/#Data-sets","content":" Data sets"},{"id":3568,"pagetitle":"Data","title":"Signals on manifolds","ref":"/manoptexamples/stable/data/#Signals-on-manifolds","content":" Signals on manifolds"},{"id":3569,"pagetitle":"Data","title":"ManoptExamples.Lemniscate","ref":"/manoptexamples/stable/data/#ManoptExamples.Lemniscate-Tuple{Number}","content":" ManoptExamples.Lemniscate  ‚Äî  Method Lemniscate(t::Float; kwargs...)\nLemniscate(n::integer; interval=[0.0, 2œÄ], kwargs...) generate the  Lemniscate of Bernoulli  as a curve on a manifold, by generating the curve emplying the keyword arguments below. To be precise on the manifold  M  we use the tangent space at  p  and generate the curve \\[Œ≥(t) \\frac{a}{}\\sin^2(t) + 1 \\begin{pmatrix} \\cos(t) \\\\ \\cos(t)\\sin(t) \\end{pmatrix}\\] in the plane spanned by  X  and  Y  in the tangent space. Note that this curve is  $2œÄ$ -periodic and  a  is the  half-width  of the curve. To reproduce the first examples from [ BBSW16 ] as a default, on the sphere  p  defaults to the North pole. THe second variant generates  n  points equispaced in √¨nterval` and calls the first variant. Keywords manifold  - ( Sphere (2) ) the manifold to build the lemniscate on p         - ( [0.0, 0.0, 1.0]  on the sphere, `rand(M) else) the center point of the Lemniscate a         ‚Äì ( œÄ/2.0 ) half-width of the Lemniscate X         ‚Äì ( [1.0, 0.0, 0.0]  for the 2-sphere with default p, the first  DefaultOrthonormalBasis ()  vector otherwise) first direction for the plane to define the Lemniscate in, unit vector recommended. Y         ‚Äì ( [0.0, 1.0, 0.0]  if p is the default, the second  DefaultOrthonormalBasis ()  vector otherwise) second direction for the plane to define the Lemniscate in, unit vector orthogonal to  X  recommended. source"},{"id":3570,"pagetitle":"Data","title":"ManoptExamples.artificial_S1_signal","ref":"/manoptexamples/stable/data/#ManoptExamples.artificial_S1_signal","content":" ManoptExamples.artificial_S1_signal  ‚Äî  Function artificial_S1_signal([pts=500]) generate a real-valued signal having piecewise constant, linear and quadratic intervals with jumps in between. If the resulting manifold the data lives on, is the  Circle  the data is also wrapped to ``[ BLSW14 ]. Optional pts : ( 500 ) number of points to sample the function source"},{"id":3571,"pagetitle":"Data","title":"ManoptExamples.artificial_S1_signal","ref":"/manoptexamples/stable/data/#ManoptExamples.artificial_S1_signal-Tuple{Real}","content":" ManoptExamples.artificial_S1_signal  ‚Äî  Method artificial_S1_signal(x) evaluate the example signal  $f(x), x ‚àà  [0,1]$ , of phase-valued data introduces in Sec. 5.1 of  [ BLSW14 ] for values outside that interval, this Signal is  missing . source"},{"id":3572,"pagetitle":"Data","title":"ManoptExamples.artificial_S1_slope_signal","ref":"/manoptexamples/stable/data/#ManoptExamples.artificial_S1_slope_signal","content":" ManoptExamples.artificial_S1_slope_signal  ‚Äî  Function artificial_S1_slope_signal([pts=500, slope=4.]) Creates a Signal of (phase-valued) data represented on the  Circle  with increasing slope. Optional pts :    ( 500 ) number of points to sample the function. slope :  ( 4.0 ) initial slope that gets increased afterwards This data set was introduced for the numerical examples in [ BLSW14 ] source"},{"id":3573,"pagetitle":"Data","title":"ManoptExamples.artificial_S2_composite_Bezier_curve","ref":"/manoptexamples/stable/data/#ManoptExamples.artificial_S2_composite_Bezier_curve-Tuple{}","content":" ManoptExamples.artificial_S2_composite_Bezier_curve  ‚Äî  Method artificial_S2_composite_Bezier_curve() Generate a composite B√©zier curve on the [ BG18 ]. It consists of 4 egments connecting the points \\[\\mathbf d_0 = \\begin{pmatrix} 0\\\\0\\\\1\\end{pmatrix},\\quad\n\\mathbf d_1 = \\begin{pmatrix} 0\\\\-1\\\\0\\end{pmatrix},\\quad\n\\mathbf d_2 = \\begin{pmatrix} -1\\\\0\\\\0\\end{pmatrix},\\text{ and }\n\\mathbf d_3 = \\begin{pmatrix} 0\\\\0\\\\-1\\end{pmatrix}.\\] where instead of providing the two center control points explicitly we provide them as velocities from the corresponding points, such thtat we can directly define the curve to be  $C^1$ . We define \\[X_0 = \\frac{œÄ}{8\\sqrt{2}}\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix},\\quad\nX_1 = \\frac{œÄ}{4\\sqrt{2}}\\begin{pmatrix}1\\\\0\\\\1\\end{pmatrix},\\quad\nX_2 = \\frac{œÄ}{4\\sqrt{2}}\\begin{pmatrix}0\\\\1\\\\-1\\end{pmatrix},\\text{ and }\nX_3 = \\frac{œÄ}{8\\sqrt{2}}\\begin{pmatrix}-1\\\\1\\\\0\\end{pmatrix},\\] where we defined each  $X_i \\in T_{d_i}\\mathbb S^2$ . We defined three  BezierSegment s of cubic B√©zier curves as follows \\[\\begin{align*}\nb_{0,0} &= d_0, \\quad & b_{1,0} &= \\exp_{d_0}X_0, \\quad & b_{2,0} &= \\exp_{d_1}X_1, \\quad & b_{3,0} &= d_1\\\\\nb_{0,1} &= d_1, \\quad & b_{1,1} &= \\exp_{d_1}(-X_1), \\quad & b_{2,1} &= \\exp_{d_2}X_2, \\quad & b_{3,1} &= d_2\\\\\nb_{0,2} &= d_2, \\quad & b_{1,1} &= \\exp_{d_2}(-X_2), \\quad & b_{2,2} &= \\exp_{d_3}X_3, \\quad & b_{3,2} &= d_3.\n\\end{align*}\\] source"},{"id":3574,"pagetitle":"Data","title":"images on manifolds","ref":"/manoptexamples/stable/data/#images-on-manifolds","content":" images on manifolds"},{"id":3575,"pagetitle":"Data","title":"ManoptExamples.artificialIn_SAR_image","ref":"/manoptexamples/stable/data/#ManoptExamples.artificialIn_SAR_image-Tuple{Integer}","content":" ManoptExamples.artificialIn_SAR_image  ‚Äî  Method artificialIn_SAR_image([pts=500]) generate an artificial InSAR image, i.e. phase valued data, of size  pts  x  pts  points. This data set was introduced for the numerical examples in [ BLSW14 ]. source"},{"id":3576,"pagetitle":"Data","title":"ManoptExamples.artificial_S2_rotation_image","ref":"/manoptexamples/stable/data/#ManoptExamples.artificial_S2_rotation_image","content":" ManoptExamples.artificial_S2_rotation_image  ‚Äî  Function artificial_S2_rotation_image([pts=64, rotations=(.5,.5)]) Create an image with a rotation on each axis as a parametrization. Optional Parameters pts :       ( 64 ) number of pixels along one dimension rotations : ( (.5,.5) ) number of total rotations performed on the axes. This dataset was used in the numerical example of Section 5.1 of [ BBSW16 ]. source"},{"id":3577,"pagetitle":"Data","title":"ManoptExamples.artificial_S2_whirl_image","ref":"/manoptexamples/stable/data/#ManoptExamples.artificial_S2_whirl_image","content":" ManoptExamples.artificial_S2_whirl_image  ‚Äî  Function artificial_S2_whirl_image([pts::Int=64]) Generate an artificial image of data on the 2 sphere, Arguments pts : ( 64 ) size of the image in  pts √ó pts  pixel. This example dataset was used in the numerical example in Section 5.5 of [ LNPS17 ] It is based on  artificial_S2_rotation_image  extended by small whirl patches. source"},{"id":3578,"pagetitle":"Data","title":"ManoptExamples.artificial_S2_whirl_patch","ref":"/manoptexamples/stable/data/#ManoptExamples.artificial_S2_whirl_patch","content":" ManoptExamples.artificial_S2_whirl_patch  ‚Äî  Function artificial_S2_whirl_patch([pts=5]) create a whirl within the  pts √ó pts  patch of  Sphere (@ref) (2) -valued image data. These patches are used within  artificial_S2_whirl_image . Optional Parameters pts : ( 5 ) size of the patch. If the number is odd, the center is the north pole. source"},{"id":3579,"pagetitle":"Data","title":"ManoptExamples.artificial_SPD_image","ref":"/manoptexamples/stable/data/#ManoptExamples.artificial_SPD_image","content":" ManoptExamples.artificial_SPD_image  ‚Äî  Function artificial_SPD_image([pts=64, stepsize=1.5]) create an artificial image of symmetric positive definite matrices of size  pts √ó pts  pixel with a jump of size  stepsize . This dataset was used in the numerical example of Section 5.2 of [ BBSW16 ]. source"},{"id":3580,"pagetitle":"Data","title":"ManoptExamples.artificial_SPD_image2","ref":"/manoptexamples/stable/data/#ManoptExamples.artificial_SPD_image2","content":" ManoptExamples.artificial_SPD_image2  ‚Äî  Function artificial_SPD_image2([pts=64, fraction=.66]) create an artificial image of symmetric positive definite matrices of size  pts √ó pts  pixel with right hand side  fraction  is moved upwards. This data set was introduced in the numerical examples of Section of [ BPS16 ] source"},{"id":3581,"pagetitle":"Data","title":"Literature","ref":"/manoptexamples/stable/data/#Literature","content":" Literature [BBSW16] M.¬†Baƒç√°k, R.¬†Bergmann, G.¬†Steidl and A.¬†Weinmann.  A second order non-smooth variational model for restoring manifold-valued images .  SIAM¬†Journal¬†on¬†Scientific¬†Computing  38 , A567‚ÄìA597  (2016),  arXiv:1506.02409 . [BG18] R.¬†Bergmann and P.-Y.¬†Gousenbourger.  A variational model for data fitting on manifolds by minimizing the acceleration of a B√©zier curve .  Frontiers¬†in¬†Applied¬†Mathematics¬†and¬†Statistics  4  (2018),  arXiv:1807.10090 . [BLSW14] R.¬†Bergmann, F.¬†Laus, G.¬†Steidl and A.¬†Weinmann.  Second order differences of cyclic data and applications in variational denoising .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  7 , 2916‚Äì2953  (2014),  arXiv:1405.5349 . [BPS16] R.¬†Bergmann, J.¬†Persch and G.¬†Steidl.  A parallel Douglas Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  9 , 901‚Äì937  (2016),  arXiv:1512.02814 . [LNPS17] F.¬†Laus, M.¬†Nikolova, J.¬†Persch and G.¬†Steidl.  A nonlocal denoising algorithm for manifold-valued images using second order statistics .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  10 , 416‚Äì448  (2017)."},{"id":3584,"pagetitle":"Overview","title":"List of Examples","ref":"/manoptexamples/stable/examples/#List-of-Examples","content":" List of Examples Name provides Documentation Comment A Benchmark for Difference of Convex contains a few simple functions B√©zier Curves and Minimizing their Acceleration tools B√©zier curves and their acceleration üìö Solving Rosenbrock with Difference of Convex DoC split of Rosenbrock üìö uses a Rosenbrock based metric Difference of Convex vs. Frank-Wolfe closed-form sub solver Riemannian Mean $f$ ,  $\\operatorname{grad}f$  (A/I), objective üìö Robust PCA $f$ ,  $\\operatorname{grad}f$  (A/I), objective üìö Rosenbrock $f$ ,  $\\operatorname{grad}f$  (A/I), objective, minimizer üìö The Rayleigh Quotient $f$ ,  $\\operatorname{grad}f$  (A/I),  $\\operatorname{Hess}f$  (A/I), objective üìö Total Variation Minimization $f$ ,  $\\operatorname{prox}f$  (A/I), objective üìö Symbols: A  Allocating variant I  In-place variant üìö  link to documented functions in the documentation"},{"id":3587,"pagetitle":"Minimizing the Acceleration of B√©zier Curves on the Sphere","title":"Minimizing the Acceleration of B√©zier Curves on the Sphere","ref":"/manoptexamples/stable/examples/Bezier-curves/#Minimizing-the-Acceleration-of-B√©zier-Curves-on-the-Sphere","content":" Minimizing the Acceleration of B√©zier Curves on the Sphere Ronny Bergmann 2023-06-06 using Manifolds, Manopt, ManoptExamples"},{"id":3588,"pagetitle":"Minimizing the Acceleration of B√©zier Curves on the Sphere","title":"Introduction","ref":"/manoptexamples/stable/examples/Bezier-curves/#Introduction","content":" Introduction B√©zier Curves can be generalized to manifolds by generalizing the  de Casteljau algorithm üìñ  to work with geodesics instead of straight lines. An implementation in just a few lines as we demonstrated in [ ABBR23 ] as function bezier(M::AbstractManifold, t, pts::NTuple)\n    p = bezier(M, t, pts[1:(end - 1)])\n    q = bezier(M, t, pts[2:end])\n    return shortest_geodesic(M, p, q, t)\nend\nfunction bezier(M::AbstractManifold, t, pts::NTuple{2})\n    return shortest_geodesic(M, pts[1], pts[2], t)\nend which is also available within this package as  de_Casteljau  using a simple  BezierSegment struct  to make it easier to also discuss the case where we compose a set of segments into a composite B√©zier course. In the following we will need the following packages and functions. They are documented in the section on  Bezier Curves  and were derived in [ BG18 ] based on [ PN07 ]. using ManoptExamples:\n    artificial_S2_composite_Bezier_curve,\n    BezierSegment,\n    de_Casteljau,\n    get_Bezier_degrees,\n    get_Bezier_inner_points,\n    get_Bezier_junctions,\n    get_Bezier_junction_tangent_vectors,\n    get_Bezier_points,\n    get_Bezier_segments,\n    grad_L2_acceleration_Bezier,\n    L2_acceleration_Bezier This notebook reproduces the example form Section 5.2 in [ BG18 ]. The following image illustrates how the de-Casteljau algorithm works for one segment. A Bezier segment and illustration of the de-Casteljau algorithm"},{"id":3589,"pagetitle":"Minimizing the Acceleration of B√©zier Curves on the Sphere","title":"Approximating data by a curve with minimal accelartion","ref":"/manoptexamples/stable/examples/Bezier-curves/#Approximating-data-by-a-curve-with-minimal-accelartion","content":" Approximating data by a curve with minimal accelartion We first load our example data M = Sphere(2)\nB = artificial_S2_composite_Bezier_curve()\ndata_points = get_Bezier_junctions(M, B) Which is the following cure, which clearly starts and ends slower than its speed in the middle, which can be seen by the increasing length of the tangent vectors in the middle. The original curve We continue to recude the points, since we ‚Äúknow‚Äù sme points due to the  $C^1$  property: the second to last control point of the first segment  $b_{0,2}$ , the joint junction point connecting both segments  $b_{0,3}=b_{1,0}$  and the second control point  $b_{1,1}$  of the second segment have to line in the tangent space of the joint junction point. Hence we only have to store one of the control points. We can use this reduced form as the variable to optimize and the one from the data as our initial point. pB = get_Bezier_points(M, B, :differentiable)\nN = PowerManifold(M, NestedPowerRepresentation(), length(pB)) PowerManifold(Sphere(2, ‚Ñù), NestedPowerRepresentation(), 8) And we further define the acceleration of the curve as our cost function, where we discretize the acceleration at a certain set of points and set the  $Œª=10$ curve_samples = [range(0, 3; length=101)...] # sample curve for the gradient\nŒª = 10.0\nfunction f(M, pB)\n    return L2_acceleration_Bezier(\n        M.manifold, pB, get_Bezier_degrees(M.manifold, B), curve_samples, Œª, data_points\n    )\nend\nfunction grad_f(M, pB)\n    return grad_L2_acceleration_Bezier(\n        M.manifold, pB, get_Bezier_degrees(M.manifold, B), curve_samples, Œª, data_points\n    )\nend grad_f (generic function with 1 method) Then we can optimize x0 = pB\npB_opt = gradient_descent(\n    N,\n    f,\n    grad_f,\n    x0;\n    stepsize=ArmijoLinesearch(N;\n        initial_stepsize=1.0,\n        retraction_method=ExponentialRetraction(),\n        contraction_factor=0.5,\n        sufficient_decrease=0.001,\n    ),\n    stopping_criterion=StopWhenChangeLess(N, 1e-5) |\n                       StopWhenGradientNormLess(1e-7) |\n                       StopAfterIteration(300),\n    debug=[\n        :Iteration,\n        \" | \",\n        :Cost,\n        \" | \",\n        DebugGradientNorm(),\n        \" | \",\n        DebugStepsize(),\n        \" | \",\n        :Change,\n        \"\\n\",\n        25,\n        :Stop,\n    ],\n); Initial  | f(x): 10.647244 |  |  | \n# 25     | f(x): 2.667564 | |grad f(p)|:0.890845571434862 | s:0.01326670131422904 | Last Change: 0.763281\n# 50     | f(x): 2.650064 | |grad f(p)|:0.05536989605165708 | s:0.05306680525691616 | Last Change: 0.081780\n# 75     | f(x): 2.649707 | |grad f(p)|:0.02135638585837997 | s:0.01326670131422904 | Last Change: 0.011590\n# 100    | f(x): 2.649700 | |grad f(p)|:0.0012887575647752057 | s:0.05306680525691616 | Last Change: 0.001745\nAt iteration 109 the algorithm performed a step with a change (2.9063044690733034e-7) less than 1.0e-5. And we can again look at the result The result looks as The resulting curve where all control points are evenly spaced and we hence have less acceleration as the final cost compared to the initial one indicates. Note that the cost is not zero, since we always have a tradeoff between approximating the initial junctinons (data points) and minimizing the acceleration. [ABBR23] S.¬†D.¬†Axen, M.¬†Baran, R.¬†Bergmann and K.¬†Rzecki.  Manifolds.jl: An Extensible Julia Framework for Data Analysis on Manifolds .  ACM¬†Transactions¬†on¬†Mathematical¬†Software  (2023),  arXiv:2021.08777 . [BG18] R.¬†Bergmann and P.-Y.¬†Gousenbourger.  A variational model for data fitting on manifolds by minimizing the acceleration of a B√©zier curve .  Frontiers¬†in¬†Applied¬†Mathematics¬†and¬†Statistics  4  (2018),  arXiv:1807.10090 . [PN07] T.¬†Popiel and L.¬†Noakes.  B√©zier curves and  $C^2$  interpolation in Riemannian manifolds .  Journal¬†of¬†Approximation¬†Theory  148 , 111‚Äì127  (2007)."},{"id":3592,"pagetitle":"Mean on mathbb H^2","title":"The Constrained mean on Hyperbolic space.","ref":"/manoptexamples/stable/examples/Constrained-Mean-H2/#The-Constrained-mean-on-Hyperbolic-space.","content":" The Constrained mean on Hyperbolic space. Ronny Bergmann 2027-03-03"},{"id":3593,"pagetitle":"Mean on mathbb H^2","title":"Introduction","ref":"/manoptexamples/stable/examples/Constrained-Mean-H2/#Introduction","content":" Introduction In this example we compare the Pprojected Gradient Algorithm (PGA) as introduced in [ BFNZ25 ] with both the Augmented Lagrangian Method (ALM) and the Exact Penalty Method (EPM) [ LB19 ]. using Chairmarks, CSV, DataFrames, Manifolds, Manopt, CairoMakie, Random Consider the constrained Riemannian center of mass for a given set of points ``q_i M $ $i=1,\\ldots,N$  given by \\[\\operatorname*{arg\\,min}_{p\\in\\mathcal C}\n\\sum_{i=1}^N d_{\\mathrm{M}}^2(p,q_i)\\] constrained to a set  $\\mathcal C \\subset \\mathcal M$ . For this experiment set  $\\mathcal M = \\mathbb H^2$  is the  Hyperbolic space  and the constrained set  $\\mathcal C = C_{c,r}$  as the ball of radius  $r$  around the center point  $c$ , where we choose here  $r=1$  and  $c = (0,0,1)^{\\mathrm{T}}$ . M = Hyperbolic(2)\nc = Manifolds._hyperbolize(M, [0, 0])\nradius = 1.0\n# Sample the boundary\nunit_circle = [\n    exp(M, c, get_vector(M, c, radius .* [cos(Œ±), sin(Œ±)], DefaultOrthonormalBasis())) for\n    Œ± in 0:(2œÄ / 720):(2œÄ)\n] Our data consists of  $N=200$  points, where we skew the data a bit to force the mean to be outside of the constrained set  $\\mathcal C$ . N = 200;\nœÉ = 1.5\nRandom.seed!(42)\n# N random points moved to top left to have a mean outside\ndata_pts = [\n    exp(\n        M,\n        c,\n        get_vector(\n            M, c, œÉ .* randn(manifold_dimension(M)) .+ [2.5, 2.5], DefaultOrthonormalBasis()\n        ),\n    ) for _ in 1:N\n]"},{"id":3594,"pagetitle":"Mean on mathbb H^2","title":"Cost, gradient and projection","ref":"/manoptexamples/stable/examples/Constrained-Mean-H2/#Cost,-gradient-and-projection","content":" Cost, gradient and projection We can formulate the constrained problem above in two different forms. Both share a cost adn require a gradient. For performance reasons, we also provide a mutating variant of the gradient f(M, p; pts=[op]) = 1 / (2 * length(pts)) .* sum(distance(M, p, q)^2 for q in pts);\n\ngrad_f(M, p; pts=[op]) = -1 / length(pts) .* sum(log(M, p, q) for q in pts);\n\nfunction grad_f!(M, X, p; pts=[op])\n    zero_vector!(M, X, p)\n    Y = zero_vector(M, p)\n    for q in pts\n        log!(M, Y, p, q)\n        X .+= Y\n    end\n    X .*= -1 / length(pts)\n    return X\nend; We can model the constrained either with an inequality constraint  $g(p) \\geq 0$  or using a projection onto the set. For the gradient of  $g$  and the projection we again also provide mutating variants. g(M, p) = distance(M, c, p)^2 - radius^2;\nindicator_C(M, p) = (g(M, p) ‚â§ 0) ? 0 : Inf;\n\nfunction project_C(M, p, r=radius)\n    X = log(M, c, p)\n    n = norm(M, c, X)\n    q = (n > r) ? exp(M, c, (r / n) * X) : copy(M, p)\n    return q\nend;\nfunction project_C!(M, q, p; X=zero_vector(M, c), r=radius)\n    log!(M, X, c, p)\n    n = norm(M, c, X)\n    if (n > r)\n        exp!(M, q, c, (r / n) * X)\n    else\n        copyto!(M, q, p)\n    end\n    return q\nend;\n\ngrad_g(M, p) = -2 * log(M, p, c);\nfunction grad_g!(M, X, p)\n    log!(M, X, p, c)\n    X .*= -2\n    return X\nend"},{"id":3595,"pagetitle":"Mean on mathbb H^2","title":"The mean","ref":"/manoptexamples/stable/examples/Constrained-Mean-H2/#The-mean","content":" The mean For comparison, we first compute the Riemannian center of mass, that is the minimization above but not constrained to  $\\mathcal C$ . We can then project this onto  $\\mathcal C$ . For the projected mean we obtain  $g(p) = 0$  since the original mean is outside of the set, the projected one lies on the boundary. mean_data = mean(M, data_pts)\nmean_projected = project_C(M, mean_data)\ng(M, mean_projected) 0.0"},{"id":3596,"pagetitle":"Mean on mathbb H^2","title":"The experiment","ref":"/manoptexamples/stable/examples/Constrained-Mean-H2/#The-experiment","content":" The experiment We first define the specific data cost functions _f(M, p) = f(M, p; pts=data_pts)\n_grad_f(M, p) = grad_f(M, p; pts=data_pts)\n_grad_f!(M, X, p) = grad_f!(M, X, p; pts=data_pts) and in a first run record a projected gradient method solver run mean_pg = copy(M, c) # start at the center\n@time pgms = projected_gradient_method!(\n    M, _f, _grad_f!, project_C!, mean_pg;\n    evaluation=InplaceEvaluation(),\n    indicator=indicator_C,\n    debug=[:Iteration, :Cost, \" \", :GradientNorm, \"\\n\", 1, :Stop],\n    record=[:Iteration, :Iterate, :Cost, :Gradient],\n    stopping_criterion=StopAfterIteration(150) |\n                       StopWhenProjectedGradientStationary(M, 1e-7),\n    return_state=true,\n) Initial f(x): 8.519830\n# 1     f(x): 5.741908 |grad f(p)|:3.560737798355543\n# 2     f(x): 5.741846 |grad f(p)|:1.881900575821152\n# 3     f(x): 5.741846 |grad f(p)|:1.8819696248924744\n# 4     f(x): 5.741846 |grad f(p)|:1.881964795224877\n# 5     f(x): 5.741846 |grad f(p)|:1.8819649705365404\n# 6     f(x): 5.741846 |grad f(p)|:1.8819649640556793\nAt iteration 6 algorithm has reached a stationary point, since the distance from the last iterate to the projected gradient (1.0030679901141345e-8) less than 1.0e-7.\n  1.589012 seconds (7.34 M allocations: 370.937 MiB, 3.65% gc time, 99.67% compilation time)\n\n# Solver state for `Manopt.jl`s Projected Gradient Method\nAfter 6 iterations\n\n## Parameters\n* inverse retraction method: LogarithmicInverseRetraction()\n* retraction method: ExponentialRetraction()\n\n## Stepsize for the gradient step\nConstantLength(1.0; type=:relative)\n\n## Stepsize for the complete step\nArmijoLinesearch(;\n    initial_stepsize=1.0\n    retraction_method=ExponentialRetraction()\n    contraction_factor=0.95\n    sufficient_decrease=0.1\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 150:  not reached\n  * projected gradient stationary (<1.0e-7):    reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-6d\"), (:Cost, \"f(x): %f\"), \" \", (:GradientNorm, \"|grad f(p)|:%s\"), \"\\n\", 1]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Vector{Float64}), RecordCost(), RecordGradient{Vector{Float64}}()]),) and similarly perform a first run of both the  augmented Lagrangian method  and the  exact penalty method mean_alm = copy(M, c)\n@time alms = augmented_Lagrangian_method!(\n    M, _f, _grad_f!, mean_alm;\n    evaluation=InplaceEvaluation(),\n    g=[g], grad_g=[grad_g!],\n    debug=[:Iteration, :Cost, \" \", \"\\n\", 10, :Stop],\n    record=[:Iteration, :Iterate, :Cost],\n    return_state=true,\n) Initial f(x): 8.519830\n# 10    f(x): 5.741814\n# 20    f(x): 5.741845\nThe algorithm computed a step size (5.830448990119683e-11) less than 1.0e-10.\n  1.748130 seconds (9.92 M allocations: 503.845 MiB, 4.47% gc time, 99.07% compilation time)\n\n# Solver state for `Manopt.jl`s Augmented Lagrangian Method\nAfter 29 iterations\n\n## Parameters\n* œµ: 0.0001348962882591652 (œµ_min: 1.0e-6, Œ∏_œµ: 0.933254300796991)\n* Œª: Float64[] (Œª_min: -20.0, Œª_max: 20.0)\n* Œº: [0.94098958634295] (Œº_max: 20.0)\n* œÅ: 15241.579027587262 (Œ∏_œÅ: 0.3)\n* œÑ: 0.8\n* current penalty: 1.1472098826459387e-9\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 300:  not reached\n  * Stop When _all_ of the following are fulfilled:\n      * Field :œµ ‚â§ 1.0e-6:  not reached\n      * |Œîp| < 0.00014454397707459258: not reached\n    Overall: not reached\n  * Stepsize s < 1.0e-10:   reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-6d\"), (:Cost, \"f(x): %f\"), \" \", \"\\n\", 10]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Vector{Float64}), RecordCost()]),) mean_epm = copy(M, c)\n@time epms = exact_penalty_method!(\n    M, _f, _grad_f!, mean_epm;\n    evaluation = InplaceEvaluation(),\n    g = [g], grad_g = [grad_g!],\n    debug = [:Iteration, :Cost, \" \", \"\\n\", 25, :Stop],\n    record = [:Iteration, :Iterate, :Cost],\n    return_state = true,\n) Initial f(x): 8.519830\n# 25    f(x): 5.747352\n# 50    f(x): 5.742157\n# 75    f(x): 5.741863\n# 100   f(x): 5.741847\nThe value of the variable (œµ) is smaller than or equal to its threshold (1.0e-6).\nAt iteration 101 the algorithm performed a step with a change (5.712257693422003e-8) less than 1.0e-6.\n  1.078051 seconds (7.59 M allocations: 359.751 MiB, 4.62% gc time, 89.39% compilation time)\n\n# Solver state for `Manopt.jl`s Exact Penalty Method\nAfter 101 iterations\n\n## Parameters\n* œµ: 1.0e-6 (œµ_min: 1.0e-6, Œ∏_œµ: 0.933254300796991)\n* u: 1.0e-6 (œµ_min: 1.0e-6, Œ∏_u: 0.8912509381337456)\n* œÅ: 3.3333333333333335 (Œ∏_œÅ: 0.3)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 300:  not reached\n  * Stop When _all_ of the following are fulfilled:\n      * Field :œµ ‚â§ 1.0e-6:  reached\n      * |Œîp| < 1.0e-6: reached\n    Overall: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-6d\"), (:Cost, \"f(x): %f\"), \" \", \"\\n\", 25]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Vector{Float64}), RecordCost()]),)"},{"id":3597,"pagetitle":"Mean on mathbb H^2","title":"Benchmark","ref":"/manoptexamples/stable/examples/Constrained-Mean-H2/#Benchmark","content":" Benchmark After a first run we now Benchmark the three algorithms with  Chairmarks.jl pg_b = @be projected_gradient_method!(\n    $M, $_f, $_grad_f!, $project_C!, $(copy(M, c));\n    evaluation=$(InplaceEvaluation()),\n    indicator=$indicator_C,\n    stopping_criterion=$(\n        StopAfterIteration(150) | StopWhenProjectedGradientStationary(M, 1e-7)\n    ),\n) evals = 1 samples = 5 seconds = 100 Benchmark: 5 samples with 1 evaluation\n min    183.583 Œºs (3862 allocs: 145.734 KiB)\n median 231.000 Œºs (5486 allocs: 208.797 KiB)\n mean   290.992 Œºs (7353.60 allocs: 281.319 KiB)\n max    605.583 Œºs (17260 allocs: 666.000 KiB) alm_b = @be augmented_Lagrangian_method!(\n    $M, $_f, $_grad_f!, $(copy(M, c));\n    evaluation = $(InplaceEvaluation()),\n    g = $([g]),\n    grad_g = $([grad_g!]),\n) evals = 1 samples = 5 seconds = 100 Benchmark: 5 samples with 1 evaluation\n min    13.538 ms (322539 allocs: 11.890 MiB)\n median 15.662 ms (391018 allocs: 14.427 MiB)\n mean   15.472 ms (369894.80 allocs: 13.646 MiB, 2.63% compile time)\n max    16.310 ms (400764 allocs: 14.791 MiB, 13.15% compile time) epm_b = @be exact_penalty_method!(\n    $M, $_f, $_grad_f!, $(copy(M, c));\n    evaluation = $(InplaceEvaluation()),\n    g = $([g]),\n    grad_g = $([grad_g!]),\n) evals = 1 samples = 5 seconds = 100 Benchmark: 5 samples with 1 evaluation\n min    87.115 ms (1981548 allocs: 73.062 MiB)\n median 90.660 ms (1981548 allocs: 73.062 MiB)\n mean   94.963 ms (1981548 allocs: 73.062 MiB, 6.10% gc time)\n max    109.907 ms (1981548 allocs: 73.062 MiB, 16.78% gc time)"},{"id":3598,"pagetitle":"Mean on mathbb H^2","title":"Plots & results","ref":"/manoptexamples/stable/examples/Constrained-Mean-H2/#Plots-and-results","content":" Plots & results pb_x(data) = [convert(PoincareBallPoint, p).value[1] for p in data]\npb_y(data) = [convert(PoincareBallPoint, p).value[2] for p in data] The results are fig = Figure()\naxis = Axis(fig[1, 1], title = \"The ball constrained mean comparison\", aspect = 1)\narc!(Point2f(0, 0), 1, -œÄ, œÄ; color = :black)\nlines!(axis, pb_x(unit_circle), pb_y(unit_circle); label = L\"Œ¥C\")\nscatter!(axis, pb_x(data_pts), pb_y(data_pts), label = L\"d_i\")\nscatter!(axis, pb_x([mean_data]), pb_y([mean_data]), label = L\"m\")\nscatter!(\n    axis,\n    pb_x([mean_projected]),\n    pb_y([mean_projected]),\n    label = L\"m_{\\text{proj}}\",\n)\nscatter!(axis, pb_x([mean_alm]), pb_y([mean_alm]), label = L\"m_{\\text{alm}}\")\nscatter!(axis, pb_x([mean_epm]), pb_y([mean_epm]), label = L\"m_{\\text{epm}}\")\nscatter!(axis, pb_x([mean_pg]), pb_y([mean_pg]), label = L\"m_{\\text{pg}}\")\naxislegend(axis, position = :rt)\nxlims!(axis, -1.02, 1.5)\nylims!(axis, -1.02, 1.5)\nhidespines!(axis)\nhidedecorations!(axis)\nfig min_cost = minimum(map(p -> _f(M, p), [mean_pg, mean_alm, mean_epm])) 5.7418455315254855 fig2 = Figure()\naxis2 = Axis(\n    fig2[1, 1];\n    title=\"Cost over iterations (log scale x)\",\n    xscale=log10,\n    yscale=identity,\n    xticks=[1, 10, 100],\n)\nlines!(\n    axis2,\n    get_record(pgms, :Iteration, :Iteration),\n    get_record(pgms, :Iteration, :Cost);\n    label=\"PG\",\n)\nlines!(\n    axis2,\n    get_record(alms, :Iteration, :Iteration),\n    get_record(alms, :Iteration, :Cost);\n    label=\"ALM\",\n)\nlines!(\n    axis2,\n    get_record(epms, :Iteration, :Iteration),\n    get_record(epms, :Iteration, :Cost);\n    label=\"EPM\",\n)\naxislegend(axis2; position=:rb)\n#ylims!(axis2, min_cost-0.001,)\naxis2.xlabel = \"Iterations (log scale)\"\naxis2.ylabel = \"Cost f\"\nfig2"},{"id":3599,"pagetitle":"Mean on mathbb H^2","title":"Literature","ref":"/manoptexamples/stable/examples/Constrained-Mean-H2/#Literature","content":" Literature [BFNZ25] R.¬†Bergmann, O.¬†P.¬†Ferreira, S.¬†Z.¬†N√©meth and J.¬†Zhu.  On projection mappings and the gradient projection method                on hyperbolic space forms . Preprint,¬†in¬†preparation (2025). [LB19] C.¬†Liu and N.¬†Boumal.  Simple algorithms for optimization on Riemannian manifolds with constraints .  Applied¬†Mathematics¬†&¬†Optimization  (2019),  arXiv:1091.10000 ."},{"id":3600,"pagetitle":"Mean on mathbb H^2","title":"Technical details","ref":"/manoptexamples/stable/examples/Constrained-Mean-H2/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. Status `/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.13.4\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.29.0\n‚åÖ [5ae59095] Colors v0.12.11\n  [a93c6f00] DataFrames v1.7.0\n  [7073ff75] IJulia v1.27.0\n  [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [d3d80556] LineSearches v7.3.0\n  [ee78f7c6] Makie v0.22.4\n  [af67fdf4] ManifoldDiff v0.4.2\n  [1cead3c2] Manifolds v0.10.16\n  [3362f125] ManifoldsBase v1.0.3\n  [0fc0a36d] Manopt v0.5.12\n  [5b8d5e80] ManoptExamples v0.1.14 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.40.12\n  [08abe8d2] PrettyTables v2.4.0\n  [6099a3de] PythonCall v0.9.24\n  [f468eda6] QuadraticModels v0.9.8\n  [1e40b3f8] RipQP v0.6.4\nInfo Packages marked with ‚åÖ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated` This tutorial was last rendered April 13, 2025, 14:06:35."},{"id":3603,"pagetitle":"Mean on mathbb H^n","title":"The Constrained mean on high-dimensional Hyperbolic space.","ref":"/manoptexamples/stable/examples/Constrained-Mean-Hn/#The-Constrained-mean-on-high-dimensional-Hyperbolic-space.","content":" The Constrained mean on high-dimensional Hyperbolic space. Ronny Bergmann 2027-03-03"},{"id":3604,"pagetitle":"Mean on mathbb H^n","title":"Introduction","ref":"/manoptexamples/stable/examples/Constrained-Mean-Hn/#Introduction","content":" Introduction In this example we compare the Pprojected Gradient Algorithm (PGA) as introduced in [ BFNZ25 ] with both the Augmented Lagrangian Method (ALM) and the Exact Penalty Method (EPM) [ LB19 ]. using Chairmarks, CSV, DataFrames, Manifolds, Manopt, CairoMakie, Random Consider the constrained Riemannian center of mass for a given set of points ``q_i M $ $i=1,\\ldots,N$  given by \\[\\operatorname*{arg\\,min}_{p\\in\\mathcal C}\n\\sum_{i=1}^N d_{\\mathrm{M}}^2(p,q_i)\\] constrained to a set  $\\mathcal C \\subset \\mathcal M$ . For this experiment set  $\\mathcal M = \\mathbb H^d$  for  $d=2,\\ldots,200$ , the  Hyperbolic space  and the constrained set  $\\mathcal C = C_{c,r}$  as the ball of radius  $r$  around the center point  $c$ , where we choose here  $r=\\frac{1}{\\sqrt{n}}$  and  $c = (0,\\ldots,0,1)^{\\mathrm{T}}$  and a  $œÉ = \\frac{3}{2}n^{1/4}$ . n_range = Vector(2:200)\nradius_range = [1 / sqrt(n) for n in n_range]\nN_range = [400 for n ‚àà n_range]\nM_range = [Hyperbolic(n) for n ‚àà n_range]\nœÉ_range = [ 1.5/sqrt(sqrt(n-1)) for n ‚àà n_range] Our data consists of  $N=200$  points, where we skew the data a bit to force the mean to be outside of the constrained set  $\\mathcal C$ ."},{"id":3605,"pagetitle":"Mean on mathbb H^n","title":"Cost, gradient and projection","ref":"/manoptexamples/stable/examples/Constrained-Mean-Hn/#Cost,-gradient-and-projection","content":" Cost, gradient and projection We can formulate the constrained problem above in two different forms. Both share a cost adn require a gradient. For performance reasons, we also provide a mutating variant of the gradient f(M, p; pts=[]) = 1 / (2 * length(pts)) .* sum(distance(M, p, q)^2 for q in pts)\n\ngrad_f(M, p; pts=[]) = -1 / length(pts) .* sum(log(M, p, q) for q in pts)\n\nfunction grad_f!(M, X, p; pts=[])\n    zero_vector!(M, X, p)\n    Y = zero_vector(M, p)\n    for q in pts\n        log!(M, Y, p, q)\n        X .+= Y\n    end\n    X .*= -1 / length(pts)\n    return X\nend We can model the constrained either with an inequality constraint  $g(p) \\geq 0$  or using a projection onto the set. For the gradient of  $g$  and the projection we again also provide mutating variants. g(M, p; op=[], radius=1) = distance(M, op, p)^2 - radius^2;\nindicator_C(M, p; op=[], radius=1) = (g(M, p; op=op, radius=radius) ‚â§ 0) ? 0 : Inf;\n\nfunction project_C(M, p; op=[], radius=1)\n    X = log(M, op, p)\n    n = norm(M, op, X)\n    q = (n > radius) ? exp(M, op, (radius / n) * X) : copy(M, p)\n    return q\nend;\n\nfunction project_C!(M, q, p; radius=1, op=[], X=zero_vector(M, op))\n    log!(M, X, op, p)\n    n = norm(M, op, X)\n    if (n > radius)\n        exp!(M, q, op, (radius / n) * X)\n    else\n        copyto!(M, q, p)\n    end\n    return q\nend;\n\ngrad_g(M, p; op=[]) = -2 * log(M, p, op)\nfunction grad_g!(M, X, p; op=[])\n    log!(M, X, p, op)\n    X .*= -2\n    return X\nend"},{"id":3606,"pagetitle":"Mean on mathbb H^n","title":"the mean","ref":"/manoptexamples/stable/examples/Constrained-Mean-Hn/#the-mean","content":" the mean For comparison, we first compute the Riemannian center of mass, that is the minimization above but not constrained to  $\\mathcal C$ . We can then project this onto  $\\mathcal C$ . For the projected mean we obtain  $g(p) = 0$  since the original mean is outside of the set, the projected one lies on the bounday. We first generate all data centers = [[zeros(n)..., 1.0] for n in n_range]\nbegin\n    Random.seed!(5)\n    data = [\n        [\n            exp(\n                M,\n                c,\n                get_vector(\n                    M, c, œÉ * randn(n) .+ 2 * r .* ones(n), DefaultOrthonormalBasis()\n                ),\n            ) for _ in 1:N\n        ] for\n        (c, r, n, N, M, œÉ) in zip(centers, radius_range, n_range, N_range, M_range, œÉ_range)\n    ]\nend means = [mean(M, d) for (M, d) in zip(M_range, data)]\ndc = [\n    indicator_C(M, m; op=c, radius=r) for\n    (M, m, c, r) in zip(M_range, means, centers, radius_range)\n]\nminimum(dc) # Sanity Check, this should be inf Inf Proj_means = [\n    project_C(M, m; op=c, radius=r) for\n    (M, m, c, r) in zip(M_range, means, centers, radius_range)\n]\n# Samll sanity check, these should all be about zero\nds = [distance(M, m, c) - r for (M, m, c, r) in zip(M_range, Proj_means, centers, radius_range)]\nmaximum(abs.(ds)) 1.1102230246251565e-16"},{"id":3607,"pagetitle":"Mean on mathbb H^n","title":"The experiment","ref":"/manoptexamples/stable/examples/Constrained-Mean-Hn/#The-experiment","content":" The experiment We first define a single test function for one set of data for a manifold function bench_aep(Manifold, center, radius, data)\n    # local functions\n    _f(M, p) = f(M, p; pts=data)\n    _grad_f!(M, X, p) = grad_f!(M, X, p; pts=data)\n    _proj_C!(M, q, p) = project_C!(M, q, p; radius=radius, op=center)\n    _g(M, p) = g(M, p; radius=radius, op=center)\n    _grad_g!(M, X, p) = grad_g!(M, X, p; op=center)\n    #\n    #\n    # returns\n    stats = Dict(:PGA => Dict(), :ALM => Dict(), :EPM => Dict())\n    #\n    #\n    # first runs\n    # println(manifold_dimension(Manifold), \": \")\n    mean_pga = copy(Manifold, center)\n    pgas = projected_gradient_method!(\n        Manifold,\n        _f,\n        _grad_f!,\n        _proj_C!,\n        mean_pga;\n        evaluation=InplaceEvaluation(),\n        record=[:Cost],\n        stopping_criterion=StopAfterIteration(150) |\n                           StopWhenProjectedGradientStationary(Manifold, 1e-7),\n        return_state=true,\n    )\n    stats[:PGA][:Iter] = length(get_record(pgas, :Iteration))\n    mean_alm = copy(Manifold, center)\n    alms = augmented_Lagrangian_method!(\n        Manifold,\n        _f,\n        _grad_f!,\n        mean_alm;\n        evaluation=InplaceEvaluation(),\n        g=[_g],\n        grad_g=[_grad_g!],\n        record=[:Cost],\n        return_state=true,\n    )\n    stats[:ALM][:Iter] = length(get_record(alms, :Iteration))\n    mean_epm = copy(Manifold, center)\n    epms = exact_penalty_method!(\n        Manifold, _f, _grad_f!, mean_epm;\n        evaluation=InplaceEvaluation(), return_state=true,\n        g=[_g], grad_g=[_grad_g!], record=[:Cost],\n    )\n    stats[:EPM][:Iter] = length(get_record(epms, :Iteration))\n    #\n    #\n    # Benchmarks\n    pga_b = @be projected_gradient_method!($Manifold, $_f, $_grad_f!, $_proj_C!,\n        $(copy(Manifold, center)); evaluation=$(InplaceEvaluation()),\n        stopping_criterion=$(\n            StopAfterIteration(150) | StopWhenProjectedGradientStationary(Manifold, 1e-7)\n        ),\n    ) evals = 1 samples = 10 seconds = 100\n    stats[:PGA][:time] = mean(pga_b).time\n    alm_b = @be augmented_Lagrangian_method!($Manifold, $_f, $_grad_f!,\n        $(copy(Manifold, center)); evaluation=$(InplaceEvaluation()),\n        g=$([_g]), grad_g=$([_grad_g!]),\n    ) evals = 1 samples = 10 seconds = 100\n    stats[:ALM][:time] = mean(alm_b).time\n    epm_b = @be exact_penalty_method!($Manifold, $_f, $_grad_f!,\n        $(copy(Manifold, center)); evaluation=$(InplaceEvaluation()),\n        g=$([_g]), grad_g=$([_grad_g!]),\n    ) evals = 1 samples = 10 seconds = 100\n    stats[:EPM][:time] = mean(epm_b).time\n    return stats\nend bench_aep (generic function with 1 method) and run these The resulting plot of runtime is fig = Figure()\naxis = Axis(fig[1, 1]; title=raw\"Time needed per dimension $\\mathbb H^d$\")\nlines!(axis, n_range, [bi[:PGA][:time] for bi in b]; label=\"PGA\")\nlines!(axis, n_range, [bi[:ALM][:time] for bi in b]; label=\"ALM\")\nlines!(axis, n_range, [bi[:EPM][:time] for bi in b]; label=\"EPM\")\naxis.xlabel = \"Manifold dimension d\"\naxis.ylabel = \"runtime (sec.)\"\naxislegend(axis; position=:lt)\nfig and the number of iterations reads fig2 = Figure()\naxis2 = Axis(fig2[1, 1]; title=raw\"Iterations needed per dimension $\\mathbb H^d$\")\nlines!(axis2, n_range, [bi[:PGA][:Iter] for bi in b]; label=\"PGA\")\nlines!(axis2, n_range, [bi[:ALM][:Iter] for bi in b]; label=\"ALM\")\nlines!(axis2, n_range, [bi[:EPM][:Iter] for bi in b]; label=\"EPM\")\naxis2.xlabel = \"Manifold dimension d\"\naxis2.ylabel = \"# Iterations\"\naxislegend(axis2; position=:lt)\nfig2"},{"id":3608,"pagetitle":"Mean on mathbb H^n","title":"Literature","ref":"/manoptexamples/stable/examples/Constrained-Mean-Hn/#Literature","content":" Literature [BFNZ25] R.¬†Bergmann, O.¬†P.¬†Ferreira, S.¬†Z.¬†N√©meth and J.¬†Zhu.  On projection mappings and the gradient projection method                on hyperbolic space forms . Preprint,¬†in¬†preparation (2025). [LB19] C.¬†Liu and N.¬†Boumal.  Simple algorithms for optimization on Riemannian manifolds with constraints .  Applied¬†Mathematics¬†&¬†Optimization  (2019),  arXiv:1091.10000 ."},{"id":3609,"pagetitle":"Mean on mathbb H^n","title":"Technical details","ref":"/manoptexamples/stable/examples/Constrained-Mean-Hn/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. Status `/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.13.4\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.29.0\n‚åÖ [5ae59095] Colors v0.12.11\n  [a93c6f00] DataFrames v1.7.0\n  [7073ff75] IJulia v1.27.0\n  [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [d3d80556] LineSearches v7.3.0\n  [ee78f7c6] Makie v0.22.4\n  [af67fdf4] ManifoldDiff v0.4.2\n  [1cead3c2] Manifolds v0.10.16\n  [3362f125] ManifoldsBase v1.0.3\n  [0fc0a36d] Manopt v0.5.12\n  [5b8d5e80] ManoptExamples v0.1.14 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.40.12\n  [08abe8d2] PrettyTables v2.4.0\n  [6099a3de] PythonCall v0.9.24\n  [f468eda6] QuadraticModels v0.9.8\n  [1e40b3f8] RipQP v0.6.4\nInfo Packages marked with ‚åÖ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated` This tutorial was last rendered April 13, 2025, 15:12:24."},{"id":3612,"pagetitle":"A Benchmark","title":"Benchmark of the Difference of Convex Algorithms","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Benchmark/#Benchmark-of-the-Difference-of-Convex-Algorithms","content":" Benchmark of the Difference of Convex Algorithms Ronny Bergmann 2023-06-06"},{"id":3613,"pagetitle":"A Benchmark","title":"Introduction","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Benchmark/#Introduction","content":" Introduction In this Benchmark we compare the Difference of Convex Algprithm (DCA) [ BFSS24 ] and the Difference of Convex Proximal Point Algorithm (DCPPA) [ SO15 ] which solve Difference of Convex (DC) problems of the form. This Benchmark reproduces the results from [ BFSS24 ], Section 7.1. \\[\\operatorname*{arg\\,min}_{p\\in\\mathcal M}\\ \\  g(p) - h(p)\\] where  $g,h\\colon \\mathcal M \\to \\mathbb R$  are geodesically convex function on the Riemannian manifold  $\\mathcal M$ . using LinearAlgebra, Random, Statistics, BenchmarkTools\nusing Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots\nRandom.seed!(42) and we load a few nice colors paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\nteal = paul_tol[\"mutedteal\"]"},{"id":3614,"pagetitle":"A Benchmark","title":"The DC Problem","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Benchmark/#The-DC-Problem","content":" The DC Problem We start with defining the two convex functions  $g,h$  and their gradients as well as the DC problem  $f$  and its gradient for the problem \\[    \\operatorname*{arg\\,min}_{p\\in\\mathcal M}\\ \\ \\bigl( \\log\\bigr(\\det(p)\\bigr)\\bigr)^4 - \\bigl(\\log \\det(p) \\bigr)^2.\\] where the critical points obtain a functional value of  $-\\frac{1}{4}$ . where  $\\mathcal M$  is the manifold of  symmetric positive definite (SPD) matrices  with the  affine invariant metric , which is the default. We first define the corresponding functions g(M, p) = log(det(p))^4\nh(M, p) = log(det(p))^2\nf(M, p) = g(M, p) - h(M, p) and their gradients grad_g(M, p) = 4 * (log(det(p)))^3 * p\ngrad_h(M, p) = 2 * log(det(p)) * p\ngrad_f(M, p) = grad_g(M, p) - grad_h(M, p) which we can use to verify that the gradients of  $g$  and  $h$  are correct. We use for that n = 6\nM = SymmetricPositiveDefinite(n)\np0 = log(n) * Matrix{Float64}(I, n, n);\nX0 = 1 / n * Matrix{Float64}(I, n, n); to tall both checks check_gradient(M, g, grad_g, p0, X0; plot=true) and check_gradient(M, h, grad_h, p0, X0; plot=true) which both pass the test. We continue to define their inplace variants function grad_g!(M, X, p)\n    copyto!(M, X, p)\n    X .*= 4 * (log(det(p)))^3\n    return X\nend\nfunction grad_h!(M, X, p)\n    copyto!(M, X, p)\n    X .*= 2 * (log(det(p)))\n    return X\nend\nfunction grad_f!(M, X, p)\n    grad_g!(M, X, p)\n    Y = copy(M, p, X)\n    grad_h!(M, Y, p)\n    X .-= Y\n    return X\nend And compare times for both algorithms, with a bit of debug output. @time p_min_dca = difference_of_convex_algorithm(\n    M,\n    f,\n    g,\n    grad_h!,\n    p0;\n    grad_g=grad_g!,\n    gradient=grad_f!,\n    evaluation=InplaceEvaluation(),\n    debug=[\n        :Iteration,\n        (:Cost, \"f(p): %1.9f\"),\n        (:GradientNorm, \" |grad_f(p)|: %1.9f\"),\n        (:Change, \" |Œ¥p|: %1.9f\"),\n        :Stop,\n        5,\n        \"\\n\",\n    ],\n    stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n    sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n); Initial f(p): 137.679053470\n# 5     f(p): -0.249956120 |grad_f(p)|: 0.046196628 |Œ¥p|: 0.201349127\n# 10    f(p): -0.249999999 |grad_f(p)|: 0.000187633 |Œ¥p|: 0.000626103\n# 15    f(p): -0.250000000 |grad_f(p)|: 0.000000772 |Œ¥p|: 0.000002574\n# 20    f(p): -0.250000000 |grad_f(p)|: 0.000000005 |Œ¥p|: 0.000000011\nThe algorithm reached approximately critical point after 24 iterations; the gradient norm (7.619584706652929e-11) is less than 1.0e-10.\n  3.531235 seconds (8.71 M allocations: 628.709 MiB, 3.52% gc time, 67.16% compilation time) The cost is f(M, p_min_dca) -0.25000000000000006 Similarly the DCPPA performs @time p_min_dcppa = difference_of_convex_proximal_point(\n    M,\n    grad_h!,\n    p0;\n    g=g,\n    grad_g=grad_g!,\n    Œª=i -> 1 / (2 * n),\n    cost=f,\n    gradient=grad_f!,\n    debug=[\n        :Iteration,\n        (:Cost, \"f(p): %1.9f\"),\n        \" \",\n        (:GradientNorm, \"|grad_f(p)|: %1.10f\"),\n        (:Change, \"|Œ¥p|: %1.10f\"),\n        :Stop,\n        5,\n        \"\\n\",\n    ],\n    evaluation=InplaceEvaluation(),\n    stepsize=ConstantStepsize(1.0),\n    stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n    sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n); Initial f(p): 137.679053470\n# 5     f(p): -0.248491803 |grad_f(p)|: 0.2793140152|Œ¥p|: 0.2753827692\n# 10    f(p): -0.249998655 |grad_f(p)|: 0.0080437374|Œ¥p|: 0.0050891316\n# 15    f(p): -0.249999999 |grad_f(p)|: 0.0002507329|Œ¥p|: 0.0001567676\n# 20    f(p): -0.250000000 |grad_f(p)|: 0.0000078348|Œ¥p|: 0.0000048968\n# 25    f(p): -0.250000000 |grad_f(p)|: 0.0000002448|Œ¥p|: 0.0000001530\n# 30    f(p): -0.250000000 |grad_f(p)|: 0.0000000076|Œ¥p|: 0.0000000048\n# 35    f(p): -0.250000000 |grad_f(p)|: 0.0000000002|Œ¥p|: 0.0000000001\nThe algorithm reached approximately critical point after 37 iterations; the gradient norm (5.458071707233144e-11) is less than 1.0e-10.\n  1.341931 seconds (2.55 M allocations: 180.474 MiB, 2.46% gc time, 59.94% compilation time) It needs a few more iterations, but the single iterations are slightly faster. Both obtain the same cost f(M, p_min_dcppa) -0.25"},{"id":3615,"pagetitle":"A Benchmark","title":"Benchmark I: Time comparison","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Benchmark/#Benchmark-I:-Time-comparison","content":" Benchmark I: Time comparison We compare both solvers first with respect to time. We initialise two vectors to collect the results and a range of natrix sizes to test dca_benchmarks = Dict{Int,BenchmarkTools.Trial}()\ndcppa_benchmarks = Dict{Int, BenchmarkTools.Trial}()\nN_max=14\nN = 2:N_max and run a benchmark for both algorithms for n in N\n    Mn = SymmetricPositiveDefinite(n)\n    pn = log(n) * Matrix{Float64}(I, n, n)\n    bdca = @benchmark difference_of_convex_algorithm(\n        $Mn,\n        $f,\n        $g,\n        $grad_h!,\n        $pn;\n        grad_g=$grad_g!,\n        gradient=$grad_f!,\n        evaluation=InplaceEvaluation(),\n        stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n    )\n    dca_benchmarks[n] = bdca\n    bdcppa = @benchmark difference_of_convex_proximal_point(\n        $Mn,\n        $grad_h!,\n        $pn;\n        g=$g,\n        grad_g=$grad_g!,\n        Œª=i -> 1 / (2 * n),\n        cost=f,\n        gradient=grad_f!,\n        evaluation=InplaceEvaluation(),\n        stepsize=ConstantStepsize(1.0),\n        stopping_criterion=StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion=StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n    )\n    dcppa_benchmarks[n] = bdcppa\nend Since we want to plot this versus the manifold dimension, we also create a vector for those and convert the times to seconds dims = [manifold_dimension(SymmetricPositiveDefinite(n)) for n in N]\ndca_times = [mean(dca_benchmarks[n]).time / 1e9 for n in N]\ndcppa_times = [mean(dcppa_benchmarks[n]).time / 1e9 for n in N] plot(; legend=:bottomright, xlabel=\"manifold dimension\", ylabel=\"Time (sec.)\")\nplot!(dims, dca_times; label=\"DCA\", color=indigo, linewidth=2)\nplot!(dims, dcppa_times; label=\"DCPPA\", color=teal, linewidth=2)"},{"id":3616,"pagetitle":"A Benchmark","title":"Benchmark II: Iterations and cost.","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Benchmark/#Benchmark-II:-Iterations-and-cost.","content":" Benchmark II: Iterations and cost. As a second benchmark, let‚Äôs collect the number of iterations needed and the development of the cost over dimensions. N2 = [5,10,20,40,80]\ndims2 = [manifold_dimension(SymmetricPositiveDefinite(n)) for n in N2]\ndca_iterations = Dict{Int,Int}()\ndca_costs = Dict{Int,Vector{Float64}}()\ndcppa_iterations = Dict{Int,Int}()\ndcppa_costs = Dict{Int,Vector{Float64}}() @time for n in N2\n    println(n)\n    Mn = SymmetricPositiveDefinite(n)\n    pn = log(n) * Matrix{Float64}(I,n,n);\n    @time dca_st = difference_of_convex_algorithm(\n        Mn, f, g, grad_h!, pn;\n        grad_g=grad_g!,\n        gradient=grad_f!,\n        evaluation = InplaceEvaluation(),\n        stopping_criterion = StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion = StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n        record = [:Iteration, :Cost],\n        return_state = true,\n    );\n    dca_costs[n] = get_record(dca_st, :Iteration, :Cost)\n    dca_iterations[n] = length(dca_costs[n])\n    @time dcppa_st = difference_of_convex_proximal_point(\n        Mn, grad_h!, pn;\n        g=g,\n        grad_g=grad_g!,\n        Œª = i -> 1/(2*n),\n        cost = f,\n        gradient= grad_f!,\n        evaluation = InplaceEvaluation(),\n        stepsize = ConstantStepsize(1.0),\n        stopping_criterion = StopAfterIteration(5000) | StopWhenGradientNormLess(1e-10),\n        sub_stopping_criterion = StopAfterIteration(100) | StopWhenGradientNormLess(1e-10),\n        record = [:Iteration, :Cost],\n        return_state = true,\n    );\n    dcppa_costs[n] = get_record(dcppa_st, :Iteration, :Cost)\n    dcppa_iterations[n] = length(dcppa_costs[n])\nend The iterations are like plot(; legend=:bottomright, xlabel=\"manifold dimension\", ylabel=\"Iterations\")\nplot!(dims2, [values(dca_iterations)...]; label=\"DCA\", color=indigo, linewidth=2)\nplot!(dims2, [values(dcppa_iterations)...]; label=\"DCPPA\", color=teal, linewidth=2) And for the developtment of the cost where we can see that the DCA needs less iterations than the DCPPA."},{"id":3617,"pagetitle":"A Benchmark","title":"Literature","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Benchmark/#Literature","content":" Literature [BFSS24] R.¬†Bergmann, O.¬†P.¬†Ferreira, E.¬†M.¬†Santos and J.¬†C.¬†Souza.  The difference of convex algorithm on Hadamard manifolds .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  (2024). [SO15] J.¬†C.¬†Souza and P.¬†R.¬†Oliveira.  A proximal point algorithm for DC fuctions on Hadamard manifolds .  Journal¬†of¬†Global¬†Optimization  63 , 797‚Äì810  (2015)."},{"id":3620,"pagetitle":"Frank Wolfe comparison","title":"A comparison of the Difference of Convex and Frank Wolfe Algorithm","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Frank-Wolfe/#A-comparison-of-the-Difference-of-Convex-and-Frank-Wolfe-Algorithm","content":" A comparison of the Difference of Convex and Frank Wolfe Algorithm Ronny Bergmann 2023-11-06"},{"id":3621,"pagetitle":"Frank Wolfe comparison","title":"Introduction","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Frank-Wolfe/#Introduction","content":" Introduction In this example we compare the Difference of Convex Algorithm (DCA) [ BFSS24 ] with the Frank-Wolfe Algorithm, which was introduced in [ WS22 ]. This example reproduces the results from [ BFSS24 ], Section 7.3. using LinearAlgebra, Random, Statistics, BenchmarkTools\nusing ManifoldsBase, Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots and we load a few nice colors paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\nteal = paul_tol[\"mutedteal\"] We consider the following constraint maximization problem of the Fr√©chet mean on the  symmetric positive definite matrices $\\mathcal P(n)$  with the  affine invariant metric . Let  $q_1,\\ldots,q_m \\in \\mathcal P(n)$  be a set of points and  $\\mu_1,\\ldots,\\mu_m$  be a set of weights, such that they sum to one. We consider then \\[\\operatorname*{arg\\,max}_{p\\in\\mathcal C}\\ \\ h(p)\\] with \\[h(p) =\n\\sum_{j=1}^m \\mu_j d^2(p,q_i),\n\\quad \\text{ where }\nd^2(p,q_i) = \\operatorname{tr}\\bigl(\n  \\log^2(p^{-\\frac{1}{2}}q_jp^{-\\frac{1}{2}})\n\\big)\n\\qquad\\text{and}\\qquad\n\\mathcal C = \\{ p\\in {\\mathcal M}\\ |\\ \\bar L\\preceq p \\preceq \\bar U \\},\\] for a lower bound  $L$  and an upper bound  $U$  for the matrices in the positive definite sense  $A \\preceq B \\Leftrightarrow (B-A)$  is positive semi-definite When every one of the weights  ${\\mu}_1, \\ldots {\\mu}_m$  are equal, this function  $h$  is known as the of the set  $\\{q_1, \\dots, q_m\\}$ . And for our example we set Random.seed!(42)\nn = 20\nm = 100\nM = SymmetricPositiveDefinite(n)\nq = [rand(M) for _ in 1:m];\nw = rand(m)\nw ./=sum(w) We use as lower and upper bound the arithmetic and geometric mean  $L$  and  $U$ , respectively. L = inv( sum( wi * inv(qi) for (wi, qi) in zip(w,q) ) )\nU = sum( wi * qi for (wi, qi) in zip(w,q) ) As a starting point, the Frank-Wolfe algorithm requires a feasible point. We use p0 = (L+U)/2 And we can check that it is feasible"},{"id":3622,"pagetitle":"Frank Wolfe comparison","title":"Common Functions","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Frank-Wolfe/#Common-Functions","content":" Common Functions Given  $p \\in \\mathcal M$ ,  $X \\in T_p\\mathcal M$  on the symmetric positive definite matrices  M , this method computes the closed form solution to \\[\\operatorname*{arg\\,min}_{q\\in  {\\mathcal C}}\\ \\langle X, \\log_p q\\rangle\n  = \\operatorname*{arg\\,min}_{q\\in  {\\mathcal C}}\\ \\operatorname{tr}(S\\log(YqY))\\] where  $\\mathcal C = \\{ q | L \\preceq q \\preceq U \\}$ ,  $S = p^{-1/2}Xp^{-1/2}$ , and  $Y=p^{-1/2}$ . The solution is given by  $Z=X^{-1}Q\\bigl( P^{\\mathrm{T}}[-\\operatorname{sgn}(D)]_{+}P+\\hat{L}\\bigr)Q^{\\mathrm{T}}X^{-1}$ ,@ where  $S=QDQ^{\\mathrm{T}}$  is a diagonalization of  $S$ ,  $\\hat{U}-\\hat{L}=P^{\\mathrm{T}}P$  with  $\\hat{L}=Q^{\\mathrm{T}}XLXQ$  and  $\\hat{U}=Q^{\\mathrm{T}}XUXQ$ , where  $[-\\mbox{sgn}(D)]_{+}$  is the diagonal matrix \\[\\operatorname{diag}\\bigl(\n  [-\\operatorname{sgn}(d_{11})]_{+}, \\ldots, [-\\operatorname{sgn}(d_{nn})]_{+}\n\\bigr)\\] and  $D=(d_{ij})$ . @doc raw\"\"\"\n    closed_form_solution!(M, q, L, U, p X)\n\nCompute the closed form solution of the constraint sub problem in place of ``q``.\n\"\"\"\nfunction closed_form_solution!(M::SymmetricPositiveDefinite, q, L, U, p, X)\n    # extract p^1/2 and p^{-1/2}\n    (p_sqrt_inv, p_sqrt) = Manifolds.spd_sqrt_and_sqrt_inv(p)\n    # Compute D & Q\n    e2 = eigen(p_sqrt_inv * X * p_sqrt_inv) # decompose Sk  = QDQ'\n    D = Diagonal(1.0 .* (e2.values .< 0))\n    Q = e2.vectors\n    #println(p)\n    Uprime = Q' * p_sqrt_inv * U * p_sqrt_inv * Q\n    Lprime = Q' * p_sqrt_inv * L * p_sqrt_inv * Q\n    P = cholesky(Hermitian(Uprime - Lprime))\n    z = P.U' * D * P.U + Lprime\n    copyto!(M, q, p_sqrt * Q * z * Q' * p_sqrt)\n    return q\nend"},{"id":3623,"pagetitle":"Frank Wolfe comparison","title":"The Difference of Convex Formulation","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Frank-Wolfe/#The-Difference-of-Convex-Formulation","content":" The Difference of Convex Formulation We use  $g(p) = \\iota_{\\mathcal C}(p)$  as the indicator function of the set  $\\mathcal C$ . We use function is_pos_def(p; atol=5e-13)\n    e = eigen(Symmetric(p))\n    return all((e.values .+ atol) .> 0)\nend\nfunction g(p, L, U)\n    return (is_pos_def(p-L) && is_pos_def(U-p)) ? 0.0 : Inf\nend\nh(M, p, w, q) = sum(wi * distance(M, p, qi)^2 for (wi, qi) in zip(w,q) ) So we can first check that  p0  is feasible g(p0,L,U) == 0.0 true Now setting \\[\\operatorname*{arg\\,min}_{p\\in\\mathcal M}\\ g(p) - h(p)\\] We look for a maximum of  $h$ , where  $g$  is minimal, i.e.¬† $g(p)$  is zero or in other words  $p \\in \\mathcal C$ . The gradient of  $h$  can also be implemented in closed form as grad_h(M, p, w, q) = -2.0 * sum(wi * log(M, p, qi) for (wi, qi) in zip(w, q))\nfunction grad_h!(M, X, p, w, q)\n    Y = copy(M, p, X)\n    zero_vector!(M, X, p)\n    for (wi, qi) in zip(w,q)\n        log!(M, Y, p, qi)\n        Y .*= - 2.0*wi\n        X .+= Y\n    end\n    return X\nend And we can further define the cost, which will just be  $+\\infty$  outside of  $\\mathcal C$ . We define f_dc(M, p) = g(p, L, U) - h(M, p, w, q)\ngrad_h!(M, X, p) = grad_h!(M, X, p, w, q)\nfunction grad_f_dc!(M,X, p)\n    grad_h!(M, X, p, w, q)\n    X .*= -1.0\n    return X\nend Here we can omit the gradient of  $g$  in the definition of  $\\operatorname{grad} f$ , since the gradient is zero at the points there it is defined, that is on any point that is not on the boundary of  $\\mathcal C$ . As the last step, we can provide the closed form solver for the DC sub problem given at iteration  $k$  by \\[\\operatorname*{arg\\,min}_{p\\in \\mathcal C}\\\n  \\big\\langle -\\operatorname{grad} h(p^{(k)}), \\exp^{-1}_{p^{(k)}}p\\big\\rangle.\\] Which we con compute function dc_sub_solution!(M, q, p, X)\n    closed_form_solution!(M, q, L, U, p, -X)\n    return q\nend For safety, we might want to avoid ending up at the boundary of  $\\mathcal C$ . That is we reduce the distance we walk towards the solution  $q$  a bit. function dc_sub_solution_safe!(M, q, p, X)\n    p_last = copy(M,p) # since p=q might be in place\n    closed_form_solution!(M, q, L, U, p, -X)\n    q_orig = copy(M,q) # since we do the following in place of q\n    a = minimum(real.(eigen(q-L).values))\n    b = minimum(real.(eigen(U-q).values))\n    s = 1.0\n    d = distance(M, p_last, q_orig);\n    # if we are close to zero, we reduce faster.\n    Œ± = d < 1/(n^2) ? 0.66 : 0.9995;\n    i=0\n    while (a < 0) || (b < 0)\n        s *= Œ±\n        shortest_geodesic!(M, q, p_last, q_orig, s)\n        a = minimum(real.(eigen(q-L).values))\n        b = minimum(real.(eigen(U-q).values))\n        #println(\"$i a: $a, b = $b with s=$s\")\n        i=i+1\n        if (i>100) # safety fallback\n            #@warn \" $i steps where not enough $s ($Œ±)\\n$a $b\\n $(distance(M, p_last, q_orig)). Fixing by shifting EVs\"\n            qe = eigen(q)\n            if a < 0\n                qe.values .+= min(1e-8, n*abs(min(a,b)))\n            else\n                qe.values .-= min(1e-8, n*abs(min(a,b)))\n            end\n            q .= qe.vectors * Diagonal(qe.values) * (qe.vectors)'\n            a = minimum(real.(eigen(q-L).values))\n            b = minimum(real.(eigen(U-q).values))\n            return q\n        end\n    end\n    return q\nend"},{"id":3624,"pagetitle":"Frank Wolfe comparison","title":"The DoC solver run","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Frank-Wolfe/#The-DoC-solver-run","content":" The DoC solver run Let‚Äôs compare both methods when they have the same stopping criteria @time state1_dc = difference_of_convex_algorithm(M, f_dc, g, grad_h!, p0;\n    gradient=grad_f_dc!,\n    sub_problem=dc_sub_solution_safe!,\n    evaluation=InplaceEvaluation(),\n    stopping_criterion = StopAfterIteration(300) |\n        StopWhenChangeLess(M, 1e-14) | StopWhenGradientChangeLess(M, 1e-9),\n    debug = [\n        (:Iteration, \"# %-8d \"), (:Cost, \"F(p): %0.14f\"), (:Change, \" |Œîp|: %0.14f \"),\n        (:GradientNorm, \" |grad f(p)|: %0.8f \"),\n        (:GradientChange, \" |Œîgrad f(p)|: %0.8f\"),\n        30, :Stop, \"\\n\"],\n    record = [:Iteration, :Iterate, :Cost, RecordGradientNorm(), :Change],\n    return_state=true,\n) Initial F(p): -0.81628444040735\n# 30       F(p): -0.82476426867975 |Œîp|: 0.08995893096792  |grad f(p)|: 0.18471629  |Œîgrad f(p)|: 0.18414762\n# 60       F(p): -0.82474701550902 |Œîp|: 0.02777382004997  |grad f(p)|: 0.18453262  |Œîgrad f(p)|: 0.05679999\n# 90       F(p): -0.82473955142600 |Œîp|: 0.01323514050663  |grad f(p)|: 0.18444796  |Œîgrad f(p)|: 0.02766167\n# 120      F(p): -0.82473728355230 |Œîp|: 0.00759483209486  |grad f(p)|: 0.18442329  |Œîgrad f(p)|: 0.01510815\nAt iteration 129 the change of the gradient (2.3583539765525604e-13) was less than 1.0e-9.\n 17.568295 seconds (40.50 M allocations: 3.712 GiB, 5.67% gc time, 82.17% compilation time)\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 129 iterations\n\n## Parameters\n* sub solver state:\n    | Manopt.ClosedFormSubSolverState{InplaceEvaluation}()\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 300:  not reached\n  * |Œîp| < 1.0e-14: not reached\n  * |Œîgrad f| < 1.0e-9: reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(p): %0.14f\"), (:Change, \" |Œîp|: %0.14f \"), (:GradientNorm, \" |grad f(p)|: %0.8f \"), (:GradientChange, \" |Œîgrad f(p)|: %0.8f\"), \"\\n\", 30]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Matrix{Float64}), RecordCost(), RecordGradientNorm(), RecordChange(; inverse_retraction_method=LogarithmicInverseRetraction())]),) Let‚Äôs extract the final point and look at its cost p1_dc = get_solver_result(state1_dc);\nf_dc(M, p1_dc) -0.8247367071753671 As well as whether (and how well) it is feasible, that is the following values should all be larger than zero. [ extrema(eigen(p1_dc-L).values), extrema(eigen(U-p1_dc).values)] 2-element Vector{Tuple{Float64, Float64}}:\n (4.968516688516737e-14, 0.07131706912126361)\n (1.7139967154158035e-9, 0.06356415609937403) For the statistics we extract the recordings from the state"},{"id":3625,"pagetitle":"Frank Wolfe comparison","title":"Define the Frank-Wolfe functions","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Frank-Wolfe/#Define-the-Frank-Wolfe-functions","content":" Define the Frank-Wolfe functions For Frank wolfe, the cost is just defined as  $-h(p)$  but the minimisation is constraint to  $\\mathcal C$ , which is enforced by the oracle. f_fw(M, p) = -h(M, p, w, q)\nfunction grad_f_fw!(M,X, p)\n    grad_h!(M, X, p, w, q)\n    X .*= -1.0\n    return X\nend\noracle_fw!(M, q, p, X) = closed_form_solution!(M, q, L, U, p, X)"},{"id":3626,"pagetitle":"Frank Wolfe comparison","title":"The FW Solver Run","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Frank-Wolfe/#The-FW-Solver-Run","content":" The FW Solver Run Similarly we can run the Frank-Wolfe algorithm with @time state1_fw = Frank_Wolfe_method(M, f_fw, grad_f_fw!, p0;\n    sub_problem=oracle_fw!,\n    evaluation=InplaceEvaluation(),\n    stopping_criterion = StopAfterIteration(10^4) |\n        StopWhenChangeLess(M, 1e-14) |¬†StopWhenGradientChangeLess(M, 1e-9),\n    debug = [\n        (:Iteration, \"# %-8d \"), :Cost, (:Change, \" |Œîp|: %0.14f \"),\n        (:GradientNorm, \" |grad f(p)|: %0.8f \"),\n        (:GradientChange, \" |Œîgrad f(p)|: %0.8f\"),\n        2*10^3, :Stop, \"\\n\"],\n    record = [:Iteration, :Iterate, :Cost, RecordGradientNorm(), :Change],\n    return_state=true,\n) Initial f(x): -0.816284\n# 2000     f(x): -0.824757 |Œîp|: 0.06259164233805  |grad f(p)|: 0.18462554  |Œîgrad f(p)|: 0.18383392\n# 4000     f(x): -0.824760 |Œîp|: 0.00533792248763  |grad f(p)|: 0.18465855  |Œîgrad f(p)|: 0.01075022\n# 6000     f(x): -0.824761 |Œîp|: 0.00292669259433  |grad f(p)|: 0.18467288  |Œîgrad f(p)|: 0.00589304\n# 8000     f(x): -0.824762 |Œîp|: 0.00199428568529  |grad f(p)|: 0.18468099  |Œîgrad f(p)|: 0.00401528\n# 10000    f(x): -0.824762 |Œîp|: 0.00150207512138  |grad f(p)|: 0.18468619  |Œîgrad f(p)|: 0.00302414\nAt iteration 10000 the algorithm reached its maximal number of iterations (10000).\n153.755945 seconds (121.01 M allocations: 92.332 GiB, 8.40% gc time, 0.68% compilation time)\n\n# Solver state for `Manopt.jl`s Frank Wolfe Method\nAfter 10000 iterations\n\n## Parameters\n* inverse retraction method: LogarithmicInverseRetraction()\n* retraction method: ExponentialRetraction()\n* sub solver state:\n    | Manopt.ClosedFormSubSolverState{InplaceEvaluation}()\n\n## Stepsize\nDecreasingLength(; length=2.0,  factor=1.0,  subtrahend=0.0,  shift=2.0, type=relative)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 10000:    reached\n  * |Œîp| < 1.0e-14: not reached\n  * |Œîgrad f| < 1.0e-9: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"f(x): %f\"), (:Change, \" |Œîp|: %0.14f \"), (:GradientNorm, \" |grad f(p)|: %0.8f \"), (:GradientChange, \" |Œîgrad f(p)|: %0.8f\"), \"\\n\", 2000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordIterate(Matrix{Float64}), RecordCost(), RecordGradientNorm(), RecordChange(; inverse_retraction_method=LogarithmicInverseRetraction())]),) And we take a look at this result as well p1_fw = get_solver_result(state1_fw);\nf_dc(M, p1_fw) -0.8247621344833183 And its feasibility [extrema(eigen(p1_fw-L).values), extrema(eigen(U-p1_fw).values)] 2-element Vector{Tuple{Float64, Float64}}:\n (5.358772865048796e-10, 0.07048653710020873)\n (5.6534370618234825e-6, 0.06727144535674022)"},{"id":3627,"pagetitle":"Frank Wolfe comparison","title":"Statistics","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Frank-Wolfe/#Statistics","content":" Statistics We extract the recorded values # DoC\niter1_dc = get_record(state1_dc, :Iteration, :Iteration)\npk_dc = get_record(state1_dc,:Iteration,:Iterate)\ncosts1_dc = -h.(Ref(M), pk_dc, Ref(w), Ref(q))\ndc_min = minimum(costs1_dc)\n# FW\niter1_fw = get_record(state1_fw,:Iteration,:Iteration)[1:5:end]\npk_fw = get_record(state1_fw,:Iteration,:Iterate)[1:5:end]\ncosts1_fw = -h.(Ref(M), pk_fw, Ref(w), Ref(q)) And let‚Äôs plot the result, where we measure the cost versus the minimum the difference of convex algorithm attains. fig = plot(;\n    legend=:topright,\n    xlabel=raw\"Iterations $k$ (log. scale)\", ylabel=raw\"Cost $f(x_k)-f^*$ (log. scale)\",\n    yaxis=:log,\n    ylims=(1e-8, 10^-2),\n    xaxis=:log,\n    xlims=(1,10^4),\n)\nplot!(fig, iter1_dc, costs1_dc .- dc_min, color=indigo, label=\"Difference of Convex\")\nplot!(fig, iter1_fw, costs1_fw .- dc_min, color=teal, label=\"Frank-Wolfe\") This indicates, that the difference off convex algorithm could even stop earlier with a proper stopping criterion, since after that the cost increases a bit again. On the other hand, Frank-Wolfe still has not reached this level function value after  10^4  iterations."},{"id":3628,"pagetitle":"Frank Wolfe comparison","title":"Literature","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Frank-Wolfe/#Literature","content":" Literature [BFSS24] R.¬†Bergmann, O.¬†P.¬†Ferreira, E.¬†M.¬†Santos and J.¬†C.¬†Souza.  The difference of convex algorithm on Hadamard manifolds .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  (2024). [WS22] M.¬†Weber and S.¬†Sra.  Riemannian Optimization via Frank-Wolfe Methods .  Mathematical¬†Programming  199 , 525‚Äì556  (2022)."},{"id":3631,"pagetitle":"Rosenbrock Metric","title":"Solving Rosenbrock with the Difference of Convex Algorithm","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Rosenbrock/#Solving-Rosenbrock-with-the-Difference-of-Convex-Algorithm","content":" Solving Rosenbrock with the Difference of Convex Algorithm Ronny Bergmann 2023-06-06"},{"id":3632,"pagetitle":"Rosenbrock Metric","title":"Introduction","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Rosenbrock/#Introduction","content":" Introduction This example illustrates how the  üìñ Rosenbrock  problem can be rephrased as a difference of convex problem and with a new metric on Euclidean space. This example is the code that produces the results in [ BFSS24 ], Section 7.2. Both the Rosenbrock problem \\[    \\operatorname*{argmin}_{x\\in ‚Ñù^2} a\\bigl( x_1^2-x_2\\bigr)^2 + \\bigl(x_1-b\\bigr)^2,\\] where  $a,b>0$  and usually  $b=1$  and  $a \\gg b$ , we know the minimizer  $x^* = (b,b^2)^\\mathrm{T}$ , and also the (Euclidean) gradient \\[\\nabla f(x) =\n  \\begin{pmatrix}\n  4a(x_1^2-x_2)\\\\ -2a(x_1^2-x_2)\n  \\end{pmatrix}\n  +\n  \\begin{pmatrix}\n  2(x_1-b)\\\\ 0\n  \\end{pmatrix}.\\] They are even available already here in  ManifoldExamples.jl , see  RosenbrockCost  and  RosenbrockGradient!! . Furthermore, the  RosenbrockMetric  can be used on  $‚Ñù^2$ , that is \\[‚ü®X,Y‚ü©_{\\mathrm{Rb},p} = X^\\mathrm{T}G_pY, \\qquad\nG_p = \\begin{pmatrix}\n  1+4p_1^2 & -2p_1 \\\\\n  -2p_1 & 1\n\\end{pmatrix},\\] In this example we want to explore four different approaches to minimizing the Rosenbrock example, that are all based on first-order methods, i.e.¬†using a gradient but not a Hessian. The Euclidean Gradient The Riemannian gradient descent with respect to the  RosenbrockMetric The Euclidean Difference of Convex Algorithm The Riemannian Difference of Convex Algorithm respect to the  RosenbrockMetric Where we obtain a difference of convex problem by writing \\[f(x) = a\\bigl( x_1^2-x_2\\bigr)^2 + \\bigl(x_1-b\\bigr)^2\n = a\\bigl( x_1^2-x_2\\bigr)^2 + 2\\bigl(x_1-b\\bigr)^2 - \\bigl(x_1-b\\bigr)^2\\] that is \\[g(x) = a\\bigl( x_1^2-x_2\\bigr)^2 + 2\\bigl(x_1-b\\bigr)^2 \\quad\\text{ and }\\quad h(x) = \\bigl(x_1-b\\bigr)^2\\] using LinearAlgebra, Random, Statistics\nusing Manifolds, Manopt, ManoptExamples\nusing NamedColors, Plots\nimport Manopt: set_parameter!\nRandom.seed!(42) paul_tol = load_paul_tol()\nindigo = paul_tol[\"mutedindigo\"]\ngreen = paul_tol[\"mutedgreen\"]\nsand = paul_tol[\"mutedsand\"]\nteal = paul_tol[\"mutedteal\"]\ngrey = paul_tol[\"mutedgrey\"] To emphasize the effect, we choose a quite large value of  a . a = 2*10^5\nb = 1 and use the starting point and a direction to check gradients p0 = [0.1, 0.2]"},{"id":3633,"pagetitle":"Rosenbrock Metric","title":"The Euclidean Gradient Descent.","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Rosenbrock/#The-Euclidean-Gradient-Descent.","content":" The Euclidean Gradient Descent. For the Euclidean gradient we can just use the same approach as in the  Rosenbrock example M = ‚Ñù^2\nf = ManoptExamples.RosenbrockCost(M; a=a, b=b)\n‚àáf!! = ManoptExamples.RosenbrockGradient!!(M; a=a, b=b) define a common debug vector debug_vec = [\n        (:Iteration, \"# %-8d \"),\n        (:Cost, \"F(x): %1.4e\"),\n        \" \",\n        (:Change, \"|Œ¥p|: %1.4e | \"),\n        (:GradientNorm, \"|grad f|: %1.6e\"),\n        :Stop,\n        \"\\n\",\n    ] and call the  gradient descent algorithm Eucl_GD_state = gradient_descent(M, f, ‚àáf!!, p0;\n    evaluation=InplaceEvaluation(),\n    debug=[debug_vec...,10^7],\n    stopping_criterion=StopAfterIteration(10^7) | StopWhenChangeLess(M, 1e-16),\n    record=[:Iteration, :Cost],\n    return_state=true,\n) Initial F(x): 7.2208e+03 \n# 10000000 F(x): 8.9937e-06 |Œ¥p|: 1.3835e+00 | |grad f|: 8.170355e-03\nAt iteration 10000000 the algorithm reached its maximal number of iterations (10000000).\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 10000000 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0\n    retraction_method=ExponentialRetraction()\n    contraction_factor=0.95\n    sufficient_decrease=0.1\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 10000000: reached\n  * |Œîp| < 1.0e-16: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|Œ¥p|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 10000000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)"},{"id":3634,"pagetitle":"Rosenbrock Metric","title":"The Riemannian Gradient Descent.","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Rosenbrock/#The-Riemannian-Gradient-Descent.","content":" The Riemannian Gradient Descent. For the Riemannian case, we define M_rb = MetricManifold(M, ManoptExamples.RosenbrockMetric()) MetricManifold(Euclidean(2; field=‚Ñù), ManoptExamples.RosenbrockMetric()) and the gradient is now adopted to the new metric function grad_f!(M, X, p)\n    ‚àáf!!(M, X, p)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_f(M, p)\n    X = zero_vector(M, p)\n    return grad_f!(M, X, p)\nend R_GD_state = gradient_descent(M_rb, f, grad_f!, p0;\n    evaluation=InplaceEvaluation(),\n    debug=[debug_vec...,10^6],\n    stopping_criterion=StopAfterIteration(10^7) | StopWhenChangeLess(M_rb, 1e-16),\n    record=[:Iteration, :Cost],\n    return_state=true,\n) Initial F(x): 7.2208e+03 \n# 1000000  F(x): 1.3571e-09 |Œ¥p|: 9.1006e-01 | |grad f|: 1.974939e-04\n# 2000000  F(x): 2.7921e-18 |Œ¥p|: 3.6836e-05 | |grad f|: 9.240792e-09\nAt iteration 2443750 the algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 2443750 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0\n    retraction_method=ExponentialRetraction()\n    contraction_factor=0.95\n    sufficient_decrease=0.1\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 10000000: not reached\n  * |Œîp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|Œ¥p|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 1000000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)"},{"id":3635,"pagetitle":"Rosenbrock Metric","title":"The Euclidean Difference of Convex","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Rosenbrock/#The-Euclidean-Difference-of-Convex","content":" The Euclidean Difference of Convex For the convex case, we have to first introduce the two parts of the cost. f1(M, p; a=100, b=1) = a * (p[1]^2 - p[2])^2;\nf2(M, p; a=100, b=1) = (p[1] - b[1])^2;\ng(M, p; a=100, b=1) = f1(M, p; a=a, b=b) + 2 * f2(M, p; a=a, b=b)\nh(M, p; a=100, b=1) = f2(M, p; a=a, b=b) and their (Euclidan) gradients function ‚àáh!(M, X, p; a=100, b=1)\n    X[1] = 2*(p[1]-b)\n    X[2] = 0\n    return X\nend\nfunction ‚àáh(M, p; a=100, b=1)\n    X = zero(p)\n    ‚àáh!(M, X, p; a=a, b=b)\n    return X\nend\nfunction ‚àág!(M, X, p; a=100, b=1)\n    X[1] = 4*a*(p[1]^2-p[2])*p[1] + 2*2*(p[1]-b)\n    X[2] = -2*a*(p[1]^2-p[2])\n    return X\nend\nfunction ‚àág(M, p; a=100, b=1)\n    X = zero(p)\n    ‚àág!(M, X, p; a=a, b=b)\n    return X\nend and we define for convenience docE_g(M, p) = g(M, p; a=a, b=b)\ndocE_f(M,p) = docE_g(M,p) - h(M, p; a=a, b=b)\ndocE_‚àáh!(M, X, p) = ‚àáh!(M, X, p; a=a, b=b)\ndocE_‚àág!(M, X, p) = ‚àág!(M, X, p; a=a, b=b)\nfunction docE_‚àáf!(M, X, p)\n  Y = zero_vector(M, p)\n  docE_‚àág!(M, X, p)\n  docE_‚àáh!(M, Y, p)\n  X .-= Y\n  return X\nend Then we call the  difference of convex algorithm  on Eucldiean space  $‚Ñù^2$ . E_doc_state = difference_of_convex_algorithm(\n    M, docE_f, docE_g, docE_‚àáh!, p0;\n    gradient=docE_‚àáf!,\n    grad_g = docE_‚àág!,\n    debug=[debug_vec..., 10^4],\n    evaluation=InplaceEvaluation(),\n    record=[:Iteration, :Cost],\n    stopping_criterion=StopAfterIteration(10^7) | StopWhenChangeLess(M, 1e-16),\n    sub_hess=nothing, # Use gradient descent\n    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),\n    return_state=true,\n) Initial F(x): 7.2208e+03 \n# 10000    F(x): 2.9705e-09 |Œ¥p|: 1.3270e+00 | |grad f|: 1.388203e-04\n# 20000    F(x): 3.3302e-16 |Œ¥p|: 1.2173e-04 | |grad f|: 4.541087e-08\nAt iteration 26549 the algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 26549 iterations\n\n## Parameters\n* sub solver state:\n    | # Solver state for `Manopt.jl`s Gradient Descent\n    | After 2000 iterations\n    | \n    | ## Parameters\n    | * retraction method: ExponentialRetraction()\n    | \n    | ## Stepsize\n    | ArmijoLinesearch(;\n    |     initial_stepsize=1.0\n    |     retraction_method=ExponentialRetraction()\n    |     contraction_factor=0.95\n    |     sufficient_decrease=0.1\n    | )\n    | \n    | ## Stopping criterion\n    | \n    | Stop When _one_ of the following are fulfilled:\n    |   * Max Iteration 2000:   reached\n    |   * |grad f| < 1.0e-16: not reached\n    | Overall: reached\n    | This indicates convergence: No\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 10000000: not reached\n  * |Œîp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|Œ¥p|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 10000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)"},{"id":3636,"pagetitle":"Rosenbrock Metric","title":"The Riemannian Difference of Convex","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Rosenbrock/#The-Riemannian-Difference-of-Convex","content":" The Riemannian Difference of Convex We first have to again defined the gradients with respect to the new metric function grad_h!(M, X, p; a=100, b=1)\n    ‚àáh!(M, X, p; a=a, b=b)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_h(M, p; a=100, b=1)\n    X = zero(p)\n    grad_h!(M, X, p; a=a, b=b)\n    return X\nend\nfunction grad_g!(M, X, p; a=100, b=1)\n    ‚àág!(M, X, p; a=a,b=b)\n    riemannian_gradient!(M, X, p, X)\n    return X\nend\nfunction grad_g(M, p; a=100, b=1)\n    X = zero(p)\n    grad_g!(M, X, p; a=a, b=b)\n    return X\nend While the cost of the subgradient can be infered automaticallty, we also have to provide the gradient of the sub problem. For  $X \\in   ‚àÇh(p^{(k)})$  the sunproblem top determine  $p^{(k+1)}$  reads \\[\\operatorname*{argmin}_{p\\in\\mathcal M} g(p) - \\langle X, \\log_{p^{(k)}}p\\rangle\\] for which usually the cost and gradient functions are computed automatically in the difference of convex algorithm. However, in our case first the closed form solution for the adjoint differential of the logaithmic map is complicated to compute and second the gradint can even be given in a nicer form. We can first simplify in our case with  $X = \\operatorname{grad} h(p^{(k)})$  that \\[\\phi(p) = g(p) - \\langle X, \\log_{p^{(k)}}p\\rangle\n= a\\bigl( p_{1}^2-p_{2}\\bigr)^2\n        + 2\\bigl(p_{1}-b\\bigr)^2 - 2(p^{(k)}_1-b)p_1 + 2(p^{(k)}_1-b)p^{(k)}_1,\\] its Euclidean gradient reads \\[\\operatorname{grad}\\phi(p) =\n    \\nabla \\varphi(p)\n    = \\begin{pmatrix}\n        4a p_1(p_1^2-p_2) + 4(p_1-b) - 2(p^{(k)}_1-b)\\\\\n        -2a(p_1^2-p_2)\n    \\end{pmatrix}\\] where we can again employ the gradient conversion from before to obtain the Riemannian gradient. mutable struct SubGrad{P,T,V}\n    pk::P\n    Xk::T\n    a::V\n    b::V\nend\nfunction (œï::SubGrad)(M, p)\n    X = zero_vector(M, p)\n    œï(M, X, p)\n    return X\nend\nfunction (œï::SubGrad)(M, X, p)\n    X .= [\n        4 * œï.a * p[1] * (p[1]^2 - p[2]) + 4 * (p[1] - œï.b) - 2 * (œï.pk[1] - œï.b),\n        -2 * œï.a * (p[1]^2 - p[2]),\n    ]\n    riemannian_gradient!(M, X, p, X) # convert\n    return X\nend And in orer to update the sub solvers gradient correctly, we have to overwrite set_parameter!(œï::SubGrad, ::Val{:p}, p) = (œï.pk .= p)\nset_parameter!(œï::SubGrad, ::Val{:X}, X) = (œï.Xk .= X) And we again introduce for ease of use docR_g(M, p) = g(M, p; a=a, b=b)\ndocR_f(M, p) = docR_g(M, p) - h(M, p; a=a, b=b)\ndocR_grad_h!(M, X, p) = grad_h!(M, X, p; a=a, b=b)\ndocR_grad_g!(M, X, p) = grad_g!(M, X, p; a=a, b=b)\nfunction docR_grad_f!(M, X, p)\n    Y = zero_vector(M, p)\n    docR_grad_g!(M, X, p)\n    docR_grad_h!(M, Y, p)\n    X .-= Y\n    return X\nend\ndocR_sub_grad = SubGrad(copy(M, p0), zero_vector(M, p0), a, b) Then we can finally call the last of our four algorithms to compare, the difference of convex algorithm with the Riemannian metric. R_doc_state = difference_of_convex_algorithm(\n    M_rb, docR_f, docR_g, docR_grad_h!, p0;\n    gradient=docR_grad_f!,\n    grad_g = docR_grad_g!,\n    debug=[debug_vec..., 10^6],\n    evaluation=InplaceEvaluation(),\n    record=[:Iteration, :Cost],\n    stopping_criterion=StopAfterIteration(10^7) | StopWhenChangeLess(M_rb, 1e-16),\n    sub_grad=docR_sub_grad,\n    sub_hess = nothing, # Use gradient descent\n    sub_stopping_criterion=StopAfterIteration(2000) | StopWhenGradientNormLess(1e-16),\n    return_state=true,\n) Initial F(x): 7.2208e+03 \nAt iteration 1235 the algorithm performed a step with a change (0.0) less than 1.0e-16.\n\n# Solver state for `Manopt.jl`s Difference of Convex Algorithm\nAfter 1235 iterations\n\n## Parameters\n* sub solver state:\n    | # Solver state for `Manopt.jl`s Gradient Descent\n    | After 2000 iterations\n    | \n    | ## Parameters\n    | * retraction method: ExponentialRetraction()\n    | \n    | ## Stepsize\n    | ArmijoLinesearch(;\n    |     initial_stepsize=1.0\n    |     retraction_method=ExponentialRetraction()\n    |     contraction_factor=0.95\n    |     sufficient_decrease=0.1\n    | )\n    | \n    | ## Stopping criterion\n    | \n    | Stop When _one_ of the following are fulfilled:\n    |   * Max Iteration 2000:   reached\n    |   * |grad f| < 1.0e-16: not reached\n    | Overall: reached\n    | This indicates convergence: No\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 10000000: not reached\n  * |Œîp| < 1.0e-16: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-8d \"), (:Cost, \"F(x): %1.4e\"), \" \", (:Change, \"|Œ¥p|: %1.4e | \"), (:GradientNorm, \"|grad f|: %1.6e\"), \"\\n\", 1000000]\n    :Stop = :Stop\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),)"},{"id":3637,"pagetitle":"Rosenbrock Metric","title":"Comparison in Iterations","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Rosenbrock/#Comparison-in-Iterations","content":" Comparison in Iterations fig = plot(;\n    legend=:topright,\n    xlabel=raw\"Iterations $k$ (log. scale)\", ylabel=raw\"Cost $f(x)$ (log. scale)\",\n    yaxis=:log,\n    ylims=(1e-16, 5*1e5),\n    xaxis=:log,\n    xlims=(1,10^7),\n)\nscatter!(fig, [1,], [f(M,p0),], label=raw\"$f(p_0)$\", markercolor=grey)\negi = get_record(Eucl_GD_state, :Iteration, :Iteration)[1:10000:end] #5308 entries\negc = get_record(Eucl_GD_state, :Iteration, :Cost)[1:10000:end] #5308 entries\nplot!(fig, egi, egc, color=teal, label=\"Euclidean GD\")\n#\nrgi = get_record(R_GD_state, :Iteration, :Iteration)[1:1000:end] # 2444 entries\nrgc = get_record(R_GD_state, :Iteration, :Cost)[1:1000:end] # 2444 entries\nplot!(fig, rgi, rgc, color=indigo, label=\"Riemannian GD\")\n#\nedi = get_record(E_doc_state, :Iteration, :Iteration) #26549 entries\nedc = get_record(E_doc_state, :Iteration, :Cost) #26549 entries\nplot!(fig, edi, edc, color=sand, label=\"Euclidean DoC\")\n#\nrdi = get_record(R_doc_state, :Iteration, :Iteration) # 1235 entries\nrdc = get_record(R_doc_state, :Iteration, :Cost) # 1235 entries\nplot!(fig, rdi, rdc, color=green, label=\"Riemannian DoC\") And we can see that using difference of convex outperforms gradient descent, and using the Riemannian approach required less iterations than their Euclidean counterparts."},{"id":3638,"pagetitle":"Rosenbrock Metric","title":"Literature","ref":"/manoptexamples/stable/examples/Difference-of-Convex-Rosenbrock/#Literature","content":" Literature [BFSS24] R.¬†Bergmann, O.¬†P.¬†Ferreira, E.¬†M.¬†Santos and J.¬†C.¬†Souza.  The difference of convex algorithm on Hadamard manifolds .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  (2024)."},{"id":3641,"pagetitle":"Hyperbolic Signal Denoising","title":"A comparison of the RCBM with the PBA, the SGM, and the CPPA for denoising a signal on the hyperbolic space","ref":"/manoptexamples/stable/examples/H2-Signal-TV/#A-comparison-of-the-RCBM-with-the-PBA,-the-SGM,-and-the-CPPA-for-denoising-a-signal-on-the-hyperbolic-space","content":" A comparison of the RCBM with the PBA, the SGM, and the CPPA for denoising a signal on the hyperbolic space Hajg Jasa 2024-06-27"},{"id":3642,"pagetitle":"Hyperbolic Signal Denoising","title":"Introduction","ref":"/manoptexamples/stable/examples/H2-Signal-TV/#Introduction","content":" Introduction In this example we compare the Riemannian Convex Bundle Method (RCBM) [ BHJ24 ] with the Proximal Bundle Algorithm, which was introduced in [ HNP23 ], and with the Subgradient Method (SGM), introduced in [ FO98 ], to denoise an artificial signal on the Hyperbolic space  $\\mathcal H^2$ . This example reproduces the results from [ BHJ24 ], Section 5.2. using PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots\nusing QuadraticModels, RipQP\nusing Random, LinearAlgebra, LRUCache\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples"},{"id":3643,"pagetitle":"Hyperbolic Signal Denoising","title":"The Problem","ref":"/manoptexamples/stable/examples/H2-Signal-TV/#The-Problem","content":" The Problem Let  $\\mathcal M = \\mathcal H^2$  be the  $2$ -dimensional hyperbolic space and let  $p, q \\in \\mathcal M^n$  be two manifold-valued signals, for  $n \\in \\mathbb N$ . Let  $f \\colon \\mathcal M \\to \\mathbb R$  be defined by \\[    f_q (p)\n    =\n    \\frac{1}{n}\n    \\left{\n    \\frac{1}{2} \\sum_{i = 1}^n \\mathrm{dist}(p_i, q_i)^2\n    +\n    \\alpha \\operatorname{TV}(p)\n    \\right}\n    ,\\] where  $\\operatorname{TV}(p)$ , is the total variation term given by \\[    \\operatorname{TV}(p)\n    =\n    \\sum_{i = 1}^{n-1} \\mathrm{dist}(p_i, p_{i+1})\n    .\\]"},{"id":3644,"pagetitle":"Hyperbolic Signal Denoising","title":"Numerical Experiment","ref":"/manoptexamples/stable/examples/H2-Signal-TV/#Numerical-Experiment","content":" Numerical Experiment We initialize the experiment parameters, as well as some utility functions. Random.seed!(33)\nn = 496\nœÉ = 0.1 # Noise parameter\nŒ± = 0.5 # TV parameter\natol = 1e-8\nk_max = 0.0\nk_min = -1.0\nmax_iters = 5000\n#\n# Colors\ndata_color = RGBA{Float64}(colorant\"#BBBBBB\")\nnoise_color = RGBA{Float64}(colorant\"#33BBEE\") # Tol Vibrant Teal\nresult_color = RGBA{Float64}(colorant\"#EE7733\") # Tol Vibrant Orange function artificial_H2_signal(\n    pts::Integer=100; a::Real=0.0, b::Real=1.0, T::Real=(b - a) / 2\n)\n    t = range(a, b; length=pts)\n    x = [[s, sign(sin(2 * œÄ / T * s))] for s in t]\n    y = [\n        [x[1]]\n        [\n            x[i] for\n            i in 2:(length(x) - 1) if (x[i][2] != x[i + 1][2] || x[i][2] != x[i - 1][2])\n        ]\n        [x[end]]\n    ]\n    y = map(z -> Manifolds._hyperbolize(Hyperbolic(2), z), y)\n    data = []\n    geodesics = []\n    l = Int(round(pts * T / (2 * (b - a))))\n    for i in 1:2:(length(y) - 1)\n        append!(\n            data,\n            shortest_geodesic(Hyperbolic(2), y[i], y[i + 1], range(0.0, 1.0; length=l)),\n        )\n        if i + 2 ‚â§ length(y) - 1\n            append!(\n                geodesics,\n                shortest_geodesic(Hyperbolic(2), y[i], y[i + 1], range(0.0, 1.0; length=l)),\n            )\n            append!(\n                geodesics,\n                shortest_geodesic(\n                    Hyperbolic(2), y[i + 1], y[i + 2], range(0.0, 1.0; length=l)\n                ),\n            )\n        end\n    end\n    return data, geodesics\nend\nfunction matrixify_Poincare_ball(input)\n    input_x = []\n    input_y = []\n    for p in input\n        push!(input_x, p.value[1])\n        push!(input_y, p.value[2])\n    end\n    return hcat(input_x, input_y)\nend We now fix the data for the experiment‚Ä¶ H = Hyperbolic(2)\nsignal, geodesics = artificial_H2_signal(n; a=-6.0, b=6.0, T=3)\nnoise = map(p -> exp(H, p, rand(H; vector_at=p, œÉ=œÉ)), signal)\ndiameter = 3 * maximum([distance(H, noise[i], noise[j]) for i in 1:n, j in 1:n])\nHn = PowerManifold(H, NestedPowerRepresentation(), length(noise)) ‚Ä¶ As well as objective, subdifferential, and proximal map. function f(M, p)\n    return 1 / length(noise) *\n           (1 / 2 * distance(M, noise, p)^2 + Œ± * ManoptExamples.Total_Variation(M, p))\nend\ndomf(M, p) = distance(M, p, noise) < diameter / 2 ? true : false\nfunction ‚àÇf(M, p)\n    return 1 / length(noise) * (\n        ManifoldDiff.grad_distance(M, noise, p) +\n        Œ± * ManoptExamples.subgrad_Total_Variation(M, p; atol=atol)\n    )\nend\nproxes = (\n    (M, Œª, p) -> ManifoldDiff.prox_distance(M, Œª, noise, p, 2),\n    (M, Œª, p) -> ManoptExamples.prox_Total_Variation(M, Œ± * Œª, p),\n) We can now plot the initial setting. global ball_scene = plot()\nif export_orig\n    ball_signal = convert.(PoincareBallPoint, signal)\n    ball_noise = convert.(PoincareBallPoint, noise)\n    ball_geodesics = convert.(PoincareBallPoint, geodesics)\n    plot!(ball_scene, H, ball_signal; geodesic_interpolation=100, label=\"Geodesics\")\n    plot!(\n        ball_scene,\n        H,\n        ball_signal;\n        markercolor=data_color,\n        markerstrokecolor=data_color,\n        label=\"Signal\",\n    )\n    plot!(\n        ball_scene,\n        H,\n        ball_noise;\n        markercolor=noise_color,\n        markerstrokecolor=noise_color,\n        label=\"Noise\",\n    )\n    matrix_data = matrixify_Poincare_ball(ball_signal)\n    matrix_noise = matrixify_Poincare_ball(ball_noise)\n    matrix_geodesics = matrixify_Poincare_ball(ball_geodesics)\n    CSV.write(\n        joinpath(results_folder, experiment_name * \"-noise.csv\"),\n        DataFrame(matrix_data, :auto);\n        header=[\"x\", \"y\"],\n    )\n    CSV.write(\n        joinpath(results_folder, experiment_name * \"-noise.csv\"),\n        DataFrame(matrix_noise, :auto);\n        header=[\"x\", \"y\"],\n    )\n    CSV.write(\n        joinpath(results_folder, experiment_name * \"-geodesics.csv\"),\n        DataFrame(matrix_geodesics, :auto);\n        header=[\"x\", \"y\"],\n    )\n    display(ball_scene)\nend We introduce some keyword arguments for the solvers we will use in this experiment rcbm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :diameter => diameter,\n    :debug => [\n        :Iteration,\n        (:Cost, \"F(p): %1.8f \"),\n        (:Œæ, \"Œæ: %1.16f \"),\n        (:Œµ, \"Œµ: %1.16f \"),\n        :WarnBundle,\n        :Stop,\n        1000,\n        \"\\n\",\n        ],\n    :domain => domf,\n    :k_max => k_max,\n    :k_min => k_min,\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(max_iters),\n]\nrcbm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :diameter => diameter,\n    :domain => domf,\n    :k_max => k_max,\n    :k_min => k_min,\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(max_iters),\n]\npba_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :debug => [\n        :Iteration,\n        :Stop,\n        (:Cost, \"F(p): %1.16f \"),\n        (:ŒΩ, \"ŒΩ: %1.16f \"),\n        (:c, \"c: %1.16f \"),\n        (:Œº, \"Œº: %1.8f \"),\n        :Stop,\n        1000,\n        \"\\n\",\n    ],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(max_iters),\n]\npba_bm_kwargs = [\n    :cache =>(:LRU, [:Cost, :SubGradient], 50),\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(max_iters),\n]\nsgm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :debug => [:Iteration, (:Cost, \"F(p): %1.16f \"), :Stop, 1000, \"\\n\"],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => DecreasingLength(; exponent=1, factor=1, subtrahend=0, length=1, shift=0, type=:absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(‚àöatol) | StopAfterIteration(max_iters),\n]\nsgm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stopping_criterion => StopWhenSubgradientNormLess(‚àöatol) | StopAfterIteration(max_iters),\n]\ncppa_kwargs = [\n    :debug => [\n        :Iteration,\n        \" | \",\n        DebugProximalParameter(),\n        \" | \",\n        (:Cost, \"F(p): %1.16f \"),\n        \" | \",\n        :Change,\n        \"\\n\",\n        1000,\n        :Stop,\n    ],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stopping_criterion => StopWhenAny(StopAfterIteration(max_iters), StopWhenChangeLess(Hn, atol)),\n]\ncppa_bm_kwargs = [\n    :stopping_criterion => StopWhenAny(StopAfterIteration(max_iters), StopWhenChangeLess(Hn, atol)),\n] Finally, we run the optimization algorithms‚Ä¶ rcbm = convex_bundle_method(Hn, f, ‚àÇf, noise; rcbm_kwargs...)\nrcbm_result = get_solver_result(rcbm)\nrcbm_record = get_record(rcbm)\n#\npba = proximal_bundle_method(Hn, f, ‚àÇf, noise; pba_kwargs...)\npba_result = get_solver_result(pba)\npba_record = get_record(pba)\n#\nsgm = subgradient_method(Hn, f, ‚àÇf, noise; sgm_kwargs...)\nsgm_result = get_solver_result(sgm)\nsgm_record = get_record(sgm)\n#\ncppa = cyclic_proximal_point(Hn, f, proxes, noise; cppa_kwargs...)\ncppa_result = get_solver_result(cppa)\ncppa_record = get_record(cppa) ‚Ä¶ And we benchmark their performance. if benchmarking\n    pba_bm = @benchmark proximal_bundle_method($Hn, $f, $‚àÇf, $noise; $pba_bm_kwargs...)\n    rcbm_bm = @benchmark convex_bundle_method($Hn, $f, $‚àÇf, $noise; $rcbm_bm_kwargs...)\n    sgm_bm = @benchmark subgradient_method($Hn, $f, $‚àÇf, $noise; $sgm_bm_kwargs...)\n    cppa_bm = @benchmark cyclic_proximal_point($Hn, $f, $proxes, $noise; $cppa_bm_kwargs...)\n    #\n    experiments = [\"RCBM\", \"PBA\", \"SGM\", \"CPPA\"]\n    records = [rcbm_record, pba_record, sgm_record, cppa_record]\n    results = [rcbm_result, pba_result, sgm_result, cppa_result]\n    times = [\n        median(rcbm_bm).time * 1e-9,\n        median(pba_bm).time * 1e-9,\n        median(sgm_bm).time * 1e-9,\n        median(cppa_bm).time * 1e-9,\n    ]\n    #\n    global B = cat(\n        experiments,\n        [maximum(first.(record)) for record in records],\n        [t for t in times],\n        [minimum([r[2] for r in record]) for record in records],\n        [distance(Hn, noise, result) / length(noise) for result in results];\n        dims=2,\n    )\n    #\n    global header = [\"Algorithm\", \"Iterations\", \"Time (s)\", \"Objective\", \"Error\"]\n    #\n    # Finalize - export costs\n    if export_table\n        for (time, record, result, experiment) in zip(times, records, results, experiments)\n            A = cat(first.(record), [r[2] for r in record]; dims=2)\n            CSV.write(\n                joinpath(results_folder, experiment_name * \"_\" * experiment * \"-result.csv\"),\n                DataFrame(A, :auto);\n                header=[\"i\", \"cost\"],\n            )\n        end\n        CSV.write(\n            joinpath(results_folder, experiment_name * \"-comparisons.csv\"),\n            DataFrame(B, :auto);\n            header=header,\n        )\n    end\nend We can take a look at how the algorithms compare to each other in their performance with the following table‚Ä¶ Algorithm Iterations Time (s) Objective Error RCBM 5000 13.8923 0.140232 0.0136919 PBA 5000 9.5191 0.142887 0.0130322 SGM 5000 7.89653 0.146216 0.0124605 CPPA 5000 3.73857 0.131913 0.0173612 Lastly, we plot the results. if export_result\n    # Convert hyperboloid points to Poincar√© ball points\n    ball_b = convert.(PoincareBallPoint, rcbm_result)\n    ball_p = convert.(PoincareBallPoint, pba_result)\n    ball_s = convert.(PoincareBallPoint, sgm_result)\n    ball_c = convert.(PoincareBallPoint, cppa_result)\n    #\n    # Plot results\n    plot!(\n        ball_scene,\n        H,\n        ball_b;\n        markercolor=result_color,\n        markerstrokecolor=result_color,\n        label=\"Convex Bundle Method\",\n    )\n    #\n    # Write csv files\n    matrix_b = matrixify_Poincare_ball(ball_b)\n    CSV.write(\n        joinpath(results_folder, experiment_name * \"-bundle_optimum.csv\"),\n        DataFrame(matrix_b, :auto);\n        header=[\"x\", \"y\"],\n    )\n    #\n    # Suppress some plots for clarity, since they are visually indistinguishable\n    # plot!(ball_scene, H, ball_p; label=\"Proximal Bundle Method\")\n    # plot!(ball_scene, H, ball_s; label=\"Subgradient Method\")\n    # plot!(ball_scene, H, ball_c; label=\"CPPA\")\n    display(ball_scene)\nend"},{"id":3645,"pagetitle":"Hyperbolic Signal Denoising","title":"Technical details","ref":"/manoptexamples/stable/examples/H2-Signal-TV/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. using Pkg\nPkg.status() Status `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.5.0\n  [336ed68f] CSV v0.10.15\n  [35d6a980] ColorSchemes v3.27.1\n‚åÖ [5ae59095] Colors v0.12.11\n  [a93c6f00] DataFrames v1.7.0\n  [7073ff75] IJulia v1.26.0\n  [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.1\n  [d3d80556] LineSearches v7.3.0\n  [af67fdf4] ManifoldDiff v0.3.13\n  [1cead3c2] Manifolds v0.10.7\n  [3362f125] ManifoldsBase v0.15.22\n  [0fc0a36d] Manopt v0.5.3 `../../Manopt.jl`\n  [5b8d5e80] ManoptExamples v0.1.10 `..`\n  [51fcb6bd] NamedColors v0.2.2\n  [91a5bcdd] Plots v1.40.9\n‚åÉ [08abe8d2] PrettyTables v2.3.2\n  [6099a3de] PythonCall v0.9.23\n  [f468eda6] QuadraticModels v0.9.7\n  [1e40b3f8] RipQP v0.6.4\nInfo Packages marked with ‚åÉ and ‚åÖ have new versions available. Those with ‚åÉ may be upgradable, but those with ‚åÖ are restricted by compatibility constraints from upgrading. To see why use `status --outdated` using Dates\nnow() 2024-11-29T17:29:02.109"},{"id":3646,"pagetitle":"Hyperbolic Signal Denoising","title":"Literature","ref":"/manoptexamples/stable/examples/H2-Signal-TV/#Literature","content":" Literature [BHJ24] R.¬†Bergmann, R.¬†Herzog and H.¬†Jasa.  The Riemannian Convex Bundle Method , preprint (2024),  arXiv:2402.13670 . [FO98] O.¬†Ferreira and P.¬†R.¬†Oliveira.  Subgradient algorithm on Riemannian manifolds .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  97 , 93‚Äì104  (1998). [HNP23] N.¬†Hoseini Monjezi, S.¬†Nobakhtian and M.¬†R.¬†Pouryayevali.  A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds .  IMA¬†Journal¬†of¬†Numerical¬†Analysis  43 , 293‚Äì325  (2023)."},{"id":3649,"pagetitle":"Hyperparameter optimziation","title":"Hyperparameter optimization","ref":"/manoptexamples/stable/examples/HyperparameterOptimization/#Hyperparameter-optimization","content":" Hyperparameter optimization Mateusz Baran 2024-08-03"},{"id":3650,"pagetitle":"Hyperparameter optimziation","title":"Introduction","ref":"/manoptexamples/stable/examples/HyperparameterOptimization/#Introduction","content":" Introduction This example shows how to automatically select the best values of hyperparameters of optimization procedures such as retraction, vector transport, size of memory in L-BFGS or line search coefficients. Hyperparameter optimization relies on the  Optuna  [ ASY+19 ] Python library because it is much more advanced than similar Julia projects, offering Bayesian optimization with conditional hyperparameters and early stopping."},{"id":3651,"pagetitle":"Hyperparameter optimziation","title":"General definitions","ref":"/manoptexamples/stable/examples/HyperparameterOptimization/#General-definitions","content":" General definitions Here are some general definitions that you will most likely be able to directly use for your problem without any changes. Just remember to install  optuna , for example using  CondaPkg  Julia library. using Manifolds, Manopt\nusing PythonCall\nusing BenchmarkTools\nusing LineSearches\n\n# This script requires optuna to be available through PythonCall\n# You can install it for example using\n# using CondaPkg\n# ]conda add optuna\n\noptuna = pyimport(\"optuna\")\n\nnorm_inf(M::AbstractManifold, p, X) = norm(X, Inf)\n\n# TTsuggest_ structs collect data from a calibrating optimization run\n# that is handled by compute_pruning_losses function\n\nstruct TTsuggest_int\n    suggestions::Dict{String,Int}\nend\nfunction (s::TTsuggest_int)(name::String, a, b)\n    return s.suggestions[name]\nend\nstruct TTsuggest_float\n    suggestions::Dict{String,Float64}\nend\nfunction (s::TTsuggest_float)(name::String, a, b; log::Bool=false)\n    return s.suggestions[name]\nend\nstruct TTsuggest_categorical\n    suggestions::Dict{String,Any}\nend\nfunction (s::TTsuggest_categorical)(name::String, vals)\n    return s.suggestions[name]\nend\nstruct TTreport\n    reported_vals::Vector{Float64}\nend\nfunction (r::TTreport)(val, i)\n    return push!(r.reported_vals, val)\nend\nstruct TTshould_prune end\n(::TTshould_prune)() = Py(false)\nstruct TracingTrial\n    suggest_int::TTsuggest_int\n    suggest_float::TTsuggest_float\n    suggest_categorical::TTsuggest_categorical\n    report::TTreport\n    should_prune::TTshould_prune\nend\n\nfunction compute_pruning_losses(\n    od,\n    int_suggestions::Dict{String,Int},\n    float_suggestions::Dict{String,Float64},\n    categorical_suggestions::Dict{String,Int},\n)\n    tt = TracingTrial(\n        TTsuggest_int(int_suggestions),\n        TTsuggest_float(float_suggestions),\n        TTsuggest_categorical(categorical_suggestions),\n        TTreport(Float64[]),\n        TTshould_prune(),\n    )\n    od(tt)\n    return tt.report.reported_vals\nend The next part is your hyperparameter optimization objective. The  ObjectiveData  struct contains all relevant information about the sequence of specific problems. The outermost key part is the  N_range  field. Early stopping requires a series of progressively more complex problems. They will be attempted from the most simple one to the most complex one, and are specified by the values of  N  in that vector. mutable struct ObjectiveData{TObj,TGrad}\n    obj::TObj\n    grad::TGrad\n    N_range::Vector{Int}\n    gtol::Float64\n    vts::Vector{AbstractVectorTransportMethod}\n    retrs::Vector{AbstractRetractionMethod}\n    manifold_constructors::Vector{Tuple{String,Any}}\n    pruning_losses::Vector{Float64}\n    manopt_stepsize::Vector{Tuple{String,Any}}\n    obj_loss_coeff::Float64\nend In the example below we optimize hyperparameters on a sequence of Rosenbrock-type problems restricted to spheres: \\[\\arg\\min_{p \\in S^{N-1}} \\sum_{i=1}^{N/2} (1-p_{2i})^2 + 100 (p_{2i+1} - p_{2i}^2)^2,\\] where  $N \\in [2, 16, 128, 1024, 8192, 65536]$ . obj  and  grad  are the objective and gradient, here defined as below. Note that gradient works in-place and variants without manifolds are also provided for easier comparison with other libraries like  Optim.jl . It is easiest when problems for different values  N  can be distinguished by being defined on successively larger manifolds but the script could be modified so that it‚Äôs not necessary. pruning_losses  and  compute_pruning_losses  are related to early pruning used in Optuna and you shouldn‚Äôt have to modify them. function f_rosenbrock(x)\n    result = 0.0\n    for i in 1:2:length(x)\n        result += (1.0 - x[i])^2 + 100.0 * (x[i + 1] - x[i]^2)^2\n    end\n    return result\nend\nfunction f_rosenbrock(::AbstractManifold, x)\n    return f_rosenbrock(x)\nend\n\nfunction g_rosenbrock!(storage, x)\n    for i in 1:2:length(x)\n        storage[i] = -2.0 * (1.0 - x[i]) - 400.0 * (x[i + 1] - x[i]^2) * x[i]\n        storage[i + 1] = 200.0 * (x[i + 1] - x[i]^2)\n    end\n    return storage\nend\nfunction g_rosenbrock!(M::AbstractManifold, storage, x)\n    g_rosenbrock!(storage, x)\n    riemannian_gradient!(M, storage, x, storage)\n    return storage\nend Next,  gtol  is the tolerance used for the stopping criterion in optimization.  vts  and  retrs  are, respectively, vector transports and retraction methods selected through hyperparameter optimization. Some items need to be different for different values of  N , for example the manifold over which the problem is defined. This is handled by  manifold_constructors  which is then defined as  Tuple{String,Any}[(\"Sphere\", N -> Manifolds.Sphere(N - 1))] , where the string  \"Sphere\"  is used to identify the manifold family and the next element is a function that transforms the value of  N  to the manifold for the problem of size  N . Similarly, different stepsize selection methods may be considered. This is handled by the field  manopt_stepsize . It will be easiest to see how it works by looking at how it is initialized: Tuple{String,Any}[\n    (\"LS-HZ\", M -> Manopt.LineSearchesStepsize(ls_hz)),\n    (\"Wolfe-Powell\", (M, c1, c2) -> Manopt.WolfePowellLinesearch(M, c1, c2)),\n] We have a string that identifies the line search method name and a constructor of the line search which takes relevant arguments like the manifold or a numerical parameter. The next part is the trial evaluation procedure. This is one of the more important places which need to be customized to your problem. This is the point where we tell Optuna about the relevant optimization hyperparameters and use them to define specific problems. The hyperparameter optimization is a multiobjective problem: we want as good problem objectives as possible and as low times as possible. As Optuna doesn‚Äôt currently support multicriteria pruning, which is important for obtaining a solution in a reasonable amount of time, we use a linear combination of sub-objectives to turn the problem into a single-criterion optimization. The hyperparameter optimization objective is a linear combination of achieved objectives the relative weight is controlled by  objective.obj_loss_coeff . function (objective::ObjectiveData)(trial)\n    # Here we use optuna to select memory length for L-BFGS -- an integer in the range between 2 and 30, referenced by name \"mem_len\"\n    mem_len = trial.suggest_int(\"mem_len\", 2, 30)\n\n    # Here we select a vector transport and retraction methods, one of those specified in the `ObjectiveData`.\n    vt = objective.vts[pyconvert(\n        Int,\n        trial.suggest_categorical(\n            \"vector_transport_method\", Vector(eachindex(objective.vts))\n        ),\n    )]\n    retr = objective.retrs[pyconvert(\n        Int,\n        trial.suggest_categorical(\"retraction_method\", Vector(eachindex(objective.retrs))),\n    )]\n\n    # Here we select the manifold constructor, in case we want to try different manifolds for our problem. For example one could try defining a problem with orthogonality constraints on Stiefel, Grassmann or flag manifold.\n    manifold_name, manifold_constructor = objective.manifold_constructors[pyconvert(\n        Int,\n        trial.suggest_categorical(\n            \"manifold\", Vector(eachindex(objective.manifold_constructors))\n        ),\n    )]\n\n    # Here the stepsize selection method type is selected.\n    manopt_stepsize_name, manopt_stepsize_constructor = objective.manopt_stepsize[pyconvert(\n        Int,\n        trial.suggest_categorical(\n            \"manopt_stepsize\", Vector(eachindex(objective.manopt_stepsize))\n        ),\n    )]\n\n    # This parametrizes stepsize selection methods with relevant numerical parameters.\n    local c1_val, c2_val, hz_sigma\n    if manopt_stepsize_name == \"Wolfe-Powell\"\n        c1_val = pyconvert(\n            Float64, trial.suggest_float(\"Wolfe-Powell c1\", 1e-5, 1e-2; log=true)\n        )\n        c2_val =\n            1.0 - pyconvert(\n                Float64, trial.suggest_float(\"Wolfe-Powell 1-c2\", 1e-4, 1e-2; log=true)\n            )\n    elseif manopt_stepsize_name == \"Improved HZ\"\n        hz_sigma = pyconvert(Float64, trial.suggest_float(\"Improved HZ sigma\", 0.1, 0.9))\n    end\n\n    # The current loss estimate, taking into account estimated loss values for larger, not-yet-evaluated values of `N`.\n    loss = sum(objective.pruning_losses)\n\n    # Here iterate over problems we want to optimize for\n    # from smallest to largest; pruning should stop the iteration early\n    # if the hyperparameter set is not promising\n    cur_i = 0\n    for N in objective.N_range\n        # Here we define the initial point for the optimization procedure\n        p0 = zeros(N)\n        p0[1] = 1\n        M = manifold_constructor(N)\n        # Here we construct the specific line search to be used\n        local ls\n        if manopt_stepsize_name == \"Wolfe-Powell\"\n            ls = manopt_stepsize_constructor(M, c1_val, c2_val)\n        elseif manopt_stepsize_name == \"Improved HZ\"\n            ls = manopt_stepsize_constructor(M, hz_sigma)\n        else\n            ls = manopt_stepsize_constructor(M)\n        end\n        manopt_time, manopt_iters, manopt_obj = benchmark_time_state(\n            ManoptQN(),\n            M,\n            N,\n            objective.obj,\n            objective.grad,\n            p0,\n            ls,\n            pyconvert(Int, mem_len),\n            objective.gtol;\n            vector_transport_method=vt,\n            retraction_method=retr,\n        )\n        # TODO: turn this into multi-criteria optimization when Optuna starts supporting\n        # pruning in such problems\n        loss -= objective.pruning_losses[cur_i + 1]\n        loss += manopt_time + objective.obj_loss_coeff * manopt_obj\n        trial.report(loss, cur_i)\n        if pyconvert(Bool, trial.should_prune().__bool__())\n            throw(PyException(optuna.TrialPruned()))\n        end\n        cur_i += 1\n    end\n    return loss\nend In the following benchmarking code you will most likely have to adapt solver parameters. This is designed around  quasi_Newton  but can be adapted to any solver as needed. The example below performs a small number of trials for quick rendering but in practice you should aim for at least a few thousand trials (the  n_trials  parameter). # An abstract type in case we want to try different optimization packages.\nabstract type AbstractOptimConfig end\nstruct ManoptQN <: AbstractOptimConfig end\n\n# Benchmark that evaluates hyperparameters. Returns time to reach the solution, number of iterations and final value of the objective.\nfunction benchmark_time_state(\n    ::ManoptQN,\n    M::AbstractManifold,\n    N,\n    f,\n    g!,\n    p0,\n    stepsize::Manopt.Stepsize,\n    mem_len::Int,\n    gtol::Real;\n    kwargs...,\n)\n    manopt_sc = StopWhenGradientNormLess(gtol; norm=norm_inf) | StopAfterIteration(1000)\n    mem_len = min(mem_len, manifold_dimension(M))\n    manopt_state = quasi_Newton(\n        M,\n        f,\n        g!,\n        p0;\n        stepsize=stepsize,\n        evaluation=InplaceEvaluation(),\n        return_state=true,\n        memory_size=mem_len,\n        stopping_criterion=manopt_sc,\n        debug=[],\n        kwargs...,\n    )\n    bench_manopt = @benchmark quasi_Newton(\n        $M,\n        $f,\n        $g!,\n        $p0;\n        stepsize=$(stepsize),\n        evaluation=$(InplaceEvaluation()),\n        memory_size=$mem_len,\n        stopping_criterion=$(manopt_sc),\n        debug=[],\n        $kwargs...,\n    )\n    iters = get_count(manopt_state, :Iterations)\n    final_val = f(M, manopt_state.p)\n    return median(bench_manopt.times) / 1000, iters, final_val\nend\n\n\"\"\"\n    lbfgs_study(; pruning_coeff::Float64=0.95)\n\nSet up the example hyperparameter optimization study.\n\"\"\"\nfunction lbfgs_study(; pruning_coeff::Float64=0.95)\n    Ns = [2^n for n in 1:3:12]\n    ls_hz = LineSearches.HagerZhang()\n    od = ObjectiveData(\n        f_rosenbrock,\n        g_rosenbrock!,\n        Ns,\n        1e-5,\n        AbstractVectorTransportMethod[ParallelTransport(), ProjectionTransport()],\n        [ExponentialRetraction(), ProjectionRetraction()],\n        Tuple{String,Any}[(\"Sphere\", N -> Manifolds.Sphere(N - 1))],\n        zeros(Float64, eachindex(Ns)),\n        Tuple{String,Any}[\n            (\"LS-HZ\", M -> Manopt.LineSearchesStepsize(ls_hz)),\n            #(\"Improved HZ\", (M, sigma) -> HagerZhangLinesearch(M; sigma=sigma)),\n            (\"Wolfe-Powell\", (M, c1, c2) -> Manopt.WolfePowellLinesearch(M, c1, c2)),\n        ],\n        10.0,\n    )\n\n    # Here you need to define baseline values of all hyperparameters\n    baseline_pruning_losses = compute_pruning_losses(\n        od,\n        Dict(\"mem_len\" => 4),\n        Dict(\n            \"Wolfe-Powell c1\" => 1e-4,\n            \"Wolfe-Powell 1-c2\" => 1e-3,\n            \"Improved HZ sigma\" => 0.9,\n        ),\n        Dict(\n            \"vector_transport_method\" => 1,\n            \"retraction_method\" => 1,\n            \"manifold\" => 1,\n            \"manopt_stepsize\" => 1,\n        ),\n    )\n    od.pruning_losses = pruning_coeff * baseline_pruning_losses\n\n    study = optuna.create_study(; study_name=\"L-BFGS\")\n    # Here you can specify number of trials and timeout (in seconds).\n    study.optimize(od; n_trials=1000, timeout=500)\n    println(\"Best params is $(study.best_params) with value $(study.best_value)\")\n    selected_manifold = od.manifold_constructors[pyconvert(Int, study.best_params[\"manifold\"])][1]\n    selected_retraction_method = od.retrs[pyconvert(Int, study.best_params[\"retraction_method\"])]\n    selected_vector_transport = od.vts[pyconvert(Int, study.best_params[\"vector_transport_method\"])]\n    println(\"Selected manifold: $(selected_manifold)\")\n    println(\"Selected retraction method: $(selected_retraction_method)\")\n    println(\"Selected vector transport method: $(selected_vector_transport)\")\n    return study\nend\n\nlbfgs_study() Best params is {'mem_len': 3, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.0006125542888545935, 'Wolfe-Powell 1-c2': 0.0010744467792321093} with value 5510.963227438757\nSelected manifold: Sphere\nSelected retraction method: ExponentialRetraction()\nSelected vector transport method: ProjectionTransport()\n\n[I 2024-03-16 18:04:17,965] A new study created in memory with name: L-BFGS\n[I 2024-03-16 18:04:45,996] Trial 0 finished with value: 5639.789870295856 and parameters: {'mem_len': 26, 'vector_transport_method': 1, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.00027288064367948073, 'Wolfe-Powell 1-c2': 0.00026503788892114045}. Best is trial 0 with value: 5639.789870295856.\n[I 2024-03-16 18:05:11,860] Trial 1 finished with value: 5635.936370295855 and parameters: {'mem_len': 11, 'vector_transport_method': 1, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.002743250060163298, 'Wolfe-Powell 1-c2': 0.00037986521186922096}. Best is trial 1 with value: 5635.936370295855.\n[I 2024-03-16 18:05:39,386] Trial 2 finished with value: 5673.101441724422 and parameters: {'mem_len': 26, 'vector_transport_method': 2, 'retraction_method': 2, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.00043339485784312605, 'Wolfe-Powell 1-c2': 0.0027302649933974173}. Best is trial 1 with value: 5635.936370295855.\n[I 2024-03-16 18:06:10,279] Trial 3 finished with value: 7410.818084581564 and parameters: {'mem_len': 26, 'vector_transport_method': 1, 'retraction_method': 2, 'manifold': 1, 'manopt_stepsize': 1}. Best is trial 1 with value: 5635.936370295855.\n[I 2024-03-16 18:06:37,995] Trial 4 finished with value: 5756.566226449636 and parameters: {'mem_len': 25, 'vector_transport_method': 1, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 1}. Best is trial 1 with value: 5635.936370295855.\n[I 2024-03-16 18:06:42,755] Trial 5 pruned. \n[I 2024-03-16 18:06:58,577] Trial 6 pruned. \n[I 2024-03-16 18:07:15,366] Trial 7 pruned. \n[I 2024-03-16 18:07:40,605] Trial 8 finished with value: 5581.7437274386975 and parameters: {'mem_len': 7, 'vector_transport_method': 1, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.0010567355712112379, 'Wolfe-Powell 1-c2': 0.003948002490203636}. Best is trial 8 with value: 5581.7437274386975.\n[I 2024-03-16 18:07:46,021] Trial 9 pruned. \n[I 2024-03-16 18:08:11,512] Trial 10 finished with value: 5510.963227438757 and parameters: {'mem_len': 3, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.0006125542888545935, 'Wolfe-Powell 1-c2': 0.0010744467792321093}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:08:35,914] Trial 11 finished with value: 5521.388656010121 and parameters: {'mem_len': 2, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.0006738829952322474, 'Wolfe-Powell 1-c2': 0.0010639659137420014}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:09:00,317] Trial 12 finished with value: 5521.36958458155 and parameters: {'mem_len': 2, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.00010975606104676191, 'Wolfe-Powell 1-c2': 0.0007663843095951679}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:09:24,680] Trial 13 finished with value: 5520.7020845815505 and parameters: {'mem_len': 2, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 6.743450835567536e-05, 'Wolfe-Powell 1-c2': 0.0008779759729737719}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:09:50,268] Trial 14 pruned. \n[I 2024-03-16 18:10:15,494] Trial 15 finished with value: 5595.119584581556 and parameters: {'mem_len': 6, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 8.147444451747575e-05, 'Wolfe-Powell 1-c2': 0.00012268601197923553}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:10:25,264] Trial 16 pruned. \n[I 2024-03-16 18:10:50,209] Trial 17 finished with value: 5572.474513153012 and parameters: {'mem_len': 5, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.0015998473664092935, 'Wolfe-Powell 1-c2': 0.0005109172020536229}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:10:54,772] Trial 18 pruned. \n[I 2024-03-16 18:11:04,534] Trial 19 pruned. \n[I 2024-03-16 18:11:28,873] Trial 20 finished with value: 5512.3824417244705 and parameters: {'mem_len': 3, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 1.1581668103921961e-05, 'Wolfe-Powell 1-c2': 0.0002691056199427656}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:11:53,327] Trial 21 finished with value: 5529.088227438692 and parameters: {'mem_len': 4, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 1.3645031886009879e-05, 'Wolfe-Powell 1-c2': 0.0001863385753491203}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:12:17,911] Trial 22 finished with value: 5522.041370295835 and parameters: {'mem_len': 2, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 1.0030173525937465e-05, 'Wolfe-Powell 1-c2': 0.000543991948003312}. Best is trial 10 with value: 5510.963227438757.\n[I 2024-03-16 18:12:27,645] Trial 23 pruned. \n[I 2024-03-16 18:12:52,163] Trial 24 finished with value: 5528.840941724406 and parameters: {'mem_len': 4, 'vector_transport_method': 2, 'retraction_method': 1, 'manifold': 1, 'manopt_stepsize': 2, 'Wolfe-Powell c1': 0.000245400433292576, 'Wolfe-Powell 1-c2': 0.000133639324295565}. Best is trial 10 with value: 5510.963227438757.\n\nPython: <optuna.study.study.Study object at 0x70dd985d9b50>"},{"id":3652,"pagetitle":"Hyperparameter optimziation","title":"Summary","ref":"/manoptexamples/stable/examples/HyperparameterOptimization/#Summary","content":" Summary We‚Äôve shown how to automatically select the best hyperparameter values for your optimization problem."},{"id":3653,"pagetitle":"Hyperparameter optimziation","title":"Literature","ref":"/manoptexamples/stable/examples/HyperparameterOptimization/#Literature","content":" Literature [ASY+19] T.¬†Akiba, S.¬†Sano, T.¬†Yanase, T.¬†Ohta and M.¬†Koyama.  Optuna: A Next-generation Hyperparameter Optimization Framework . In:  Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  (2019),  arXiv:1907.10902 ."},{"id":3656,"pagetitle":"Riemannian Median","title":"A comparison of the RCBM with the PBA and the SGM for the Riemannian median","ref":"/manoptexamples/stable/examples/RCBM-Median/#A-comparison-of-the-RCBM-with-the-PBA-and-the-SGM-for-the-Riemannian-median","content":" A comparison of the RCBM with the PBA and the SGM for the Riemannian median Hajg Jasa 2024-06-27"},{"id":3657,"pagetitle":"Riemannian Median","title":"Introduction","ref":"/manoptexamples/stable/examples/RCBM-Median/#Introduction","content":" Introduction In this example we compare the Riemannian Convex Bundle Method (RCBM) [ BHJ24 ] with the Proximal Bundle Algorithm, which was introduced in [ HNP23 ], and with the Subgradient Method (SGM), introduced in [FerreiraOliveira:1998:1], to find the Riemannian median. This example reproduces the results from [ BHJ24 ], Section 5. The runtimes reported in the tables are measured in seconds. using PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots\nusing QuadraticModels, RipQP\nusing LinearAlgebra, LRUCache, Random\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples Let  $\\mathcal M$  be a Hadamard manifold and  $\\{q_1,\\ldots,q_N\\} \\in \\mathcal M$  denote  $N = 1000$  Gaussian random data points. Let  $f \\colon \\mathcal M \\to \\mathbb R$  be defined by \\[f(p) = \\sum_{j = 1}^N w_j \\, \\mathrm{dist}(p, q_j),\\] where  $w_j$ ,  $j = 1, \\ldots, N$  are positive weights such that  $\\sum_{j = 1}^N w_j = 1$ . The Riemannian geometric median  $p^*$  of the dataset \\[\\mathcal D = \\{\n    q_1,\\ldots,q_N \\, \\vert \\, q_j \\in \\mathcal M\\text{ for all } j = 1,\\ldots,N\n\\}\\] is then defined as \\[    p^* \\coloneqq \\operatorname*{arg\\,min}_{p \\in \\mathcal M} f(p),\\] where equality is justified since  $p^*$  is uniquely determined on Hadamard manifolds. In our experiments, we choose the weights  $w_j = \\frac{1}{N}$ . We initialize the experiment parameters, as well as utility functions. experiment_name = \"RCBM-Median\"\nresults_folder = joinpath(@__DIR__, experiment_name)\n!isdir(results_folder) && mkdir(results_folder)\nseed_argument = 57\n\natol = 1e-8\nN = 1000 # number of data points\nspd_dims = [2, 5, 10, 15]\nhn_sn_dims = [1, 2, 5, 10, 15]\n\n# Generate a point that is at most `tol` close to the point `p` on `M`\nfunction close_point(M, p, tol; retraction_method=Manifolds.default_retraction_method(M, typeof(p)))\n    X = rand(M; vector_at = p)\n    X .= tol * rand() * X / norm(M, p, X)\n    return retract(M, p, X, retraction_method)\nend # Objective and subdifferential\nf(M, p, data) = sum(1 / length(data) * distance.(Ref(M), Ref(p), data))\ndomf(M, p, centroid, diameter) = distance(M, p, centroid) < diameter / 2 ? true : false\nfunction ‚àÇf(M, p, data, atol=atol)\n    return sum(\n        1 / length(data) *\n        ManifoldDiff.subgrad_distance.(Ref(M), data, Ref(p), 1; atol=atol),\n    )\nend maxiter = 5000\nrcbm_kwargs(diameter, domf, k_max, k_min) = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :count => [:Cost, :SubGradient],\n    :domain => domf,\n    :debug => [\n        :Iteration,\n        (:Cost, \"F(p): %1.16f \"),\n        (:Œæ, \"Œæ: %1.8f \"),\n        (:last_stepsize, \"step size: %1.8f\"),\n        :Stop,\n        1000,\n        \"\\n\",\n    ],\n    :diameter => diameter,\n    :k_max => k_max,\n    :k_min => k_min,\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\nrcbm_bm_kwargs(diameter, domf, k_max, k_min) = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :diameter => diameter,\n    :domain => domf,\n    :k_max => k_max,\n    :k_min => k_min,\n]\npba_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :count => [:Cost, :SubGradient],\n    :debug => [\n        :Iteration,\n        :Stop,\n        (:Cost, \"F(p): %1.16f \"),\n        (:ŒΩ, \"ŒΩ: %1.16f \"),\n        (:c, \"c: %1.16f \"),\n        (:Œº, \"Œº: %1.8f \"),\n        :Stop,\n        1000,\n        \"\\n\",\n    ],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stopping_criterion => StopWhenLagrangeMultiplierLess(atol) | StopAfterIteration(maxiter),\n]\npba_bm_kwargs = [:cache => (:LRU, [:Cost, :SubGradient], 50),]\nsgm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :count => [:Cost, :SubGradient],\n    :debug => [:Iteration, (:Cost, \"F(p): %1.16f \"), :Stop, 1000, \"\\n\"],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n    :stepsize => DecreasingLength(; exponent=1, factor=1, subtrahend=0, length=1, shift=0, type=:absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(‚àöatol) | StopAfterIteration(maxiter),\n]\nsgm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stepsize => DecreasingLength(; exponent=1, factor=1, subtrahend=0, length=1, shift=0, type=:absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(‚àöatol) | StopAfterIteration(maxiter),\n] Before running the experiments, we initialize data collection functions that we will use later global col_names_1 = [\n    :Dimension,\n    :Iterations_1,\n    :Time_1,\n    :Objective_1,\n    :Iterations_2,\n    :Time_2,\n    :Objective_2,\n]\ncol_types_1 = [\n    Int64,\n    Int64,\n    Float64,\n    Float64,\n    Int64,\n    Float64,\n    Float64,\n]\nnamed_tuple_1 = (; zip(col_names_1, type[] for type in col_types_1 )...)\nglobal col_names_2 = [\n    :Dimension,\n    :Iterations,\n    :Time,\n    :Objective,\n]\ncol_types_2 = [\n    Int64,\n    Int64,\n    Float64,\n    Float64,\n]\nnamed_tuple_2 = (; zip(col_names_2, type[] for type in col_types_2 )...)\nfunction initialize_dataframes(results_folder, experiment_name, subexperiment_name, named_tuple_1, named_tuple_2)\n    A1 = DataFrame(named_tuple_1)\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name * \"_$subexperiment_name\" * \"-Comparisons-Convex-Prox.csv\",\n        ),\n        A1;\n        header=false,\n    )\n    A2 = DataFrame(named_tuple_2)\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name * \"_$subexperiment_name\" * \"-Comparisons-Subgrad.csv\",\n        ),\n        A2;\n        header=false,\n    )\n    return A1, A2\nend function export_dataframes(M, records, times, results_folder, experiment_name, subexperiment_name, col_names_1, col_names_2)\n    B1 = DataFrame(;\n        Dimension=manifold_dimension(M),\n        Iterations_1=maximum(first.(records[1])),\n        Time_1=times[1],\n        Objective_1=minimum([r[2] for r in records[1]]),\n        Iterations_2=maximum(first.(records[2])),\n        Time_2=times[2],\n        Objective_2=minimum([r[2] for r in records[2]]),\n    )\n    B2 = DataFrame(;\n        Dimension=manifold_dimension(M),\n        Iterations=maximum(first.(records[3])),\n        Time=times[3],\n        Objective=minimum([r[2] for r in records[3]]),\n    )\n    return B1, B2\nend\nfunction write_dataframes(B1, B2, results_folder, experiment_name, subexperiment_name)\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name *\n            \"_$subexperiment_name\" *\n            \"-Comparisons-Convex-Prox.csv\",\n        ),\n        B1;\n        append=true,\n    )\n    CSV.write(\n        joinpath(\n            results_folder,\n            experiment_name *\n            \"_$subexperiment_name\" *\n            \"-Comparisons-Subgrad.csv\",\n        ),\n        B2;\n        append=true,\n    )\nend"},{"id":3658,"pagetitle":"Riemannian Median","title":"The Median on the Hyperboloid Model","ref":"/manoptexamples/stable/examples/RCBM-Median/#The-Median-on-the-Hyperboloid-Model","content":" The Median on the Hyperboloid Model subexperiment_name = \"Hn\"\nk_max_hn = -1.0\nk_min_hn = -1.0\n\nglobal A1, A2 = initialize_dataframes(\n    results_folder,\n    experiment_name,\n    subexperiment_name,\n    named_tuple_1,\n    named_tuple_2\n)\n\nfor n in hn_sn_dims\n    Random.seed!(seed_argument)\n\n    M = Hyperbolic(Int(2^n))\n    data_hn = [rand(M) for _ in 1:N]\n    dists = [distance(M, z, y) for z in data_hn, y in data_hn]\n    diameter_hn = 2 * maximum(dists)\n    p0 = data_hn[minimum(Tuple(findmax(dists)[2]))]\n\n    f_hn(M, p) = f(M, p, data_hn)\n    domf_hn(M, p) = domf(M, p, p0, diameter_hn)\n    ‚àÇf_hn(M, p) = ‚àÇf(M, p, data_hn, atol)\n\n    # Optimization\n    rcbm = convex_bundle_method(M, f_hn, ‚àÇf_hn, p0; rcbm_kwargs(diameter_hn, domf_hn, k_max_hn, k_min_hn)...)\n    rcbm_result = get_solver_result(rcbm)\n    rcbm_record = get_record(rcbm)\n\n    pba = proximal_bundle_method(M, f_hn, ‚àÇf_hn, p0; pba_kwargs...)\n    pba_result = get_solver_result(pba)\n    pba_record = get_record(pba)\n\n    sgm = subgradient_method(M, f_hn, ‚àÇf_hn, p0; sgm_kwargs...)\n    sgm_result = get_solver_result(sgm)\n    sgm_record = get_record(sgm)\n\n    records = [\n        rcbm_record,\n        pba_record,\n        sgm_record,\n    ]\n\n    if benchmarking\n        rcbm_bm = @benchmark convex_bundle_method($M, $f_hn, $‚àÇf_hn, $p0; rcbm_bm_kwargs($diameter_hn, $domf_hn, $k_max_hn, $k_min_hn)...)\n        pba_bm = @benchmark proximal_bundle_method($M, $f_hn, $‚àÇf_hn, $p0; $pba_bm_kwargs...)\n        sgm_bm = @benchmark subgradient_method($M, $f_hn, $‚àÇf_hn, $p0; $sgm_bm_kwargs...)\n        \n        times = [\n            median(rcbm_bm).time * 1e-9,\n            median(pba_bm).time * 1e-9,\n            median(sgm_bm).time * 1e-9,\n        ]\n\n        B1, B2 = export_dataframes(\n            M,\n            records,\n            times,\n            results_folder,\n            experiment_name,\n            subexperiment_name,\n            col_names_1,\n            col_names_2,\n        )\n\n        append!(A1, B1)\n        append!(A2, B2)\n        (export_table) && (write_dataframes(B1, B2, results_folder, experiment_name, subexperiment_name))\n    end\nend We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA‚Ä¶ Dimension Iterations_1 Time_1 Objective_1 Iterations_2 Time_2 Objective_2 2 9 0.00523775 1.05192 251 0.132011 1.05192 4 8 0.00469981 1.07516 230 0.132091 1.07516 32 15 0.0151958 1.08559 234 0.180374 1.08559 1024 16 0.284984 1.09706 234 4.00771 1.09706 32768 16 7.34017 1.0681 229 91.2803 1.0681 ‚Ä¶ Whereas the following table refers to the SGM Dimension Iterations Time Objective 2 18 0.00811254 1.04748 4 19 0.00953129 1.05518 32 25 0.0208788 1.08559 1024 23 0.400038 1.09706 32768 21 8.81869 1.06488"},{"id":3659,"pagetitle":"Riemannian Median","title":"The Median on the Symmetric Positive Definite Matrix Space","ref":"/manoptexamples/stable/examples/RCBM-Median/#The-Median-on-the-Symmetric-Positive-Definite-Matrix-Space","content":" The Median on the Symmetric Positive Definite Matrix Space subexperiment_name = \"SPD\"\nk_max_spd = 0.0\nk_min_spd = -1/2\n\nglobal A1_SPD, A2_SPD = initialize_dataframes(\n    results_folder,\n    experiment_name,\n    subexperiment_name,\n    named_tuple_1,\n    named_tuple_2\n)\n\nfor n in spd_dims\n    Random.seed!(seed_argument)\n\n    M = SymmetricPositiveDefinite(Int(n))\n    data_spd = [rand(M) for _ in 1:N]\n    dists = [distance(M, z, y) for z in data_spd, y in data_spd]\n    diameter_spd = 2 * maximum(dists)\n    p0 = data_spd[minimum(Tuple(findmax(dists)[2]))]\n    \n    f_spd(M, p) = f(M, p, data_spd)\n    domf_spd(M, p) = domf(M, p, p0, diameter_spd)\n    ‚àÇf_spd(M, p) = ‚àÇf(M, p, data_spd, atol)\n\n    # Optimization\n    rcbm = convex_bundle_method(M, f_spd, ‚àÇf_spd, p0; rcbm_kwargs(diameter_spd, domf_spd, k_max_spd, k_min_spd)...)\n    rcbm_result = get_solver_result(rcbm)\n    rcbm_record = get_record(rcbm)\n\n    pba = proximal_bundle_method(M, f_spd, ‚àÇf_spd, p0; pba_kwargs...)\n    pba_result = get_solver_result(pba)\n    pba_record = get_record(pba)\n\n    sgm = subgradient_method(M, f_spd, ‚àÇf_spd, p0; sgm_kwargs...)\n    sgm_result = get_solver_result(sgm)\n    sgm_record = get_record(sgm)\n\n    records = [\n        rcbm_record,\n        pba_record,\n        sgm_record,\n    ]\n\n    if benchmarking\n        rcbm_bm = @benchmark convex_bundle_method($M, $f_spd, $‚àÇf_spd, $p0; rcbm_bm_kwargs($diameter_spd, $domf_spd, $k_max_spd, $k_min_spd)...)\n        pba_bm = @benchmark proximal_bundle_method($M, $f_spd, $‚àÇf_spd, $p0; $pba_bm_kwargs...)\n        sgm_bm = @benchmark subgradient_method($M, $f_spd, $‚àÇf_spd, $p0; $sgm_bm_kwargs...)\n\n        times = [\n            median(rcbm_bm).time * 1e-9,\n            median(pba_bm).time * 1e-9,\n            median(sgm_bm).time * 1e-9,\n        ]\n\n        B1_SPD, B2_SPD = export_dataframes(\n            M,\n            records,\n            times,\n            results_folder,\n            experiment_name,\n            subexperiment_name,\n            col_names_1,\n            col_names_2,\n        )\n\n        append!(A1_SPD, B1_SPD)\n        append!(A2_SPD, B2_SPD)\n        (export_table) && (write_dataframes(B1_SPD, B2_SPD, results_folder, experiment_name, subexperiment_name))\n    end\nend We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA‚Ä¶ Dimension Iterations_1 Time_1 Objective_1 Iterations_2 Time_2 Objective_2 3 43 0.303751 0.260846 57 0.441796 0.260846 15 49 2.01407 0.436536 75 1.74885 0.436536 55 15 1.30749 0.618059 89 6.15426 0.618059 120 6 1.20377 0.764031 123 15.4064 0.764031 ‚Ä¶ Whereas the following table refers to the SGM Dimension Iterations Time Objective 3 4629 46.5469 0.260846 15 1727 40.4873 0.436536 55 776 53.3628 0.618059 120 438 53.5932 0.764031"},{"id":3660,"pagetitle":"Riemannian Median","title":"The Median on the Sphere","ref":"/manoptexamples/stable/examples/RCBM-Median/#The-Median-on-the-Sphere","content":" The Median on the Sphere For the last experiment, note that a major difference here is that the sphere has constant positive sectional curvature equal to  $1$ . In this case, we lose the global convexity of the Riemannian distance and thus of the objective. Minimizers still exist, but they may, in general, be non-unique. subexperiment_name = \"Sn\"\nk_max_sn = 1.0\nk_min_sn = 1.0\ndiameter_sn = œÄ / 3\n\nglobal A1_Sn, A2_Sn = initialize_dataframes(\n    results_folder,\n    experiment_name,\n    subexperiment_name,\n    named_tuple_1,\n    named_tuple_2\n)\n\nfor n in hn_sn_dims\n    Random.seed!(seed_argument)\n\n    M = Sphere(Int(2^n))\n    north = [0.0 for _ in 1:manifold_dimension(M)]\n    push!(north, 1.0)\n    data_sn = [close_point(M, north, diameter_sn / 2)]\n    distance(M, data_sn[1], north) < diameter_sn / 2 ? pop!(data_sn) : nothing\n    while length(data_sn) < N\n        q = close_point(M, north, diameter_sn / 2)\n        distance(M, q, north) < diameter_sn / 2 ? push!(data_sn, q) : nothing \n    end\n    dists = [distance(M, z, y) for z in data_sn, y in data_sn]\n    p0 = data_sn[minimum(Tuple(findmax(dists)[2]))]\n\n    f_sn(M, p) = f(M, p, data_sn)\n    domf_sn(M, p) = domf(M, p, north, diameter_sn)\n    ‚àÇf_sn(M, p) = ‚àÇf(M, p, data_sn, atol)\n\n    # Optimization\n    rcbm = convex_bundle_method(M, f_sn, ‚àÇf_sn, p0; rcbm_kwargs(diameter_sn, domf_sn, k_max_sn, k_min_sn)...)\n    rcbm_result = get_solver_result(rcbm)\n    rcbm_record = get_record(rcbm)\n\n    pba = proximal_bundle_method(M, f_sn, ‚àÇf_sn, p0; pba_kwargs...)\n    pba_result = get_solver_result(pba)\n    pba_record = get_record(pba)\n\n    sgm = subgradient_method(M, f_sn, ‚àÇf_sn, p0; sgm_kwargs...)\n    sgm_result = get_solver_result(sgm)\n    sgm_record = get_record(sgm)\n\n    records = [\n        rcbm_record,\n        pba_record,\n        sgm_record,\n    ]\n\n    if benchmarking\n        rcbm_bm = @benchmark convex_bundle_method($M, $f_sn, $‚àÇf_sn, $p0; rcbm_bm_kwargs($diameter_sn, $domf_sn, $k_max_sn, $k_min_sn)...)\n        pba_bm = @benchmark proximal_bundle_method($M, $f_sn, $‚àÇf_sn, $p0; $pba_bm_kwargs...)\n        sgm_bm = @benchmark subgradient_method($M, $f_sn, $‚àÇf_sn, $p0; $sgm_bm_kwargs...)\n\n        times = [\n            median(rcbm_bm).time * 1e-9,\n            median(pba_bm).time * 1e-9,\n            median(sgm_bm).time * 1e-9,\n        ]\n\n        B1_Sn, B2_Sn = export_dataframes(\n            M,\n            records,\n            times,\n            results_folder,\n            experiment_name,\n            subexperiment_name,\n            col_names_1,\n            col_names_2,\n        )\n\n        append!(A1_Sn, B1_Sn)\n        append!(A2_Sn, B2_Sn)\n        (export_table) && (write_dataframes(B1_Sn, B2_Sn, results_folder, experiment_name, subexperiment_name))\n    end\nend We can take a look at how the algorithms compare to each other in their performance with the following table, where columns 2 to 4 relate to the RCBM, while columns 5 to 7 refer to the PBA‚Ä¶ Dimension Iterations_1 Time_1 Objective_1 Iterations_2 Time_2 Objective_2 2 43 0.0158139 0.258898 71 0.0184203 0.258898 4 74 0.0230197 0.253525 62 0.0168082 0.253525 32 102 0.043011 0.259886 64 0.0272739 0.259886 1024 103 0.890434 0.266993 68 0.622527 0.266993 32768 80 27.1998 0.259302 65 29.1502 0.259302 ‚Ä¶ Whereas the following table refers to the SGM Dimension Iterations Time Objective 2 401 0.0970337 0.258898 4 5000 1.33258 0.253525 32 231 0.0963392 0.259886 1024 185 1.98786 0.266993 32768 157 205.02 0.259302"},{"id":3661,"pagetitle":"Riemannian Median","title":"Technical details","ref":"/manoptexamples/stable/examples/RCBM-Median/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. using Pkg\nPkg.status() Status `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.5.0\n  [336ed68f] CSV v0.10.15\n  [35d6a980] ColorSchemes v3.27.1\n‚åÖ [5ae59095] Colors v0.12.11\n  [a93c6f00] DataFrames v1.7.0\n  [7073ff75] IJulia v1.26.0\n  [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.1\n  [d3d80556] LineSearches v7.3.0\n  [af67fdf4] ManifoldDiff v0.3.13\n  [1cead3c2] Manifolds v0.10.7\n  [3362f125] ManifoldsBase v0.15.22\n  [0fc0a36d] Manopt v0.5.3 `../../Manopt.jl`\n  [5b8d5e80] ManoptExamples v0.1.10 `..`\n  [51fcb6bd] NamedColors v0.2.2\n  [91a5bcdd] Plots v1.40.9\n‚åÉ [08abe8d2] PrettyTables v2.3.2\n  [6099a3de] PythonCall v0.9.23\n  [f468eda6] QuadraticModels v0.9.7\n  [1e40b3f8] RipQP v0.6.4\nInfo Packages marked with ‚åÉ and ‚åÖ have new versions available. Those with ‚åÉ may be upgradable, but those with ‚åÖ are restricted by compatibility constraints from upgrading. To see why use `status --outdated` using Dates\nnow() 2024-11-28T00:40:27.330"},{"id":3662,"pagetitle":"Riemannian Median","title":"Literature","ref":"/manoptexamples/stable/examples/RCBM-Median/#Literature","content":" Literature [BHJ24] R.¬†Bergmann, R.¬†Herzog and H.¬†Jasa.  The Riemannian Convex Bundle Method , preprint (2024),  arXiv:2402.13670 . [HNP23] N.¬†Hoseini Monjezi, S.¬†Nobakhtian and M.¬†R.¬†Pouryayevali.  A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds .  IMA¬†Journal¬†of¬†Numerical¬†Analysis  43 , 293‚Äì325  (2023)."},{"id":3665,"pagetitle":"The Rayleigh Quotient","title":"The Rayleigh Quotient","ref":"/manoptexamples/stable/examples/RayleighQuotient/#The-Rayleigh-Quotient","content":" The Rayleigh Quotient Ronny Bergmann 2024-03-09"},{"id":3666,"pagetitle":"The Rayleigh Quotient","title":"Introduction","ref":"/manoptexamples/stable/examples/RayleighQuotient/#Introduction","content":" Introduction This example reproduces a few conceptual ideas of Optimization on Manifolds that are used throughout [ Bou23 ] using the Rayleigh quotient and explores several different ways to use the algorithms from  Manopt.jl . For a symmetric matrix  $A \\in \\mathbb R^{n\\times n}$  we consider the  üìñ Rayleigh Quotient \\[\\operatorname*{arg\\,min}_{x \\in \\mathbb R^n \\backslash \\{0\\}}\n\\frac{x^{\\mathrm{T}}Ax}{\\lVert x¬†\\rVert^2}.\\] On the sphere we can omit the denominator and obtain \\[f(p) = p^{\\mathrm{T}}Ap,\\qquad p ‚àà ùïä^{n-1},\\] which by itself we can again continue in the embedding as \\[\\tilde f(x) = x^{\\mathrm{T}}Ax,\\qquad x \\in \\mathbb R^n.\\] This cost has the nice feature that at the minimizer  $p^*\\in\\mathbb S^{n-1}$  the function falue  $f(p^*)$  is the smalles eigenvalue of  $A$ . For the embedded function  $\\tilde f$  the gradient and Hessian can be computed with classical methods as \\[\\begin{align*}\n‚àá\\tilde f(x) &= 2Ax, \\qquad x ‚àà ‚Ñù^n,\n\\\\\n‚àá^2\\tilde f(x)[V] &= 2AV, \\qquad x, V ‚àà ‚Ñù^n.\n\\end{align*}\\] Similarly, cf.¬†Examples 3.62 and 5.27 of [ Bou23 ], the Riemannian gradient and Hessian on the manifold  $\\mathcal M = \\mathbb S^{n-1}$  are given by \\[\\begin{align*}\n\\operatorname{grad} f(p) &= 2Ap - 2(p^{\\mathrm{T}}Ap)*p,\\qquad p ‚àà ùïä^{n-1},\n\\\\\n\\operatorname{Hess} f(p)[X] &=  2AX - 2(p^{\\mathrm{T}}AX)p - 2(p^{\\mathrm{T}}Ap)X,\\qquad p ‚àà ùïä^{n-1}, X \\in T_pùïä^{n-1}\n\\end{align*}\\] Let‚Äôs first generate an example martrx  $A$ . using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment, using LRUCache, BenchmarkTools, LinearAlgebra, Manifolds, ManoptExamples, Manopt, Random\nRandom.seed!(42)\nn = 500\nA = Symmetric(randn(n, n) / n) And the manifolds M = Sphere(n-1) Sphere(499, ‚Ñù) E = get_embedding(M) Euclidean(500; field=‚Ñù)"},{"id":3667,"pagetitle":"The Rayleigh Quotient","title":"Setup the corresponding functions","ref":"/manoptexamples/stable/examples/RayleighQuotient/#Setup-the-corresponding-functions","content":" Setup the corresponding functions Since  RayleighQuotientCost ,  RayleighQuotientGrad!! , and  RayleighQuotientHess!!  are themselves manifold agnostic we only need to initialize them once. Agnostic here means that they would compute  $f$  is called with  M  as their first argument and  $\\tilde f$  if called with  E . We instantiate f = ManoptExamples.RayleighQuotientCost(A)\ngrad_f = ManoptExamples.RayleighQuotientGrad!!(A)\nHess_f = ManoptExamples.RayleighQuotientHess!!(A) the suffix  !!  also indicates that these functions both work as allocating and in-place variants. Given a starting point and some memory p0 = [1.0, zeros(n-1)...]\nX = zero_vector(M, p0) we can both call Y = grad_f(M, p0)  # Allocates memory\ngrad_f(M, X, p0)    # Computes in place of X and returns the result in X.\nnorm(M, p0, X-Y) 0.0 Now we can use a few different variants of solvers to approaach this and this tutorial will walk you through a few of them. First of all let‚Äôs construct the actual result ‚Äì¬†since Rayleigh quotient minimization is not necessarily the best way to compute the smallest Eigenvalue. We can also compute Œª = min(eigvals(A)...) -0.08924035897103727"},{"id":3668,"pagetitle":"The Rayleigh Quotient","title":"A Solver based on gradient information","ref":"/manoptexamples/stable/examples/RayleighQuotient/#A-Solver-based-on-gradient-information","content":" A Solver based on gradient information Let‚Äôs first just use first-order information and since we are just starting, maybe we only derived the Euclidean gradient  $\\nabla \\tilde f$ . We can ‚Äútell‚Äù the solver, that the provided function and the gradient are defined as the Euclidean variants in the embedding. internally,  Manopt.jl  then issues the conversion for Euclidean gradients to the corresponding Riemannian one, cf.¬†e.g.¬† this tutorial section  or Section 3.8 or more precisely Example 3.62 in [ Bou23 ]. But instead of diving into all the tecnical details, we can just specify  objective_type=:Euclidean  to trigger the conversion. We start with a simple  gradient descent s = gradient_descent(M, f, grad_f, p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 50, \"\\n\"],\n    return_state=true,\n)\nq1 = get_solver_result(s)\ns Initial f(x): -0.000727\n# 50    f(x): -0.088242|grad f(p)|:0.003870474326981599\n# 100   f(x): -0.088680|grad f(p)|:0.0034956568288634616\n# 150   f(x): -0.089026|grad f(p)|:0.0026514781676923237\n# 200   f(x): -0.089178|grad f(p)|:0.001531160335922979\n\n# Solver state for `Manopt.jl`s Gradient Descent\nAfter 200 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0\n    retraction_method=ExponentialRetraction()\n    contraction_factor=0.95\n    sufficient_decrease=0.1\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 200:  reached\n  * |grad f| < 1.0e-8: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Debug\n    :Iteration = [(:Iteration, \"# %-6d\"), (:Cost, \"f(x): %f\"), (:GradientNorm, \"|grad f(p)|:%s\"), \"\\n\", 50] From the final cost we can already see that  q1  is an eigenvector to the smallest eigenvalue we obtaines above. And we can compare this to running with the Riemannian gradient, since the  RayleighQuotientGrad!!  returns this one as well, when just called with the sphere as first Argument, we just have to remove the  objective_type . q2 = gradient_descent(M, f, grad_f, p0;\n    debug = [:Iteration, :Cost, :GradientNorm, 50, \"\\n\"],\n)\n#Test that both are the same\nisapprox(M, q1,q2) Initial f(x): -0.000727\n# 50    f(x): -0.088242|grad f(p)|:0.0038704743269815734\n# 100   f(x): -0.088680|grad f(p)|:0.0034956568288633714\n# 150   f(x): -0.089026|grad f(p)|:0.002651478167692353\n# 200   f(x): -0.089178|grad f(p)|:0.0015311603359229782\n\ntrue We can also benchmark both @benchmark gradient_descent($M, $f, $grad_f, $p0; objective_type=:Euclidean) BenchmarkTools.Trial: 15 samples with 1 evaluation per sample.\n Range (min ‚Ä¶ max):  313.438 ms ‚Ä¶ 365.884 ms  ‚îä GC (min ‚Ä¶ max): 10.02% ‚Ä¶ 4.56%\n Time  (median):     341.265 ms               ‚îä GC (median):    10.18%\n Time  (mean ¬± œÉ):   338.854 ms ¬±  15.925 ms  ‚îä GC (mean ¬± œÉ):   9.95% ¬± 1.56%\n\n  ‚ñÅ    ‚ñÅ  ‚ñÅ ‚ñÅ      ‚ñÅ      ‚ñà       ‚ñÅ‚ñÅ   ‚ñÅ    ‚ñÅ‚ñÅ  ‚ñÅ        ‚ñÅ    ‚ñÅ  \n  ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà ‚ñÅ\n  313 ms           Histogram: frequency by time          366 ms <\n\n Memory estimate: 1.13 GiB, allocs estimate: 7964. @benchmark gradient_descent($M, $f, $grad_f, $p0) BenchmarkTools.Trial: 161 samples with 1 evaluation per sample.\n Range (min ‚Ä¶ max):  28.766 ms ‚Ä¶ 40.709 ms  ‚îä GC (min ‚Ä¶ max): 0.00% ‚Ä¶ 19.53%\n Time  (median):     30.280 ms              ‚îä GC (median):    2.78%\n Time  (mean ¬± œÉ):   31.098 ms ¬±  2.404 ms  ‚îä GC (mean ¬± œÉ):  2.83% ¬±  3.53%\n\n    ‚ñá   ‚ñÉ‚ñÑ‚ñà‚ñÇ                                                   \n  ‚ñÉ‚ñÖ‚ñà‚ñà‚ñÉ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÖ‚ñá‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÉ ‚ñÉ\n  28.8 ms         Histogram: frequency by time        39.4 ms <\n\n Memory estimate: 11.20 MiB, allocs estimate: 8750. From these results we see, that the conversion from the Euclidean to the Riemannian gradient does require a small amount of effort and hence reduces the performance slighly. Still, if the Euclidean Gradient is easier to compute or already available, this is in terms of coding the faster way. Finally this is a tradeoff between derivation and implementation efforts for the Riemannian gradient and a slight performance reduction when using the Euclidean one."},{"id":3669,"pagetitle":"The Rayleigh Quotient","title":"A Solver based (also) on (approximate) Hessian information","ref":"/manoptexamples/stable/examples/RayleighQuotient/#A-Solver-based-(also)-on-(approximate)-Hessian-information","content":" A Solver based (also) on (approximate) Hessian information To also involve the Hessian, we consider the  trust regions  solver with three cases: Euclidean, approximating the Hessian Euclidean, providing the Hessian Riemannian, providing the Hessian but also using in-place evaluations. q3 = trust_regions(M, f, grad_f, p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n); Initial f(x): -0.000727\n# 10    f(x): -0.088412|grad f(p)|:0.020989167846023046\n# 20    f(x): -0.089079|grad f(p)|:0.007420373217153763\n# 30    f(x): -0.089095|grad f(p)|:0.0022962557538450884\n# 40    f(x): -0.089095|grad f(p)|:0.0022962557538450884\n# 50    f(x): -0.089095|grad f(p)|:0.002296255752372694\n# 60    f(x): -0.089095|grad f(p)|:0.002296255750900326\n# 70    f(x): -0.089095|grad f(p)|:0.002296255749427973\n# 80    f(x): -0.089095|grad f(p)|:0.0022962557479555895\n# 90    f(x): -0.089095|grad f(p)|:0.002296255746483256\n# 100   f(x): -0.089095|grad f(p)|:0.002296255745010824\n# 110   f(x): -0.089095|grad f(p)|:0.0022962557435384336\n# 120   f(x): -0.089095|grad f(p)|:0.0022962557420660814\n# 130   f(x): -0.089095|grad f(p)|:0.0022962557405937175\n# 140   f(x): -0.089095|grad f(p)|:0.0022962557391213406\n# 150   f(x): -0.089095|grad f(p)|:0.0022962557376489472\n# 160   f(x): -0.089095|grad f(p)|:0.0022962557361765603\n# 170   f(x): -0.089095|grad f(p)|:0.0022962557347042086\n# 180   f(x): -0.089095|grad f(p)|:0.0022962557332318594\n# 190   f(x): -0.089095|grad f(p)|:0.002296255731759444\n# 200   f(x): -0.089095|grad f(p)|:0.0022962557302870605\n# 210   f(x): -0.089095|grad f(p)|:0.002296255728814736\n# 220   f(x): -0.089095|grad f(p)|:0.0022962557273423162\n# 230   f(x): -0.089095|grad f(p)|:0.002296255725869944\n# 240   f(x): -0.089095|grad f(p)|:0.002296255724397584\n# 250   f(x): -0.089095|grad f(p)|:0.0022962557229252085\n# 260   f(x): -0.089095|grad f(p)|:0.0022962557214528346\n# 270   f(x): -0.089095|grad f(p)|:0.0022962557199804855\n# 280   f(x): -0.089095|grad f(p)|:0.0022962557185080726\n# 290   f(x): -0.089095|grad f(p)|:0.002296255717035685\n# 300   f(x): -0.089095|grad f(p)|:0.0022962557155633144\n# 310   f(x): -0.089095|grad f(p)|:0.0022962557140909605\n# 320   f(x): -0.089095|grad f(p)|:0.002296255712618571\n# 330   f(x): -0.089095|grad f(p)|:0.0022962557111461876\n# 340   f(x): -0.089095|grad f(p)|:0.002296255709673808\n# 350   f(x): -0.089095|grad f(p)|:0.0022962557082014416\n# 360   f(x): -0.089095|grad f(p)|:0.00229625570672907\n# 370   f(x): -0.089095|grad f(p)|:0.002296255705256684\n# 380   f(x): -0.089095|grad f(p)|:0.0022962557037843165\n# 390   f(x): -0.089095|grad f(p)|:0.002296255702311948\n# 400   f(x): -0.089095|grad f(p)|:0.0022962557008395722\n# 410   f(x): -0.089095|grad f(p)|:0.002296255699367198\n# 420   f(x): -0.089095|grad f(p)|:0.0022962556978947854\n# 430   f(x): -0.089095|grad f(p)|:0.002296255696422434\n# 440   f(x): -0.089095|grad f(p)|:0.0022962556949500715\n# 450   f(x): -0.089095|grad f(p)|:0.002296255693477701\n# 460   f(x): -0.089095|grad f(p)|:0.0022962556920053264\n# 470   f(x): -0.089095|grad f(p)|:0.002296255690532953\n# 480   f(x): -0.089095|grad f(p)|:0.0022962556890605674\n# 490   f(x): -0.089095|grad f(p)|:0.002296255687588196\n# 500   f(x): -0.089095|grad f(p)|:0.002296255686115812\n# 510   f(x): -0.089095|grad f(p)|:0.0022962556846434557\n# 520   f(x): -0.089095|grad f(p)|:0.00229625568317109\n# 530   f(x): -0.089095|grad f(p)|:0.0022962556816987227\n# 540   f(x): -0.089095|grad f(p)|:0.002296255680226299\n# 550   f(x): -0.089095|grad f(p)|:0.0022962556787539364\n# 560   f(x): -0.089095|grad f(p)|:0.0022962556772815833\n# 570   f(x): -0.089095|grad f(p)|:0.0022962556758091917\n# 580   f(x): -0.089095|grad f(p)|:0.0022962556743368473\n# 590   f(x): -0.089095|grad f(p)|:0.0022962556728644322\n# 600   f(x): -0.089095|grad f(p)|:0.002296255671392082\n# 610   f(x): -0.089095|grad f(p)|:0.0022962556699197066\n# 620   f(x): -0.089095|grad f(p)|:0.002296255668447314\n# 630   f(x): -0.089095|grad f(p)|:0.002296255666974941\n# 640   f(x): -0.089095|grad f(p)|:0.002296255665502539\n# 650   f(x): -0.089095|grad f(p)|:0.002296255664030171\n# 660   f(x): -0.089095|grad f(p)|:0.002296255662557798\n# 670   f(x): -0.089095|grad f(p)|:0.0022962556610854248\n# 680   f(x): -0.089095|grad f(p)|:0.002296255659613032\n# 690   f(x): -0.089095|grad f(p)|:0.0022962556581407082\n# 700   f(x): -0.089095|grad f(p)|:0.002296255656668318\n# 710   f(x): -0.089095|grad f(p)|:0.0022962556551958985\n# 720   f(x): -0.089095|grad f(p)|:0.002296255653723546\n# 730   f(x): -0.089095|grad f(p)|:0.002296255652251183\n# 740   f(x): -0.089095|grad f(p)|:0.0022962556507788168\n# 750   f(x): -0.089095|grad f(p)|:0.0022962556493064516\n# 760   f(x): -0.089095|grad f(p)|:0.002296255647834051\n# 770   f(x): -0.089095|grad f(p)|:0.002296255646361663\n# 780   f(x): -0.089095|grad f(p)|:0.0022962556448893287\n# 790   f(x): -0.089095|grad f(p)|:0.0022962556434169014\n# 800   f(x): -0.089095|grad f(p)|:0.0022962556419445497\n# 810   f(x): -0.089095|grad f(p)|:0.0022962556404721797\n# 820   f(x): -0.089095|grad f(p)|:0.0022962556389998293\n# 830   f(x): -0.089095|grad f(p)|:0.002296255637527417\n# 840   f(x): -0.089095|grad f(p)|:0.0022962556360550538\n# 850   f(x): -0.089095|grad f(p)|:0.00229625563458268\n# 860   f(x): -0.089095|grad f(p)|:0.002296255633110347\n# 870   f(x): -0.089095|grad f(p)|:0.0022962556316379126\n# 880   f(x): -0.089095|grad f(p)|:0.0022962556301655314\n# 890   f(x): -0.089095|grad f(p)|:0.0022962556286931957\n# 900   f(x): -0.089095|grad f(p)|:0.002296255627220805\n# 910   f(x): -0.089095|grad f(p)|:0.0022962556257484328\n# 920   f(x): -0.089095|grad f(p)|:0.0022962556242760554\n# 930   f(x): -0.089095|grad f(p)|:0.0022962556228036954\n# 940   f(x): -0.089095|grad f(p)|:0.002296255621331309\n# 950   f(x): -0.089095|grad f(p)|:0.0022962556198589204\n# 960   f(x): -0.089095|grad f(p)|:0.0022962556183865473\n# 970   f(x): -0.089095|grad f(p)|:0.002296255616914149\n# 980   f(x): -0.089095|grad f(p)|:0.0022962556154418005\n# 990   f(x): -0.089095|grad f(p)|:0.0022962556139694153\n# 1000  f(x): -0.089095|grad f(p)|:0.0022962556124970193 To provide the Hessian in the high-level interface we need to prodive it as an anonymous function, since any  struct  is considered to (eventually) be the also optional starting point. For space reasons, let‚Äôs also shorten the debug print to only iterations 7 and 14. q4 = trust_regions(M, f, grad_f, (E, p, X) -> Hess_f(E, p, X), p0; objective_type=:Euclidean,\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n); Initial f(x): -0.000727\n# 10    f(x): -0.089234|grad f(p)|:0.0013561755210368023 q5 = trust_regions(M, f, grad_f, (M, Y, p, X) -> Hess_f(M, Y, p, X), p0;\n    evaluation=InplaceEvaluation(),\n    debug = [:Iteration, :Cost, :GradientNorm, 10, \"\\n\"],\n); Initial f(x): -0.000727\n# 10    f(x): -0.089234|grad f(p)|:0.0013561755210368012 Let‚Äôs also here compare them in benchmarks. Let‚Äôs here compare all variants in their (more performant) in-place versions. @benchmark trust_regions($M, $f, $grad_f, $p0;\n  objective_type=:Euclidean,\n  evaluation=InplaceEvaluation(),\n) BenchmarkTools.Trial: 7 samples with 1 evaluation per sample.\n Range (min ‚Ä¶ max):  750.045 ms ‚Ä¶ 806.410 ms  ‚îä GC (min ‚Ä¶ max):  7.09% ‚Ä¶ 10.98%\n Time  (median):     782.691 ms               ‚îä GC (median):    10.68%\n Time  (mean ¬± œÉ):   781.670 ms ¬±  19.942 ms  ‚îä GC (mean ¬± œÉ):  10.21% ¬±  1.39%\n\n  ‚ñà           ‚ñà                   ‚ñà  ‚ñà          ‚ñà    ‚ñà        ‚ñà  \n  ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà ‚ñÅ\n  750 ms           Histogram: frequency by time          806 ms <\n\n Memory estimate: 1.96 GiB, allocs estimate: 75417. @benchmark trust_regions($M, $f, $grad_f, $((E, Y, p, X) -> Hess_f(E, Y, p, X)), $p0;\n  evaluation=InplaceEvaluation(),\n  objective_type=:Euclidean\n) BenchmarkTools.Trial: 217 samples with 1 evaluation per sample.\n Range (min ‚Ä¶ max):  18.994 ms ‚Ä¶ 42.563 ms  ‚îä GC (min ‚Ä¶ max):  0.00% ‚Ä¶ 8.23%\n Time  (median):     22.067 ms              ‚îä GC (median):    10.55%\n Time  (mean ¬± œÉ):   23.041 ms ¬±  3.516 ms  ‚îä GC (mean ¬± œÉ):   9.60% ¬± 3.83%\n\n  ‚ñÅ   ‚ñÇ‚ñÇ‚ñÑ‚ñà‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÉ  ‚ñÇ                                             \n  ‚ñà‚ñá‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ ‚ñÜ\n  19 ms        Histogram: log(frequency) by time      40.6 ms <\n\n Memory estimate: 44.92 MiB, allocs estimate: 12651. @benchmark trust_regions($M, $f, $grad_f, $((M, Y, p, X) -> Hess_f(M, Y, p, X)), $p0;\n    evaluation=InplaceEvaluation(),\n) BenchmarkTools.Trial: 336 samples with 1 evaluation per sample.\n Range (min ‚Ä¶ max):  12.458 ms ‚Ä¶ 46.158 ms  ‚îä GC (min ‚Ä¶ max): 0.00% ‚Ä¶ 0.00%\n Time  (median):     14.305 ms              ‚îä GC (median):    9.22%\n Time  (mean ¬± œÉ):   14.887 ms ¬±  3.059 ms  ‚îä GC (mean ¬± œÉ):  7.51% ¬± 6.54%\n\n  ‚ñÇ‚ñÑ‚ñÉ   ‚ñà‚ñà‚ñÇ‚ñÜ‚ñÅ                                                  \n  ‚ñà‚ñà‚ñà‚ñÜ‚ñÇ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ ‚ñÉ\n  12.5 ms         Histogram: frequency by time        27.3 ms <\n\n Memory estimate: 16.35 MiB, allocs estimate: 12633. We see that Hessian approximation is quite costly, and Gradient and Hessian conversion somewhat costly; still, they also might serve as a good starting point, before deciding to delve into computing Riemannian gradients and Hessians. Of course all 5 runs obtained solutions close by; one might consider the gradient based runs to not have fully converged. [distance(M, q1, q) for q ‚àà [q2,q3] ] 2-element Vector{Float64}:\n 8.835276992173579e-16\n 0.8953986994946139 [distance(M, q3, q) for q ‚àà [q4,q5] ] 2-element Vector{Float64}:\n 1.1159334410358788\n 1.1159334410358788 Which we can also see in the final cost, comparing it to the Eigenvalue [f(M, q) - Œª for q ‚àà [q1, q2, q3, q4, q5] ] 5-element Vector{Float64}:\n  6.211293387550776e-5\n  6.211293387559103e-5\n  0.0001451688142172225\n -5.551115123125783e-17\n -5.551115123125783e-17"},{"id":3670,"pagetitle":"The Rayleigh Quotient","title":"Summary","ref":"/manoptexamples/stable/examples/RayleighQuotient/#Summary","content":" Summary We illustrated several possibilities to call solvers, with both Euclidean gradient and Hessian and Riemannian gradient and Hessian, allocating and in-place function. While the performance is better for the Riemannian case, the Euclidean one is a worthy alternative, when those are easier to compute."},{"id":3671,"pagetitle":"The Rayleigh Quotient","title":"Literature","ref":"/manoptexamples/stable/examples/RayleighQuotient/#Literature","content":" Literature [Bou23] N.¬†Boumal.  An Introduction to Optimization on Smooth Manifolds . First¬†Edition ( Cambridge University Press, 2023 )."},{"id":3674,"pagetitle":"Riemannian Mean","title":"The Riemannian Center of Mass (mean)","ref":"/manoptexamples/stable/examples/Riemannian-mean/#The-Riemannian-Center-of-Mass-(mean)","content":" The Riemannian Center of Mass (mean) Ronny Bergmann 2023-07-02"},{"id":3675,"pagetitle":"Riemannian Mean","title":"Preliminary Notes","ref":"/manoptexamples/stable/examples/Riemannian-mean/#Preliminary-Notes","content":" Preliminary Notes Each of the example objectives or problems stated in this package should be accompanied by a  Quarto  notebook that illustrates their usage, like this one. For this first example, the objective is a very common one, for example also used in the  üèîÔ∏è Get started with Manopt.jl  tutorial of  Manopt.jl . The second goal of this tutorial is to also illustrate how this package provides these examples, namely in both an easy-to-use and a performant way. There are two recommended ways to activate a reproducible environment. For most cases the recommended environment is the one in  examples/ . If you are programming a new, relatively short example, consider using the packages main environment, which is the same as having  ManoptExamples.jl  in development mode. this requires that your example does not have any (additional) dependencies beyond the ones  ManoptExamples.jl  has anyways. For registered versions of  ManoptExamples.jl  use the environment of  examples/  and ‚Äì under development ‚Äì add  ManoptExamples.jl  in development mode from the parent folder. This should be changed after a new example is within a registered version to just use the  examples/  environment again. using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment,"},{"id":3676,"pagetitle":"Riemannian Mean","title":"Loading packages and defining data","ref":"/manoptexamples/stable/examples/Riemannian-mean/#Loading-packages-and-defining-data","content":" Loading packages and defining data Loading the necessary packages and defining a data set on a manifold using ManoptExamples, Manopt, Manifolds, ManifoldDiff, Random\nRandom.seed!(42)\nM = Sphere(2)\nn = 100\nœÉ = œÄ / 8\np = 1 / sqrt(2) * [1.0, 0.0, 1.0]\ndata = [exp(M, p,  œÉ * rand(M; vector_at=p)) for i in 1:n];"},{"id":3677,"pagetitle":"Riemannian Mean","title":"Variant 1: Using the functions","ref":"/manoptexamples/stable/examples/Riemannian-mean/#Variant-1:-Using-the-functions","content":" Variant 1: Using the functions We can define both the cost and gradient,  RiemannianMeanCost  and  RiemannianMeanGradient!! , respectively. For their mathematical derivation and further explanations, we again refer to  üèîÔ∏è Get started with Manopt.jl . f = ManoptExamples.RiemannianMeanCost(data)\ngrad_f = ManoptExamples.RiemannianMeanGradient!!(M, data) Then we can for example directly call a  gradient descent  as x1 = gradient_descent(M, f, grad_f, first(data)) 3-element Vector{Float64}:\n 0.6868392794567202\n 0.006531600696673591\n 0.7267799821044285"},{"id":3678,"pagetitle":"Riemannian Mean","title":"Variant 2: Using the objective","ref":"/manoptexamples/stable/examples/Riemannian-mean/#Variant-2:-Using-the-objective","content":" Variant 2: Using the objective A shorter way to directly obtain the  Manifold objective  including these two functions. Here, we want to specify that the objective can do in-place-evaluations using the  evaluation= -keyword. The objective can be obtained calling  Riemannian_mean_objective  as rmo = ManoptExamples.Riemannian_mean_objective(\n    M, data,\n    evaluation=InplaceEvaluation(),\n) Together with a manifold, this forms a  Manopt Problem , which would usually enable to switch manifolds between solver runs. Here we could for example switch to using  Euclidean(3)  instead for the same data the objective is build upon. rmp = DefaultManoptProblem(M, rmo) This enables us to for example solve the task with different, gradient based, solvers. The first is the same as above, just not using the high-level interface s1 = GradientDescentState(M; p=copy(M, first(data)))\nsolve!(rmp, s1)\nx2 = get_solver_result(s1) 3-element Vector{Float64}:\n 0.6868392794567202\n 0.006531600696673591\n 0.7267799821044285 but we can easily use a conjugate gradient instead s2 = ConjugateGradientDescentState(\n    M;\n    p=copy(M, first(data)),\n    stopping_criterion=StopAfterIteration(100),\n    stepsize=ArmijoLinesearch(M)(),\n    coefficient=FletcherReevesCoefficient(),\n)\nsolve!(rmp, s2)\nx3 = get_solver_result(s2) 3-element Vector{Float64}:\n 0.6868393613136017\n 0.006531541407458413\n 0.7267799052788726"},{"id":3679,"pagetitle":"Riemannian Mean","title":"Technical details","ref":"/manoptexamples/stable/examples/Riemannian-mean/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. using Pkg\nPkg.status() Status `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.13.4\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.29.0\n‚åÖ [5ae59095] Colors v0.12.11\n  [a93c6f00] DataFrames v1.7.0\n  [7073ff75] IJulia v1.27.0\n  [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [d3d80556] LineSearches v7.3.0\n  [ee78f7c6] Makie v0.22.4\n  [af67fdf4] ManifoldDiff v0.4.2\n  [1cead3c2] Manifolds v0.10.16\n  [3362f125] ManifoldsBase v1.0.3\n  [0fc0a36d] Manopt v0.5.12\n  [5b8d5e80] ManoptExamples v0.1.14 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.40.12\n  [08abe8d2] PrettyTables v2.4.0\n  [6099a3de] PythonCall v0.9.24\n  [f468eda6] QuadraticModels v0.9.8\n  [1e40b3f8] RipQP v0.6.4\nInfo Packages marked with ‚åÖ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated` using Dates\nnow() 2025-04-13T14:03:53.318"},{"id":3682,"pagetitle":"Robust PCA","title":"The Robust PCA computed on the Grassmann manifold","ref":"/manoptexamples/stable/examples/Robust-PCA/#The-Robust-PCA-computed-on-the-Grassmann-manifold","content":" The Robust PCA computed on the Grassmann manifold Ronny Bergmann, Laura Weigl 2023-07-02 For this example we first load the necessary packages. using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment, using LinearAlgebra, Random, Statistics\nusing Manifolds, Manopt, ManoptExamples\nusing Plots\nRandom.seed!(42)"},{"id":3683,"pagetitle":"Robust PCA","title":"Computing a Robust PCA","ref":"/manoptexamples/stable/examples/Robust-PCA/#Computing-a-Robust-PCA","content":" Computing a Robust PCA For a given matrix  $D ‚àà ‚Ñù^{d√ón}$  whose columns represent points in  $‚Ñù^d$ , a matrix  $p ‚àà ‚Ñù^{d√óm}$  is computed for a given dimension  $m < n$ :  $p$  represents an ONB of  $‚Ñù^{d√óm}$  such that the column space of  $p$  approximates the points (columns of  $D$ ), i.e.¬†the vectors  $D_i$  as well as possible. We compute  $p$  as a minimizer over the Grassmann manifold of the cost function: \\[\\begin{split}\nf(p)\n& = \\frac{1}{n}\\sum_{i=1}^{n}{\\operatorname{dist}(D_i, \\operatorname{span}(p))}\n\\\\\n& = \\frac{1}{n} \\sum_{i=1}^{n}\\lVert pp^TD_i - D_i\\rVert\n\\end{split}\\] The output cost represents the average distance achieved with the returned  $p$ , an orthonormal basis (or a point on the Stiefel manifold) representing the subspace (a point on the Grassmann manifold). Notice that norms are not squared, so we have a robust cost function. This means that  $f$  is nonsmooth, therefore we regularize with a pseudo-Huber loss function of smoothing parameter  $Œµ$ . \\[f_œµ(p) = \\frac{1}{n} \\sum_{i=1}^n{‚Ñì_œµ(\\lVert pp^{\\mathrm{T}}D_i - D_i\\rVert)},\\] where  $‚Ñì_œµ(x) = \\sqrt{x^2 + œµ^2} - œµ$ . The smoothing parameter is iteratively reduced in the final optimisation runs(with warm starts). First, we generate random data. For illustration purposes we take points in  $\\mathbb R^2$  and  $m=1$ , that is we aim to find a robust regression line. n = 40\nd = 2\noutliers = 15\ndata = randn(d, 1) * (1:n)' + 0.05 * randn(2, n) .* [1:n 1:n]'\n# Outliers:\npermute = shuffle(1:size(data, 2))'\ndata[:, permute[1:outliers]] = 30 * randn(2, outliers)\n# We are looking for a line here so we set\nm = 1 We use the Manopt toolbox to optimize the regularized cost function over the Grassmann manifold. To do this, we first need to define the problem structure. M = Grassmann(d,m); For the initial matrix  $p_0$  we use classical PCA via singular value decomposition. Thus, we use the first  $d$  left singular vectors. Then, we compute an optimum of the cost function over the Grassmann manifold. We use a trust-region method which is implemented in  Manopt.jl . Furthermore the cost and gradient are implemented in  ManoptExamples.jl . Since these are Huber regularized, both functors have the  œµ  as a parameter. To compute the Riemannian gradient we first compute the Euclidian gradient. Afterwards it is projected onto the tangent space by using the orthogonal projection  $pp^T - I$ , which converts the Euclidean to the Riemannian gradient. The trust-region method also requires the Hessian Matrix. By using  ApproxHessianFiniteDifference  using a finite difference scheme we get an approximation of the Hessian Matrix. We run the procedure several times, where the smoothing parameter  $Œµ$  is reduced iteratively. Œµ = 1.0\niterations = 6\nreduction = 0.5\nU, S, V = svd(data);\np0 = U[:, 1:m] 2√ó1 Matrix{Float64}:\n -0.14057143527189225\n  0.9900705386918653 Let‚Äôs generate the cost and gradient we aim to use here f = ManoptExamples.RobustPCACost(M, data, Œµ)\ngrad_f = ManoptExamples.RobustPCAGrad!!(M, data, Œµ) ManoptExamples.RobustPCAGrad!!{Matrix{Float64}, Float64}([0.7726062030458439 1.6583418797018163 ‚Ä¶ 30.833523701909474 30.512999245062304; -0.8954212160206203 -1.7120433539256108 ‚Ä¶ -35.85943792458936 -32.93976007215313], 1.0, [0.0 0.0 ‚Ä¶ 0.0 0.0; 0.0 0.0 ‚Ä¶ 0.0 0.0]) and check the initial cost f(M, p0) 16.158396619704558 Now we iterate the opimization with reducing  Œµ  after every iteration, which we update in  f  and  grad_f . q = copy(M, p0)\nŒµi = Œµ\nfor i in 1:iterations\n    f.Œµ = Œµi\n    grad_f.Œµ = Œµi\n    global q = trust_regions(\n        M,\n        f,\n        grad_f,\n        ApproxHessianFiniteDifference(\n            M, q, f;\n            vector_transport_method=ProjectionTransport(),\n            retraction_method=PolarRetraction(),\n        ),\n        q;\n        (project!)=project!,\n    )\n    global Œµi *= reduction\nend When finally setting  Œµ  we can investigate the final cost f.Œµ = 0.0\nf(M, q) 12.890044794413498 Finally, the results are presented visually. The data points are visualized in a scatter plot. The result of the robust PCA and (for comparison) the standard SVD solution are plotted as straight lines. fig = plot(data[1, :], data[2, :]; seriestype=:scatter, label=\"Data points\");\nplot!(\n    fig,\n    q[1] * [-1, 1] * 100,\n    q[2] * [-1, 1] * 100;\n    linecolor=:red,\n    linewidth=2,\n    label=\"Robust PCA\",\n);\nplot!(\n    fig,\n    p0[1] * [-1, 1] * 100,\n    p0[2] * [-1, 1] * 100;\n    xlims=1.1 * [minimum(data[1, :]), maximum(data[1, :])],\n    ylims=1.1 * [minimum(data[2, :]), maximum(data[2, :])],\n    linewidth=2,\n    linecolor=:black,\n    label=\"Standard SVD\",\n)"},{"id":3686,"pagetitle":"Rosenbrock","title":"The Rosenbrock Function","ref":"/manoptexamples/stable/examples/Rosenbrock/#The-Rosenbrock-Function","content":" The Rosenbrock Function Ronny Bergmann 2023-01-03 After loading the necessary packages using Pkg;\ncd(@__DIR__)\nPkg.activate(\".\"); # use the example environment, using Manifolds, Manopt, ManoptExamples\nusing Plots We fix the parameters for the  üìñ Rosenbrock  (where the wikipedia page has a slightly different parameter naming). a = 100.0\nb = 1.0\np0 = [1/10, 2/10] which is defined on  $\\mathbb R^2$ , so we need M = ‚Ñù^2 Euclidean(2; field=‚Ñù) and can then generate both the cost and the gradient f = ManoptExamples.RosenbrockCost(M; a=a, b=b)\ngrad_f = ManoptExamples.RosenbrockGradient!!(M; a=a, b=b) ManoptExamples.RosenbrockGradient!!{Float64}(100.0, 1.0) For comparison, we look at the initial cost f(M, p0) 4.42 And to illustrate, we run two small solvers with their default settings as a comparison."},{"id":3687,"pagetitle":"Rosenbrock","title":"Gradient Descent","ref":"/manoptexamples/stable/examples/Rosenbrock/#Gradient-Descent","content":" Gradient Descent We start with the  gradient descent solver . Since we need the state anyways to access the record, we also get from the  return_state=true  a short summary of the solver run. gd_state = gradient_descent(M, f, grad_f, p0; record = [:Iteration, :Cost], return_state=true) # Solver state for `Manopt.jl`s Gradient Descent\nAfter 200 iterations\n\n## Parameters\n* retraction method: ExponentialRetraction()\n\n## Stepsize\nArmijoLinesearch(;\n    initial_stepsize=1.0\n    retraction_method=ExponentialRetraction()\n    contraction_factor=0.95\n    sufficient_decrease=0.1\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 200:  reached\n  * |grad f| < 1.0e-8: not reached\nOverall: reached\nThis indicates convergence: No\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),) From the summary we see, that the gradient is not yet small enough, but we hit the 200 iterations (default) iteration limit. Collecting the cost recording and printing the final cost gd_x = get_record(gd_state, :Iteration, :Iteration)\ngd_y =  get_record(gd_state, :Iteration, :Cost)\nf(M, get_solver_result(gd_state)) 0.10562873187751265"},{"id":3688,"pagetitle":"Rosenbrock","title":"Quasi Newton","ref":"/manoptexamples/stable/examples/Rosenbrock/#Quasi-Newton","content":" Quasi Newton We can improve this using the  quasi Newton  algorithm qn_state = quasi_Newton(M, f, grad_f, p0;\n    record = [:Iteration, :Cost], return_state=true\n) # Solver state for `Manopt.jl`s Quasi Newton Method\nAfter 26 iterations\n\n## Parameters\n* direction update:        limited memory InverseBFGS (size 2) initial scaling 1.0and ParallelTransport() as vector transport.\n* retraction method:       ExponentialRetraction()\n* vector transport method: ParallelTransport()\n\n## Stepsize\nWolfePowellLinesearch(;\n    sufficient_descrease=0.0001\n    sufficient_curvature=0.999,\n    retraction_method = ExponentialRetraction()\n    vector_transport_method = ParallelTransport()\n    stop_when_stepsize_less = 1.0e-10\n)\n\n## Stopping criterion\n\nStop When _one_ of the following are fulfilled:\n  * Max Iteration 1000: not reached\n  * |grad f| < 1.0e-6: reached\nOverall: reached\nThis indicates convergence: Yes\n\n## Record\n(Iteration = RecordGroup([RecordIteration(), RecordCost()]),) And we see it stops far earlier, after 45 Iterations. We again collect the recorded values qn_x = get_record(qn_state, :Iteration, :Iteration)\nqn_y =  get_record(qn_state, :Iteration, :Cost)\nf(M, get_solver_result(qn_state)) 1.4404666436813376e-18 and see that the final value is close to the one of the minimizer f(M, ManoptExamples.minimizer(f)) 0.0 which we also see if we plot the recorded cost. fig = plot(gd_x, gd_y; linewidth=1, label=\"Gradient Descent\");\nplot!(fig, qn_x, qn_y; linewidth=1, label=\"Quasi Newton\")"},{"id":3691,"pagetitle":"Spectral Procrustes","title":"A comparison of the RCBM with the PBA, the SGM for solving the spectral Procrustes problem","ref":"/manoptexamples/stable/examples/Spectral-Procrustes/#A-comparison-of-the-RCBM-with-the-PBA,-the-SGM-for-solving-the-spectral-Procrustes-problem","content":" A comparison of the RCBM with the PBA, the SGM for solving the spectral Procrustes problem Hajg Jasa 2024-06-27"},{"id":3692,"pagetitle":"Spectral Procrustes","title":"Introduction","ref":"/manoptexamples/stable/examples/Spectral-Procrustes/#Introduction","content":" Introduction In this example we compare the Riemannian Convex Bundle Method (RCBM) [ BHJ24 ] with the Proximal Bundle Algorithm, which was introduced in [ HNP23 ], and with the Subgradient Method (SGM), introduced in [FerreiraOliveira:1998:1], to solve the spectral Procrustes problem on  $\\mathrm{SO}(250)$ . This example reproduces the results from [ BHJ24 ], Section 5. using PrettyTables\nusing BenchmarkTools\nusing CSV, DataFrames\nusing ColorSchemes, Plots\nusing QuadraticModels, RipQP\nusing Random, LinearAlgebra, LRUCache\nusing ManifoldDiff, Manifolds, Manopt, ManoptExamples"},{"id":3693,"pagetitle":"Spectral Procrustes","title":"The Problem","ref":"/manoptexamples/stable/examples/Spectral-Procrustes/#The-Problem","content":" The Problem Given two matrices  $A, B \\in \\mathbb R^{n \\times d}$  we aim to solve the Procrustes problem \\[    {\\arg\\min}_{p \\in \\mathrm{SO}(d)}\\ \\Vert A - B \\, p \\Vert_2\n    ,\\] where  $\\mathrm{SO}(d)$  is equipped with the standard bi-invariant metric, and where  $\\Vert \\,\\cdot\\, \\Vert_2$  denotes the spectral norm of a matrix, , its largest singular value. We aim to find the best matrix  $p \\in \\mathbb R^{d \\times d}$  such that  $p^\\top p = \\mathrm{id}$  is the identity matrix, or in other words  $p$  is the best rotation. Note that the spectral norm is convex in the Euclidean sense, but not geodesically convex on  $\\mathrm{SO}(d)$ . Let us define the objective as \\[    f (p)\n    =\n    \\Vert A - B \\, p \\Vert_2\n    .\\] To obtain subdifferential information, we use \\[    \\mathrm{proj}_p(-B^\\top UV^\\top)\\] as a substitute for  $\\partial f(p)$ , where  $U$  and  $V$  are some left and right singular vectors, respectively, corresponding to the largest singular value of  $A - B \\, p$ , and  $\\mathrm{proj}_p$  is the projection onto \\[    \\mathcal T_p \\mathrm{SO}(d)\n    =\n    \\{\n    A \\in \\mathbb R^{d,d} \\, \\vert \\, pA^\\top + Ap^\\top = 0, \\, \\mathrm{trace}(p^{-1}A)=0\n    \\}\n    .\\]"},{"id":3694,"pagetitle":"Spectral Procrustes","title":"Numerical Experiment","ref":"/manoptexamples/stable/examples/Spectral-Procrustes/#Numerical-Experiment","content":" Numerical Experiment We initialize the experiment parameters, as well as some utility functions. Random.seed!(33)\nn = 1000\nd = 250\nA = rand(n, d)\nB = randn(n, d)\ntol = 1e-8\nmax_iters = 5000\n#\n# Compute the orthogonal Procrustes minimizer given A and B\nfunction orthogonal_procrustes(A, B)\n    s =  svd((A'*B)')\n    R = s.U* s.Vt\n    return R\nend\n#\n# Algorithm parameters\nk_max = 1/4\nk_min = 0.0\ndiameter = œÄ/(3 * ‚àök_max)\n#\n# Manifolds and data\nM = SpecialOrthogonal(d)\np0 = orthogonal_procrustes(A, B)\nproject!(M, p0, p0) We now define objective and subdifferential (first the Euclidean one, then the projected one). f(M, p) = opnorm(A - B*p)\nfunction ‚àÇ‚Çëf(M, p)\n    cost_svd = svd(A - B*p)\n    # Find all maxima in S ‚Äì¬†since S is sorted, these are the first n ones\n    indices = [i for (i, v) in enumerate(cost_svd.S) if abs(v - cost_svd.S[1]) < eps()]\n    ind = rand(indices)\n    return -B'*(cost_svd.U[:,ind]*cost_svd.Vt[ind,:]')\nend\nrpb = Manifolds.RiemannianProjectionBackend(Manifolds.ExplicitEmbeddedBackend(M; gradient=‚àÇ‚Çëf))\n‚àÇf(M, p) = Manifolds.gradient(M, f, p, rpb)\ndomf(M, p) = distance(M, p, p0) < diameter/2 ? true : false We introduce some keyword arguments for the solvers we will use in this experiment rcbm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :diameter => diameter,\n    :debug => [\n        :Iteration,\n        (:Cost, \"F(p): %1.16f \"),\n        (:Œæ, \"Œæ: %1.8f \"),\n        (:Œµ, \"Œµ: %1.8f \"),\n        (:last_stepsize, \"step size: %1.8f\"),\n        :WarnBundle,\n        :Stop,\n        10,\n        \"\\n\",\n    ],\n    :domain => domf,\n    :k_max => k_max,\n    :k_min => k_min,\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\nrcbm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :diameter => diameter,\n    :domain => domf,\n    :k_max => k_max,\n    :k_min => k_min,\n]\npba_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :debug =>[\n        :Iteration,\n        :Stop,\n        (:Cost, \"F(p): %1.16f \"),\n        (:ŒΩ, \"ŒΩ: %1.16f \"),\n        (:c, \"c: %1.16f \"),\n        (:Œº, \"Œº: %1.8f \"),\n        :Stop,\n        :WarnBundle,\n        10,\n        \"\\n\",\n    ],\n    :record => [:Iteration, :Cost, :Iterate],\n    :return_state => true,\n]\npba_bm_kwargs = [\n    :cache =>(:LRU, [:Cost, :SubGradient], 50),\n]\nsgm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :debug => [:Iteration, (:Cost, \"F(p): %1.16f \"), :Stop, 1000, \"\\n\"],\n    :record => [:Iteration, :Cost, :p_star],\n    :return_state => true,\n    :stepsize => DecreasingLength(; exponent=1, factor=1, subtrahend=0, length=1, shift=0, type=:absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(‚àötol) | StopAfterIteration(max_iters),\n]\nsgm_bm_kwargs = [\n    :cache => (:LRU, [:Cost, :SubGradient], 50),\n    :stepsize => DecreasingLength(; exponent=1, factor=1, subtrahend=0, length=1, shift=0, type=:absolute),\n    :stopping_criterion => StopWhenSubgradientNormLess(‚àötol) | StopAfterIteration(max_iters),\n]\nglobal header = [\"Algorithm\", \"Iterations\", \"Time (s)\", \"Objective\"] We run the optimization algorithms‚Ä¶ rcbm = convex_bundle_method(M, f, ‚àÇf, p0; rcbm_kwargs...)\nrcbm_result = get_solver_result(rcbm)\nrcbm_record = get_record(rcbm)\n#\npba = proximal_bundle_method(M, f, ‚àÇf, p0; pba_kwargs...)\npba_result = get_solver_result(pba)\npba_record = get_record(pba)\n#\nsgm = subgradient_method(M, f, ‚àÇf, p0; sgm_kwargs...)\nsgm_result = get_solver_result(sgm)\nsgm_record = get_record(sgm) ‚Ä¶ And we benchmark their performance. if benchmarking\n    pba_bm = @benchmark proximal_bundle_method($M, $f, $‚àÇf, $p0; $pba_bm_kwargs...)\n    rcbm_bm = @benchmark convex_bundle_method($M, $f, $‚àÇf, $p0; $rcbm_bm_kwargs...)\n    sgm_bm = @benchmark subgradient_method($M, $f, $‚àÇf, $p0; $sgm_bm_kwargs...)\n    #\n    experiments = [\"RCBM\", \"PBA\", \"SGM\"]\n    records = [rcbm_record, pba_record, sgm_record]\n    results = [rcbm_result, pba_result, sgm_result]\n    times = [\n        median(rcbm_bm).time * 1e-9,\n        median(pba_bm).time * 1e-9,\n        median(sgm_bm).time * 1e-9,\n    ]\n    if show_plot\n        global fig = plot(xscale=:log10)\n    end\n    #\n    global D = cat(\n        experiments,\n        [maximum(first.(record)) for record in records],\n        [t for t in times],\n        [minimum([r[2] for r in record]) for record in records];\n        dims=2,\n    )\n    # \n    \n    #\n    # Finalize - export costs\n    if export_table\n        for (time, record, result, experiment) in zip(times, records, results, experiments)\n            C1 = [0.5 f(M, p0)]\n            C = cat(first.(record), [r[2] for r in record]; dims=2)\n            bm_data = vcat(C1, C)\n            CSV.write(\n                joinpath(results_folder, experiment_name * \"_\" * experiment * \"-result.csv\"),\n                DataFrame(bm_data, :auto);\n                header=[\"i\", \"cost\"],\n            )\n            if show_plot\n                plot!(fig, bm_data[:,1], bm_data[:,2]; label=experiment)\n            end\n        end\n        CSV.write(\n            joinpath(results_folder, experiment_name * \"-comparisons.csv\"),\n            DataFrame(D, :auto);\n            header=header,\n        )\n    end\nend We can take a look at how the algorithms compare to each other in their performance with the following table‚Ä¶ Algorithm Iterations Time (s) Objective RCBM 99 102.036 235.46 PBA 31 5.8049 235.46 SGM 5000 402.739 235.46 ‚Ä¶ and this cost versus iterations plot"},{"id":3695,"pagetitle":"Spectral Procrustes","title":"Technical details","ref":"/manoptexamples/stable/examples/Spectral-Procrustes/#Technical-details","content":" Technical details This tutorial is cached. It was last run on the following package versions. using Pkg\nPkg.status() Status `~/Repositories/Julia/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.5.0\n  [336ed68f] CSV v0.10.15\n  [35d6a980] ColorSchemes v3.27.1\n‚åÖ [5ae59095] Colors v0.12.11\n  [a93c6f00] DataFrames v1.7.0\n  [7073ff75] IJulia v1.26.0\n  [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.1\n  [d3d80556] LineSearches v7.3.0\n  [af67fdf4] ManifoldDiff v0.3.13\n  [1cead3c2] Manifolds v0.10.7\n  [3362f125] ManifoldsBase v0.15.22\n  [0fc0a36d] Manopt v0.5.3 `../../Manopt.jl`\n  [5b8d5e80] ManoptExamples v0.1.10 `..`\n  [51fcb6bd] NamedColors v0.2.2\n  [91a5bcdd] Plots v1.40.9\n‚åÉ [08abe8d2] PrettyTables v2.3.2\n  [6099a3de] PythonCall v0.9.23\n  [f468eda6] QuadraticModels v0.9.7\n  [1e40b3f8] RipQP v0.6.4\nInfo Packages marked with ‚åÉ and ‚åÖ have new versions available. Those with ‚åÉ may be upgradable, but those with ‚åÖ are restricted by compatibility constraints from upgrading. To see why use `status --outdated` using Dates\nnow() 2024-11-29T09:36:53.667"},{"id":3696,"pagetitle":"Spectral Procrustes","title":"Literature","ref":"/manoptexamples/stable/examples/Spectral-Procrustes/#Literature","content":" Literature [BHJ24] R.¬†Bergmann, R.¬†Herzog and H.¬†Jasa.  The Riemannian Convex Bundle Method , preprint (2024),  arXiv:2402.13670 . [HNP23] N.¬†Hoseini Monjezi, S.¬†Nobakhtian and M.¬†R.¬†Pouryayevali.  A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds .  IMA¬†Journal¬†of¬†Numerical¬†Analysis  43 , 293‚Äì325  (2023)."},{"id":3699,"pagetitle":"Total Variation","title":"Total Variation Minimization","ref":"/manoptexamples/stable/examples/Total-Variation/#Total-Variation-Minimization","content":" Total Variation Minimization Ronny Bergmann 2023-06-06"},{"id":3700,"pagetitle":"Total Variation","title":"Introduction","ref":"/manoptexamples/stable/examples/Total-Variation/#Introduction","content":" Introduction Total Variation denoising  is an optimization problem used to denoise signals and images. The corresponding (Euclidean) objective is often called Rudin-Osher-Fatemi (ROF) model based on the paper [ ROF92 ]. This was generalized to manifolds in [ WDS14 ]. In this short example we will look at the ROF model for manifold-valued data, its generalizations, and how they can be solved using  Manopt.jl ."},{"id":3701,"pagetitle":"Total Variation","title":"The manifold-valued ROF model","ref":"/manoptexamples/stable/examples/Total-Variation/#The-manifold-valued-ROF-model","content":" The manifold-valued ROF model Generalizing the ROF model to manifolds can be phrased as follows: Given a (discrete) signal on a manifold  $s = (s_i)_{i=1}^N \\in \\mathbb M^n$  of length  $n \\in \\mathbb N$ , we usually assume that this signal might be noisy. For the (Euclidean) ROF model we assume that the noise is Gaussian. Then variational models for denoising usually consist of a data term  $D(p,s)$  to ‚Äústay close to‚Äù  $s$  and a regularizer  $R(p)$ . For TV regularization the data term is the squared distance and the regularizer models that without noise, neighboring values are close. We obtain \\[\\operatorname*{arg\\,min}_{p\\in\\mathcal M^n}\nf(p),\n\\qquad\nf(p) = D(p,s) + Œ± R(p) = \\sum_{i=1}^n d_{\\mathcal M}^2(s_i,p_i) + Œ±\\sum_{i=1}^{n-1} d_{\\mathcal M}(p_i,p_{i+1}),\\] where  $Œ± > 0$  is a weight parameter. The challenge here is that most classical algorithm, like gradient descent or Quasi Newton, assume the cost  $f(p)$  to be smooth such that the gradient exists at every point. In our setting that is not the case since the distacen is not differentiable for any  $p_i=p_{i+1}$ . So we have to use another technique."},{"id":3702,"pagetitle":"Total Variation","title":"The Cyclic Proximal Point algorithm","ref":"/manoptexamples/stable/examples/Total-Variation/#The-Cyclic-Proximal-Point-algorithm","content":" The Cyclic Proximal Point algorithm If the cost consists of a sum of functions, where each of the proximal maps is ‚Äúeasy to evaluate‚Äù, for best of cases in closed form, we can ‚Äúapply the proximal maps in a cyclic fashion‚Äù and optain the  Cyclic Proximal Point Algorithm  [ Bac14 ]. Both for the distance and the squared distance, we have  generic implementations ; since this happens in a cyclic manner, there is also always one of the arguments involved in the prox and never both. We can improve the performance slightly by computing all proes in parallel that do not interfer. To be precise we can compute first all proxes of distances in the regularizer that start with an odd index in parallel. Afterwards all that start with an even index."},{"id":3703,"pagetitle":"Total Variation","title":"The Optimsation","ref":"/manoptexamples/stable/examples/Total-Variation/#The-Optimsation","content":" The Optimsation using Manifolds, Manopt, ManoptExamples, ManifoldDiff\nusing ManifoldDiff: prox_distance\nusing ManoptExamples: prox_Total_Variation\nn = 500 #Signal length\nœÉ = 0.2 # amount of noise\nŒ± = 0.5# in the TV model We define a few colors using Colors, NamedColors, ColorSchemes, Plots, Random\ndata_color = RGBA{Float64}(colorant\"black\")\nlight_color = RGBA{Float64}(colorant\"brightgrey\")\nrecon_color = RGBA{Float64}(colorant\"vibrantorange\")\nnoisy_color = RGBA{Float64}(colorant\"vibrantteal\") And we generate our data on the  Circle , since that is easy to plot and nice to compare to the Euclidean case of a real-valued signal. Random.seed!(23)\nM = Circle()\nN = PowerManifold(M, n)\ndata = ManoptExamples.artificial_S1_signal(n)\ns = [exp(M, d, rand(M; vector_at=d, œÉ=0.2)) for d in data]\nt = range(0.0, 1.0; length=n)\nscene = scatter(\n    t,\n    data;\n    markercolor=data_color,\n    markerstrokecolor=data_color,\n    markersize=2,\n    lab=\"original\",\n)\nscatter!(\n    scene,\n    t,\n    s;\n    markersize=2,\n    markercolor=noisy_color,\n    markerstrokecolor=noisy_color,\n    lab=\"noisy\",\n)\nyticks!(\n    [-œÄ, -œÄ / 2, 0, œÄ / 2, œÄ],\n    [raw\"$-\\pi$\", raw\"$-\\frac{\\pi}{2}$\", raw\"$0$\", raw\"$\\frac{\\pi}{2}$\", raw\"$\\pi$\"],\n) As mentioned above, total variation now minimized different neighbors ‚Äì¬†while keeping jumps if the are large enough. One notable difference between Euclidean and Cyclic data is, that the y-axis is in our case periodic, hence the first jump is actually not a jump but a ‚Äúlinear increase‚Äù that ‚Äúwraps around‚Äù and the second large jump ‚Äìor third overall‚Äì is actually only as small as the second jump. Defining cost and the proximal maps, which are actually 3 proxes to be precise. f(N, p) = ManoptExamples.L2_Total_Variation(N, s, Œ±, p)\nproxes_f = ((N, Œª, p) -> prox_distance(N, Œª, s, p, 2), (N, Œª, p) -> prox_Total_Variation(N, Œ± * Œª, p)) We run the algorithm o = cyclic_proximal_point(\n    N,\n    f,\n    proxes_f,\n    s;\n    Œª=i -> œÄ / (2 * i),\n    debug=[\n        :Iteration,\n        \" | \",\n        DebugProximalParameter(),\n        \" | \",\n        :Cost,\n        \" | \",\n        :Change,\n        \"\\n\",\n        1000,\n        :Stop,\n    ],\n    record=[:Iteration, :Cost, :Change, :Iterate],\n    return_state=true,\n); Initial  |  | f(x): 59.187445 | \n# 1000   | Œª:0.0015707963267948967 | f(x): 13.963912 | Last Change: 1.773283\n# 2000   | Œª:0.0007853981633974483 | f(x): 13.947124 | Last Change: 0.011678\n# 3000   | Œª:0.0005235987755982988 | f(x): 13.941538 | Last Change: 0.003907\n# 4000   | Œª:0.00039269908169872416 | f(x): 13.938748 | Last Change: 0.001957\n# 5000   | Œª:0.0003141592653589793 | f(x): 13.937075 | Last Change: 0.001175\nAt iteration 5000 the algorithm reached its maximal number of iterations (5000). We can see that the cost reduces nicely. Let‚Äôs extract the result an the recorded values recon = get_solver_result(o)\nrecord = get_record(o) We get scene = scatter(\n    t,\n    data;\n    markercolor=data_color,\n    markerstrokecolor=data_color,\n    markersize=2,\n    lab=\"original\",\n)\nscatter!(\n    scene,\n    t,\n    s;\n    markersize=2,\n    markercolor=light_color,\n    markerstrokecolor=light_color,\n    lab=\"noisy\",\n)\nscatter!(\n    scene,\n    t,\n    recon;\n    markersize=2,\n    markercolor=recon_color,\n    markerstrokecolor=recon_color,\n    lab=\"reconstruction\",\n) Which contains the usual stair casing one expects for TV regularization, but here in a ‚Äúcyclic manner‚Äù"},{"id":3704,"pagetitle":"Total Variation","title":"Outlook","ref":"/manoptexamples/stable/examples/Total-Variation/#Outlook","content":" Outlook We can generalize the total variation also to a second order total variation. Again intuitively, while TV prefers constant areas, the  $\\operatorname{TV}_2$  yields a cost 0 for anything linear, which on manifolds can be generalized to equidistant on a geodesic [ BBSW16 ]. Here we can again derive proximal maps, which for the circle again have a closed form solutoin [ BLSW14 ] but on general manifolds these have again to be approximated. Another extension for both first and second order TV is to apply this for manifold-valued images  $S = (S_{i,j})_{i,j=1}^{m,n} \\in \\mathcal M^{m,n}$ , where the distances in the regularizer are then used in both the first dimension  $i$  and the second dimension  $j$  in the data."},{"id":3705,"pagetitle":"Total Variation","title":"Technical details","ref":"/manoptexamples/stable/examples/Total-Variation/#Technical-details","content":" Technical details This version of the example was generated with the following package versions. Pkg.status() Status `~/work/ManoptExamples.jl/ManoptExamples.jl/examples/Project.toml`\n  [6e4b80f9] BenchmarkTools v1.6.0\n  [336ed68f] CSV v0.10.15\n  [13f3f980] CairoMakie v0.13.4\n  [0ca39b1e] Chairmarks v1.3.1\n  [35d6a980] ColorSchemes v3.29.0\n‚åÖ [5ae59095] Colors v0.12.11\n  [a93c6f00] DataFrames v1.7.0\n  [7073ff75] IJulia v1.27.0\n  [682c06a0] JSON v0.21.4\n  [8ac3fa9e] LRUCache v1.6.2\n  [d3d80556] LineSearches v7.3.0\n  [ee78f7c6] Makie v0.22.4\n  [af67fdf4] ManifoldDiff v0.4.2\n  [1cead3c2] Manifolds v0.10.16\n  [3362f125] ManifoldsBase v1.0.3\n  [0fc0a36d] Manopt v0.5.12\n  [5b8d5e80] ManoptExamples v0.1.14 `..`\n  [51fcb6bd] NamedColors v0.2.3\n  [91a5bcdd] Plots v1.40.12\n  [08abe8d2] PrettyTables v2.4.0\n  [6099a3de] PythonCall v0.9.24\n  [f468eda6] QuadraticModels v0.9.8\n  [1e40b3f8] RipQP v0.6.4\nInfo Packages marked with ‚åÖ have new versions available but compatibility constraints restrict them from upgrading. To see why use `status --outdated`"},{"id":3706,"pagetitle":"Total Variation","title":"Literature","ref":"/manoptexamples/stable/examples/Total-Variation/#Literature","content":" Literature [Bac14] M.¬†Baƒç√°k.  Computing medians and means in Hadamard spaces .  SIAM¬†Journal¬†on¬†Optimization  24 , 1542‚Äì1566  (2014),  arXiv:1210.2145 . [BBSW16] M.¬†Baƒç√°k, R.¬†Bergmann, G.¬†Steidl and A.¬†Weinmann.  A second order non-smooth variational model for restoring manifold-valued images .  SIAM¬†Journal¬†on¬†Scientific¬†Computing  38 , A567‚ÄìA597  (2016),  arXiv:1506.02409 . [BLSW14] R.¬†Bergmann, F.¬†Laus, G.¬†Steidl and A.¬†Weinmann.  Second order differences of cyclic data and applications in variational denoising .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  7 , 2916‚Äì2953  (2014),  arXiv:1405.5349 . [ROF92] L.¬†I.¬†Rudin, S.¬†Osher and E.¬†Fatemi.  Nonlinear total variation based noise removal algorithms .  Physica¬†D:¬†Nonlinear¬†Phenomena  60 , 259‚Äì268  (1992). [WDS14] A.¬†Weinmann, L.¬†Demaret and M.¬†Storath.  Total variation regularization for manifold-valued data .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  7 , 2226‚Äì2257  (2014)."},{"id":3709,"pagetitle":"Error measures","title":"Error measures","ref":"/manoptexamples/stable/helpers/error_measures/#Error-measures","content":" Error measures"},{"id":3710,"pagetitle":"Error measures","title":"ManoptExamples.mean_average_error","ref":"/manoptexamples/stable/helpers/error_measures/#ManoptExamples.mean_average_error-Tuple{ManifoldsBase.AbstractManifold, Any, Any}","content":" ManoptExamples.mean_average_error  ‚Äî  Method mean_average_error(M,x,y) Compute the (mean) squared error between the two points  x  and  y  on the  PowerManifold  manifold  M . source"},{"id":3711,"pagetitle":"Error measures","title":"ManoptExamples.mean_squared_error","ref":"/manoptexamples/stable/helpers/error_measures/#ManoptExamples.mean_squared_error-Union{Tuple{mT}, Tuple{mT, Any, Any}} where mT<:ManifoldsBase.AbstractManifold","content":" ManoptExamples.mean_squared_error  ‚Äî  Method mean_squared_error(M, p, q) Compute the (mean) squared error between the two points  p  and  q  on the (power) manifold  M . source"},{"id":3714,"pagetitle":"Objectives","title":"List of Objectives defined for the Examples","ref":"/manoptexamples/stable/objectives/#List-of-Objectives-defined-for-the-Examples","content":" List of Objectives defined for the Examples"},{"id":3715,"pagetitle":"Objectives","title":"Rayleigh Quotient on the Sphere","ref":"/manoptexamples/stable/objectives/#Rayleigh","content":" Rayleigh Quotient on the Sphere See the Rayleigh example (TODO) to see these in use."},{"id":3716,"pagetitle":"Objectives","title":"ManoptExamples.RayleighQuotientCost","ref":"/manoptexamples/stable/objectives/#ManoptExamples.RayleighQuotientCost","content":" ManoptExamples.RayleighQuotientCost  ‚Äî  Type RayleighQuotientCost A functor representing the Rayleigh Quotient cost function. Let  $A ‚àà ‚Ñù^{n√ón}$  be a symmetric matrix. Then we can specify the  Rayleigh Quotient  in two forms. Either \\[f(p) = p^{\\mathrm{T}}Ap,\\qquad p ‚àà ùïä^{n-1},\\] or extended into the embedding as \\[f(x) = x^{\\mathrm{T}}Ax, \\qquad x ‚àà ‚Ñù^n,\\] which is not the orignal Rayleigh quotient for performance reasons, but useful if you want to use this as the Euclidean cost in the emedding of  $ùïä^{n-1}$ . Fields A  ‚Äì storing the matrix internally Constructor RayleighQuotientCost(A) Create the Rayleigh cost function. See also RayleighQuotientGrad!! ,  RayleighQuotientHess!! source"},{"id":3717,"pagetitle":"Objectives","title":"ManoptExamples.RayleighQuotientGrad!!","ref":"/manoptexamples/stable/objectives/#ManoptExamples.RayleighQuotientGrad!!","content":" ManoptExamples.RayleighQuotientGrad!!  ‚Äî  Type RayleighQuotientGrad!! A functor representing the Rayleigh Quotient gradient function. Let  $A ‚àà ‚Ñù^{n√ón}$  be a symmetric matrix. Then we can specify the gradient of the  Rayleigh Quotient  in two forms. Either \\[\\operatorname{grad} f(p) = 2 Ap - 2 (p^{\\mathrm{T}}Ap)*p,\\qquad p ‚àà ùïä^{n-1},\\] or taking the Euclidean gradient of the Rayleigh quotient on the sphere as \\[‚àáf(x) = 2Ax, \\qquad x ‚àà ‚Ñù^n.\\] For details, see Example 3.62 of [ Bou23 ]. Fields A  ‚Äì storing the matrix internally Constructor RayleighQuotientGrad!!(A) Create the Rayleigh quotient gradient function. See also RayleighQuotientCost ,  RayleighQuotientHess!! source"},{"id":3718,"pagetitle":"Objectives","title":"ManoptExamples.RayleighQuotientHess!!","ref":"/manoptexamples/stable/objectives/#ManoptExamples.RayleighQuotientHess!!","content":" ManoptExamples.RayleighQuotientHess!!  ‚Äî  Type RayleighQuotientHess!! A functor representing the Rayleigh Quotient Hessian. Let  $A ‚àà ‚Ñù^{n√ón}$  be a symmetric matrix. Then we can specify the Hessian of the  Rayleigh Quotient  in two forms. Either \\[\\operatorname{Hess} f(p)[X] = 2 \\bigl(AX - (p^{mathrm{T}}AX)p - (p^{\\mathrm{T}}Ap)X\\bigr),\\qquad p ‚àà ùïä^{n-1}, X \\in T_pùïä^{n-1}\\] or taking the Euclidean Hessian of the Rayleigh quotient on the sphere as \\[‚àá^2f(x)[V] = 2AV, \\qquad x, V ‚àà ‚Ñù^n.\\] For details, see Example 5.27 of [ Bou23 ]. Fields A  ‚Äì storing the matrix internally Constructor RayleighQuotientHess!!(A) Create the Rayleigh quotient Hessian function. See also RayleighQuotientCost ,  RayleighQuotientGrad!! source"},{"id":3719,"pagetitle":"Objectives","title":"B√©zier Curves","ref":"/manoptexamples/stable/objectives/#BezierCurves","content":" B√©zier Curves See the  Bezier Curves example  to see these in use."},{"id":3720,"pagetitle":"Objectives","title":"ManoptExamples.BezierSegment","ref":"/manoptexamples/stable/objectives/#ManoptExamples.BezierSegment","content":" ManoptExamples.BezierSegment  ‚Äî  Type BezierSegment A type to capture a Bezier segment. With  $n$  points, a B√©zier segment of degree  $n-1$  is stored. On the Euclidean manifold, this yields a polynomial of degree  $n-1$ . This type is mainly used to encapsulate the points within a composite Bezier curve, which consist of an  AbstractVector  of  BezierSegments  where each of the points might be a nested array on a  PowerManifold  already. Not that this can also be used to represent tangent vectors on the control points of a segment. See also:  de_Casteljau . Constructor BezierSegment(pts::AbstractVector) Given an abstract vector of  pts  generate the corresponding B√©zier segment. source"},{"id":3721,"pagetitle":"Objectives","title":"ManoptExamples.L2_acceleration_Bezier","ref":"/manoptexamples/stable/objectives/#ManoptExamples.L2_acceleration_Bezier-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, AbstractVector{P}, AbstractVector{<:Integer}, AbstractVector{<:AbstractFloat}, AbstractFloat, AbstractVector{P}}} where P","content":" ManoptExamples.L2_acceleration_Bezier  ‚Äî  Method L2_acceleration_Bezier(M,B,pts,Œª,d) compute the value of the discrete Acceleration of the composite Bezier curve together with a data term, i.e. \\[\\frac{Œª}{2}\\sum_{i=0}^{N} d_{\\mathcal M}(d_i, c_B(i))^2+\n\\sum_{i=1}^{N-1}\\frac{d^2_2 [ B(t_{i-1}), B(t_{i}), B(t_{i+1})]}{\\Delta_t^3}\\] where for this formula the  pts  along the curve are equispaced and denoted by  $t_i$  and  $d_2$  refers to the second order absolute difference  second_order_Total_Variation  (squared), the junction points are denoted by  $p_i$ , and to each  $p_i$  corresponds one data item in the manifold points given in  d . For details on the acceleration approximation, see  acceleration_Bezier . Note that the B√©zier-curve is given in reduces form as a point on a  PowerManifold , together with the  degrees  of the segments and assuming a differentiable curve, the segments can internally be reconstructed. See also grad_L2_acceleration_Bezier ,  acceleration_Bezier ,  grad_acceleration_Bezier source"},{"id":3722,"pagetitle":"Objectives","title":"ManoptExamples.acceleration_Bezier","ref":"/manoptexamples/stable/objectives/#ManoptExamples.acceleration_Bezier-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, AbstractVector{P}, AbstractVector{<:Integer}, AbstractVector{<:AbstractFloat}}} where P","content":" ManoptExamples.acceleration_Bezier  ‚Äî  Method acceleration_Bezier(\n    M::AbstractManifold,\n    B::AbstractVector{P},\n    degrees::AbstractVector{<:Integer},\n    T::AbstractVector{<:AbstractFloat},\n) where {P} compute the value of the discrete Acceleration of the composite Bezier curve \\[\\sum_{i=1}^{N-1}\\frac{d^2_2 [ B(t_{i-1}), B(t_{i}), B(t_{i+1})]}{\\Delta_t^3}\\] where for this formula the  pts  along the curve are equispaced and denoted by  $t_i$ ,  $i=1,‚Ä¶,N$ , and  $d_2$  refers to the second order absolute difference  second_order_Total_Variation  (squared). Note that the B√©zier-curve is given in reduces form as a point on a  PowerManifold , together with the  degrees  of the segments and assuming a differentiable curve, the segments can internally be reconstructed. This acceleration discretization was introduced in  Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018 . See also grad_acceleration_Bezier ,  L2_acceleration_Bezier ,  grad_L2_acceleration_Bezier source"},{"id":3723,"pagetitle":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","ref":"/manoptexamples/stable/objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, AbstractVector, AbstractVector}","content":" ManoptExamples.adjoint_differential_Bezier_control_points  ‚Äî  Method adjoint_differential_Bezier_control_points(\n    M::AbstractManifold,\n    T::AbstractVector,\n    X::AbstractVector,\n)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment},\n    T::AbstractVector,\n    X::AbstractVector,\n) Evaluate the adjoint of the differential with respect to the controlpoints at several times  T . This can be computed in place of  Y . See  de_Casteljau  for more details on the curve. source"},{"id":3724,"pagetitle":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","ref":"/manoptexamples/stable/objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, Any, Any}","content":" ManoptExamples.adjoint_differential_Bezier_control_points  ‚Äî  Method adjoint_differential_Bezier_control_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X\n)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment},\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X\n) evaluate the adjoint of the differential of a composite B√©zier curve on the manifold  M  with respect to its control points  b  based on a points  T $=(t_i)_{i=1}^n$  that are pointwise in  $t_i‚àà[0,1]$  on the curve and given corresponding tangential vectors  $X = (Œ∑_i)_{i=1}^n$ ,  $Œ∑_i‚ààT_{Œ≤(t_i)}\\mathcal M$  This can be computed in place of  Y . See  de_Casteljau  for more details on the curve. source"},{"id":3725,"pagetitle":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","ref":"/manoptexamples/stable/objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, AbstractVector, AbstractVector}","content":" ManoptExamples.adjoint_differential_Bezier_control_points  ‚Äî  Method adjoint_differential_Bezier_control_points(\n    M::AbstractManifold,\n    b::BezierSegment,\n    t::AbstractVector,\n    X::AbstractVector,\n)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::BezierSegment,\n    b::BezierSegment,\n    t::AbstractVector,\n    X::AbstractVector,\n) evaluate the adjoint of the differential of a B√©zier curve on the manifold  M  with respect to its control points  b  based on a points  T $=(t_i)_{i=1}^n$  that are pointwise in  $t_i‚àà[0,1]$  on the curve and given corresponding tangential vectors  $X = (Œ∑_i)_{i=1}^n$ ,  $Œ∑_i‚ààT_{Œ≤(t_i)}\\mathcal M$  This can be computed in place of  Y . See  de_Casteljau  for more details on the curve and  Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018 source"},{"id":3726,"pagetitle":"Objectives","title":"ManoptExamples.adjoint_differential_Bezier_control_points","ref":"/manoptexamples/stable/objectives/#ManoptExamples.adjoint_differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, Any, Any}","content":" ManoptExamples.adjoint_differential_Bezier_control_points  ‚Äî  Method adjoint_differential_Bezier_control_points(M::AbstractManifold, b::BezierSegment, t, Œ∑)\nadjoint_differential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::BezierSegment,\n    b::BezierSegment,\n    t,\n    Œ∑,\n) evaluate the adjoint of the differential of a B√©zier curve on the manifold  M  with respect to its control points  b  based on a point  t $‚àà[0,1]$  on the curve and a tangent vector  $Œ∑‚ààT_{Œ≤(t)}\\mathcal M$ . This can be computed in place of  Y . See  de_Casteljau  for more details on the curve. source"},{"id":3727,"pagetitle":"Objectives","title":"ManoptExamples.de_Casteljau","ref":"/manoptexamples/stable/objectives/#ManoptExamples.de_Casteljau-Tuple{ManifoldsBase.AbstractManifold, Vararg{Any}}","content":" ManoptExamples.de_Casteljau  ‚Äî  Method de_Casteljau(M::AbstractManifold, b::BezierSegment NTuple{N,P}) -> Function return the  B√©zier curve $Œ≤(‚ãÖ;b_0,‚Ä¶,b_n): [0,1] ‚Üí \\mathcal M$  defined by the control points  $b_0,‚Ä¶,b_n‚àà\\mathcal M$ ,  $n‚àà\\mathbb N$ , as a  BezierSegment . This function implements de Casteljau's algorithm  Casteljau, 1959 ,  Casteljau, 1963  generalized to manifolds by  Popiel, Noakes, J Approx Theo, 2007 : Let  $Œ≥_{a,b}(t)$  denote the shortest geodesic connecting  $a,b‚àà\\mathcal M$ . Then the curve is defined by the recursion \\[\\begin{aligned}\n    Œ≤(t;b_0,b_1) &= \\gamma_{b_0,b_1}(t)\\\\\n    Œ≤(t;b_0,‚Ä¶,b_n) &= \\gamma_{Œ≤(t;b_0,‚Ä¶,b_{n-1}), Œ≤(t;b_1,‚Ä¶,b_n)}(t),\n\\end{aligned}\\] and  P  is the type of a point on the  Manifold M . de_Casteljau(M::AbstractManifold, B::AbstractVector{<:BezierSegment}) -> Function Given a vector of B√©zier segments, i.e. a vector of control points  $B=\\bigl( (b_{0,0},‚Ä¶,b_{n_0,0}),‚Ä¶,(b_{0,m},‚Ä¶ b_{n_m,m}) \\bigr)$ , where the different segments might be of different degree(s)  $n_0,‚Ä¶,n_m$ . The resulting composite B√©zier curve  $c_B:[0,m] ‚Üí \\mathcal M$  consists of  $m$  segments which are B√©zier curves. \\[c_B(t) :=\n    \\begin{cases}\n        Œ≤(t; b_{0,0},‚Ä¶,b_{n_0,0}) & \\text{ if } t ‚àà[0,1]\\\\\n        Œ≤(t-i; b_{0,i},‚Ä¶,b_{n_i,i}) & \\text{ if }\n            t‚àà(i,i+1], \\quad i‚àà\\{1,‚Ä¶,m-1\\}.\n    \\end{cases}\\] de_Casteljau(M::AbstractManifold, b::BezierSegment, t::Real)\nde_Casteljau(M::AbstractManifold, B::AbstractVector{<:BezierSegment}, t::Real)\nde_Casteljau(M::AbstractManifold, b::BezierSegment, T::AbstractVector) -> AbstractVector\nde_Casteljau(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n) -> AbstractVector Evaluate the B√©zier curve at time  t  or at times  t  in  T . source"},{"id":3728,"pagetitle":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","ref":"/manoptexamples/stable/objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, AbstractVector, AbstractVector{<:ManoptExamples.BezierSegment}}","content":" ManoptExamples.differential_Bezier_control_points  ‚Äî  Method differential_Bezier_control_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n    Œû::AbstractVector{<:BezierSegment}\n)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Œò::AbstractVector{<:BezierSegment}\n    B::AbstractVector{<:BezierSegment},\n    T::AbstractVector\n    Œû::AbstractVector{<:BezierSegment}\n) evaluate the differential of the composite B√©zier curve with respect to its control points  B  and tangent vectors  Œû  in the tangent spaces of the control points. The result is the ‚Äúchange‚Äù of the curve at the points in  T , which are elementwise in  $[0,N]$ , and each depending the corresponding segment(s). Here,  $N$  is the length of  B . For the mutating variant the result is computed in  Œò . See  de_Casteljau  for more details on the curve and  Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018 . source"},{"id":3729,"pagetitle":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","ref":"/manoptexamples/stable/objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}, Any, AbstractVector{<:ManoptExamples.BezierSegment}}","content":" ManoptExamples.differential_Bezier_control_points  ‚Äî  Method differential_Bezier_control_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X::AbstractVector{<:BezierSegment}\n)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y::AbstractVector{<:BezierSegment}\n    B::AbstractVector{<:BezierSegment},\n    t,\n    X::AbstractVector{<:BezierSegment}\n) evaluate the differential of the composite B√©zier curve with respect to its control points  B  and tangent vectors  Œû  in the tangent spaces of the control points. The result is the ‚Äúchange‚Äù of the curve at  t $‚àà[0,N]$ , which depends only on the corresponding segment. Here,  $N$  is the length of  B . The computation can be done in place of  Y . See  de_Casteljau  for more details on the curve. source"},{"id":3730,"pagetitle":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","ref":"/manoptexamples/stable/objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, AbstractVector, ManoptExamples.BezierSegment}","content":" ManoptExamples.differential_Bezier_control_points  ‚Äî  Method differential_Bezier_control_points(\n    M::AbstractManifold,\n    b::BezierSegment,\n    T::AbstractVector,\n    X::BezierSegment,\n)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y,\n    b::BezierSegment,\n    T::AbstractVector,\n    X::BezierSegment,\n) evaluate the differential of the B√©zier curve with respect to its control points  b  and tangent vectors  X  in the tangent spaces of the control points. The result is the ‚Äúchange‚Äù of the curve at the points  T , elementwise in  $t‚àà[0,1]$ . The computation can be done in place of  Y . See  de_Casteljau  for more details on the curve. source"},{"id":3731,"pagetitle":"Objectives","title":"ManoptExamples.differential_Bezier_control_points","ref":"/manoptexamples/stable/objectives/#ManoptExamples.differential_Bezier_control_points-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment, Any, ManoptExamples.BezierSegment}","content":" ManoptExamples.differential_Bezier_control_points  ‚Äî  Method differential_Bezier_control_points(M::AbstractManifold, b::BezierSegment, t::Float, X::BezierSegment)\ndifferential_Bezier_control_points!(\n    M::AbstractManifold,\n    Y,\n    b::BezierSegment,\n    t,\n    X::BezierSegment\n) evaluate the differential of the B√©zier curve with respect to its control points  b  and tangent vectors  X  given in the tangent spaces of the control points. The result is the ‚Äúchange‚Äù of the curve at  t $‚àà[0,1]$ . The computation can be done in place of  Y . See  de_Casteljau  for more details on the curve. source"},{"id":3732,"pagetitle":"Objectives","title":"ManoptExamples.get_Bezier_degree","ref":"/manoptexamples/stable/objectives/#ManoptExamples.get_Bezier_degree-Tuple{ManifoldsBase.AbstractManifold, ManoptExamples.BezierSegment}","content":" ManoptExamples.get_Bezier_degree  ‚Äî  Method get_Bezier_degree(M::AbstractManifold, b::BezierSegment) return the degree of the B√©zier curve represented by the tuple  b  of control points on the manifold  M , i.e. the number of points minus 1. source"},{"id":3733,"pagetitle":"Objectives","title":"ManoptExamples.get_Bezier_degrees","ref":"/manoptexamples/stable/objectives/#ManoptExamples.get_Bezier_degrees-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}}","content":" ManoptExamples.get_Bezier_degrees  ‚Äî  Method get_Bezier_degrees(M::AbstractManifold, B::AbstractVector{<:BezierSegment}) return the degrees of the components of a composite B√©zier curve represented by tuples in  B  containing points on the manifold  M . source"},{"id":3734,"pagetitle":"Objectives","title":"ManoptExamples.get_Bezier_inner_points","ref":"/manoptexamples/stable/objectives/#ManoptExamples.get_Bezier_inner_points-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}}","content":" ManoptExamples.get_Bezier_inner_points  ‚Äî  Method get_Bezier_inner_points(M::AbstractManifold, B::AbstractVector{<:BezierSegment} )\nget_Bezier_inner_points(M::AbstractManifold, b::BezierSegment) returns the inner (i.e. despite start and end) points of the segments of the composite B√©zier curve specified by the control points  B . For a single segment  b , its inner points are returned source"},{"id":3735,"pagetitle":"Objectives","title":"ManoptExamples.get_Bezier_junction_tangent_vectors","ref":"/manoptexamples/stable/objectives/#ManoptExamples.get_Bezier_junction_tangent_vectors-Tuple{ManifoldsBase.AbstractManifold, AbstractVector{<:ManoptExamples.BezierSegment}}","content":" ManoptExamples.get_Bezier_junction_tangent_vectors  ‚Äî  Method get_Bezier_junction_tangent_vectors(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\nget_Bezier_junction_tangent_vectors(M::AbstractManifold, b::BezierSegment) returns the tangent vectors at start and end points of the composite B√©zier curve pointing from a junction point to the first and last inner control points for each segment of the composite Bezier curve specified by the control points  B , either a vector of segments of controlpoints. source"},{"id":3736,"pagetitle":"Objectives","title":"ManoptExamples.get_Bezier_junctions","ref":"/manoptexamples/stable/objectives/#ManoptExamples.get_Bezier_junctions","content":" ManoptExamples.get_Bezier_junctions  ‚Äî  Function get_Bezier_junctions(M::AbstractManifold, B::AbstractVector{<:BezierSegment})\nget_Bezier_junctions(M::AbstractManifold, b::BezierSegment) returns the start and end point(s) of the segments of the composite B√©zier curve specified by the control points  B . For just one segment  b , its start and end points are returned. source"},{"id":3737,"pagetitle":"Objectives","title":"ManoptExamples.get_Bezier_points","ref":"/manoptexamples/stable/objectives/#ManoptExamples.get_Bezier_points","content":" ManoptExamples.get_Bezier_points  ‚Äî  Function get_Bezier_points(\n    M::AbstractManifold,\n    B::AbstractVector{<:BezierSegment},\n    reduce::Symbol=:default\n)\nget_Bezier_points(M::AbstractManifold, b::BezierSegment, reduce::Symbol=:default) returns the control points of the segments of the composite B√©zier curve specified by the control points  B , either a vector of segments of controlpoints or a. This method reduces the points depending on the optional  reduce  symbol :default :        no reduction is performed :continuous :     for a continuous function, the junction points are doubled at  $b_{0,i}=b_{n_{i-1},i-1}$ , so only  $b_{0,i}$  is in the vector. :differentiable : for a differentiable function additionally  $\\log_{b_{0,i}}b_{1,i} = -\\log_{b_{n_{i-1},i-1}}b_{n_{i-1}-1,i-1}$  holds. hence  $b_{n_{i-1}-1,i-1}$  is omitted. If only one segment is given, all points of  b ,  b.pts , is returned. source"},{"id":3738,"pagetitle":"Objectives","title":"ManoptExamples.get_Bezier_segments","ref":"/manoptexamples/stable/objectives/#ManoptExamples.get_Bezier_segments-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, Vector{P}, Any}, Tuple{ManifoldsBase.AbstractManifold, Vector{P}, Any, Symbol}} where P","content":" ManoptExamples.get_Bezier_segments  ‚Äî  Method get_Bezier_segments(M::AbstractManifold, c::AbstractArray{P}, d[, s::Symbol=:default]) returns the array of  BezierSegment s  B  of a composite B√©zier curve reconstructed from an array  c  of points on the manifold  M  and an array of degrees  d . There are a few (reduced) representations that can get extended; see also  get_Bezier_points . For ease of the following, let  $c=(c_1,‚Ä¶,c_k)$  and  $d=(d_1,‚Ä¶,d_m)$ , where  $m$  denotes the number of components the composite B√©zier curve consists of. Then :default :  $k = m + \\sum_{i=1}^m d_i$  since each component requires one point more than its degree. The points are then ordered in tuples, i.e. \\[B = \\bigl[ [c_1,‚Ä¶,c_{d_1+1}], (c_{d_1+2},‚Ä¶,c_{d_1+d_2+2}],‚Ä¶, [c_{k-m+1+d_m},‚Ä¶,c_{k}] \\bigr]\\] :continuous :  $k = 1+ \\sum_{i=1}{m} d_i$ , since for a continuous curve start and end point of successive components are the same, so the very first start point and the end points are stored. \\[B = \\bigl[ [c_1,‚Ä¶,c_{d_1+1}], [c_{d_1+1},‚Ä¶,c_{d_1+d_2+1}],‚Ä¶, [c_{k-1+d_m},‚Ä¶,b_{k}) \\bigr]\\] :differentiable  ‚Äì for a differentiable function additionally to the last explanation, also the second point of any segment was not stored except for the first segment. Hence  $k = 2 - m + \\sum_{i=1}{m} d_i$  and at a junction point  $b_n$  with its given prior point  $c_{n-1}$ , i.e. this is the last inner point of a segment, the first inner point in the next segment the junction is computed as  $b = \\exp_{c_n}(-\\log_{c_n} c_{n-1})$  such that the assumed differentiability holds source"},{"id":3739,"pagetitle":"Objectives","title":"ManoptExamples.grad_L2_acceleration_Bezier","ref":"/manoptexamples/stable/objectives/#ManoptExamples.grad_L2_acceleration_Bezier-Union{Tuple{P}, Tuple{ManifoldsBase.AbstractManifold, AbstractVector{P}, AbstractVector{<:Integer}, AbstractVector, Any, AbstractVector{P}}} where P","content":" ManoptExamples.grad_L2_acceleration_Bezier  ‚Äî  Method grad_L2_acceleration_Bezier(\n    M::AbstractManifold,\n    B::AbstractVector{P},\n    degrees::AbstractVector{<:Integer},\n    T::AbstractVector,\n    Œª,\n    d::AbstractVector{P}\n) where {P} compute the gradient of the discretized acceleration of a composite B√©zier curve on the  Manifold M  with respect to its control points  B  together with a data term that relates the junction points  p_i  to the data  d  with a weight  $Œª$  compared to the acceleration. The curve is evaluated at the points given in  pts  (elementwise in  $[0,N]$ ), where  $N$  is the number of segments of the B√©zier curve. The summands are  grad_distance  for the data term and  grad_acceleration_Bezier  for the acceleration with interpolation constrains. Here the  get_Bezier_junctions  are included in the optimization, i.e. setting  $Œª=0$  yields the unconstrained acceleration minimization. Note that this is ill-posed, since any B√©zier curve identical to a geodesic is a minimizer. Note that the B√©zier-curve is given in reduces form as a point on a  PowerManifold , together with the  degrees  of the segments and assuming a differentiable curve, the segments can internally be reconstructed. See also grad_acceleration_Bezier ,  L2_acceleration_Bezier ,  acceleration_Bezier . source"},{"id":3740,"pagetitle":"Objectives","title":"ManoptExamples.grad_acceleration_Bezier","ref":"/manoptexamples/stable/objectives/#ManoptExamples.grad_acceleration_Bezier-Tuple{ManifoldsBase.AbstractManifold, AbstractVector, AbstractVector{<:Integer}, AbstractVector}","content":" ManoptExamples.grad_acceleration_Bezier  ‚Äî  Method grad_acceleration_Bezier(\n    M::AbstractManifold,\n    B::AbstractVector,\n    degrees::AbstractVector{<:Integer}\n    T::AbstractVector\n) compute the gradient of the discretized acceleration of a (composite) B√©zier curve  $c_B(t)$  on the  Manifold M  with respect to its control points  B  given as a point on the  PowerManifold  assuming C1 conditions and known  degrees . The curve is evaluated at the points given in  T  (elementwise in  $[0,N]$ , where  $N$  is the number of segments of the B√©zier curve). The  get_Bezier_junctions  are fixed for this gradient (interpolation constraint). For the unconstrained gradient, see  grad_L2_acceleration_Bezier  and set  $Œª=0$  therein. This gradient is computed using  adjoint_Jacobi_field s. For details, see  Bergmann, Gousenbourger, Front. Appl. Math. Stat., 2018 . See  de_Casteljau  for more details on the curve. See also acceleration_Bezier ,   grad_L2_acceleration_Bezier ,  L2_acceleration_Bezier . source"},{"id":3741,"pagetitle":"Objectives","title":"Riemannian Mean","ref":"/manoptexamples/stable/objectives/#RiemannianMean","content":" Riemannian Mean See the  Riemannian mean example  to see these in use."},{"id":3742,"pagetitle":"Objectives","title":"ManoptExamples.RiemannianMeanCost","ref":"/manoptexamples/stable/objectives/#ManoptExamples.RiemannianMeanCost","content":" ManoptExamples.RiemannianMeanCost  ‚Äî  Type RiemannianMeanCost{P} A functor representing the Riemannian center of mass (or Riemannian mean) cost function. For a given set of points  $d_1,\\ldots,d_N$  this cost function is defined as \\[f(p) = \\sum_{j=i}^N d_{mathcal M}^2(d_i, p),\\] where  $d_{\\mathcal M}$  is the  distance  on a Riemannian manifold. Constructor RiemannianMeanCost(M::AbstractManifold, data::AbstractVector{<:P}) where {P} Initialize the cost function to a data set  data  of points on a manfiold  M , where each point is of type  P . See also RiemannianMeanGradient!! ,  Riemannian_mean_objective source"},{"id":3743,"pagetitle":"Objectives","title":"ManoptExamples.RiemannianMeanGradient!!","ref":"/manoptexamples/stable/objectives/#ManoptExamples.RiemannianMeanGradient!!","content":" ManoptExamples.RiemannianMeanGradient!!  ‚Äî  Type RiemannianMeanGradient!!{P} where P A functor representing the Riemannian center of mass (or Riemannian mean) cost function. For a given set of points  $d_1,\\ldots,d_N$  this cost function is defined as \\[\\operatorname{grad}f(p) = \\sum_{j=i}^N \\log_p d_i\\] where  $d_{\\mathcal M}$  is the  distance  on a Riemannian manifold and we employ  grad_distance  to compute the single summands. This functor provides both the allocating variant  grad_f(M,p)  as well as the in-place variant  grad_f!(M, X, p)  which computes the gradient in-place of  X . Constructors RiemannianMeanGradient!!(data::AbstractVector{P}, initial_vector::T=nothing) where {P,T} Generate the Riemannian mean gradient based on some data points  data  an intial tangent vector  initial_vector . If you do not provide an initial tangent vector to allocate the intermediate storage of a tangent vector, you can only use the allocating variant. RiemannianMeanGradient!!(\n    M::AbstractManifold,\n    data::AbstractVector{P};\n    initial_vector::T=zero_vector(M, first(data)),\n) where {P,T} Initialize the Riemannian mean gradient, where the internal storage for tangent vectors can be created automatically, since the Riemannian manifold  M  is provideed. See also RiemannianMeanCost ,  Riemannian_mean_objective source"},{"id":3744,"pagetitle":"Objectives","title":"ManoptExamples.Riemannian_mean_objective","ref":"/manoptexamples/stable/objectives/#ManoptExamples.Riemannian_mean_objective","content":" ManoptExamples.Riemannian_mean_objective  ‚Äî  Function Riemannian_mean_objective(data, initial_vector=nothing, evaluation=Manopt.AllocatingEvaluation())\nRiemannian_mean_objective(M, data;\ninitial_vector=zero_vector(M, first(data)),\nevaluation=AllocatingEvaluton()\n) Generate the objective for the Riemannian mean task for some given vector of  data  points on the Riemannian manifold  M . See also RiemannianMeanCost ,  RiemannianMeanGradient!! Note The first constructor should only be used if an additional storage of the vector is not feasible, since initialising the  initial_vector  to  nothing  disables the in-place variant. Hence the evaluation is a positional argument, since it only can be changed, if a vector is provided. Note The objective is available when  Manopt.jl  is loaded. source"},{"id":3745,"pagetitle":"Objectives","title":"Robust PCA","ref":"/manoptexamples/stable/objectives/#RobustPCA","content":" Robust PCA See the  Robust PCA example  to see these in use."},{"id":3746,"pagetitle":"Objectives","title":"ManoptExamples.RobustPCACost","ref":"/manoptexamples/stable/objectives/#ManoptExamples.RobustPCACost","content":" ManoptExamples.RobustPCACost  ‚Äî  Type RobustPCACost{D,F} A functor representing the Riemannian robust PCA function on the  Grassmann  manifold. For some given (column) data  $D‚àà\\mathbb R^{d\\times n}$  the cost function is defined on some  $\\operatorname{Gr}(d,m)$ ,  $m<n$  as the sum of the distances of the columns  $D_i$  to the subspace spanned by  $p\\in\\operatorname{Gr}(d,m)$  (represented as a point on the Stiefel manifold). The function reads \\[f(U) = \\frac{1}{n}\\sum_{i=1}^n \\lVert pp^{\\mathrm{T}}D_i - D_i\\rVert\\] This cost additionally provides a  Huber regularisation  of the cost, that is for some  $Œµ>0$  one use  $‚Ñì_Œµ(x) = \\sqrt{x^2+Œµ^2} - Œµ$  in \\[f_{Œµ}(p) = \\frac{1}{n}\\sum_{i=1}^n ‚Ñì_Œµ\\bigl(\\lVert pp^{\\mathrm{T}}D_i - D_i\\rVert\\bigr)\\] Note that this is a mutable struct so you can adapt the  $Œµ$  later on. Constructor RobustPCACost(data::AbstractMatrix, Œµ=1.0)\nRobustPCACost(M::Grassmann, data::AbstractMatrix, Œµ=1.0) Initialize the robust PCA cost to some  data $D$ , and some regularization  $Œµ$ . The manifold is optional to comply with all examples, but it is not needed here to construct the cost. source"},{"id":3747,"pagetitle":"Objectives","title":"ManoptExamples.RobustPCAGrad!!","ref":"/manoptexamples/stable/objectives/#ManoptExamples.RobustPCAGrad!!","content":" ManoptExamples.RobustPCAGrad!!  ‚Äî  Type RobustPCAGrad!!{D,F} A functor representing the Riemannian robust PCA gradient on the  Grassmann  manifold. For some given (column) data  $X‚àà\\mathbb R^{p\\times n}$  the gradient of the  RobustPCACost  can be computed by projecting the Euclidean gradient onto the corresponding tangent space. Note that this is a mutable struct so you can adapt the  $Œµ$  later on. Constructor RobustPCAGrad!!(data, Œµ=1.0)\nRobustPCAGrad!!(M::Grassmannian{d,m}, data, Œµ=1.0; evaluation=AllocatingEvaluation()) Initialize the robust PCA cost to some  data $D$ , and some regularization  $Œµ$ . The manifold is optional to comply with all examples, but it is not needed here to construct the cost. Also the  evaluation=  keyword is present only for unification of the interfaces. Indeed, independent of that keyword the functor always works in both variants. source"},{"id":3748,"pagetitle":"Objectives","title":"ManoptExamples.robust_PCA_objective","ref":"/manoptexamples/stable/objectives/#ManoptExamples.robust_PCA_objective","content":" ManoptExamples.robust_PCA_objective  ‚Äî  Function robust_PCA_objective(data::AbstractMatrix, Œµ=1.0; evaluation=AllocatingEvaluation())\nrobust_PCA_objective(M, data::AbstractMatrix, Œµ=1.0; evaluation=AllocatingEvaluton()) Generate the objective for the robust PCA task for some given  data $D$  and Huber regularization parameter  $Œµ$ . See also RobustPCACost ,  RobustPCAGrad!! Note Since the construction is independent of the manifold, that argument is optional and mainly provided to comply with other objectives. Similarly, independent of the  evaluation , indeed the gradient always allows for both the allocating and the in-place variant to be used, though that keyword is used to setup the objective. Note The objective is available when  Manopt.jl  is loaded. source"},{"id":3749,"pagetitle":"Objectives","title":"Rosenbrock Function","ref":"/manoptexamples/stable/objectives/#Rosenbrock","content":" Rosenbrock Function See the  Rosenbrock example   and  The Difference of Convex Rosenbrock Example  to see these in use."},{"id":3750,"pagetitle":"Objectives","title":"ManoptExamples.RosenbrockCost","ref":"/manoptexamples/stable/objectives/#ManoptExamples.RosenbrockCost","content":" ManoptExamples.RosenbrockCost  ‚Äî  Type RosenbrockCost Provide the Rosenbrock function in 2D, i.e. for some  $a,b ‚àà ‚Ñù$ \\[f(\\mathcal M, p) = a(p_1^2-p_2)^2 + (p_1-b)^2\\] which means that for the 2D case, the manifold  $\\mathcal M$  is ignored. See also  üìñ Rosenbrock  (with slightly different parameter naming). Constructor f = Rosenbrock(a,b) generates the struct/function of the Rosenbrock cost. source"},{"id":3751,"pagetitle":"Objectives","title":"ManoptExamples.RosenbrockGradient!!","ref":"/manoptexamples/stable/objectives/#ManoptExamples.RosenbrockGradient!!","content":" ManoptExamples.RosenbrockGradient!!  ‚Äî  Type RosenbrockGradient Provide Euclidean gradient fo the Rosenbrock function in 2D, i.e. for some  $a,b ‚àà ‚Ñù$ \\[\\nabla f(\\mathcal M, p) = \\begin{pmatrix}\n    4a(p_1^2-p_2)p_1 + 2(p_1-b) \\\\\n    -2a(p_1^2-p_2)\n\\end{pmatrix}\\] i.e. also here the manifold is ignored. Constructor RosenbrockGradient(a,b) Functors grad_f!!(M,p)\ngrad_f!!(M, X, p) evaluate the gradient at  $p$  the manifold $\\mathcal M$  is ignored. source"},{"id":3752,"pagetitle":"Objectives","title":"ManoptExamples.RosenbrockMetric","ref":"/manoptexamples/stable/objectives/#ManoptExamples.RosenbrockMetric","content":" ManoptExamples.RosenbrockMetric  ‚Äî  Type RosenbrockMetric <: AbstractMetric A metric related to the Rosenbrock problem, where the metric at a point  $p‚àà\\mathbb R^2$  is given by \\[‚ü®X,Y‚ü©_{\\mathrm{Rb},p} = X^\\mathrm{T}G_pY, \\qquad\nG_p = \\begin{pmatrix}\n  1+4p_{1}^2 & -2p_{1} \\\\\n  -2p_{1} & 1\n\\end{pmatrix},\\] where the  $\\mathrm{Rb}$  stands for Rosenbrock source"},{"id":3753,"pagetitle":"Objectives","title":"Base.log","ref":"/manoptexamples/stable/objectives/#Base.log-Tuple{Manifolds.MetricManifold{‚Ñù, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s8\"} where var\"#s8\"<:Int64}, ‚Ñù}, ManoptExamples.RosenbrockMetric}, Any, Any}","content":" Base.log  ‚Äî  Method X = log(::MetricManifold{‚Ñù,Euclidean{Tuple{2},‚Ñù},RosenbrockMetric}, p, q)\nlog!(::MetricManifold{‚Ñù,Euclidean{Tuple{2},‚Ñù},RosenbrockMetric}, X, p, q) Compute the logarithmic map with respect to the  RosenbrockMetric . The formula reads for any  $j ‚àà \\{1,‚Ä¶,m\\}$ \\[X = \\begin{pmatrix}\n  q_1 - p_1 \\\\\n  q_2 - p_2 + (q_1 - p_1)^2\n\\end{pmatrix}\\] source"},{"id":3754,"pagetitle":"Objectives","title":"Manifolds.inverse_local_metric","ref":"/manoptexamples/stable/objectives/#Manifolds.inverse_local_metric-Tuple{Manifolds.MetricManifold{‚Ñù, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}, ‚Ñù}, ManoptExamples.RosenbrockMetric}, Any}","content":" Manifolds.inverse_local_metric  ‚Äî  Method inverse_local_metric(::MetricManifold{‚Ñù,Euclidean{Tuple{2},‚Ñù},RosenbrockMetric}, p) Return the inverse of the local metric matrix of the  RosenbrockMetric  in the canonical unit vector basis of the tangent space  $T_p\\mathbb R^2$  given as \\[G^{-1}_p =\n\\begin{pmatrix}\n    1 & 2p_1\\\\\n    2p_1 & 1+4p_1^2 \\\\\n\\end{pmatrix}.\\] source"},{"id":3755,"pagetitle":"Objectives","title":"Manifolds.local_metric","ref":"/manoptexamples/stable/objectives/#Manifolds.local_metric-Tuple{Manifolds.MetricManifold{‚Ñù, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}, ‚Ñù}, ManoptExamples.RosenbrockMetric}, Any}","content":" Manifolds.local_metric  ‚Äî  Method local_metric(::MetricManifold{‚Ñù,Euclidean{Tuple{2},‚Ñù},RosenbrockMetric}, p) Return the local metric matrix of the  RosenbrockMetric  in the canonical unit vector basis of the tangent space  $T_p\\mathbb R^2$  given as \\[G_p = \\begin{pmatrix}\n  1+4p_1^2 & -2p_1 \\\\\n  -2p_1 & 1\n\\end{pmatrix}\\] source"},{"id":3756,"pagetitle":"Objectives","title":"ManifoldsBase.change_representer","ref":"/manoptexamples/stable/objectives/#ManifoldsBase.change_representer-Tuple{Manifolds.MetricManifold{‚Ñù, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s8\"} where var\"#s8\"<:Int64}, ‚Ñù}, ManoptExamples.RosenbrockMetric}, ManifoldsBase.EuclideanMetric, Any, Any}","content":" ManifoldsBase.change_representer  ‚Äî  Method Y = change_representer(M::MetricManifold{‚Ñù,Euclidean{Tuple{2},‚Ñù},RosenbrockMetric}, ::EuclideanMetric, p, X)\nchange_representer!(M::MetricManifold{‚Ñù,Euclidean{Tuple{2},‚Ñù},RosenbrockMetric}, Y, ::EuclideanMetric, p, X) Given the Euclidean gradient  X  at  p , this function computes the corresponding Riesz representer  Y such that ‚ü®X,Z‚ü© = ‚ü® Y, Z ‚ü©_{\\mathrm{Rb},p} holds for all Z , in other words Y = G(p)^{-1}X `. this function is used in  riemannian_gradient  to convert a Euclidean into a Riemannian gradient. source"},{"id":3757,"pagetitle":"Objectives","title":"ManifoldsBase.inner","ref":"/manoptexamples/stable/objectives/#ManifoldsBase.inner-Tuple{Manifolds.MetricManifold{‚Ñù, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}, ‚Ñù}, ManoptExamples.RosenbrockMetric}, Any, Any, Any}","content":" ManifoldsBase.inner  ‚Äî  Method inner(M::MetricManifold{‚Ñù,Euclidean{Tuple{2},‚Ñù},RosenbrockMetric}, p, X, Y) Compute the inner product on  $\\mathbb R^2$  with respect to the  RosenbrockMetric , i.e. for  $X,Y \\in T_p\\mathcal M$  we have \\[‚ü®X,Y‚ü©_{\\mathrm{Rb},p} = X^\\mathrm{T}G_pY, \\qquad\nG_p = \\begin{pmatrix}\n  1+4p_1^2 & -2p_1\\\\\n  -2p_1 & 1\n\\end{pmatrix},\\] source"},{"id":3758,"pagetitle":"Objectives","title":"ManoptExamples.Rosenbrock_objective","ref":"/manoptexamples/stable/objectives/#ManoptExamples.Rosenbrock_objective","content":" ManoptExamples.Rosenbrock_objective  ‚Äî  Function Rosenbrock_objective(M::AbstractManifold=DefaultManifold(), a=100.0, b=1.0, evaluation=AllocatingEvaluation()) Return the gradient objective of the Rosenbrock example. See also  RosenbrockCost ,  RosenbrockGradient!! Note The objective is available when  Manopt.jl  is loaded. source"},{"id":3759,"pagetitle":"Objectives","title":"ManoptExamples.expt","ref":"/manoptexamples/stable/objectives/#ManoptExamples.expt-Tuple{Manifolds.MetricManifold{‚Ñù, <:Manifolds.Euclidean{<:Union{ManifoldsBase.TypeParameter{Tuple{2}}, Tuple{var\"#s1\"} where var\"#s1\"<:Int64}, ‚Ñù}, ManoptExamples.RosenbrockMetric}, Any, Any, Number}","content":" ManoptExamples.expt  ‚Äî  Method q = exp(::MetricManifold{‚Ñù,Euclidean{Tuple{2},‚Ñù},RosenbrockMetric}, p, X)\nexp!(::MetricManifold{‚Ñù,Euclidean{Tuple{2},‚Ñù},RosenbrockMetric}, q, p, X) Compute the exponential map with respect to the  RosenbrockMetric . \\[    q = \\begin{pmatrix} p_1 + X_1 \\\\ p_2+X_2+X_1^2\\end{pmatrix}\\] source"},{"id":3760,"pagetitle":"Objectives","title":"ManoptExamples.minimizer","ref":"/manoptexamples/stable/objectives/#ManoptExamples.minimizer-Tuple{ManoptExamples.RosenbrockCost}","content":" ManoptExamples.minimizer  ‚Äî  Method minimizer(::RosenbrockCost) Return the minimizer of the  RosenbrockCost , which is given by \\[p^* = \\begin{pmatrix} b\\\\b^2 \\end{pmatrix}\\] source"},{"id":3761,"pagetitle":"Objectives","title":"Total Variation","ref":"/manoptexamples/stable/objectives/#Total-Variation","content":" Total Variation See the  Total Variation example  to see these in use."},{"id":3762,"pagetitle":"Objectives","title":"ManoptExamples.Intrinsic_infimal_convolution_TV12","ref":"/manoptexamples/stable/objectives/#ManoptExamples.Intrinsic_infimal_convolution_TV12-Tuple{ManifoldsBase.AbstractManifold, Vararg{Any, 5}}","content":" ManoptExamples.Intrinsic_infimal_convolution_TV12  ‚Äî  Method Intrinsic_infimal_convolution_TV12(M, f, u, v, Œ±, Œ≤) Compute the intrinsic infimal convolution model, where the addition is replaced by a mid point approach and the two functions involved are  second_order_Total_Variation  and  Total_Variation . The model reads \\[E(u,v) =\n  \\frac{1}{2}\\sum_{i ‚àà \\mathcal G}\n    d_{\\mathcal M}\\bigl(g(\\frac{1}{2},v_i,w_i),f_i\\bigr)\n  +\\alpha\\bigl( Œ≤\\mathrm{TV}(v) + (1-Œ≤)\\mathrm{TV}_2(w) \\bigr).\\] for more details see [ BFPS17 ,  BFPS18 ]. See also Total_Variation ,  second_order_Total_Variation source"},{"id":3763,"pagetitle":"Objectives","title":"ManoptExamples.L2_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.L2_Total_Variation-NTuple{4, Any}","content":" ManoptExamples.L2_Total_Variation  ‚Äî  Method L2_Total_Variation(M, p_data, Œ±, p) compute the  $‚Ñì^2$ -TV functional on the  PowerManifold M  for given (fixed) data  p_data  (on  M ), a nonnegative weight  Œ± , and evaluated at  p  (on  M ), i.e. \\[E(p) = d_{\\mathcal M}^2(f,p) + \\alpha \\operatorname{TV}(p)\\] See also Total_Variation source"},{"id":3764,"pagetitle":"Objectives","title":"ManoptExamples.L2_Total_Variation_1_2","ref":"/manoptexamples/stable/objectives/#ManoptExamples.L2_Total_Variation_1_2-Tuple{ManifoldsBase.PowerManifold, Vararg{Any, 4}}","content":" ManoptExamples.L2_Total_Variation_1_2  ‚Äî  Method L2_Total_Variation_1_2(M, f, Œ±, Œ≤, x) compute the  $‚Ñì^2$ -TV-TV2 functional on the  PowerManifold  manifold  M  for given (fixed) data  f  (on  M ), nonnegative weight  Œ± ,  Œ≤ , and evaluated at  x  (on  M ), i.e. \\[E(x) = d_{\\mathcal M}^2(f,x) + \\alpha\\operatorname{TV}(x)\n  + Œ≤\\operatorname{TV}_2(x)\\] See also Total_Variation ,  second_order_Total_Variation source"},{"id":3765,"pagetitle":"Objectives","title":"ManoptExamples.L2_second_order_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.L2_second_order_Total_Variation-Tuple{ManifoldsBase.PowerManifold, Any, Any, Any}","content":" ManoptExamples.L2_second_order_Total_Variation  ‚Äî  Method L2_second_order_Total_Variation(M, f, Œ≤, x) compute the  $‚Ñì^2$ -TV2 functional on the  PowerManifold  manifold  M  for given data  f , nonnegative parameter  Œ≤ , and evaluated at  x , i.e. \\[E(x) = d_{\\mathcal M}^2(f,x) + Œ≤\\operatorname{TV}_2(x)\\] as used in [ BBSW16 ]. See also second_order_Total_Variation source"},{"id":3766,"pagetitle":"Objectives","title":"ManoptExamples.Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.Total_Variation","content":" ManoptExamples.Total_Variation  ‚Äî  Function Total_Variation(M,x [,p=2,q=1]) Compute the  $\\operatorname{TV}^p$  functional for data  x on the  PowerManifold  manifold  M , i.e.  $\\mathcal M = \\mathcal N^n$ , where  $n ‚àà \\mathbb N^k$  denotes the dimensions of the data  x . Let  $\\mathcal I_i$  denote the forward neighbors, i.e. with  $\\mathcal G$  as all indices from  $\\mathbf{1} ‚àà \\mathbb N^k$  to  $n$  we have  $\\mathcal I_i = \\{i+e_j, j=1,‚Ä¶,k\\}\\cap \\mathcal G$ . The formula reads \\[E^q(x) = \\sum_{i ‚àà \\mathcal G}\n  \\bigl( \\sum_{j ‚àà  \\mathcal I_i} d^p_{\\mathcal M}(x_i,x_j) \\bigr)^{q/p},\\] see [ WDS14 ] for more details. In long function names, this might be shortened to  TV1  and the  1  might be ommitted if only total variation is present. See also grad_Total_Variation ,  prox_Total_Variation ,  second_order_Total_Variation source"},{"id":3767,"pagetitle":"Objectives","title":"ManoptExamples.adjoint_differential_forward_logs","ref":"/manoptexamples/stable/objectives/#ManoptExamples.adjoint_differential_forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{ùîΩ}, Tuple{ManifoldsBase.PowerManifold{ùîΩ, TM, TSize, TPR}, Any, Any}} where {ùîΩ, TM, TSize, TPR}","content":" ManoptExamples.adjoint_differential_forward_logs  ‚Äî  Method Y = adjoint_differential_forward_logs(M, p, X)\nadjoint_differential_forward_logs!(M, Y, p, X) Compute the adjoint differential of  forward_logs $F$  occurring, in the power manifold array  p , the differential of the function $F_i(p) = \\sum_{j ‚àà \\mathcal I_i} \\log_{p_i} p_j$ where  $i$  runs over all indices of the  PowerManifold  manifold  M  and  $\\mathcal I_i$  denotes the forward neighbors of  $i$  Let  $n$  be the number dimensions of the  PowerManifold  manifold (i.e.  length(size(x) )). Then the input tangent vector lies on the manifold  $\\mathcal M' = \\mathcal M^n$ . The adjoint differential can be computed in place of  Y . Input M      ‚Äì a  PowerManifold  manifold p      ‚Äì an array of points on a manifold X      ‚Äì a tangent vector to from the n-fold power of  p , where n is the  ndims  of  p Output Y  ‚Äì resulting tangent vector in  $T_p\\mathcal M$  representing the adjoint   differentials of the logs. source"},{"id":3768,"pagetitle":"Objectives","title":"ManoptExamples.differential_forward_logs","ref":"/manoptexamples/stable/objectives/#ManoptExamples.differential_forward_logs-Tuple{ManifoldsBase.PowerManifold, Any, Any}","content":" ManoptExamples.differential_forward_logs  ‚Äî  Method Y = differential_forward_logs(M, p, X)\ndifferential_forward_logs!(M, Y, p, X) compute the differential of  forward_logs $F$  on the  PowerManifold  manifold  M  at  p  and direction  X  , in the power manifold array, the differential of the function \\[F_i(x) = \\sum_{j ‚àà \\mathcal I_i} \\log_{p_i} p_j, \\quad i ‚àà \\mathcal G,\\] where  $\\mathcal G$  is the set of indices of the  PowerManifold  manifold  M  and  $\\mathcal I_i$  denotes the forward neighbors of  $i$ . Input M      ‚Äì a  PowerManifold  manifold p      ‚Äì a point. X      ‚Äì a tangent vector. Output Y  ‚Äì resulting tangent vector in  $T_x\\mathcal N$  representing the differentials of the   logs, where  $\\mathcal N$  is the power manifold with the number of dimensions added   to  size(x) . The computation can also be done in place. source"},{"id":3769,"pagetitle":"Objectives","title":"ManoptExamples.forward_logs","ref":"/manoptexamples/stable/objectives/#ManoptExamples.forward_logs-Union{Tuple{TPR}, Tuple{TSize}, Tuple{TM}, Tuple{ùîΩ}, Tuple{ManifoldsBase.PowerManifold{ùîΩ, TM, TSize, TPR}, Any}} where {ùîΩ, TM, TSize, TPR}","content":" ManoptExamples.forward_logs  ‚Äî  Method Y = forward_logs(M,x)\nforward_logs!(M, Y, x) compute the forward logs  $F$  (generalizing forward differences) occurring, in the power manifold array, the function \\[F_i(x) = \\sum_{j ‚àà \\mathcal I_i} \\log_{x_i} x_j,\\quad i  ‚àà  \\mathcal G,\\] where  $\\mathcal G$  is the set of indices of the  PowerManifold  manifold  M  and  $\\mathcal I_i$  denotes the forward neighbors of  $i$ . This can also be done in place of  Œæ . Input M  ‚Äì a  PowerManifold  manifold x  ‚Äì a point. Output Y  ‚Äì resulting tangent vector in  $T_x\\mathcal M$  representing the logs, where  $\\mathcal N$  is the power manifold with the number of dimensions added to  size(x) . The computation can be done in place of  Y . source"},{"id":3770,"pagetitle":"Objectives","title":"ManoptExamples.grad_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.grad_Total_Variation","content":" ManoptExamples.grad_Total_Variation  ‚Äî  Function X = grad_Total_Variation(M, Œª, x[, p=1])\ngrad_Total_Variation!(M, X, Œª, x[, p=1]) Compute the (sub)gradient  $‚àÇf$  of all forward differences occurring, in the power manifold array, i.e. of the function \\[f(p) = \\sum_{i}\\sum_{j ‚àà \\mathcal I_i} d^p(x_i,x_j)\\] where  $i$  runs over all indices of the  PowerManifold  manifold  M  and  $\\mathcal I_i$  denotes the forward neighbors of  $i$ . Input M  ‚Äì a  PowerManifold  manifold x  ‚Äì a point. Output X ‚Äì resulting tangent vector in  $T_x\\mathcal M$ . The computation can also be done in place. source"},{"id":3771,"pagetitle":"Objectives","title":"ManoptExamples.grad_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.grad_Total_Variation-Union{Tuple{T}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}, Any}} where T","content":" ManoptExamples.grad_Total_Variation  ‚Äî  Method X = grad_Total_Variation(M, (x,y)[, p=1])\ngrad_Total_Variation!(M, X, (x,y)[, p=1]) compute the (deterministic) (sub) gradient of  $\\frac{1}{p}d^p_{\\mathcal M}(x,y)$  with respect to both  $x$  and  $y$  (in place of  X  and  Y ). source"},{"id":3772,"pagetitle":"Objectives","title":"ManoptExamples.grad_intrinsic_infimal_convolution_TV12","ref":"/manoptexamples/stable/objectives/#ManoptExamples.grad_intrinsic_infimal_convolution_TV12-Tuple{ManifoldsBase.AbstractManifold, Vararg{Any, 5}}","content":" ManoptExamples.grad_intrinsic_infimal_convolution_TV12  ‚Äî  Method grad_u, grad_v = grad_intrinsic_infimal_convolution_TV12(M, f, u, v, Œ±, Œ≤) compute (sub)gradient of the intrinsic infimal convolution model using the mid point model of second order differences, see  second_order_Total_Variation , i.e. for some  $f ‚àà \\mathcal M$  on a  PowerManifold  manifold  $\\mathcal M$  this function computes the (sub)gradient of \\[E(u,v) =\n\\frac{1}{2}\\sum_{i ‚àà \\mathcal G} d_{\\mathcal M}(g(\\frac{1}{2},v_i,w_i),f_i)\n+ \\alpha\n\\bigl(\nŒ≤\\mathrm{TV}(v) + (1-Œ≤)\\mathrm{TV}_2(w)\n\\bigr),\\] where both total variations refer to the intrinsic ones,  grad_Total_Variation  and  grad_second_order_Total_Variation , respectively. source"},{"id":3773,"pagetitle":"Objectives","title":"ManoptExamples.grad_second_order_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.grad_second_order_Total_Variation","content":" ManoptExamples.grad_second_order_Total_Variation  ‚Äî  Function Y = grad_second_order_Total_Variation(M, q[, p=1])\ngrad_second_order_Total_Variation!(M, Y, q[, p=1]) computes the (sub) gradient of  $\\frac{1}{p}d_2^p(q_1, q_2, q_3)$  with respect to all three components of  $q‚àà\\mathcal M^3$ , where  $d_2$  denotes the second order absolute difference using the mid point model, i.e. let \\[\\mathcal C = \\bigl\\{ c ‚àà \\mathcal M \\ |\\ g(\\tfrac{1}{2};q_1,q_3) \\text{ for some geodesic }g\\bigr\\}\\] denote the mid points between  $q_1$  and  $q_3$  on the manifold  $\\mathcal M$ . Then the absolute second order difference is defined as \\[d_2(q_1,q_2,q_3) = \\min_{c ‚àà \\mathcal C_{q_1,q_3}} d(c, q_2).\\] While the (sub)gradient with respect to  $q_2$  is easy, the other two require the evaluation of an  adjoint_Jacobi_field . The derivation of this gradient can be found in [ BBSW16 ]. source"},{"id":3774,"pagetitle":"Objectives","title":"ManoptExamples.grad_second_order_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.grad_second_order_Total_Variation","content":" ManoptExamples.grad_second_order_Total_Variation  ‚Äî  Function grad_second_order_Total_Variation(M::PowerManifold, q[, p=1]) computes the (sub) gradient of  $\\frac{1}{p}d_2^p(q_1,q_2,q_3)$  with respect to all  $q_1,q_2,q_3$  occurring along any array dimension in the point  q , where  M  is the corresponding  PowerManifold . source"},{"id":3775,"pagetitle":"Objectives","title":"ManoptExamples.project_collaborative_TV","ref":"/manoptexamples/stable/objectives/#ManoptExamples.project_collaborative_TV","content":" ManoptExamples.project_collaborative_TV  ‚Äî  Function project_collaborative_TV(M, Œª, x, Œû[, p=2,q=1])\nproject_collaborative_TV!(M, Œò, Œª, x, Œû[, p=2,q=1]) compute the projection onto collaborative Norm unit (or Œ±-) ball, i.e. of the function \\[F^q(x) = \\sum_{i‚àà\\mathcal G}\n  \\Bigl( \\sum_{j‚àà\\mathcal I_i}\n    \\sum_{k=1}^d \\lVert X_{i,j}\\rVert_x^p\\Bigr)^\\frac{q}{p},\\] where  $\\mathcal G$  is the set of indices for  $x‚àà\\mathcal M$  and  $\\mathcal I_i$  is the set of its forward neighbors. The computation can also be done in place of  Œò . This is adopted from the paper  Duran, M√∂ller, Sbert, Cremers, SIAM J Imag Sci, 2016 , see their Example 3 for details. source"},{"id":3776,"pagetitle":"Objectives","title":"ManoptExamples.prox_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.prox_Total_Variation","content":" ManoptExamples.prox_Total_Variation  ‚Äî  Function Œæ = prox_Total_Variation(M,Œª,x [,p=1]) compute the proximal maps  $\\operatorname{prox}_{Œª\\varphi}$  of all forward differences occurring in the power manifold array, i.e.  $\\varphi(xi,xj) = d_{\\mathcal M}^p(xi,xj)$  with  xi  and  xj  are array elements of  x  and  j = i+e_k , where  e_k  is the  $k$ th unit vector. The parameter  Œª  is the prox parameter. Input M  ‚Äì a manifold  M Œª  ‚Äì a real value, parameter of the proximal map x  ‚Äì a point. Optional (default is given in brackets) p  ‚Äì (1) exponent of the distance of the TV term Output y  ‚Äì resulting  point containing with all mentioned proximal points evaluated (in a cyclic order). The computation can also be done in place source"},{"id":3777,"pagetitle":"Objectives","title":"ManoptExamples.prox_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.prox_Total_Variation-Union{Tuple{T}, Tuple{ManifoldsBase.AbstractManifold, Number, Tuple{T, T}}, Tuple{ManifoldsBase.AbstractManifold, Number, Tuple{T, T}, Int64}} where T","content":" ManoptExamples.prox_Total_Variation  ‚Äî  Method [y1,y2] = prox_Total_Variation(M, Œª, [x1,x2] [,p=1])\nprox_Total_Variation!(M, [y1,y2] Œª, [x1,x2] [,p=1]) Compute the proximal map  $\\operatorname{prox}_{Œª\\varphi}$  of  $œÜ(x,y) = d_{\\mathcal M}^p(x,y)$  with parameter  Œª . A derivation of this closed form solution is given in see [ WDS14 ]. Input M  ‚Äì a manifold  M Œª  ‚Äì a real value, parameter of the proximal map (x1,x2)  ‚Äì a tuple of two points, Optional (default is given in brackets) p  ‚Äì (1) exponent of the distance of the TV term Output (y1,y2)  ‚Äì resulting tuple of points of the  $\\operatorname{prox}_{ŒªœÜ}($ (x1,x2) $)$ . The result can also be computed in place. source"},{"id":3778,"pagetitle":"Objectives","title":"ManoptExamples.prox_parallel_TV","ref":"/manoptexamples/stable/objectives/#ManoptExamples.prox_parallel_TV","content":" ManoptExamples.prox_parallel_TV  ‚Äî  Function y = prox_parallel_TV(M, Œª, x [,p=1])\nprox_parallel_TV!(M, y, Œª, x [,p=1]) compute the proximal maps  $\\operatorname{prox}_{ŒªœÜ}$  of all forward differences occurring in the power manifold array, i.e.  $œÜ(x_i,x_j) = d_{\\mathcal M}^p(x_i,x_j)$  with  xi  and  xj  are array elements of  x  and  j = i+e_k , where  e_k  is the  $k$ th unit vector. The parameter  Œª  is the prox parameter. Input M      ‚Äì a  PowerManifold  manifold Œª      ‚Äì a real value, parameter of the proximal map x      ‚Äì a point Optional (default is given in brackets) p  ‚Äì ( 1 ) exponent of the distance of the TV term Output y   ‚Äì resulting Array of points with all mentioned proximal points evaluated (in a parallel within the arrays elements). The computation can also be done in place. See also prox_Total_Variation source"},{"id":3779,"pagetitle":"Objectives","title":"ManoptExamples.prox_second_order_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.prox_second_order_Total_Variation","content":" ManoptExamples.prox_second_order_Total_Variation  ‚Äî  Function (y1,y2,y3) = prox_second_order_Total_Variation(M,Œª,(x1,x2,x3),[p=1], kwargs...)\nprox_second_order_Total_Variation!(M, y, Œª,(x1,x2,x3),[p=1], kwargs...) Compute the proximal map  $\\operatorname{prox}_{Œª\\varphi}$  of  $\\varphi(x_1,x_2,x_3) = d_{\\mathcal M}^p(c(x_1,x_3),x_2)$  with parameter  Œª >0, where  $c(x,z)$  denotes the mid point of a shortest geodesic from  x1  to  x3  that is closest to  x2 . The result can be computed in place of  y . Note that this function does not have a closed form solution but is solbed by a few steps of the  subgradient mehtod  from  manopt.jl  by default. See [ BBSW16 ] for a derivation. Input M           ‚Äì a manifold Œª           ‚Äì a real value, parameter of the proximal map (x1,x2,x3)  ‚Äì a tuple of three points p  ‚Äì ( 1 ) exponent of the distance of the TV term Optional kwargs...  ‚Äì parameters for the internal  subgradient_method      (if  M  is neither  Euclidean  nor  Circle , since for these a closed form     is given) Output (y1,y2,y3)  ‚Äì resulting tuple of points of the proximal map. The computation can also be done in place. Note This function requires  Manopt.jl  to be loaded source"},{"id":3780,"pagetitle":"Objectives","title":"ManoptExamples.prox_second_order_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.prox_second_order_Total_Variation-Union{Tuple{T}, Tuple{N}, Tuple{ManifoldsBase.PowerManifold{N, T}, Any, Any}, Tuple{ManifoldsBase.PowerManifold{N, T}, Any, Any, Int64}} where {N, T}","content":" ManoptExamples.prox_second_order_Total_Variation  ‚Äî  Method y = prox_second_order_Total_Variation(M, Œª, x[, p=1])\nprox_second_order_Total_Variation!(M, y, Œª, x[, p=1]) compute the proximal maps  $\\operatorname{prox}_{Œª\\varphi}$  of all centered second order differences occurring in the power manifold array, i.e.  $\\varphi(x_k,x_i,x_j) = d_2(x_k,x_i.x_j)$ , where  $k,j$  are backward and forward neighbors (along any dimension in the array of  x ). The parameter  Œª  is the prox parameter. Input M  ‚Äì a manifold  M Œª  ‚Äì a real value, parameter of the proximal map x  ‚Äì a points. Optional (default is given in brackets) p  ‚Äì ( 1 ) exponent of the distance of the TV term Output y  ‚Äì resulting point with all mentioned proximal points evaluated (in a cyclic order). The computation can also be done in place. Note This function requires  Manopt.jl  to be loaded source"},{"id":3781,"pagetitle":"Objectives","title":"ManoptExamples.second_order_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.second_order_Total_Variation","content":" ManoptExamples.second_order_Total_Variation  ‚Äî  Function second_order_Total_Variation(M,x [,p=1]) compute the  $\\operatorname{TV}_2^p$  functional for data  x  on the  PowerManifold  manifold  M , i.e.  $\\mathcal M = \\mathcal N^n$ , where  $n ‚àà \\mathbb N^k$  denotes the dimensions of the data  x . Let  $\\mathcal I_i^{\\pm}$  denote the forward and backward neighbors, respectively, i.e. with  $\\mathcal G$  as all indices from  $\\mathbf{1} ‚àà \\mathbb N^k$  to  $n$  we have  $\\mathcal I^\\pm_i = \\{i\\pm e_j, j=1,‚Ä¶,k\\}\\cap \\mathcal I$ . The formula then reads \\[E(x) = \\sum_{i ‚àà \\mathcal I,\\ j_1 ‚àà  \\mathcal I^+_i,\\ j_2 ‚àà  \\mathcal I^-_i}\nd^p_{\\mathcal M}(c_i(x_{j_1},x_{j_2}), x_i),\\] where  $c_i(‚ãÖ,‚ãÖ)$  denotes the mid point between its two arguments that is nearest to  $x_i$ , see [ BBSW16 ] for a derivation. In long function names, this might be shortened to  TV2 . See also grad_second_order_Total_Variation ,  prox_second_order_Total_Variation source"},{"id":3782,"pagetitle":"Objectives","title":"ManoptExamples.second_order_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.second_order_Total_Variation-Union{Tuple{T}, Tuple{MT}, Tuple{MT, Tuple{T, T, T}}, Tuple{MT, Tuple{T, T, T}, Any}} where {MT<:ManifoldsBase.AbstractManifold, T}","content":" ManoptExamples.second_order_Total_Variation  ‚Äî  Method second_order_Total_Variation(M,(x1,x2,x3) [,p=1]) Compute the  $\\operatorname{TV}_2^p$  functional for the 3-tuple of points  (x1,x2,x3) on the manifold  M . Denote by \\[  \\mathcal C = \\bigl\\{ c ‚àà  \\mathcal M \\ |\\ g(\\tfrac{1}{2};x_1,x_3) \\text{ for some geodesic }g\\bigr\\}\\] the set of mid points between  $x_1$  and  $x_3$ . Then the function reads \\[d_2^p(x_1,x_2,x_3) = \\min_{c ‚àà \\mathcal C} d_{\\mathcal M}(c,x_2),\\] see [ BBSW16 ] for a derivation. In long function names, this might be shortened to  TV2 . See also grad_second_order_Total_Variation ,  prox_second_order_Total_Variation ,  Total_Variation source"},{"id":3783,"pagetitle":"Objectives","title":"ManoptExamples.subgrad_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.subgrad_Total_Variation","content":" ManoptExamples.subgrad_Total_Variation  ‚Äî  Function X = subgrad_TV(M, Œª, p[, k=1; atol=0])\nsubgrad_TV!(M, X, Œª, p[, k=1; atol=0]) Compute the (randomized) subgradient  $\\partial F$  of all forward differences occurring, in the power manifold array, i.e. of the function \\[F(p) = \\sum_{i}\\sum_{j ‚àà \\mathcal I_i} d^k(p_i,p_j)\\] where  $i$  runs over all indices of the  PowerManifold  manifold  M  and  $\\mathcal I_i$  denotes the forward neighbors of  $i$ . Input M  ‚Äì a  PowerManifold  manifold p  ‚Äì a point. Ouput X ‚Äì resulting tangent vector in  $T_p\\mathcal M$ . The computation can also be done in place. source"},{"id":3784,"pagetitle":"Objectives","title":"ManoptExamples.subgrad_Total_Variation","ref":"/manoptexamples/stable/objectives/#ManoptExamples.subgrad_Total_Variation-Union{Tuple{T}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}}, Tuple{ManifoldsBase.AbstractManifold, Tuple{T, T}, Int64}} where T","content":" ManoptExamples.subgrad_Total_Variation  ‚Äî  Method X = subgrad_TV(M, (p,q)[, k=1; atol=0])\nsubgrad_TV!(M, X, (p,q)[, k=1; atol=0]) compute the (randomized) subgradient of  $\\frac{1}{k}d^k_{\\mathcal M}(p,q)$  with respect to both  $p$  and  $q$  (in place of  X  and  Y ). source"},{"id":3785,"pagetitle":"Objectives","title":"Literature","ref":"/manoptexamples/stable/objectives/#Literature","content":" Literature [BBSW16] M.¬†Baƒç√°k, R.¬†Bergmann, G.¬†Steidl and A.¬†Weinmann.  A second order non-smooth variational model for restoring manifold-valued images .  SIAM¬†Journal¬†on¬†Scientific¬†Computing  38 , A567‚ÄìA597  (2016),  arXiv:1506.02409 . [BFPS18] R.¬†Bergmann, J.¬†H.¬†Fitschen, J.¬†Persch and G.¬†Steidl.  Priors with coupled first and second order differences for manifold-valued image processing .  Journal¬†of¬†Mathematical¬†Imaging¬†and¬†Vision  60 , 1459‚Äì1481  (2018),  arXiv:1709.01343 . [BFPS17] R.¬†Bergmann, J.¬†H.¬†Fitschen, J.¬†Persch and G.¬†Steidl.  Infimal convolution coupling of first and second order differences on manifold-valued images . In:  Scale Space and Variational Methods in Computer Vision: 6th International Conference, SSVM 2017, Kolding, Denmark, June 4‚Äì8, 2017, Proceedings , edited by F.¬†Lauze, Y.¬†Dong and A.¬†B.¬†Dahl (Springer International Publishing, 2017); pp.¬†447‚Äì459. [BG18] R.¬†Bergmann and P.-Y.¬†Gousenbourger.  A variational model for data fitting on manifolds by minimizing the acceleration of a B√©zier curve .  Frontiers¬†in¬†Applied¬†Mathematics¬†and¬†Statistics  4  (2018),  arXiv:1807.10090 . [Bou23] N.¬†Boumal.  An Introduction to Optimization on Smooth Manifolds . First¬†Edition ( Cambridge University Press, 2023 ). [Cas59] P.¬†de¬†Casteljau.  Outillage methodes calcul  (Enveloppe Soleau 40.040, Institute National de la Propri√©t√© Industrielle, Paris., 1959). [Cas63] P.¬†de¬†Casteljau.  Courbes et surfaces √† p√¥les  (Microfiche P 4147-1, Institute National de la Propri√©t√© Industrielle, Paris., 1963). [DMSC16] J.¬†Duran, M.¬†Moeller, C.¬†Sbert and D.¬†Cremers.  Collaborative Total Variation: A General Framework for Vectorial TV Models .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  9 , 116‚Äì151  (2016),  arXiv:1508.01308 . [PN07] T.¬†Popiel and L.¬†Noakes.  B√©zier curves and  $C^2$  interpolation in Riemannian manifolds .  Journal¬†of¬†Approximation¬†Theory  148 , 111‚Äì127  (2007). [WDS14] A.¬†Weinmann, L.¬†Demaret and M.¬†Storath.  Total variation regularization for manifold-valued data .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  7 , 2226‚Äì2257  (2014)."},{"id":3788,"pagetitle":"References","title":"Literature","ref":"/manoptexamples/stable/references/#Literature","content":" Literature [ASY+19] T.¬†Akiba, S.¬†Sano, T.¬†Yanase, T.¬†Ohta and M.¬†Koyama.  Optuna: A Next-generation Hyperparameter Optimization Framework . In:  Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  (2019),  arXiv:1907.10902 . [ABBR23] S.¬†D.¬†Axen, M.¬†Baran, R.¬†Bergmann and K.¬†Rzecki.  Manifolds.jl: An Extensible Julia Framework for Data Analysis on Manifolds .  ACM¬†Transactions¬†on¬†Mathematical¬†Software  (2023),  arXiv:2021.08777 . [Bac14] M.¬†Baƒç√°k.  Computing medians and means in Hadamard spaces .  SIAM¬†Journal¬†on¬†Optimization  24 , 1542‚Äì1566  (2014),  arXiv:1210.2145 . [BBSW16] M.¬†Baƒç√°k, R.¬†Bergmann, G.¬†Steidl and A.¬†Weinmann.  A second order non-smooth variational model for restoring manifold-valued images .  SIAM¬†Journal¬†on¬†Scientific¬†Computing  38 , A567‚ÄìA597  (2016),  arXiv:1506.02409 . [BFNZ25] R.¬†Bergmann, O.¬†P.¬†Ferreira, S.¬†Z.¬†N√©meth and J.¬†Zhu.  On projection mappings and the gradient projection method                on hyperbolic space forms . Preprint,¬†in¬†preparation (2025). [BFSS24] R.¬†Bergmann, O.¬†P.¬†Ferreira, E.¬†M.¬†Santos and J.¬†C.¬†Souza.  The difference of convex algorithm on Hadamard manifolds .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  (2024). [BFPS18] R.¬†Bergmann, J.¬†H.¬†Fitschen, J.¬†Persch and G.¬†Steidl.  Priors with coupled first and second order differences for manifold-valued image processing .  Journal¬†of¬†Mathematical¬†Imaging¬†and¬†Vision  60 , 1459‚Äì1481  (2018),  arXiv:1709.01343 . [BFPS17] R.¬†Bergmann, J.¬†H.¬†Fitschen, J.¬†Persch and G.¬†Steidl.  Infimal convolution coupling of first and second order differences on manifold-valued images . In:  Scale Space and Variational Methods in Computer Vision: 6th International Conference, SSVM 2017, Kolding, Denmark, June 4‚Äì8, 2017, Proceedings , edited by F.¬†Lauze, Y.¬†Dong and A.¬†B.¬†Dahl (Springer International Publishing, 2017); pp.¬†447‚Äì459. [BG18] R.¬†Bergmann and P.-Y.¬†Gousenbourger.  A variational model for data fitting on manifolds by minimizing the acceleration of a B√©zier curve .  Frontiers¬†in¬†Applied¬†Mathematics¬†and¬†Statistics  4  (2018),  arXiv:1807.10090 . [BHJ24] R.¬†Bergmann, R.¬†Herzog and H.¬†Jasa.  The Riemannian Convex Bundle Method , preprint (2024),  arXiv:2402.13670 . [BLSW14] R.¬†Bergmann, F.¬†Laus, G.¬†Steidl and A.¬†Weinmann.  Second order differences of cyclic data and applications in variational denoising .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  7 , 2916‚Äì2953  (2014),  arXiv:1405.5349 . [BPS16] R.¬†Bergmann, J.¬†Persch and G.¬†Steidl.  A parallel Douglas Rachford algorithm for minimizing ROF-like functionals on images with values in symmetric Hadamard manifolds .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  9 , 901‚Äì937  (2016),  arXiv:1512.02814 . [Bou23] N.¬†Boumal.  An Introduction to Optimization on Smooth Manifolds . First¬†Edition ( Cambridge University Press, 2023 ). [Cas59] P.¬†de¬†Casteljau.  Outillage methodes calcul  (Enveloppe Soleau 40.040, Institute National de la Propri√©t√© Industrielle, Paris., 1959). [Cas63] P.¬†de¬†Casteljau.  Courbes et surfaces √† p√¥les  (Microfiche P 4147-1, Institute National de la Propri√©t√© Industrielle, Paris., 1963). [DMSC16] J.¬†Duran, M.¬†Moeller, C.¬†Sbert and D.¬†Cremers.  Collaborative Total Variation: A General Framework for Vectorial TV Models .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  9 , 116‚Äì151  (2016),  arXiv:1508.01308 . [FO98] O.¬†Ferreira and P.¬†R.¬†Oliveira.  Subgradient algorithm on Riemannian manifolds .  Journal¬†of¬†Optimization¬†Theory¬†and¬†Applications  97 , 93‚Äì104  (1998). [HNP23] N.¬†Hoseini Monjezi, S.¬†Nobakhtian and M.¬†R.¬†Pouryayevali.  A proximal bundle algorithm for nonsmooth optimization on Riemannian manifolds .  IMA¬†Journal¬†of¬†Numerical¬†Analysis  43 , 293‚Äì325  (2023). [LNPS17] F.¬†Laus, M.¬†Nikolova, J.¬†Persch and G.¬†Steidl.  A nonlocal denoising algorithm for manifold-valued images using second order statistics .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  10 , 416‚Äì448  (2017). [LB19] C.¬†Liu and N.¬†Boumal.  Simple algorithms for optimization on Riemannian manifolds with constraints .  Applied¬†Mathematics¬†&¬†Optimization  (2019),  arXiv:1091.10000 . [PN07] T.¬†Popiel and L.¬†Noakes.  B√©zier curves and  $C^2$  interpolation in Riemannian manifolds .  Journal¬†of¬†Approximation¬†Theory  148 , 111‚Äì127  (2007). [ROF92] L.¬†I.¬†Rudin, S.¬†Osher and E.¬†Fatemi.  Nonlinear total variation based noise removal algorithms .  Physica¬†D:¬†Nonlinear¬†Phenomena  60 , 259‚Äì268  (1992). [SO15] J.¬†C.¬†Souza and P.¬†R.¬†Oliveira.  A proximal point algorithm for DC fuctions on Hadamard manifolds .  Journal¬†of¬†Global¬†Optimization  63 , 797‚Äì810  (2015). [WS22] M.¬†Weber and S.¬†Sra.  Riemannian Optimization via Frank-Wolfe Methods .  Mathematical¬†Programming  199 , 525‚Äì556  (2022). [WDS14] A.¬†Weinmann, L.¬†Demaret and M.¬†Storath.  Total variation regularization for manifold-valued data .  SIAM¬†Journal¬†on¬†Imaging¬†Sciences  7 , 2226‚Äì2257  (2014)."}]